\chapter{Systematic Model Development and Selection}

\section{Introduction}

Model selection is one of the most critical decisions in machine learning projects. Yet many teams approach it unsystematically: trying a few algorithms, picking the one with the highest validation accuracy, and moving to production. This naive approach often leads to models that fail under real-world conditionsâ€”overfitting to validation data, poor performance on edge cases, or unacceptable inference latency.

\subsection{The Model Selection Challenge}

Consider a fraud detection system where false negatives cost \$500 on average but false positives require manual review costing \$5. A model with 99\% accuracy might be worse than one with 95\% accuracy if the latter has a better precision-recall trade-off. Beyond predictive performance, production constraints matter: inference latency, memory footprint, model interpretability, and maintenance complexity all impact real-world success.

\subsection{Why Systematic Model Development Matters}

Studies show that 87\% of machine learning projects never make it to production. A primary reason is inadequate model selection processes that ignore:

\begin{itemize}
    \item \textbf{Statistical significance}: Performance differences may be due to random variation
    \item \textbf{Business constraints}: Best model $\neq$ most accurate model
    \item \textbf{Complexity trade-offs}: Complex models may not justify marginal gains
    \item \textbf{Production requirements}: Inference time, memory, and scalability matter
    \item \textbf{Temporal dynamics}: Models degrade over time requiring monitoring
\end{itemize}

\subsection{Chapter Overview}

This chapter presents a comprehensive framework for systematic model development:

\begin{enumerate}
    \item \textbf{Model Candidate Framework}: Standardized representation of models with performance metrics
    \item \textbf{Cross-Validation Strategies}: Specialized approaches for time series, imbalanced, and grouped data
    \item \textbf{Statistical Model Comparison}: Rigorous testing for significant differences
    \item \textbf{Complexity Analysis}: Quantifying model complexity and trade-offs
    \item \textbf{Automated Selection}: Business constraint-aware model selection
    \item \textbf{Production Monitoring}: Detecting degradation and triggering retraining
    \item \textbf{Model Registry}: Versioning, metadata, and deployment management
\end{enumerate}

\section{Model Candidate Framework}

We need a standardized way to represent models that captures not just performance metrics but also operational characteristics critical for production deployment.

\subsection{Core Model Representation}

\begin{lstlisting}[language=Python, caption={Model Candidate Framework with Comprehensive Metrics}]
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Protocol, Tuple
from enum import Enum
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_squared_error, mean_absolute_error, r2_score
)
from datetime import datetime
import time
import logging
import json
from pathlib import Path
import pickle
import hashlib
import sys

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Type of machine learning task."""
    BINARY_CLASSIFICATION = "binary_classification"
    MULTICLASS_CLASSIFICATION = "multiclass_classification"
    REGRESSION = "regression"
    RANKING = "ranking"

class ComplexityLevel(Enum):
    """Model complexity categorization."""
    LOW = "low"  # Linear models, decision trees
    MEDIUM = "medium"  # Ensembles, shallow neural networks
    HIGH = "high"  # Deep neural networks, large ensembles

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for a model."""
    # Primary metrics
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None
    roc_auc: Optional[float] = None

    # Regression metrics
    mse: Optional[float] = None
    rmse: Optional[float] = None
    mae: Optional[float] = None
    r2: Optional[float] = None

    # Confidence intervals (95% CI)
    accuracy_ci: Optional[Tuple[float, float]] = None
    precision_ci: Optional[Tuple[float, float]] = None
    recall_ci: Optional[Tuple[float, float]] = None

    # Cross-validation stats
    cv_mean: Optional[float] = None
    cv_std: Optional[float] = None
    cv_scores: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            k: v for k, v in self.__dict__.items()
            if v is not None
        }

@dataclass
class ComplexityMetrics:
    """Metrics quantifying model complexity."""
    n_parameters: int
    n_features: int
    model_size_bytes: int
    training_time_seconds: float
    inference_time_ms: float  # Per sample
    memory_mb: float
    complexity_level: ComplexityLevel

    def compute_complexity_score(self) -> float:
        """
        Compute normalized complexity score (0-100).
        Higher = more complex.
        """
        # Normalize each component (log scale for parameters/size)
        param_score = min(np.log10(self.n_parameters + 1) / 8 * 100, 100)
        size_score = min(np.log10(self.model_size_bytes + 1) / 9 * 100, 100)
        time_score = min(self.inference_time_ms / 100 * 100, 100)
        memory_score = min(self.memory_mb / 1000 * 100, 100)

        # Weighted average
        complexity_score = (
            0.3 * param_score +
            0.2 * size_score +
            0.3 * time_score +
            0.2 * memory_score
        )

        return complexity_score

@dataclass
class ModelCandidate:
    """
    Comprehensive representation of a model candidate.

    Tracks performance, complexity, metadata, and operational
    characteristics for systematic model comparison.
    """
    name: str
    model_type: ModelType
    estimator: BaseEstimator

    # Performance
    performance: PerformanceMetrics
    complexity: ComplexityMetrics

    # Metadata
    created_at: datetime
    algorithm: str
    hyperparameters: Dict[str, Any]
    feature_names: List[str]

    # Training context
    training_samples: int
    validation_samples: int
    training_duration: float

    # Versioning
    version: str = "1.0.0"
    git_commit: Optional[str] = None

    # Business metrics
    business_value_score: Optional[float] = None
    production_ready: bool = False

    def compute_model_hash(self) -> str:
        """Compute hash of model for versioning."""
        config = {
            "algorithm": self.algorithm,
            "hyperparameters": self.hyperparameters,
            "feature_names": sorted(self.feature_names),
            "model_type": self.model_type.value
        }
        config_str = json.dumps(config, sort_keys=True)
        return hashlib.sha256(config_str.encode()).hexdigest()[:16]

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions with timing."""
        start = time.time()
        predictions = self.estimator.predict(X)
        duration = (time.time() - start) * 1000 / len(X)
        logger.debug(f"Prediction time: {duration:.2f}ms per sample")
        return predictions

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Make probability predictions if supported."""
        if not hasattr(self.estimator, 'predict_proba'):
            raise AttributeError(f"{self.algorithm} does not support predict_proba")
        return self.estimator.predict_proba(X)

    def save(self, path: Path) -> None:
        """Save model and metadata to disk."""
        path.mkdir(parents=True, exist_ok=True)

        # Save estimator
        model_path = path / "model.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(self.estimator, f)

        # Save metadata
        metadata = {
            "name": self.name,
            "model_type": self.model_type.value,
            "algorithm": self.algorithm,
            "hyperparameters": self.hyperparameters,
            "feature_names": self.feature_names,
            "performance": self.performance.to_dict(),
            "complexity": {
                "n_parameters": self.complexity.n_parameters,
                "n_features": self.complexity.n_features,
                "model_size_bytes": self.complexity.model_size_bytes,
                "training_time_seconds": self.complexity.training_time_seconds,
                "inference_time_ms": self.complexity.inference_time_ms,
                "memory_mb": self.complexity.memory_mb,
                "complexity_level": self.complexity.complexity_level.value
            },
            "created_at": self.created_at.isoformat(),
            "version": self.version,
            "git_commit": self.git_commit,
            "training_samples": self.training_samples,
            "validation_samples": self.validation_samples,
            "business_value_score": self.business_value_score,
            "production_ready": self.production_ready
        }

        metadata_path = path / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"Saved model to {path}")

    @classmethod
    def load(cls, path: Path) -> 'ModelCandidate':
        """Load model and metadata from disk."""
        # Load estimator
        model_path = path / "model.pkl"
        with open(model_path, 'rb') as f:
            estimator = pickle.load(f)

        # Load metadata
        metadata_path = path / "metadata.json"
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        # Reconstruct ModelCandidate
        performance = PerformanceMetrics(**metadata["performance"])

        complexity_data = metadata["complexity"]
        complexity = ComplexityMetrics(
            n_parameters=complexity_data["n_parameters"],
            n_features=complexity_data["n_features"],
            model_size_bytes=complexity_data["model_size_bytes"],
            training_time_seconds=complexity_data["training_time_seconds"],
            inference_time_ms=complexity_data["inference_time_ms"],
            memory_mb=complexity_data["memory_mb"],
            complexity_level=ComplexityLevel(complexity_data["complexity_level"])
        )

        return cls(
            name=metadata["name"],
            model_type=ModelType(metadata["model_type"]),
            estimator=estimator,
            performance=performance,
            complexity=complexity,
            created_at=datetime.fromisoformat(metadata["created_at"]),
            algorithm=metadata["algorithm"],
            hyperparameters=metadata["hyperparameters"],
            feature_names=metadata["feature_names"],
            training_samples=metadata["training_samples"],
            validation_samples=metadata["validation_samples"],
            training_duration=complexity_data["training_time_seconds"],
            version=metadata["version"],
            git_commit=metadata.get("git_commit"),
            business_value_score=metadata.get("business_value_score"),
            production_ready=metadata.get("production_ready", False)
        )

    def __str__(self) -> str:
        """Human-readable representation."""
        primary_metric = (
            self.performance.accuracy if self.performance.accuracy is not None
            else self.performance.r2
        )
        return (f"ModelCandidate(name='{self.name}', "
                f"algorithm='{self.algorithm}', "
                f"performance={primary_metric:.4f}, "
                f"complexity_score={self.complexity.compute_complexity_score():.1f})")
\end{lstlisting}

\subsection{Model Builder}

\begin{lstlisting}[language=Python, caption={Model Builder for Creating Candidates}]
import psutil
import os

class ModelBuilder:
    """Builder for creating ModelCandidate instances with complete metrics."""

    def __init__(self, model_type: ModelType):
        self.model_type = model_type

    def build_candidate(
        self,
        name: str,
        estimator: BaseEstimator,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        feature_names: List[str],
        hyperparameters: Dict[str, Any],
        version: str = "1.0.0"
    ) -> ModelCandidate:
        """
        Build a complete ModelCandidate with all metrics computed.

        Args:
            name: Human-readable name
            estimator: Fitted sklearn-compatible estimator
            X_train, y_train: Training data
            X_val, y_val: Validation data
            feature_names: List of feature names
            hyperparameters: Hyperparameter configuration
            version: Model version string

        Returns:
            Complete ModelCandidate
        """
        logger.info(f"Building candidate: {name}")

        # Train and measure time
        start_time = time.time()
        estimator.fit(X_train, y_train)
        training_duration = time.time() - start_time

        # Compute performance metrics
        performance = self._compute_performance(estimator, X_val, y_val)

        # Compute complexity metrics
        complexity = self._compute_complexity(
            estimator, X_val, feature_names, training_duration
        )

        # Create candidate
        candidate = ModelCandidate(
            name=name,
            model_type=self.model_type,
            estimator=estimator,
            performance=performance,
            complexity=complexity,
            created_at=datetime.now(),
            algorithm=type(estimator).__name__,
            hyperparameters=hyperparameters,
            feature_names=feature_names,
            training_samples=len(X_train),
            validation_samples=len(X_val),
            training_duration=training_duration,
            version=version
        )

        logger.info(f"Built candidate: {candidate}")
        return candidate

    def _compute_performance(
        self,
        estimator: BaseEstimator,
        X_val: np.ndarray,
        y_val: np.ndarray
    ) -> PerformanceMetrics:
        """Compute comprehensive performance metrics."""
        y_pred = estimator.predict(X_val)

        metrics = PerformanceMetrics()

        if self.model_type in [ModelType.BINARY_CLASSIFICATION,
                               ModelType.MULTICLASS_CLASSIFICATION]:
            # Classification metrics
            metrics.accuracy = accuracy_score(y_val, y_pred)
            metrics.precision = precision_score(
                y_val, y_pred, average='binary' if self.model_type ==
                ModelType.BINARY_CLASSIFICATION else 'weighted', zero_division=0
            )
            metrics.recall = recall_score(
                y_val, y_pred, average='binary' if self.model_type ==
                ModelType.BINARY_CLASSIFICATION else 'weighted', zero_division=0
            )
            metrics.f1_score = f1_score(
                y_val, y_pred, average='binary' if self.model_type ==
                ModelType.BINARY_CLASSIFICATION else 'weighted', zero_division=0
            )

            # ROC AUC (requires predict_proba)
            if hasattr(estimator, 'predict_proba'):
                y_proba = estimator.predict_proba(X_val)
                if self.model_type == ModelType.BINARY_CLASSIFICATION:
                    metrics.roc_auc = roc_auc_score(y_val, y_proba[:, 1])
                else:
                    metrics.roc_auc = roc_auc_score(
                        y_val, y_proba, multi_class='ovr', average='weighted'
                    )

        elif self.model_type == ModelType.REGRESSION:
            # Regression metrics
            metrics.mse = mean_squared_error(y_val, y_pred)
            metrics.rmse = np.sqrt(metrics.mse)
            metrics.mae = mean_absolute_error(y_val, y_pred)
            metrics.r2 = r2_score(y_val, y_pred)

        return metrics

    def _compute_complexity(
        self,
        estimator: BaseEstimator,
        X_sample: np.ndarray,
        feature_names: List[str],
        training_time: float
    ) -> ComplexityMetrics:
        """Compute complexity metrics."""
        # Count parameters
        n_params = self._count_parameters(estimator)

        # Model size in bytes
        model_bytes = len(pickle.dumps(estimator))

        # Inference time (average over 100 samples)
        n_samples = min(100, len(X_sample))
        X_test = X_sample[:n_samples]

        start = time.time()
        _ = estimator.predict(X_test)
        inference_time = (time.time() - start) * 1000 / n_samples

        # Memory usage estimate
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024

        # Determine complexity level
        complexity_level = self._determine_complexity_level(estimator, n_params)

        return ComplexityMetrics(
            n_parameters=n_params,
            n_features=len(feature_names),
            model_size_bytes=model_bytes,
            training_time_seconds=training_time,
            inference_time_ms=inference_time,
            memory_mb=memory_mb,
            complexity_level=complexity_level
        )

    def _count_parameters(self, estimator: BaseEstimator) -> int:
        """Count trainable parameters in model."""
        # For sklearn models
        if hasattr(estimator, 'coef_'):
            return np.prod(estimator.coef_.shape)
        elif hasattr(estimator, 'n_features_in_'):
            return estimator.n_features_in_
        elif hasattr(estimator, 'tree_'):
            # Decision trees
            return estimator.tree_.node_count
        elif hasattr(estimator, 'estimators_'):
            # Ensembles
            return sum(
                self._count_parameters(e) for e in estimator.estimators_
            )
        else:
            # Default estimate
            return 1000

    def _determine_complexity_level(
        self,
        estimator: BaseEstimator,
        n_params: int
    ) -> ComplexityLevel:
        """Determine complexity level based on model type and size."""
        algo_name = type(estimator).__name__.lower()

        if 'linear' in algo_name or 'logistic' in algo_name:
            return ComplexityLevel.LOW
        elif 'tree' in algo_name and 'forest' not in algo_name:
            return ComplexityLevel.LOW
        elif 'forest' in algo_name or 'gradient' in algo_name or 'xgb' in algo_name:
            return ComplexityLevel.MEDIUM
        elif n_params > 100000:
            return ComplexityLevel.HIGH
        else:
            return ComplexityLevel.MEDIUM
\end{lstlisting}

\section{Cross-Validation Strategies}

Different data types require specialized cross-validation strategies to ensure valid performance estimates.

\subsection{Comprehensive Cross-Validation Framework}

\begin{lstlisting}[language=Python, caption={Cross-Validation Strategies for Different Data Types}]
from sklearn.model_selection import (
    KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold, cross_val_score
)
from typing import Iterator, Union
from abc import ABC, abstractmethod

class CrossValidationStrategy(ABC):
    """Abstract base class for cross-validation strategies."""

    @abstractmethod
    def split(self, X: np.ndarray, y: np.ndarray,
             groups: Optional[np.ndarray] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        """Generate train/test indices for cross-validation."""
        pass

    @abstractmethod
    def get_n_splits(self) -> int:
        """Return number of splits."""
        pass

class StandardCVStrategy(CrossValidationStrategy):
    """Standard k-fold cross-validation."""

    def __init__(self, n_splits: int = 5, shuffle: bool = True, random_state: int = 42):
        self.cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)

    def split(self, X: np.ndarray, y: np.ndarray,
             groups: Optional[np.ndarray] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        return self.cv.split(X, y)

    def get_n_splits(self) -> int:
        return self.cv.n_splits

class StratifiedCVStrategy(CrossValidationStrategy):
    """Stratified k-fold for imbalanced classification."""

    def __init__(self, n_splits: int = 5, shuffle: bool = True, random_state: int = 42):
        self.cv = StratifiedKFold(n_splits=n_splits, shuffle=shuffle,
                                  random_state=random_state)

    def split(self, X: np.ndarray, y: np.ndarray,
             groups: Optional[np.ndarray] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        return self.cv.split(X, y)

    def get_n_splits(self) -> int:
        return self.cv.n_splits

class TimeSeriesCVStrategy(CrossValidationStrategy):
    """
    Time series cross-validation with expanding window.

    Maintains temporal order and prevents data leakage.
    """

    def __init__(self, n_splits: int = 5, max_train_size: Optional[int] = None):
        self.cv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)

    def split(self, X: np.ndarray, y: np.ndarray,
             groups: Optional[np.ndarray] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        return self.cv.split(X)

    def get_n_splits(self) -> int:
        return self.cv.n_splits

class GroupedCVStrategy(CrossValidationStrategy):
    """
    Grouped k-fold for preventing data leakage across groups.

    Example: Customer-level splits to prevent customer data in both
    train and test sets.
    """

    def __init__(self, n_splits: int = 5):
        self.cv = GroupKFold(n_splits=n_splits)

    def split(self, X: np.ndarray, y: np.ndarray,
             groups: Optional[np.ndarray] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        if groups is None:
            raise ValueError("GroupedCVStrategy requires 'groups' parameter")
        return self.cv.split(X, y, groups=groups)

    def get_n_splits(self) -> int:
        return self.cv.n_splits

@dataclass
class CrossValidationResult:
    """Results from cross-validation."""
    model_name: str
    cv_scores: List[float]
    mean_score: float
    std_score: float
    confidence_interval: Tuple[float, float]  # 95% CI
    strategy: str
    n_splits: int

    def __str__(self) -> str:
        return (f"{self.model_name}: {self.mean_score:.4f} +/- {self.std_score:.4f} "
                f"(95% CI: [{self.confidence_interval[0]:.4f}, "
                f"{self.confidence_interval[1]:.4f}])")

class CrossValidator:
    """
    Comprehensive cross-validation with support for different data types.
    """

    def __init__(self, strategy: CrossValidationStrategy, scoring: str = 'accuracy'):
        """
        Args:
            strategy: Cross-validation strategy
            scoring: Scoring metric (sklearn scoring string)
        """
        self.strategy = strategy
        self.scoring = scoring

    def evaluate_model(
        self,
        estimator: BaseEstimator,
        X: np.ndarray,
        y: np.ndarray,
        groups: Optional[np.ndarray] = None,
        model_name: str = "model"
    ) -> CrossValidationResult:
        """
        Evaluate model using cross-validation.

        Returns:
            CrossValidationResult with statistics and confidence intervals
        """
        logger.info(f"Cross-validating {model_name} with {self.strategy.__class__.__name__}")

        # Perform cross-validation
        cv_scores = []
        for train_idx, test_idx in self.strategy.split(X, y, groups):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # Train and evaluate
            estimator.fit(X_train, y_train)
            score = self._compute_score(estimator, X_test, y_test)
            cv_scores.append(score)

        # Calculate statistics
        mean_score = np.mean(cv_scores)
        std_score = np.std(cv_scores)

        # 95% confidence interval (t-distribution)
        from scipy import stats
        n = len(cv_scores)
        ci = stats.t.interval(
            0.95, n - 1, loc=mean_score, scale=std_score / np.sqrt(n)
        )

        result = CrossValidationResult(
            model_name=model_name,
            cv_scores=cv_scores,
            mean_score=mean_score,
            std_score=std_score,
            confidence_interval=ci,
            strategy=self.strategy.__class__.__name__,
            n_splits=self.strategy.get_n_splits()
        )

        logger.info(str(result))
        return result

    def _compute_score(self, estimator: BaseEstimator,
                      X_test: np.ndarray, y_test: np.ndarray) -> float:
        """Compute score based on scoring metric."""
        from sklearn.metrics import get_scorer
        scorer = get_scorer(self.scoring)
        return scorer(estimator, X_test, y_test)

    def compare_models(
        self,
        estimators: Dict[str, BaseEstimator],
        X: np.ndarray,
        y: np.ndarray,
        groups: Optional[np.ndarray] = None
    ) -> pd.DataFrame:
        """
        Compare multiple models using cross-validation.

        Returns:
            DataFrame with comparison results
        """
        results = []

        for name, estimator in estimators.items():
            cv_result = self.evaluate_model(estimator, X, y, groups, name)
            results.append({
                "model": name,
                "mean_score": cv_result.mean_score,
                "std_score": cv_result.std_score,
                "ci_lower": cv_result.confidence_interval[0],
                "ci_upper": cv_result.confidence_interval[1]
            })

        df = pd.DataFrame(results)
        df = df.sort_values("mean_score", ascending=False)

        logger.info(f"Compared {len(estimators)} models")
        return df
\end{lstlisting}

\section{Statistical Model Comparison}

Performance differences between models must be statistically significant, not due to random variation.

\subsection{Statistical Testing Framework}

\begin{lstlisting}[language=Python, caption={Statistical Model Comparison with Multiple Tests}]
from scipy.stats import ttest_rel, wilcoxon
from sklearn.metrics import accuracy_score
from itertools import combinations

@dataclass
class ComparisonResult:
    """Result of statistical comparison between two models."""
    model_a: str
    model_b: str
    test_statistic: float
    p_value: float
    is_significant: bool
    alpha: float
    test_method: str
    winner: Optional[str] = None

    def __str__(self) -> str:
        sig = "significant" if self.is_significant else "not significant"
        winner_str = f", winner: {self.winner}" if self.winner else ""
        return (f"{self.model_a} vs {self.model_b}: "
                f"p={self.p_value:.4f} ({sig}{winner_str}) [{self.test_method}]")

class ModelComparator:
    """
    Statistical comparison of model performance.

    Supports:
    - Paired t-test (for cross-validation scores)
    - McNemar's test (for binary classification)
    - Permutation test (non-parametric)
    """

    def __init__(self, alpha: float = 0.05):
        """
        Args:
            alpha: Significance level for hypothesis tests
        """
        self.alpha = alpha

    def compare_cv_scores(
        self,
        model_a_name: str,
        model_a_scores: List[float],
        model_b_name: str,
        model_b_scores: List[float]
    ) -> ComparisonResult:
        """
        Compare two models using paired t-test on CV scores.

        Tests null hypothesis: models have equal performance.
        """
        if len(model_a_scores) != len(model_b_scores):
            raise ValueError("Score arrays must have same length")

        # Paired t-test
        statistic, p_value = ttest_rel(model_a_scores, model_b_scores)

        is_significant = p_value < self.alpha

        # Determine winner
        winner = None
        if is_significant:
            if np.mean(model_a_scores) > np.mean(model_b_scores):
                winner = model_a_name
            else:
                winner = model_b_name

        result = ComparisonResult(
            model_a=model_a_name,
            model_b=model_b_name,
            test_statistic=statistic,
            p_value=p_value,
            is_significant=is_significant,
            alpha=self.alpha,
            test_method="paired_t_test",
            winner=winner
        )

        logger.info(str(result))
        return result

    def mcnemar_test(
        self,
        model_a_name: str,
        model_a_predictions: np.ndarray,
        model_b_name: str,
        model_b_predictions: np.ndarray,
        y_true: np.ndarray
    ) -> ComparisonResult:
        """
        McNemar's test for comparing binary classifiers.

        Tests whether the disagreements between models are systematic.
        """
        # Create contingency table
        a_correct = model_a_predictions == y_true
        b_correct = model_b_predictions == y_true

        # Count agreements and disagreements
        both_correct = np.sum(a_correct & b_correct)
        both_wrong = np.sum(~a_correct & ~b_correct)
        a_correct_b_wrong = np.sum(a_correct & ~b_correct)
        a_wrong_b_correct = np.sum(~a_correct & b_correct)

        # McNemar's test statistic
        # Uses only the disagreements
        n = a_correct_b_wrong + a_wrong_b_correct

        if n == 0:
            # Models have identical predictions
            p_value = 1.0
            statistic = 0.0
        else:
            # Chi-squared test with continuity correction
            statistic = (abs(a_correct_b_wrong - a_wrong_b_correct) - 1) ** 2 / n

            from scipy.stats import chi2
            p_value = 1 - chi2.cdf(statistic, df=1)

        is_significant = p_value < self.alpha

        # Determine winner
        winner = None
        if is_significant:
            if a_correct_b_wrong > a_wrong_b_correct:
                winner = model_a_name
            else:
                winner = model_b_name

        result = ComparisonResult(
            model_a=model_a_name,
            model_b=model_b_name,
            test_statistic=statistic,
            p_value=p_value,
            is_significant=is_significant,
            alpha=self.alpha,
            test_method="mcnemar_test",
            winner=winner
        )

        logger.info(str(result))
        logger.info(f"  Contingency: both_correct={both_correct}, "
                   f"both_wrong={both_wrong}, "
                   f"A_correct_B_wrong={a_correct_b_wrong}, "
                   f"A_wrong_B_correct={a_wrong_b_correct}")

        return result

    def permutation_test(
        self,
        model_a_name: str,
        model_a_scores: np.ndarray,
        model_b_name: str,
        model_b_scores: np.ndarray,
        n_permutations: int = 10000
    ) -> ComparisonResult:
        """
        Non-parametric permutation test for comparing models.

        Tests whether the observed difference could occur by chance.
        """
        # Observed difference
        observed_diff = np.mean(model_a_scores) - np.mean(model_b_scores)

        # Combine scores
        combined = np.concatenate([model_a_scores, model_b_scores])
        n_a = len(model_a_scores)

        # Permutation test
        count_extreme = 0

        np.random.seed(42)
        for _ in range(n_permutations):
            # Randomly permute
            permuted = np.random.permutation(combined)
            perm_a = permuted[:n_a]
            perm_b = permuted[n_a:]

            # Calculate permuted difference
            perm_diff = np.mean(perm_a) - np.mean(perm_b)

            # Count if as extreme as observed
            if abs(perm_diff) >= abs(observed_diff):
                count_extreme += 1

        p_value = count_extreme / n_permutations
        is_significant = p_value < self.alpha

        # Determine winner
        winner = None
        if is_significant:
            if observed_diff > 0:
                winner = model_a_name
            else:
                winner = model_b_name

        result = ComparisonResult(
            model_a=model_a_name,
            model_b=model_b_name,
            test_statistic=observed_diff,
            p_value=p_value,
            is_significant=is_significant,
            alpha=self.alpha,
            test_method=f"permutation_test (n={n_permutations})",
            winner=winner
        )

        logger.info(str(result))
        return result

    def compare_multiple_models(
        self,
        cv_results: Dict[str, List[float]]
    ) -> List[ComparisonResult]:
        """
        Pairwise comparison of all model pairs.

        Args:
            cv_results: Dict mapping model names to CV scores

        Returns:
            List of ComparisonResults for all pairs
        """
        results = []

        model_names = list(cv_results.keys())
        for model_a, model_b in combinations(model_names, 2):
            result = self.compare_cv_scores(
                model_a, cv_results[model_a],
                model_b, cv_results[model_b]
            )
            results.append(result)

        # Sort by p-value
        results.sort(key=lambda r: r.p_value)

        logger.info(f"Completed {len(results)} pairwise comparisons")
        return results
\end{lstlisting}

\section{Model Complexity and Performance Trade-offs}

The best model balances predictive performance with operational complexity.

\subsection{Complexity-Performance Analysis}

\begin{lstlisting}[language=Python, caption={Model Complexity Trade-off Analysis}]
import matplotlib.pyplot as plt
import seaborn as sns

@dataclass
class ComplexityTradeoff:
    """Analysis of complexity-performance trade-off."""
    model_name: str
    performance_score: float
    complexity_score: float
    efficiency_score: float  # Performance per unit complexity
    is_pareto_optimal: bool = False

class ComplexityAnalyzer:
    """
    Analyze trade-offs between model performance and complexity.

    Helps identify models on the Pareto frontier: no other model
    is both simpler AND more accurate.
    """

    def analyze_tradeoffs(
        self,
        candidates: List[ModelCandidate],
        performance_metric: str = "accuracy"
    ) -> List[ComplexityTradeoff]:
        """
        Analyze complexity-performance trade-offs.

        Args:
            candidates: List of model candidates
            performance_metric: Which metric to use for performance

        Returns:
            List of ComplexityTradeoff analyses
        """
        tradeoffs = []

        for candidate in candidates:
            # Extract performance score
            perf_score = self._get_performance_metric(
                candidate.performance, performance_metric
            )

            # Get complexity score
            complexity_score = candidate.complexity.compute_complexity_score()

            # Calculate efficiency (performance per unit complexity)
            efficiency = perf_score / (complexity_score + 1)  # Add 1 to avoid div by 0

            tradeoffs.append(ComplexityTradeoff(
                model_name=candidate.name,
                performance_score=perf_score,
                complexity_score=complexity_score,
                efficiency_score=efficiency
            ))

        # Identify Pareto optimal models
        tradeoffs = self._identify_pareto_optimal(tradeoffs)

        logger.info(f"Analyzed {len(candidates)} models for complexity trade-offs")
        pareto_count = sum(1 for t in tradeoffs if t.is_pareto_optimal)
        logger.info(f"Found {pareto_count} Pareto-optimal models")

        return tradeoffs

    def _get_performance_metric(
        self,
        performance: PerformanceMetrics,
        metric_name: str
    ) -> float:
        """Extract specific performance metric."""
        metric_value = getattr(performance, metric_name, None)
        if metric_value is None:
            raise ValueError(f"Metric '{metric_name}' not available")
        return metric_value

    def _identify_pareto_optimal(
        self,
        tradeoffs: List[ComplexityTradeoff]
    ) -> List[ComplexityTradeoff]:
        """
        Identify Pareto-optimal models.

        A model is Pareto-optimal if no other model is both:
        - More accurate (higher performance score)
        - Simpler (lower complexity score)
        """
        for i, candidate in enumerate(tradeoffs):
            is_dominated = False

            for j, other in enumerate(tradeoffs):
                if i == j:
                    continue

                # Check if 'other' dominates 'candidate'
                if (other.performance_score >= candidate.performance_score and
                    other.complexity_score <= candidate.complexity_score and
                    (other.performance_score > candidate.performance_score or
                     other.complexity_score < candidate.complexity_score)):
                    is_dominated = True
                    break

            candidate.is_pareto_optimal = not is_dominated

        return tradeoffs

    def plot_tradeoff(
        self,
        tradeoffs: List[ComplexityTradeoff],
        output_path: Optional[Path] = None
    ) -> None:
        """
        Visualize complexity-performance trade-off.

        Creates scatter plot with Pareto frontier highlighted.
        """
        fig, ax = plt.subplots(figsize=(10, 6))

        # Separate Pareto and non-Pareto models
        pareto = [t for t in tradeoffs if t.is_pareto_optimal]
        non_pareto = [t for t in tradeoffs if not t.is_pareto_optimal]

        # Plot non-Pareto models
        if non_pareto:
            ax.scatter(
                [t.complexity_score for t in non_pareto],
                [t.performance_score for t in non_pareto],
                c='lightblue', s=100, alpha=0.6, label='Other models'
            )

        # Plot Pareto-optimal models
        if pareto:
            ax.scatter(
                [t.complexity_score for t in pareto],
                [t.performance_score for t in pareto],
                c='red', s=150, alpha=0.8, label='Pareto optimal', marker='*'
            )

            # Draw Pareto frontier
            pareto_sorted = sorted(pareto, key=lambda t: t.complexity_score)
            ax.plot(
                [t.complexity_score for t in pareto_sorted],
                [t.performance_score for t in pareto_sorted],
                'r--', alpha=0.5, linewidth=2
            )

        # Annotate models
        for t in tradeoffs:
            ax.annotate(
                t.model_name,
                (t.complexity_score, t.performance_score),
                xytext=(5, 5), textcoords='offset points',
                fontsize=8, alpha=0.7
            )

        ax.set_xlabel('Complexity Score', fontsize=12)
        ax.set_ylabel('Performance Score', fontsize=12)
        ax.set_title('Model Complexity vs Performance Trade-off', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved trade-off plot to {output_path}")

        plt.close()

    def generate_report(self, tradeoffs: List[ComplexityTradeoff]) -> pd.DataFrame:
        """Generate DataFrame report of trade-off analysis."""
        data = []
        for t in tradeoffs:
            data.append({
                "model": t.model_name,
                "performance": t.performance_score,
                "complexity": t.complexity_score,
                "efficiency": t.efficiency_score,
                "pareto_optimal": t.is_pareto_optimal
            })

        df = pd.DataFrame(data)
        df = df.sort_values("efficiency", ascending=False)

        return df
\end{lstlisting}

\section{Automated Model Selection}

Integrate business constraints, performance requirements, and operational limits into automated model selection.

\begin{lstlisting}[language=Python, caption={Automated Model Selection with Business Constraints}]
@dataclass
class BusinessConstraints:
    """Business and operational constraints for model selection."""
    max_inference_time_ms: Optional[float] = None
    max_model_size_mb: Optional[float] = None
    max_memory_mb: Optional[float] = None
    min_accuracy: Optional[float] = None
    min_recall: Optional[float] = None  # For high-recall applications
    min_precision: Optional[float] = None  # For high-precision applications
    require_interpretability: bool = False
    max_complexity_level: Optional[ComplexityLevel] = None

    def validate_candidate(self, candidate: ModelCandidate) -> Tuple[bool, List[str]]:
        """
        Check if candidate meets all constraints.

        Returns:
            (is_valid, list_of_violations)
        """
        violations = []

        # Check inference time
        if (self.max_inference_time_ms is not None and
            candidate.complexity.inference_time_ms > self.max_inference_time_ms):
            violations.append(
                f"Inference time {candidate.complexity.inference_time_ms:.2f}ms "
                f"exceeds limit {self.max_inference_time_ms}ms"
            )

        # Check model size
        size_mb = candidate.complexity.model_size_bytes / 1024 / 1024
        if self.max_model_size_mb is not None and size_mb > self.max_model_size_mb:
            violations.append(
                f"Model size {size_mb:.2f}MB exceeds limit {self.max_model_size_mb}MB"
            )

        # Check memory
        if (self.max_memory_mb is not None and
            candidate.complexity.memory_mb > self.max_memory_mb):
            violations.append(
                f"Memory {candidate.complexity.memory_mb:.2f}MB "
                f"exceeds limit {self.max_memory_mb}MB"
            )

        # Check accuracy
        if (self.min_accuracy is not None and
            candidate.performance.accuracy is not None and
            candidate.performance.accuracy < self.min_accuracy):
            violations.append(
                f"Accuracy {candidate.performance.accuracy:.4f} "
                f"below minimum {self.min_accuracy}"
            )

        # Check recall
        if (self.min_recall is not None and
            candidate.performance.recall is not None and
            candidate.performance.recall < self.min_recall):
            violations.append(
                f"Recall {candidate.performance.recall:.4f} "
                f"below minimum {self.min_recall}"
            )

        # Check precision
        if (self.min_precision is not None and
            candidate.performance.precision is not None and
            candidate.performance.precision < self.min_precision):
            violations.append(
                f"Precision {candidate.performance.precision:.4f} "
                f"below minimum {self.min_precision}"
            )

        # Check complexity level
        if (self.max_complexity_level is not None and
            candidate.complexity.complexity_level.value >
            self.max_complexity_level.value):
            violations.append(
                f"Complexity level {candidate.complexity.complexity_level.value} "
                f"exceeds maximum {self.max_complexity_level.value}"
            )

        # Check interpretability
        if self.require_interpretability:
            interpretable_algos = ['linear', 'logistic', 'tree', 'ridge', 'lasso']
            if not any(algo in candidate.algorithm.lower()
                      for algo in interpretable_algos):
                violations.append(
                    f"Model {candidate.algorithm} not interpretable"
                )

        is_valid = len(violations) == 0
        return is_valid, violations

@dataclass
class SelectionResult:
    """Result of automated model selection."""
    selected_model: ModelCandidate
    all_candidates: List[ModelCandidate]
    valid_candidates: List[ModelCandidate]
    selection_criteria: str
    constraints: BusinessConstraints
    selection_score: float

class AutomatedModelSelector:
    """
    Automated model selection with business constraints.

    Scoring function:
    score = performance_weight * performance +
            simplicity_weight * (100 - complexity) +
            efficiency_weight * efficiency
    """

    def __init__(self,
                 performance_weight: float = 0.6,
                 simplicity_weight: float = 0.2,
                 efficiency_weight: float = 0.2):
        """
        Args:
            performance_weight: Weight for predictive performance
            simplicity_weight: Weight for model simplicity
            efficiency_weight: Weight for inference efficiency
        """
        if abs(performance_weight + simplicity_weight + efficiency_weight - 1.0) > 1e-6:
            raise ValueError("Weights must sum to 1.0")

        self.performance_weight = performance_weight
        self.simplicity_weight = simplicity_weight
        self.efficiency_weight = efficiency_weight

    def select_best_model(
        self,
        candidates: List[ModelCandidate],
        constraints: BusinessConstraints,
        performance_metric: str = "accuracy"
    ) -> SelectionResult:
        """
        Select best model given candidates and constraints.

        Args:
            candidates: List of trained model candidates
            constraints: Business and operational constraints
            performance_metric: Primary performance metric

        Returns:
            SelectionResult with selected model and analysis
        """
        logger.info(f"Selecting from {len(candidates)} candidates")

        # Filter by constraints
        valid_candidates = []
        for candidate in candidates:
            is_valid, violations = constraints.validate_candidate(candidate)
            if is_valid:
                valid_candidates.append(candidate)
            else:
                logger.info(f"Candidate '{candidate.name}' failed constraints:")
                for violation in violations:
                    logger.info(f"  - {violation}")

        if not valid_candidates:
            raise ValueError("No candidates meet the specified constraints")

        logger.info(f"{len(valid_candidates)} candidates meet constraints")

        # Score valid candidates
        scored_candidates = []
        for candidate in valid_candidates:
            score = self._compute_selection_score(candidate, performance_metric)
            scored_candidates.append((candidate, score))

        # Select best
        scored_candidates.sort(key=lambda x: x[1], reverse=True)
        best_candidate, best_score = scored_candidates[0]

        logger.info(f"Selected model: {best_candidate.name} (score={best_score:.4f})")

        result = SelectionResult(
            selected_model=best_candidate,
            all_candidates=candidates,
            valid_candidates=valid_candidates,
            selection_criteria=f"weighted_score (perf={self.performance_weight}, "
                              f"simp={self.simplicity_weight}, eff={self.efficiency_weight})",
            constraints=constraints,
            selection_score=best_score
        )

        return result

    def _compute_selection_score(
        self,
        candidate: ModelCandidate,
        performance_metric: str
    ) -> float:
        """Compute weighted selection score."""
        # Performance score (0-100)
        perf_value = getattr(candidate.performance, performance_metric)
        if perf_value is None:
            raise ValueError(f"Metric '{performance_metric}' not available")

        # Normalize to 0-100 (assuming metrics are 0-1 or already percentages)
        if perf_value <= 1.0:
            performance_score = perf_value * 100
        else:
            performance_score = perf_value

        # Complexity score (0-100, lower is better, so invert)
        complexity_score = candidate.complexity.compute_complexity_score()
        simplicity_score = 100 - complexity_score

        # Efficiency score (performance per ms of inference time)
        efficiency_score = min(
            (performance_score / (candidate.complexity.inference_time_ms + 0.1)) * 10,
            100
        )

        # Weighted combination
        total_score = (
            self.performance_weight * performance_score +
            self.simplicity_weight * simplicity_score +
            self.efficiency_weight * efficiency_score
        )

        return total_score
\end{lstlisting}

\section{Performance Degradation Detection}

Models degrade over time due to data drift, concept drift, or operational changes. Automated monitoring detects degradation and triggers retraining.

\begin{lstlisting}[language=Python, caption={Performance Degradation Detection and Retraining Triggers}]
import sqlite3
from collections import deque

@dataclass
class PerformanceSnapshot:
    """Snapshot of model performance at a point in time."""
    timestamp: datetime
    metric_name: str
    metric_value: float
    n_samples: int
    data_hash: str  # Hash of recent data characteristics

@dataclass
class DegradationAlert:
    """Alert for detected performance degradation."""
    model_name: str
    metric_name: str
    baseline_value: float
    current_value: float
    degradation_pct: float
    timestamp: datetime
    severity: str  # 'low', 'medium', 'high', 'critical'
    should_retrain: bool

class PerformanceMonitor:
    """
    Monitor model performance over time and detect degradation.

    Triggers retraining when:
    - Performance drops below threshold
    - Consistent downward trend detected
    - Sudden sharp decline
    """

    def __init__(self,
                 db_path: Path,
                 baseline_window: int = 100,
                 monitoring_window: int = 50,
                 degradation_threshold_pct: float = 5.0,
                 critical_threshold_pct: float = 10.0):
        """
        Args:
            db_path: Path to SQLite database
            baseline_window: Window size for baseline performance
            monitoring_window: Window size for current performance
            degradation_threshold_pct: % drop to trigger alert
            critical_threshold_pct: % drop to trigger immediate retraining
        """
        self.db_path = db_path
        self.baseline_window = baseline_window
        self.monitoring_window = monitoring_window
        self.degradation_threshold = degradation_threshold_pct
        self.critical_threshold = critical_threshold_pct

        self.performance_history: deque = deque(maxlen=baseline_window * 2)
        self._init_database()

    def _init_database(self) -> None:
        """Initialize monitoring database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT NOT NULL,
                timestamp DATETIME NOT NULL,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                n_samples INTEGER NOT NULL,
                data_hash TEXT NOT NULL
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS degradation_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT NOT NULL,
                timestamp DATETIME NOT NULL,
                metric_name TEXT NOT NULL,
                baseline_value REAL NOT NULL,
                current_value REAL NOT NULL,
                degradation_pct REAL NOT NULL,
                severity TEXT NOT NULL,
                should_retrain BOOLEAN NOT NULL
            )
        ''')

        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_snapshots_model_time
            ON performance_snapshots(model_name, timestamp)
        ''')

        conn.commit()
        conn.close()

    def record_performance(
        self,
        model_name: str,
        metric_name: str,
        metric_value: float,
        n_samples: int,
        data_hash: str,
        timestamp: Optional[datetime] = None
    ) -> Optional[DegradationAlert]:
        """
        Record performance snapshot and check for degradation.

        Returns:
            DegradationAlert if degradation detected, else None
        """
        if timestamp is None:
            timestamp = datetime.now()

        # Record to database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO performance_snapshots
            (model_name, timestamp, metric_name, metric_value, n_samples, data_hash)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (model_name, timestamp.isoformat(), metric_name, metric_value,
              n_samples, data_hash))

        conn.commit()
        conn.close()

        # Update in-memory history
        snapshot = PerformanceSnapshot(
            timestamp=timestamp,
            metric_name=metric_name,
            metric_value=metric_value,
            n_samples=n_samples,
            data_hash=data_hash
        )
        self.performance_history.append(snapshot)

        # Check for degradation
        if len(self.performance_history) >= self.baseline_window + self.monitoring_window:
            alert = self._check_degradation(model_name, metric_name)
            if alert:
                self._record_alert(alert)
                return alert

        return None

    def _check_degradation(
        self,
        model_name: str,
        metric_name: str
    ) -> Optional[DegradationAlert]:
        """Check if performance has degraded significantly."""
        history = list(self.performance_history)

        # Calculate baseline (early window)
        baseline_values = [
            s.metric_value for s in history[:self.baseline_window]
            if s.metric_name == metric_name
        ]

        if not baseline_values:
            return None

        baseline_mean = np.mean(baseline_values)

        # Calculate current performance (recent window)
        current_values = [
            s.metric_value for s in history[-self.monitoring_window:]
            if s.metric_name == metric_name
        ]

        if not current_values:
            return None

        current_mean = np.mean(current_values)

        # Calculate degradation percentage
        degradation_pct = (baseline_mean - current_mean) / baseline_mean * 100

        # Check if degradation exceeds threshold
        if degradation_pct >= self.degradation_threshold:
            # Determine severity
            if degradation_pct >= self.critical_threshold:
                severity = "critical"
                should_retrain = True
            elif degradation_pct >= self.degradation_threshold * 1.5:
                severity = "high"
                should_retrain = True
            elif degradation_pct >= self.degradation_threshold:
                severity = "medium"
                should_retrain = False
            else:
                severity = "low"
                should_retrain = False

            alert = DegradationAlert(
                model_name=model_name,
                metric_name=metric_name,
                baseline_value=baseline_mean,
                current_value=current_mean,
                degradation_pct=degradation_pct,
                timestamp=datetime.now(),
                severity=severity,
                should_retrain=should_retrain
            )

            logger.warning(f"DEGRADATION ALERT: {model_name} - "
                          f"{metric_name} dropped {degradation_pct:.2f}% "
                          f"({baseline_mean:.4f} -> {current_mean:.4f}), "
                          f"severity={severity}")

            return alert

        return None

    def _record_alert(self, alert: DegradationAlert) -> None:
        """Record alert to database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO degradation_alerts
            (model_name, timestamp, metric_name, baseline_value, current_value,
             degradation_pct, severity, should_retrain)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            alert.model_name,
            alert.timestamp.isoformat(),
            alert.metric_name,
            alert.baseline_value,
            alert.current_value,
            alert.degradation_pct,
            alert.severity,
            alert.should_retrain
        ))

        conn.commit()
        conn.close()

    def get_alert_history(
        self,
        model_name: str,
        days: int = 30
    ) -> pd.DataFrame:
        """Get degradation alert history."""
        conn = sqlite3.connect(self.db_path)

        cutoff = datetime.now() - timedelta(days=days)

        query = '''
            SELECT * FROM degradation_alerts
            WHERE model_name = ? AND timestamp >= ?
            ORDER BY timestamp DESC
        '''

        df = pd.read_sql_query(query, conn, params=(model_name, cutoff.isoformat()))
        conn.close()

        return df

    def plot_performance_trend(
        self,
        model_name: str,
        metric_name: str,
        days: int = 30,
        output_path: Optional[Path] = None
    ) -> None:
        """Plot performance trend over time."""
        conn = sqlite3.connect(self.db_path)

        cutoff = datetime.now() - timedelta(days=days)

        query = '''
            SELECT timestamp, metric_value
            FROM performance_snapshots
            WHERE model_name = ? AND metric_name = ? AND timestamp >= ?
            ORDER BY timestamp
        '''

        df = pd.read_sql_query(
            query, conn,
            params=(model_name, metric_name, cutoff.isoformat()),
            parse_dates=['timestamp']
        )
        conn.close()

        if df.empty:
            logger.warning("No performance data available")
            return

        fig, ax = plt.subplots(figsize=(12, 6))

        ax.plot(df['timestamp'], df['metric_value'], 'b-', linewidth=2)
        ax.scatter(df['timestamp'], df['metric_value'], c='blue', s=30, alpha=0.6)

        # Add trend line
        from scipy.stats import linregress
        x_numeric = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds()
        slope, intercept, _, _, _ = linregress(x_numeric, df['metric_value'])
        trend_line = slope * x_numeric + intercept
        ax.plot(df['timestamp'], trend_line, 'r--', linewidth=2, alpha=0.7,
               label=f'Trend (slope={slope:.6f})')

        ax.set_xlabel('Time', fontsize=12)
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_title(f'{model_name} - {metric_name} Over Time', fontsize=14)
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved performance trend plot to {output_path}")

        plt.close()
\end{lstlisting}

\section{Advanced Model Selection Frameworks}

Modern model selection requires balancing multiple competing objectives: accuracy, fairness, interpretability, latency, and cost. This section presents advanced frameworks for systematic multi-objective optimization.

\subsection{Multi-Objective Optimization with Pareto Analysis}

Real-world model selection rarely optimizes a single metric. We must balance accuracy, inference speed, memory usage, interpretability, and fairness simultaneously.

\begin{lstlisting}[language=Python, caption={Multi-Objective Model Optimization with Pareto Frontier}]
from typing import List, Tuple, Callable, Dict
from dataclasses import dataclass
import numpy as np
import pandas as pd
from scipy.spatial.distance import euclidean
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

@dataclass
class ObjectiveMetric:
    """Single objective to optimize."""
    name: str
    value: float
    weight: float = 1.0
    minimize: bool = False  # True if lower is better

    @property
    def normalized_value(self) -> float:
        """Value adjusted for minimization vs maximization."""
        return -self.value if self.minimize else self.value

@dataclass
class MultiObjectiveScore:
    """Scores across multiple objectives."""
    model_name: str
    objectives: Dict[str, ObjectiveMetric]

    def get_objective_vector(self) -> np.ndarray:
        """Get vector of normalized objective values."""
        return np.array([obj.normalized_value for obj in self.objectives.values()])

    def dominates(self, other: "MultiObjectiveScore") -> bool:
        """
        Check if this solution Pareto-dominates another.

        A solution dominates another if it's better in at least one objective
        and not worse in any objective.
        """
        self_vec = self.get_objective_vector()
        other_vec = other.get_objective_vector()

        better_in_some = np.any(self_vec > other_vec)
        not_worse_in_any = np.all(self_vec >= other_vec)

        return better_in_some and not_worse_in_any

    def weighted_sum(self) -> float:
        """Calculate weighted sum of objectives."""
        return sum(obj.normalized_value * obj.weight
                  for obj in self.objectives.values())

class ParetoFrontierAnalyzer:
    """
    Analyze Pareto frontier for multi-objective model selection.

    Identifies non-dominated solutions and provides trade-off analysis.
    """

    def __init__(self):
        self.pareto_front: List[MultiObjectiveScore] = []
        self.all_solutions: List[MultiObjectiveScore] = []

    def add_solution(self, solution: MultiObjectiveScore):
        """Add a solution and update Pareto frontier."""
        self.all_solutions.append(solution)

        # Check if solution is dominated by existing Pareto front
        dominated = any(p.dominates(solution) for p in self.pareto_front)

        if not dominated:
            # Remove solutions dominated by new solution
            self.pareto_front = [
                p for p in self.pareto_front
                if not solution.dominates(p)
            ]
            self.pareto_front.append(solution)

    def find_pareto_frontier(self,
                            solutions: List[MultiObjectiveScore]) -> List[MultiObjectiveScore]:
        """Identify Pareto-optimal solutions."""
        self.all_solutions = solutions
        self.pareto_front = []

        for solution in solutions:
            self.add_solution(solution)

        return self.pareto_front

    def get_knee_point(self) -> Optional[MultiObjectiveScore]:
        """
        Find knee point on Pareto frontier.

        Knee point provides best balance across objectives.
        Uses maximum distance from ideal-nadir line.
        """
        if len(self.pareto_front) < 2:
            return self.pareto_front[0] if self.pareto_front else None

        # Get objective vectors for Pareto front
        vectors = np.array([s.get_objective_vector() for s in self.pareto_front])

        # Normalize to [0, 1]
        scaler = StandardScaler()
        vectors_norm = scaler.fit_transform(vectors)

        # Find ideal and nadir points
        ideal = vectors_norm.max(axis=0)
        nadir = vectors_norm.min(axis=0)

        # Calculate distance to ideal-nadir line for each point
        max_distance = -1
        knee_idx = 0

        for i, vec in enumerate(vectors_norm):
            # Distance from point to line connecting ideal and nadir
            distance = np.linalg.norm(np.cross(nadir - ideal, ideal - vec)) / \
                      np.linalg.norm(nadir - ideal)

            if distance > max_distance:
                max_distance = distance
                knee_idx = i

        return self.pareto_front[knee_idx]

    def get_solution_by_preference(self,
                                   preferences: Dict[str, float]) -> MultiObjectiveScore:
        """
        Select solution based on stakeholder preferences.

        Args:
            preferences: Weight for each objective (0-1, should sum to 1)

        Returns:
            Best solution according to weighted preferences
        """
        best_score = float('-inf')
        best_solution = None

        for solution in self.pareto_front:
            score = sum(
                solution.objectives[obj_name].normalized_value * weight
                for obj_name, weight in preferences.items()
                if obj_name in solution.objectives
            )

            if score > best_score:
                best_score = score
                best_solution = solution

        return best_solution

    def visualize_pareto_front_2d(self,
                                  obj1_name: str,
                                  obj2_name: str,
                                  output_path: Optional[str] = None):
        """Visualize 2D Pareto frontier."""
        if not self.all_solutions:
            logger.warning("No solutions to visualize")
            return

        # Extract objectives
        all_obj1 = [s.objectives[obj1_name].value for s in self.all_solutions]
        all_obj2 = [s.objectives[obj2_name].value for s in self.all_solutions]

        pareto_obj1 = [s.objectives[obj1_name].value for s in self.pareto_front]
        pareto_obj2 = [s.objectives[obj2_name].value for s in self.pareto_front]

        # Create plot
        plt.figure(figsize=(10, 6))
        plt.scatter(all_obj1, all_obj2, c='lightgray', s=50,
                   alpha=0.5, label='All Solutions')
        plt.scatter(pareto_obj1, pareto_obj2, c='red', s=100,
                   alpha=0.8, label='Pareto Front', edgecolors='black')

        # Annotate Pareto solutions
        for solution in self.pareto_front:
            plt.annotate(solution.model_name,
                        (solution.objectives[obj1_name].value,
                         solution.objectives[obj2_name].value),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)

        # Highlight knee point
        knee = self.get_knee_point()
        if knee:
            plt.scatter([knee.objectives[obj1_name].value],
                       [knee.objectives[obj2_name].value],
                       c='gold', s=200, marker='*',
                       edgecolors='black', linewidths=2,
                       label='Knee Point', zorder=10)

        plt.xlabel(obj1_name, fontsize=12)
        plt.ylabel(obj2_name, fontsize=12)
        plt.title('Pareto Frontier Analysis', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            logger.info(f"Saved Pareto frontier plot to {output_path}")

        plt.close()

    def generate_trade_off_report(self) -> pd.DataFrame:
        """Generate trade-off analysis report."""
        data = []

        for solution in self.pareto_front:
            row = {'Model': solution.model_name}
            for obj_name, obj in solution.objectives.items():
                row[obj_name] = obj.value
            row['Weighted_Score'] = solution.weighted_sum()
            data.append(row)

        df = pd.DataFrame(data)
        return df.sort_values('Weighted_Score', ascending=False)

# Example usage for model selection
class MultiObjectiveModelSelector:
    """
    Select models using multi-objective optimization.

    Balances accuracy, latency, interpretability, fairness, etc.
    """

    def __init__(self):
        self.analyzer = ParetoFrontierAnalyzer()

    def evaluate_candidates(self,
                           candidates: List[ModelCandidate]) -> List[MultiObjectiveScore]:
        """Convert model candidates to multi-objective scores."""
        solutions = []

        for candidate in candidates:
            objectives = {
                'accuracy': ObjectiveMetric(
                    name='accuracy',
                    value=candidate.metrics.accuracy or 0.0,
                    weight=1.0,
                    minimize=False
                ),
                'latency': ObjectiveMetric(
                    name='latency_ms',
                    value=candidate.complexity.inference_time_ms,
                    weight=0.8,
                    minimize=True
                ),
                'memory': ObjectiveMetric(
                    name='memory_mb',
                    value=candidate.complexity.memory_mb,
                    weight=0.5,
                    minimize=True
                ),
                'interpretability': ObjectiveMetric(
                    name='interpretability',
                    value=self._score_interpretability(candidate),
                    weight=0.7,
                    minimize=False
                )
            }

            solutions.append(MultiObjectiveScore(
                model_name=candidate.name,
                objectives=objectives
            ))

        return solutions

    def _score_interpretability(self, candidate: ModelCandidate) -> float:
        """Score model interpretability (0-1, higher is better)."""
        complexity_map = {
            ComplexityLevel.LOW: 1.0,
            ComplexityLevel.MEDIUM: 0.5,
            ComplexityLevel.HIGH: 0.2
        }
        return complexity_map.get(candidate.complexity.level, 0.5)

    def select_best_model(self,
                         candidates: List[ModelCandidate],
                         preferences: Optional[Dict[str, float]] = None) -> ModelCandidate:
        """
        Select best model using multi-objective optimization.

        Args:
            candidates: List of model candidates
            preferences: Optional stakeholder preferences for objectives

        Returns:
            Selected model candidate
        """
        solutions = self.evaluate_candidates(candidates)
        pareto_front = self.analyzer.find_pareto_frontier(solutions)

        logger.info(f"Found {len(pareto_front)} Pareto-optimal solutions "
                   f"from {len(candidates)} candidates")

        if preferences:
            best_solution = self.analyzer.get_solution_by_preference(preferences)
        else:
            # Use knee point if no preferences specified
            best_solution = self.analyzer.get_knee_point()

        # Find corresponding candidate
        for candidate in candidates:
            if candidate.name == best_solution.model_name:
                return candidate

        raise ValueError(f"Could not find candidate for {best_solution.model_name}")
\end{lstlisting}

This multi-objective framework enables principled trade-off analysis. Rather than arbitrarily weighting metrics, we identify Pareto-optimal solutions and select based on stakeholder preferences or the knee point for balanced performance.

\subsection{Advanced AutoML with Custom Search Spaces}

Automated machine learning (AutoML) can explore vast model spaces efficiently, but production systems require custom constraints and domain knowledge integration.

\begin{lstlisting}[language=Python, caption={Production-Grade AutoML with Custom Constraints}]
from typing import Dict, Any, List, Callable, Optional
import optuna
from optuna.pruners import MedianPruner, SuccessiveHalvingPruner
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import lightgbm as lgb

class CustomAutoML:
    """
    Production-grade AutoML with custom search spaces and constraints.

    Supports:
    - Custom algorithm search spaces
    - Business constraint integration
    - Multi-objective optimization
    - Early stopping for efficiency
    """

    def __init__(self,
                 task_type: str = 'classification',
                 time_budget_seconds: int = 3600,
                 n_trials: int = 100,
                 constraints: Optional[BusinessConstraints] = None):
        """
        Args:
            task_type: 'classification' or 'regression'
            time_budget_seconds: Maximum time for optimization
            n_trials: Maximum number of trials
            constraints: Business constraints for model selection
        """
        self.task_type = task_type
        self.time_budget_seconds = time_budget_seconds
        self.n_trials = n_trials
        self.constraints = constraints or BusinessConstraints()
        self.study: Optional[optuna.Study] = None
        self.best_model: Optional[Any] = None

    def define_search_space(self,
                           trial: optuna.Trial,
                           algorithm_family: Optional[List[str]] = None) -> Any:
        """
        Define custom search space for hyperparameters.

        Args:
            trial: Optuna trial object
            algorithm_family: Restrict to specific algorithms (e.g., ['tree', 'linear'])

        Returns:
            Configured model instance
        """
        if algorithm_family is None:
            algorithm_family = ['tree', 'linear', 'ensemble', 'neural']

        # Select algorithm
        algorithm = trial.suggest_categorical('algorithm', algorithm_family)

        if algorithm == 'tree':
            return self._configure_random_forest(trial)
        elif algorithm == 'linear':
            return self._configure_logistic_regression(trial)
        elif algorithm == 'ensemble':
            ensemble_type = trial.suggest_categorical('ensemble_type',
                                                     ['xgboost', 'lightgbm', 'gbm'])
            if ensemble_type == 'xgboost':
                return self._configure_xgboost(trial)
            elif ensemble_type == 'lightgbm':
                return self._configure_lightgbm(trial)
            else:
                return self._configure_gradient_boosting(trial)
        elif algorithm == 'neural':
            return self._configure_mlp(trial)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

    def _configure_random_forest(self, trial: optuna.Trial) -> RandomForestClassifier:
        """Configure Random Forest search space."""
        return RandomForestClassifier(
            n_estimators=trial.suggest_int('rf_n_estimators', 50, 500),
            max_depth=trial.suggest_int('rf_max_depth', 3, 20),
            min_samples_split=trial.suggest_int('rf_min_samples_split', 2, 20),
            min_samples_leaf=trial.suggest_int('rf_min_samples_leaf', 1, 10),
            max_features=trial.suggest_categorical('rf_max_features',
                                                   ['sqrt', 'log2', None]),
            random_state=42,
            n_jobs=-1
        )

    def _configure_logistic_regression(self, trial: optuna.Trial) -> LogisticRegression:
        """Configure Logistic Regression search space."""
        return LogisticRegression(
            C=trial.suggest_loguniform('lr_C', 1e-4, 1e2),
            penalty=trial.suggest_categorical('lr_penalty', ['l1', 'l2', 'elasticnet']),
            solver='saga',
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )

    def _configure_xgboost(self, trial: optuna.Trial) -> xgb.XGBClassifier:
        """Configure XGBoost search space."""
        return xgb.XGBClassifier(
            n_estimators=trial.suggest_int('xgb_n_estimators', 50, 500),
            max_depth=trial.suggest_int('xgb_max_depth', 3, 12),
            learning_rate=trial.suggest_loguniform('xgb_learning_rate', 1e-3, 1.0),
            subsample=trial.suggest_uniform('xgb_subsample', 0.6, 1.0),
            colsample_bytree=trial.suggest_uniform('xgb_colsample_bytree', 0.6, 1.0),
            gamma=trial.suggest_loguniform('xgb_gamma', 1e-8, 1.0),
            reg_alpha=trial.suggest_loguniform('xgb_reg_alpha', 1e-8, 10.0),
            reg_lambda=trial.suggest_loguniform('xgb_reg_lambda', 1e-8, 10.0),
            random_state=42,
            n_jobs=-1
        )

    def _configure_lightgbm(self, trial: optuna.Trial) -> lgb.LGBMClassifier:
        """Configure LightGBM search space."""
        return lgb.LGBMClassifier(
            n_estimators=trial.suggest_int('lgb_n_estimators', 50, 500),
            max_depth=trial.suggest_int('lgb_max_depth', 3, 12),
            learning_rate=trial.suggest_loguniform('lgb_learning_rate', 1e-3, 1.0),
            num_leaves=trial.suggest_int('lgb_num_leaves', 20, 300),
            subsample=trial.suggest_uniform('lgb_subsample', 0.6, 1.0),
            colsample_bytree=trial.suggest_uniform('lgb_colsample_bytree', 0.6, 1.0),
            reg_alpha=trial.suggest_loguniform('lgb_reg_alpha', 1e-8, 10.0),
            reg_lambda=trial.suggest_loguniform('lgb_reg_lambda', 1e-8, 10.0),
            random_state=42,
            n_jobs=-1
        )

    def _configure_gradient_boosting(self, trial: optuna.Trial) -> GradientBoostingClassifier:
        """Configure Gradient Boosting search space."""
        return GradientBoostingClassifier(
            n_estimators=trial.suggest_int('gbm_n_estimators', 50, 500),
            max_depth=trial.suggest_int('gbm_max_depth', 3, 10),
            learning_rate=trial.suggest_loguniform('gbm_learning_rate', 1e-3, 1.0),
            subsample=trial.suggest_uniform('gbm_subsample', 0.6, 1.0),
            random_state=42
        )

    def _configure_mlp(self, trial: optuna.Trial) -> MLPClassifier:
        """Configure MLP search space."""
        n_layers = trial.suggest_int('mlp_n_layers', 1, 3)
        hidden_layer_sizes = tuple(
            trial.suggest_int(f'mlp_n_units_l{i}', 32, 256)
            for i in range(n_layers)
        )

        return MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation=trial.suggest_categorical('mlp_activation',
                                                 ['relu', 'tanh']),
            alpha=trial.suggest_loguniform('mlp_alpha', 1e-5, 1e-1),
            learning_rate_init=trial.suggest_loguniform('mlp_learning_rate', 1e-4, 1e-2),
            max_iter=500,
            random_state=42
        )

    def objective(self, trial: optuna.Trial,
                 X: np.ndarray, y: np.ndarray) -> float:
        """
        Objective function for optimization.

        Evaluates model with cross-validation and checks constraints.
        """
        # Get model from search space
        model = self.define_search_space(trial)

        # Evaluate with cross-validation
        scores = cross_val_score(model, X, y, cv=5,
                                scoring='accuracy', n_jobs=-1)
        mean_score = scores.mean()

        # Check constraints (simplified - in production, train and measure actual metrics)
        # Here we use heuristics based on model type
        if self.constraints.require_interpretability:
            algorithm = trial.params.get('algorithm', '')
            if algorithm == 'neural':
                # Penalize complex models when interpretability required
                mean_score *= 0.8

        # Report intermediate values for pruning
        trial.report(mean_score, step=0)

        # Check if trial should be pruned
        if trial.should_prune():
            raise optuna.TrialPruned()

        return mean_score

    def optimize(self,
                X: np.ndarray,
                y: np.ndarray,
                direction: str = 'maximize') -> ModelCandidate:
        """
        Run AutoML optimization.

        Args:
            X: Feature matrix
            y: Target variable
            direction: 'maximize' or 'minimize'

        Returns:
            Best model candidate
        """
        # Create study with pruning for efficiency
        sampler = TPESampler(seed=42)
        pruner = MedianPruner(n_warmup_steps=5)

        self.study = optuna.create_study(
            direction=direction,
            sampler=sampler,
            pruner=pruner
        )

        # Optimize with timeout
        self.study.optimize(
            lambda trial: self.objective(trial, X, y),
            n_trials=self.n_trials,
            timeout=self.time_budget_seconds,
            show_progress_bar=True
        )

        # Train best model on full data
        best_params = self.study.best_params
        algorithm = best_params['algorithm']

        # Reconstruct best model
        best_trial = self.study.best_trial
        self.best_model = self.define_search_space(best_trial)
        self.best_model.fit(X, y)

        # Create ModelCandidate
        candidate = ModelCandidate(
            name=f"AutoML_{algorithm}",
            model=self.best_model,
            algorithm_name=algorithm,
            hyperparameters=best_params,
            model_type=ModelType.BINARY_CLASSIFICATION
        )

        logger.info(f"AutoML found best model: {algorithm} with "
                   f"score={self.study.best_value:.4f}")

        return candidate

    def get_optimization_history(self) -> pd.DataFrame:
        """Get history of all trials."""
        if not self.study:
            raise ValueError("Must run optimize() first")

        df = self.study.trials_dataframe()
        return df.sort_values('value', ascending=False)
\end{lstlisting}

This AutoML framework provides production-grade capabilities while allowing customization of search spaces and integration of business constraints.

\subsection{Ensemble Methods with Diversity Optimization}

Ensemble methods combine multiple models to improve predictive performance, but naive ensembling can waste computational resources on redundant models. Diversity optimization ensures ensemble members contribute unique perspectives.

\begin{lstlisting}[language=Python, caption={Ensemble Diversity Optimization and Stacking}]
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr
import itertools

class EnsembleDiversityOptimizer:
    """
    Optimize ensemble composition by maximizing diversity while
    maintaining performance.

    Diversity metrics:
    - Prediction disagreement
    - Error correlation
    - Decision boundary difference
    """

    def __init__(self,
                 max_ensemble_size: int = 5,
                 diversity_weight: float = 0.3):
        """
        Args:
            max_ensemble_size: Maximum number of models in ensemble
            diversity_weight: Weight for diversity vs performance (0-1)
        """
        self.max_ensemble_size = max_ensemble_size
        self.diversity_weight = diversity_weight

    def compute_diversity_matrix(self,
                                 predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """
        Compute pairwise diversity between model predictions.

        Args:
            predictions: Dict mapping model names to prediction arrays

        Returns:
            Diversity matrix (higher = more diverse)
        """
        model_names = list(predictions.keys())
        n_models = len(model_names)
        diversity_matrix = np.zeros((n_models, n_models))

        for i, j in itertools.combinations(range(n_models), 2):
            pred_i = predictions[model_names[i]]
            pred_j = predictions[model_names[j]]

            # Disagreement rate (classification)
            if pred_i.dtype == int or len(np.unique(pred_i)) < 10:
                diversity = np.mean(pred_i != pred_j)
            else:
                # Correlation (regression)
                diversity = 1 - abs(pearsonr(pred_i, pred_j)[0])

            diversity_matrix[i, j] = diversity
            diversity_matrix[j, i] = diversity

        return diversity_matrix

    def select_diverse_ensemble(self,
                                candidates: List[ModelCandidate],
                                X_val: np.ndarray,
                                y_val: np.ndarray) -> List[ModelCandidate]:
        """
        Select ensemble members maximizing diversity and performance.

        Args:
            candidates: List of model candidates
            X_val: Validation features
            y_val: Validation targets

        Returns:
            Selected ensemble members
        """
        # Get predictions from all candidates
        predictions = {
            c.name: c.predict(X_val) for c in candidates
        }

        # Compute diversity
        diversity_matrix = self.compute_diversity_matrix(predictions)

        # Greedy selection
        selected_indices = []
        remaining_indices = list(range(len(candidates)))

        # Start with best performing model
        performances = [
            self._compute_performance(candidates[i], X_val, y_val)
            for i in range(len(candidates))
        ]
        best_idx = np.argmax(performances)
        selected_indices.append(best_idx)
        remaining_indices.remove(best_idx)

        # Add models that maximize diversity + performance
        while len(selected_indices) < self.max_ensemble_size and remaining_indices:
            scores = []
            for idx in remaining_indices:
                # Diversity with selected ensemble
                avg_diversity = np.mean([
                    diversity_matrix[idx, sel_idx]
                    for sel_idx in selected_indices
                ])

                # Combined score
                score = (self.diversity_weight * avg_diversity +
                        (1 - self.diversity_weight) * performances[idx])
                scores.append(score)

            best_remaining = remaining_indices[np.argmax(scores)]
            selected_indices.append(best_remaining)
            remaining_indices.remove(best_remaining)

        selected_models = [candidates[i] for i in selected_indices]

        logger.info(f"Selected {len(selected_models)} diverse ensemble members")
        return selected_models

    def _compute_performance(self,
                           candidate: ModelCandidate,
                           X_val: np.ndarray,
                           y_val: np.ndarray) -> float:
        """Compute normalized performance score."""
        if candidate.model_type == ModelType.REGRESSION:
            return candidate.performance.r2 or 0.0
        else:
            return candidate.performance.accuracy or 0.0

    def build_stacking_ensemble(self,
                               base_models: List[ModelCandidate],
                               meta_learner: BaseEstimator,
                               X_train: np.ndarray,
                               y_train: np.ndarray,
                               X_val: np.ndarray,
                               y_val: np.ndarray) -> ModelCandidate:
        """
        Build stacking ensemble with meta-learner.

        Args:
            base_models: Base layer models
            meta_learner: Meta-learning model (e.g., LogisticRegression)
            X_train, y_train: Training data
            X_val, y_val: Validation data

        Returns:
            Stacking ensemble as ModelCandidate
        """
        estimators = [
            (model.name, model.estimator) for model in base_models
        ]

        stacking = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_learner,
            cv=5,
            stack_method='auto'
        )

        # Build using ModelBuilder
        builder = ModelBuilder(base_models[0].model_type)

        stacking_candidate = builder.build_candidate(
            name=f"Stacking_{len(base_models)}_models",
            estimator=stacking,
            X_train=X_train,
            y_train=y_train,
            X_val=X_val,
            y_val=y_val,
            feature_names=base_models[0].feature_names,
            hyperparameters={
                'base_models': [m.name for m in base_models],
                'meta_learner': type(meta_learner).__name__
            }
        )

        return stacking_candidate
\end{lstlisting}

\subsection{Neural Architecture Search with Efficiency Constraints}

Neural Architecture Search (NAS) automates deep learning architecture design, but production systems require efficiency constraints for deployment feasibility.

\begin{lstlisting}[language=Python, caption={NAS with Deployment Constraints}]
from typing import Tuple, List, Dict
import numpy as np
from dataclasses import dataclass

@dataclass
class ArchitectureConstraints:
    """Constraints for neural architecture search."""
    max_params: int = 10_000_000  # Maximum parameters
    max_latency_ms: float = 100.0  # Maximum inference latency
    max_memory_mb: float = 500.0   # Maximum memory footprint
    min_accuracy: float = 0.90     # Minimum acceptable accuracy

class NeuralArchitectureSearch:
    """
    Neural Architecture Search with efficiency constraints.

    Searches architecture space while respecting deployment constraints:
    - Model size (parameter count)
    - Inference latency
    - Memory usage
    - Accuracy requirements
    """

    def __init__(self,
                 constraints: ArchitectureConstraints,
                 search_space: Dict[str, List[Any]]):
        """
        Args:
            constraints: Deployment constraints
            search_space: Dictionary defining search space
                         (e.g., {'layers': [2,3,4], 'units': [64,128,256]})
        """
        self.constraints = constraints
        self.search_space = search_space
        self.search_history: List[Dict] = []

    def sample_architecture(self) -> Dict[str, Any]:
        """Sample architecture from search space."""
        architecture = {}
        for param, values in self.search_space.items():
            architecture[param] = np.random.choice(values)
        return architecture

    def evaluate_architecture(self,
                            architecture: Dict[str, Any],
                            X_train: np.ndarray,
                            y_train: np.ndarray,
                            X_val: np.ndarray,
                            y_val: np.ndarray) -> Tuple[float, Dict[str, float]]:
        """
        Evaluate architecture on validation data.

        Returns:
            (accuracy, constraints_dict) tuple
        """
        # Build model from architecture specification
        model = self._build_model(architecture, X_train.shape[1])

        # Train
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=10,
            batch_size=32,
            verbose=0
        )

        # Evaluate
        accuracy = history.history['val_accuracy'][-1]

        # Measure constraints
        n_params = model.count_params()
        latency = self._measure_latency(model, X_val[:100])
        memory = self._estimate_memory(model)

        constraints_met = {
            'params_ok': n_params <= self.constraints.max_params,
            'latency_ok': latency <= self.constraints.max_latency_ms,
            'memory_ok': memory <= self.constraints.max_memory_mb,
            'accuracy_ok': accuracy >= self.constraints.min_accuracy
        }

        metrics = {
            'accuracy': accuracy,
            'n_params': n_params,
            'latency_ms': latency,
            'memory_mb': memory,
            'all_constraints_met': all(constraints_met.values())
        }

        return accuracy, metrics

    def search(self,
              X_train: np.ndarray,
              y_train: np.ndarray,
              X_val: np.ndarray,
              y_val: np.ndarray,
              n_trials: int = 50) -> Dict[str, Any]:
        """
        Perform architecture search.

        Args:
            X_train, y_train: Training data
            X_val, y_val: Validation data
            n_trials: Number of architectures to evaluate

        Returns:
            Best architecture satisfying constraints
        """
        best_architecture = None
        best_accuracy = 0.0
        valid_architectures = []

        for trial in range(n_trials):
            architecture = self.sample_architecture()
            accuracy, metrics = self.evaluate_architecture(
                architecture, X_train, y_train, X_val, y_val
            )

            self.search_history.append({
                'trial': trial,
                'architecture': architecture,
                'accuracy': accuracy,
                **metrics
            })

            # Track valid architectures (meeting all constraints)
            if metrics['all_constraints_met']:
                valid_architectures.append({
                    'architecture': architecture,
                    'accuracy': accuracy,
                    'metrics': metrics
                })

                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_architecture = architecture

            logger.info(f"Trial {trial}: accuracy={accuracy:.4f}, "
                       f"params={metrics['n_params']}, "
                       f"latency={metrics['latency_ms']:.1f}ms, "
                       f"constraints_met={metrics['all_constraints_met']}")

        if best_architecture is None:
            logger.warning("No architecture met all constraints. "
                          "Relaxing constraints...")
            # Fall back to best accuracy regardless of constraints
            best_trial = max(self.search_history, key=lambda x: x['accuracy'])
            best_architecture = best_trial['architecture']

        logger.info(f"Best architecture: {best_architecture}, "
                   f"accuracy={best_accuracy:.4f}, "
                   f"{len(valid_architectures)}/{n_trials} met constraints")

        return best_architecture

    def _build_model(self, architecture: Dict[str, Any], input_dim: int):
        """Build Keras model from architecture specification."""
        from tensorflow import keras

        model = keras.Sequential()
        model.add(keras.layers.Input(shape=(input_dim,)))

        for i in range(architecture.get('n_layers', 2)):
            units = architecture.get(f'units_layer_{i}', 128)
            activation = architecture.get('activation', 'relu')
            dropout = architecture.get('dropout', 0.2)

            model.add(keras.layers.Dense(units, activation=activation))
            if dropout > 0:
                model.add(keras.layers.Dropout(dropout))

        # Output layer
        model.add(keras.layers.Dense(1, activation='sigmoid'))

        optimizer = architecture.get('optimizer', 'adam')
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        return model

    def _measure_latency(self, model, X_sample: np.ndarray) -> float:
        """Measure average inference latency in milliseconds."""
        import time

        n_samples = len(X_sample)
        start = time.time()
        _ = model.predict(X_sample, verbose=0)
        latency_ms = (time.time() - start) * 1000 / n_samples

        return latency_ms

    def _estimate_memory(self, model) -> float:
        """Estimate model memory footprint in MB."""
        # Rough estimate based on parameters
        n_params = model.count_params()
        bytes_per_param = 4  # Float32
        memory_mb = (n_params * bytes_per_param) / (1024 * 1024)
        return memory_mb
\end{lstlisting}

\subsection{Transfer Learning Evaluation with Domain Adaptation}

Transfer learning leverages pre-trained models for new tasks, but requires careful evaluation of domain similarity and adaptation effectiveness.

\begin{lstlisting}[language=Python, caption={Transfer Learning Evaluation Framework}]
from typing import Optional, Callable
from sklearn.metrics import accuracy_score
import numpy as np

class TransferLearningEvaluator:
    """
    Evaluate transfer learning effectiveness with domain adaptation metrics.

    Metrics:
    - Transfer performance gain vs training from scratch
    - Domain similarity assessment
    - Fine-tuning convergence analysis
    - Sample efficiency comparison
    """

    def __init__(self, source_model: BaseEstimator, task_type: ModelType):
        """
        Args:
            source_model: Pre-trained model from source domain
            task_type: Type of ML task
        """
        self.source_model = source_model
        self.task_type = task_type
        self.evaluation_results: Dict[str, Any] = {}

    def evaluate_domain_similarity(self,
                                   X_source: np.ndarray,
                                   X_target: np.ndarray) -> float:
        """
        Assess similarity between source and target domains.

        Uses maximum mean discrepancy (MMD) to measure distribution difference.

        Args:
            X_source: Features from source domain
            X_target: Features from target domain

        Returns:
            Similarity score (0-1, higher = more similar)
        """
        # Compute MMD (simplified version)
        def rbf_kernel(X1, X2, gamma=1.0):
            """RBF kernel for MMD computation."""
            from scipy.spatial.distance import cdist
            dists = cdist(X1, X2, metric='sqeuclidean')
            return np.exp(-gamma * dists)

        # Sample for efficiency
        n_samples = min(1000, len(X_source), len(X_target))
        X_source_sample = X_source[np.random.choice(len(X_source), n_samples)]
        X_target_sample = X_target[np.random.choice(len(X_target), n_samples)]

        # Compute kernels
        K_ss = rbf_kernel(X_source_sample, X_source_sample)
        K_tt = rbf_kernel(X_target_sample, X_target_sample)
        K_st = rbf_kernel(X_source_sample, X_target_sample)

        # MMD statistic
        mmd = (K_ss.mean() + K_tt.mean() - 2 * K_st.mean())

        # Convert to similarity (0-1 scale, normalized)
        similarity = np.exp(-mmd)

        self.evaluation_results['domain_similarity'] = similarity
        logger.info(f"Domain similarity: {similarity:.4f}")

        return similarity

    def compare_transfer_vs_scratch(self,
                                   X_train: np.ndarray,
                                   y_train: np.ndarray,
                                   X_val: np.ndarray,
                                   y_val: np.ndarray,
                                   scratch_model_factory: Callable,
                                   sample_sizes: List[int] = [100, 500, 1000]) -> Dict:
        """
        Compare transfer learning vs training from scratch across sample sizes.

        Args:
            X_train, y_train: Target domain training data
            X_val, y_val: Target domain validation data
            scratch_model_factory: Function returning new untrained model
            sample_sizes: Training set sizes to evaluate

        Returns:
            Dict with performance comparison results
        """
        results = {
            'sample_sizes': sample_sizes,
            'transfer_performance': [],
            'scratch_performance': [],
            'performance_gain': []
        }

        for n_samples in sample_sizes:
            # Sample training data
            indices = np.random.choice(len(X_train),
                                      min(n_samples, len(X_train)),
                                      replace=False)
            X_train_sample = X_train[indices]
            y_train_sample = y_train[indices]

            # Transfer learning: fine-tune pre-trained model
            transfer_model = self._finetune_model(
                self.source_model,
                X_train_sample,
                y_train_sample
            )
            transfer_pred = transfer_model.predict(X_val)
            transfer_acc = accuracy_score(y_val, transfer_pred)

            # Training from scratch
            scratch_model = scratch_model_factory()
            scratch_model.fit(X_train_sample, y_train_sample)
            scratch_pred = scratch_model.predict(X_val)
            scratch_acc = accuracy_score(y_val, scratch_pred)

            gain = transfer_acc - scratch_acc

            results['transfer_performance'].append(transfer_acc)
            results['scratch_performance'].append(scratch_acc)
            results['performance_gain'].append(gain)

            logger.info(f"Samples={n_samples}: transfer={transfer_acc:.4f}, "
                       f"scratch={scratch_acc:.4f}, gain={gain:.4f}")

        self.evaluation_results['transfer_comparison'] = results
        return results

    def analyze_fine_tuning_convergence(self,
                                       X_train: np.ndarray,
                                       y_train: np.ndarray,
                                       X_val: np.ndarray,
                                       y_val: np.ndarray,
                                       max_iterations: int = 100) -> Dict:
        """
        Analyze how quickly fine-tuning converges vs training from scratch.

        Returns:
            Dict with convergence analysis
        """
        # This is a simplified example - would need iterative training
        # for true convergence analysis

        transfer_scores = []
        iterations = range(1, max_iterations + 1, 10)

        for n_iter in iterations:
            # Simulate partial training (in practice, use early stopping)
            model_copy = self._copy_model(self.source_model)

            # Partial fit (simplified - actual implementation depends on algorithm)
            if hasattr(model_copy, 'partial_fit'):
                for _ in range(n_iter):
                    model_copy.partial_fit(X_train, y_train)
            else:
                # For models without partial_fit, use full training
                # and measure at different epochs/iterations
                model_copy.fit(X_train, y_train)

            score = accuracy_score(y_val, model_copy.predict(X_val))
            transfer_scores.append(score)

        results = {
            'iterations': list(iterations),
            'scores': transfer_scores,
            'convergence_rate': self._compute_convergence_rate(transfer_scores)
        }

        self.evaluation_results['convergence_analysis'] = results
        return results

    def _finetune_model(self,
                       base_model: BaseEstimator,
                       X: np.ndarray,
                       y: np.ndarray) -> BaseEstimator:
        """Fine-tune pre-trained model on target data."""
        # Create copy to avoid modifying original
        from sklearn.base import clone
        model_copy = clone(base_model)

        # Fine-tune (retrain on target data)
        model_copy.fit(X, y)

        return model_copy

    def _copy_model(self, model: BaseEstimator) -> BaseEstimator:
        """Create copy of model."""
        from sklearn.base import clone
        return clone(model)

    def _compute_convergence_rate(self, scores: List[float]) -> float:
        """Compute convergence rate from score progression."""
        if len(scores) < 2:
            return 0.0

        # Compute average improvement rate
        improvements = [scores[i+1] - scores[i]
                       for i in range(len(scores) - 1)]
        return np.mean([max(0, imp) for imp in improvements])

    def compute_transfer_score(self) -> float:
        """
        Compute overall transfer learning quality score.

        Combines:
        - Domain similarity
        - Performance gain
        - Sample efficiency

        Returns:
            Transfer score (0-1, higher = better transfer)
        """
        if not self.evaluation_results:
            raise ValueError("Must run evaluations first")

        # Weight different factors
        domain_sim = self.evaluation_results.get('domain_similarity', 0.5)

        transfer_comp = self.evaluation_results.get('transfer_comparison', {})
        avg_gain = (np.mean(transfer_comp.get('performance_gain', [0]))
                   if transfer_comp else 0)

        # Normalize gain to 0-1
        gain_score = max(0, min(1, avg_gain + 0.5))

        # Combined score
        transfer_score = 0.4 * domain_sim + 0.6 * gain_score

        logger.info(f"Transfer learning score: {transfer_score:.4f}")
        return transfer_score
\end{lstlisting}

\section{Real-World Scenario: Model Selection for Medical Diagnosis}

\subsection{MedTech's Diabetic Retinopathy Detection System}

MedTech developed an AI system to detect diabetic retinopathy from retinal images. This high-stakes medical application required careful model selection balancing accuracy, interpretability, and operational constraints.

\subsection{Initial Challenge}

The team trained 8 candidate models:

\begin{enumerate}
    \item Logistic Regression (baseline)
    \item Random Forest
    \item XGBoost
    \item LightGBM
    \item ResNet-50 (deep CNN)
    \item EfficientNet-B3
    \item Vision Transformer (ViT)
    \item Ensemble (ResNet + XGBoost)
\end{enumerate}

Initial results showed ViT had highest accuracy (94.2\%), but the team needed systematic selection considering business constraints.

\subsection{Business Constraints}

\begin{itemize}
    \item \textbf{Minimum recall: 95\%} (cannot miss true positives in medical context)
    \item \textbf{Maximum inference time: 500ms} (for clinical workflow integration)
    \item \textbf{Maximum model size: 100MB} (edge device deployment)
    \item \textbf{Interpretability preferred} (for clinical validation)
\end{itemize}

\subsection{Systematic Model Selection Process}

\textbf{Step 1: Cross-Validation with Grouped Strategy}

Using GroupedCVStrategy to prevent patient data leakage (same patient's images only in train OR test), the team found:

\begin{itemize}
    \item ViT: 94.2\% accuracy, but 92\% recall (failed minimum recall constraint)
    \item EfficientNet-B3: 93.8\% accuracy, 96\% recall
    \item Ensemble: 94.5\% accuracy, 97\% recall
\end{itemize}

\textbf{Step 2: Statistical Comparison}

McNemar's test comparing EfficientNet and Ensemble:
\begin{itemize}
    \item p-value = 0.03 (statistically significant difference)
    \item Ensemble significantly better
\end{itemize}

\textbf{Step 3: Complexity Analysis}

\begin{itemize}
    \item EfficientNet-B3: 45MB, 320ms inference, complexity score = 42
    \item Ensemble: 180MB (failed size constraint), 450ms inference
    \item ResNet-50: 98MB, 280ms, complexity score = 58, recall = 95.5\%
\end{itemize}

\textbf{Step 4: Automated Selection}

Using AutomatedModelSelector with constraints, three models passed:
\begin{itemize}
    \item EfficientNet-B3: selection score = 87.3
    \item ResNet-50: selection score = 84.1
    \item XGBoost: selection score = 79.2
\end{itemize}

EfficientNet-B3 selected as best balance.

\subsection{Production Deployment and Monitoring}

After 6 months in production:
\begin{itemize}
    \item PerformanceMonitor detected 6.2\% recall degradation
    \item Investigation revealed distribution shift in imaging equipment
    \item Automated retraining triggered with updated data
    \item Performance restored to 96.1\% recall
\end{itemize}

\subsection{Key Outcomes}

\begin{itemize}
    \item \textbf{Saved 3 months}: Systematic approach vs. trial-and-error
    \item \textbf{Met all constraints}: Business requirements guaranteed
    \item \textbf{FDA approval}: Statistical rigor supported regulatory submission
    \item \textbf{Proactive monitoring}: Degradation detected before clinical impact
\end{itemize}

\subsection{Lessons Learned}

\begin{enumerate}
    \item Highest accuracy $\neq$ best model for deployment
    \item Statistical significance testing prevented premature conclusions
    \item Grouped CV was critical to prevent patient data leakage
    \item Automated monitoring caught degradation 2 weeks before manual review would have
    \item Business constraints must be formalized and validated programmatically
\end{enumerate}

\section{Model Registry Integration}

For production ML systems, a model registry provides versioning, metadata management, and deployment tracking.

\begin{lstlisting}[language=Python, caption={Model Registry for Production Management}]
from typing import List, Optional
import shutil

@dataclass
class ModelRegistryEntry:
    """Entry in model registry."""
    model_id: str
    name: str
    version: str
    algorithm: str
    performance_metrics: Dict[str, float]
    complexity_metrics: Dict[str, float]
    registered_at: datetime
    model_path: Path
    stage: str  # 'development', 'staging', 'production', 'archived'
    tags: List[str]
    description: str

class ModelRegistry:
    """
    Central registry for managing model lifecycle.

    Features:
    - Version tracking
    - Stage management (dev -> staging -> production)
    - Metadata storage
    - Model artifact management
    """

    def __init__(self, registry_path: Path):
        """
        Args:
            registry_path: Base path for registry storage
        """
        self.registry_path = registry_path
        self.registry_path.mkdir(parents=True, exist_ok=True)

        self.metadata_file = self.registry_path / "registry.json"
        self.models_dir = self.registry_path / "models"
        self.models_dir.mkdir(exist_ok=True)

        self.entries: Dict[str, ModelRegistryEntry] = {}
        self._load_registry()

    def register_model(
        self,
        candidate: ModelCandidate,
        stage: str = "development",
        tags: Optional[List[str]] = None,
        description: str = ""
    ) -> str:
        """
        Register a model candidate in the registry.

        Returns:
            model_id: Unique identifier for registered model
        """
        # Generate model ID
        model_id = f"{candidate.name}_{candidate.version}_{candidate.compute_model_hash()}"

        # Create model directory
        model_path = self.models_dir / model_id
        model_path.mkdir(exist_ok=True)

        # Save model
        candidate.save(model_path)

        # Create registry entry
        entry = ModelRegistryEntry(
            model_id=model_id,
            name=candidate.name,
            version=candidate.version,
            algorithm=candidate.algorithm,
            performance_metrics=candidate.performance.to_dict(),
            complexity_metrics={
                "n_parameters": candidate.complexity.n_parameters,
                "inference_time_ms": candidate.complexity.inference_time_ms,
                "model_size_bytes": candidate.complexity.model_size_bytes
            },
            registered_at=datetime.now(),
            model_path=model_path,
            stage=stage,
            tags=tags or [],
            description=description
        )

        self.entries[model_id] = entry
        self._save_registry()

        logger.info(f"Registered model: {model_id} (stage={stage})")
        return model_id

    def transition_stage(self, model_id: str, new_stage: str) -> None:
        """Transition model to new stage."""
        if model_id not in self.entries:
            raise ValueError(f"Model {model_id} not found in registry")

        valid_stages = ['development', 'staging', 'production', 'archived']
        if new_stage not in valid_stages:
            raise ValueError(f"Invalid stage: {new_stage}")

        old_stage = self.entries[model_id].stage
        self.entries[model_id].stage = new_stage
        self._save_registry()

        logger.info(f"Transitioned {model_id}: {old_stage} -> {new_stage}")

    def get_production_model(self, name: str) -> Optional[ModelCandidate]:
        """Get current production model by name."""
        production_entries = [
            e for e in self.entries.values()
            if e.name == name and e.stage == 'production'
        ]

        if not production_entries:
            return None

        # Return most recently registered
        latest_entry = max(production_entries, key=lambda e: e.registered_at)
        return ModelCandidate.load(latest_entry.model_path)

    def list_models(self, stage: Optional[str] = None,
                   tags: Optional[List[str]] = None) -> List[ModelRegistryEntry]:
        """List models, optionally filtered by stage and tags."""
        results = list(self.entries.values())

        if stage:
            results = [e for e in results if e.stage == stage]

        if tags:
            results = [e for e in results if any(t in e.tags for t in tags)]

        return results

    def delete_model(self, model_id: str) -> None:
        """Delete model from registry and remove artifacts."""
        if model_id not in self.entries:
            raise ValueError(f"Model {model_id} not found")

        entry = self.entries[model_id]

        # Cannot delete production models
        if entry.stage == 'production':
            raise ValueError("Cannot delete production model. Archive it first.")

        # Remove artifacts
        if entry.model_path.exists():
            shutil.rmtree(entry.model_path)

        # Remove from registry
        del self.entries[model_id]
        self._save_registry()

        logger.info(f"Deleted model: {model_id}")

    def _load_registry(self) -> None:
        """Load registry from disk."""
        if not self.metadata_file.exists():
            return

        with open(self.metadata_file, 'r') as f:
            data = json.load(f)

        for model_id, entry_data in data.items():
            self.entries[model_id] = ModelRegistryEntry(
                model_id=entry_data["model_id"],
                name=entry_data["name"],
                version=entry_data["version"],
                algorithm=entry_data["algorithm"],
                performance_metrics=entry_data["performance_metrics"],
                complexity_metrics=entry_data["complexity_metrics"],
                registered_at=datetime.fromisoformat(entry_data["registered_at"]),
                model_path=Path(entry_data["model_path"]),
                stage=entry_data["stage"],
                tags=entry_data["tags"],
                description=entry_data["description"]
            )

    def _save_registry(self) -> None:
        """Save registry to disk."""
        data = {}
        for model_id, entry in self.entries.items():
            data[model_id] = {
                "model_id": entry.model_id,
                "name": entry.name,
                "version": entry.version,
                "algorithm": entry.algorithm,
                "performance_metrics": entry.performance_metrics,
                "complexity_metrics": entry.complexity_metrics,
                "registered_at": entry.registered_at.isoformat(),
                "model_path": str(entry.model_path),
                "stage": entry.stage,
                "tags": entry.tags,
                "description": entry.description
            }

        with open(self.metadata_file, 'w') as f:
            json.dump(data, f, indent=2)
\end{lstlisting}

\section{A/B Testing Preparation}

\begin{lstlisting}[language=Python, caption={A/B Testing Framework for Model Comparison}]
@dataclass
class ABTestConfig:
    """Configuration for A/B test."""
    model_a_id: str
    model_b_id: str
    traffic_split: float  # Fraction to model B (0.0-1.0)
    sample_size_per_variant: int
    success_metric: str
    minimum_effect_size: float  # Minimum detectable effect
    alpha: float = 0.05
    power: float = 0.80

class ABTestManager:
    """Manage A/B tests for model comparison in production."""

    def __init__(self, registry: ModelRegistry):
        self.registry = registry

    def setup_ab_test(
        self,
        model_a_id: str,
        model_b_id: str,
        success_metric: str,
        minimum_effect_size: float = 0.05,
        traffic_split: float = 0.5
    ) -> ABTestConfig:
        """
        Set up A/B test configuration.

        Calculates required sample size using power analysis.
        """
        from statsmodels.stats.power import zt_ind_solve_power

        # Calculate required sample size
        effect_size = minimum_effect_size
        sample_size = int(zt_ind_solve_power(
            effect_size=effect_size,
            alpha=0.05,
            power=0.80,
            ratio=1.0,
            alternative='two-sided'
        ))

        config = ABTestConfig(
            model_a_id=model_a_id,
            model_b_id=model_b_id,
            traffic_split=traffic_split,
            sample_size_per_variant=sample_size,
            success_metric=success_metric,
            minimum_effect_size=minimum_effect_size
        )

        logger.info(f"A/B test configured: {model_a_id} vs {model_b_id}")
        logger.info(f"Required sample size per variant: {sample_size}")

        return config

    def analyze_ab_test(
        self,
        config: ABTestConfig,
        results_a: np.ndarray,
        results_b: np.ndarray
    ) -> Dict[str, Any]:
        """
        Analyze A/B test results.

        Args:
            config: Test configuration
            results_a: Metric values for model A
            results_b: Metric values for model B

        Returns:
            Dictionary with test results
        """
        from scipy.stats import ttest_ind

        # Two-sample t-test
        statistic, p_value = ttest_ind(results_a, results_b)

        # Calculate effect size (Cohen's d)
        pooled_std = np.sqrt(
            (np.std(results_a)**2 + np.std(results_b)**2) / 2
        )
        cohens_d = (np.mean(results_b) - np.mean(results_a)) / pooled_std

        # Determine winner
        is_significant = p_value < config.alpha
        winner = None
        if is_significant:
            if np.mean(results_b) > np.mean(results_a):
                winner = config.model_b_id
            else:
                winner = config.model_a_id

        # Calculate confidence intervals
        from scipy import stats
        ci_a = stats.t.interval(
            0.95, len(results_a) - 1,
            loc=np.mean(results_a),
            scale=stats.sem(results_a)
        )
        ci_b = stats.t.interval(
            0.95, len(results_b) - 1,
            loc=np.mean(results_b),
            scale=stats.sem(results_b)
        )

        results = {
            "model_a_mean": np.mean(results_a),
            "model_a_ci": ci_a,
            "model_b_mean": np.mean(results_b),
            "model_b_ci": ci_b,
            "p_value": p_value,
            "is_significant": is_significant,
            "cohens_d": cohens_d,
            "winner": winner,
            "recommendation": self._get_recommendation(
                is_significant, winner, cohens_d, config
            )
        }

        return results

    def _get_recommendation(
        self,
        is_significant: bool,
        winner: Optional[str],
        cohens_d: float,
        config: ABTestConfig
    ) -> str:
        """Generate recommendation based on test results."""
        if not is_significant:
            return "No significant difference. Keep current model."

        if winner == config.model_b_id:
            if abs(cohens_d) >= config.minimum_effect_size:
                return f"Deploy {config.model_b_id}. Significant improvement detected."
            else:
                return "Difference significant but effect size small. Consider operational costs."
        else:
            return f"Keep {config.model_a_id}. New model did not improve performance."
\end{lstlisting}

\section{Statistical Validation Rigor}

Production model selection requires statistical rigor beyond simple cross-validation. We must quantify uncertainty, test significance, and analyze stability.

\subsection{Nested Cross-Validation with Bias-Variance Decomposition}

Standard cross-validation for hyperparameter tuning can overestimate performance. Nested CV provides unbiased estimates and enables bias-variance analysis.

\begin{lstlisting}[language=Python, caption={Nested Cross-Validation Framework}]
from sklearn.model_selection import KFold, GridSearchCV
from typing import Dict, List, Tuple
import numpy as np

class NestedCrossValidator:
    """
    Nested cross-validation for unbiased model evaluation.

    Outer loop: Estimates generalization performance
    Inner loop: Hyperparameter tuning
    """

    def __init__(self,
                 outer_cv: int = 5,
                 inner_cv: int = 3,
                 random_state: int = 42):
        """
        Args:
            outer_cv: Number of outer CV folds (for performance estimation)
            inner_cv: Number of inner CV folds (for hyperparameter tuning)
            random_state: Random seed for reproducibility
        """
        self.outer_cv = KFold(n_splits=outer_cv, shuffle=True,
                             random_state=random_state)
        self.inner_cv = inner_cv
        self.outer_scores: List[float] = []
        self.inner_scores: List[List[float]] = []
        self.best_params_per_fold: List[Dict] = []

    def evaluate(self,
                model,
                param_grid: Dict,
                X: np.ndarray,
                y: np.ndarray,
                scoring: str = 'accuracy') -> Dict[str, float]:
        """
        Perform nested cross-validation.

        Returns:
            Dictionary with performance statistics
        """
        self.outer_scores = []
        self.inner_scores = []
        self.best_params_per_fold = []

        for fold_idx, (train_idx, test_idx) in enumerate(self.outer_cv.split(X)):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # Inner loop: Hyperparameter tuning
            grid_search = GridSearchCV(
                model,
                param_grid,
                cv=self.inner_cv,
                scoring=scoring,
                n_jobs=-1
            )
            grid_search.fit(X_train, y_train)

            # Store inner CV scores
            self.inner_scores.append(grid_search.cv_results_['mean_test_score'].tolist())
            self.best_params_per_fold.append(grid_search.best_params_)

            # Outer loop: Evaluate on held-out test set
            test_score = grid_search.score(X_test, y_test)
            self.outer_scores.append(test_score)

            logger.info(f"Fold {fold_idx+1}/{self.outer_cv.n_splits}: "
                       f"test_score={test_score:.4f}, "
                       f"best_params={grid_search.best_params_}")

        return self.get_statistics()

    def get_statistics(self) -> Dict[str, float]:
        """Calculate performance statistics."""
        outer_scores_array = np.array(self.outer_scores)

        return {
            'mean_score': outer_scores_array.mean(),
            'std_score': outer_scores_array.std(),
            'min_score': outer_scores_array.min(),
            'max_score': outer_scores_array.max(),
            'score_range': outer_scores_array.max() - outer_scores_array.min(),
            'cv_coefficient': outer_scores_array.std() / outer_scores_array.mean()  # Relative stability
        }

    def bias_variance_decomposition(self) -> Dict[str, float]:
        """
        Estimate bias-variance trade-off.

        Note: This is a simplified approximation.
        True bias-variance requires knowing optimal predictor.
        """
        outer_mean = np.mean(self.outer_scores)
        outer_var = np.var(self.outer_scores)

        # Average variance across inner CV folds
        inner_variances = [np.var(scores) for scores in self.inner_scores]
        avg_inner_var = np.mean(inner_variances)

        return {
            'estimated_bias': 1.0 - outer_mean,  # Simplified: 1 - performance
            'estimated_variance': outer_var,
            'inner_variance': avg_inner_var,
            'stability_score': 1.0 / (1.0 + outer_var)  # Higher is more stable
        }

\subsection{Learning Curve Analysis}

Learning curves reveal how model performance scales with training data size, helping determine if more data would help.

class LearningCurveAnalyzer:
    """
    Analyze learning curves to assess sample efficiency and convergence.
    """

    def __init__(self,
                 train_sizes: np.ndarray = None,
                 cv: int = 5):
        """
        Args:
            train_sizes: Training set sizes to evaluate (fractions or absolute)
            cv: Number of cross-validation folds
        """
        if train_sizes is None:
            train_sizes = np.linspace(0.1, 1.0, 10)
        self.train_sizes = train_sizes
        self.cv = cv
        self.train_scores: Optional[np.ndarray] = None
        self.val_scores: Optional[np.ndarray] = None

    def analyze(self,
               model,
               X: np.ndarray,
               y: np.ndarray,
               scoring: str = 'accuracy') -> Dict[str, Any]:
        """
        Generate and analyze learning curves.

        Returns:
            Analysis results including convergence assessment
        """
        from sklearn.model_selection import learning_curve

        train_sizes_abs, train_scores, val_scores = learning_curve(
            model, X, y,
            train_sizes=self.train_sizes,
            cv=self.cv,
            scoring=scoring,
            n_jobs=-1,
            shuffle=True,
            random_state=42
        )

        self.train_scores = train_scores
        self.val_scores = val_scores

        # Calculate statistics
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        # Assess convergence
        convergence = self._assess_convergence(val_mean)

        # Calculate sample efficiency
        efficiency = self._calculate_sample_efficiency(train_sizes_abs, val_mean)

        return {
            'train_sizes': train_sizes_abs,
            'train_mean': train_mean,
            'train_std': train_std,
            'val_mean': val_mean,
            'val_std': val_std,
            'has_converged': convergence['has_converged'],
            'convergence_point': convergence['convergence_point'],
            'sample_efficiency': efficiency,
            'recommendation': self._generate_recommendation(convergence, efficiency)
        }

    def _assess_convergence(self, val_scores: np.ndarray,
                           threshold: float = 0.01) -> Dict[str, Any]:
        """Check if learning curve has converged."""
        if len(val_scores) < 3:
            return {'has_converged': False, 'convergence_point': None}

        # Check if slope of last few points is near zero
        recent_scores = val_scores[-3:]
        slope = (recent_scores[-1] - recent_scores[0]) / len(recent_scores)

        has_converged = abs(slope) < threshold

        # Find convergence point (where improvement drops below threshold)
        convergence_point = None
        for i in range(1, len(val_scores)):
            improvement = val_scores[i] - val_scores[i-1]
            if abs(improvement) < threshold:
                convergence_point = i
                break

        return {
            'has_converged': has_converged,
            'convergence_point': convergence_point,
            'final_slope': slope
        }

    def _calculate_sample_efficiency(self,
                                    train_sizes: np.ndarray,
                                    val_scores: np.ndarray) -> float:
        """
        Calculate sample efficiency.

        Higher is better - means model learns quickly from few samples.
        """
        # Area under learning curve (normalized)
        if len(train_sizes) < 2:
            return 0.0

        # Normalize train sizes to [0, 1]
        sizes_norm = (train_sizes - train_sizes.min()) / \
                    (train_sizes.max() - train_sizes.min())

        # Calculate area using trapezoidal rule
        area = np.trapz(val_scores, sizes_norm)

        return area

    def _generate_recommendation(self,
                                convergence: Dict,
                                efficiency: float) -> str:
        """Generate actionable recommendation."""
        if convergence['has_converged']:
            if efficiency > 0.8:
                return "Model has converged with good sample efficiency. Current data size is sufficient."
            else:
                return "Model has converged but sample efficiency is low. Consider model complexity or feature engineering."
        else:
            return "Model has not converged. Collecting more data would likely improve performance."

    def visualize(self, output_path: Optional[str] = None):
        """Plot learning curves."""
        if self.train_scores is None:
            raise ValueError("Must run analyze() first")

        train_mean = self.train_scores.mean(axis=1)
        train_std = self.train_scores.std(axis=1)
        val_mean = self.val_scores.mean(axis=1)
        val_std = self.val_scores.std(axis=1)

        plt.figure(figsize=(10, 6))

        plt.plot(self.train_sizes, train_mean, label='Training Score',
                marker='o', color='blue')
        plt.fill_between(self.train_sizes,
                        train_mean - train_std,
                        train_mean + train_std,
                        alpha=0.2, color='blue')

        plt.plot(self.train_sizes, val_mean, label='Validation Score',
                marker='o', color='red')
        plt.fill_between(self.train_sizes,
                        val_mean - val_std,
                        val_mean + val_std,
                        alpha=0.2, color='red')

        plt.xlabel('Training Set Size', fontsize=12)
        plt.ylabel('Score', fontsize=12)
        plt.title('Learning Curves', fontsize=14, fontweight='bold')
        plt.legend(loc='best')
        plt.grid(True, alpha=0.3)

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')

        plt.close()
\end{lstlisting}

\section{Business-Aware Model Selection}

Production model selection must account for business realities: deployment costs, interpretability requirements, and regulatory constraints.

\subsection{Cost-Sensitive Learning with Business Loss Functions}

Different errors have different costs. A fraud detection false negative might cost thousands while a false positive costs manual review time.

\begin{lstlisting}[language=Python, caption={Cost-Sensitive Model Selection}]
@dataclass
class BusinessLossFunction:
    """Define business costs for different prediction errors."""
    true_positive_value: float = 0.0  # Revenue/saving from correct positive
    true_negative_value: float = 0.0  # Value from correct negative
    false_positive_cost: float = 0.0  # Cost of false alarm
    false_negative_cost: float = 0.0  # Cost of missing positive

    def calculate_total_cost(self,
                            y_true: np.ndarray,
                            y_pred: np.ndarray) -> float:
        """Calculate total business cost/value."""
        from sklearn.metrics import confusion_matrix

        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

        total_value = (
            tp * self.true_positive_value +
            tn * self.true_negative_value -
            fp * self.false_positive_cost -
            fn * self.false_negative_cost
        )

        return total_value

    def get_expected_value_per_prediction(self,
                                         y_true: np.ndarray,
                                         y_pred: np.ndarray) -> float:
        """Average business value per prediction."""
        total = self.calculate_total_cost(y_true, y_pred)
        return total / len(y_true)

class CostSensitiveModelSelector:
    """
    Select models based on business value rather than just accuracy.
    """

    def __init__(self, loss_function: BusinessLossFunction):
        self.loss_function = loss_function

    def evaluate_candidate(self,
                          candidate: ModelCandidate,
                          X_test: np.ndarray,
                          y_test: np.ndarray) -> Dict[str, float]:
        """
        Evaluate model using business loss function.

        Returns metrics including business value.
        """
        y_pred = candidate.model.predict(X_test)

        # Standard metrics
        from sklearn.metrics import accuracy_score, precision_score, recall_score

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)

        # Business metrics
        total_value = self.loss_function.calculate_total_cost(y_test, y_pred)
        value_per_prediction = self.loss_function.get_expected_value_per_prediction(
            y_test, y_pred
        )

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'total_business_value': total_value,
            'value_per_prediction': value_per_prediction
        }

    def compare_candidates(self,
                          candidates: List[ModelCandidate],
                          X_test: np.ndarray,
                          y_test: np.ndarray) -> pd.DataFrame:
        """
        Compare candidates on both ML and business metrics.

        Returns ranked comparison table.
        """
        results = []

        for candidate in candidates:
            metrics = self.evaluate_candidate(candidate, X_test, y_test)
            metrics['model_name'] = candidate.name
            metrics['inference_time_ms'] = candidate.complexity.inference_time_ms
            results.append(metrics)

        df = pd.DataFrame(results)
        df = df.sort_values('total_business_value', ascending=False)

        return df

    def select_optimal_threshold(self,
                                candidate: ModelCandidate,
                                X_test: np.ndarray,
                                y_test: np.ndarray,
                                thresholds: np.ndarray = None) -> Tuple[float, Dict]:
        """
        Find optimal classification threshold for business value.

        Args:
            candidate: Model candidate with predict_proba
            X_test: Test features
            y_test: Test labels
            thresholds: Thresholds to test (default: 0.1 to 0.9)

        Returns:
            (optimal_threshold, metrics_at_threshold)
        """
        if thresholds is None:
            thresholds = np.arange(0.1, 0.95, 0.05)

        # Get prediction probabilities
        y_proba = candidate.model.predict_proba(X_test)[:, 1]

        best_value = float('-inf')
        best_threshold = 0.5
        best_metrics = {}

        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)

            business_value = self.loss_function.calculate_total_cost(y_test, y_pred)

            if business_value > best_value:
                best_value = business_value
                best_threshold = threshold
                best_metrics = self.evaluate_candidate(
                    candidate, X_test, y_test
                )

        logger.info(f"Optimal threshold: {best_threshold:.3f} "
                   f"with business value: ${best_value:.2f}")

        return best_threshold, best_metrics
\end{lstlisting}

\section{Advanced Scenarios: Model Selection Challenges}

Real-world model selection faces challenges beyond technical metrics. These scenarios illustrate common pitfalls and solutions.

\subsection{Scenario 1: The Accuracy Trap}

\textbf{Situation}: An e-commerce company built a product recommendation model. The deep learning ensemble achieved 94\% accuracy vs. 91\% for a simple collaborative filtering approach. The team deployed the complex model.

\textbf{Problem}: In production, the deep learning model:
\begin{itemize}
    \item Required 300ms inference time (vs. 15ms for simple model)
    \item Used 8GB RAM per instance (vs. 500MB)
    \item Needed GPU instances costing \$2,000/month (vs. \$200)
    \item Was impossible to debug when making poor recommendations
\end{itemize}

The 3\% accuracy gain generated \$500/month in additional revenue, while infrastructure costs increased by \$1,800/month.

\textbf{Lesson}: Always calculate total cost of ownership. A 3\% accuracy improvement rarely justifies 10x infrastructure costs. The team should have used the multi-objective optimization framework to balance accuracy, latency, and operational cost.

\textbf{Resolution}: Switched to the simpler model and invested engineering time in better feature engineering, achieving 92.5\% accuracy at 20ms latency and 1/10th the cost.

\subsection{Scenario 2: The Interpretability Mandate}

\textbf{Situation}: A lending institution built a credit risk model. XGBoost achieved 88\% AUC while logistic regression achieved 82\% AUC.

\textbf{Problem}: Regulators required the institution to explain all loan denials to applicants. The XGBoost model provided SHAP values, but:
\begin{itemize}
    \item Explanations varied significantly for similar applicants
    \item SHAP values didn't satisfy "clearly understandable to non-technical consumers" regulatory requirement
    \item Legal team couldn't defend model decisions in disputes
    \item Audit trails were complex and costly to maintain
\end{itemize}

\textbf{Lesson}: In regulated industries, interpretability is not optional. A 6\% performance gain means nothing if you can't deploy the model legally.

\textbf{Resolution}: Deployed the logistic regression model with clear coefficient interpretations. Each feature's impact could be explained in plain language: "Your income-to-debt ratio of 45\% is above our 40\% threshold, contributing -0.3 to your risk score."

\subsection{Scenario 3: The Resource Reality Check}

\textbf{Situation}: A startup built a computer vision model for manufacturing defect detection. Their EfficientNet model achieved 96\% accuracy on edge cases versus 93\% for MobileNet.

\textbf{Problem}: The model needed to run on edge devices in factories:
\begin{itemize}
    \item EfficientNet required 250ms inference on edge hardware (unacceptable for real-time)
    \item Model size was 180MB (exceeding device storage budget)
    \item Power consumption was too high for battery operation
    \item Device couldn't run CUDA, falling back to slow CPU inference
\end{itemize}

\textbf{Lesson}: Deployment constraints must be tested early. Lab benchmarks on GPUs don't predict edge device performance.

\textbf{Resolution}: Optimized MobileNet with quantization, achieving 94\% accuracy at 45ms inference time. The 2\% accuracy loss was acceptable given 5x speedup enabled real-time operation.

\subsection{Scenario 4: The Fairness Trade-off}

\textbf{Situation}: A hiring screening tool used deep learning to rank candidates. The model achieved 75\% precision in predicting successful hires versus 68\% for a simpler model.

\textbf{Problem}: Fairness audit revealed:
\begin{itemize}
    \item Model had 15\% lower recall for female candidates
    \item Bias stemmed from historical hiring patterns in training data
    \item SHAP analysis showed years of experience weighted heavily (correlating with gender)
    \item Legal risk and PR damage potential far exceeded hiring efficiency gains
\end{itemize}

\textbf{Lesson}: Accuracy without fairness creates existential business risk. Model selection must include bias testing.

\textbf{Resolution}: Implemented fairness-constrained model selection:
\begin{itemize}
    \item Added demographic parity constraint (max 5\% difference in selection rates)
    \item Re-trained models with fairness penalty in loss function
    \item Final model: 71\% precision with <2\% demographic disparity
    \item Accepted 4\% precision loss to mitigate legal and ethical risks
\end{itemize}

\subsection{Scenario 5: The Concept Drift Surprise}

\textbf{Situation}: A fraud detection system selected the best model using standard 80/20 train/test split from historical data. The model performed excellently in backtesting.

\textbf{Problem}: After deployment:
\begin{itemize}
    \item Precision dropped from 85\% to 62\% within 3 months
    \item Fraudsters adapted to detection patterns
    \item Seasonal patterns in legitimate transactions weren't captured in random split
    \item Model was trained on all historical data, masking temporal degradation
\end{itemize}

\textbf{Lesson}: Static validation doesn't reveal temporal dynamics. For time-dependent problems, use time-series cross-validation and test on future time periods.

\textbf{Resolution}: Implemented temporal validation strategy:
\begin{itemize}
    \item Time-series cross-validation with walk-forward analysis
    \item Required models to perform well on data from 3+ months in future
    \item Set up monthly model retraining pipeline
    \item Added drift monitoring with automatic performance degradation alerts
    \item Selected more adaptable model (online learning) over highest accuracy static model
\end{itemize}

\section{Exercises}

\subsection{Exercise 1: Building Model Candidates (Easy)}

Create ModelCandidate instances for three different algorithms (Logistic Regression, Random Forest, XGBoost) on a binary classification dataset. Compare their performance metrics and complexity scores.

\subsection{Exercise 2: Cross-Validation Strategies (Easy)}

Implement and compare StandardCVStrategy, StratifiedCVStrategy, and TimeSeriesCVStrategy on appropriate datasets. Visualize how each strategy splits the data.

\subsection{Exercise 3: Statistical Model Comparison (Medium)}

Generate synthetic cross-validation scores for 5 models with varying levels of overlap. Use paired t-test, McNemar's test, and permutation test to compare them. Identify which models have statistically significant differences.

\subsection{Exercise 4: Complexity-Performance Trade-off (Medium)}

Create 10 model candidates with varying complexity levels. Plot the Pareto frontier and identify optimal models. Implement a custom scoring function that balances performance and simplicity for your specific use case.

\subsection{Exercise 5: Automated Model Selection with Constraints (Medium)}

Define business constraints for a real-world application (e.g., fraud detection with maximum 50ms inference time, minimum 90\% recall). Train multiple models and use AutomatedModelSelector to find the best candidate that meets all constraints.

\subsection{Exercise 6: Performance Degradation Simulation (Advanced)}

Simulate model performance degradation over time by:
\begin{enumerate}
    \item Starting with baseline performance
    \item Gradually introducing distribution shift
    \item Using PerformanceMonitor to detect degradation
    \item Triggering automated retraining at appropriate thresholds
\end{enumerate}

Create visualizations showing performance trends and alert history.

\subsection{Exercise 7: End-to-End Model Development Pipeline (Advanced)}

Build a complete model development pipeline:

\begin{enumerate}
    \item Train 5-8 diverse model candidates
    \item Apply appropriate cross-validation strategy
    \item Perform statistical comparisons
    \item Analyze complexity trade-offs
    \item Apply business constraints
    \item Select best model automatically
    \item Register in model registry
    \item Set up A/B test configuration
    \item Deploy with performance monitoring
\end{enumerate}

Document all decisions and create a comprehensive report suitable for stakeholders.

\section{Summary}

This chapter presented a systematic framework for model development and selection:

\begin{itemize}
    \item \textbf{Model Candidate Framework}: Comprehensive representation with performance and complexity metrics, versioning, and metadata tracking
    \item \textbf{Cross-Validation Strategies}: Specialized approaches for standard, stratified, time series, and grouped data to prevent leakage
    \item \textbf{Statistical Comparison}: Rigorous testing with paired t-tests, McNemar's test, and permutation tests for significance
    \item \textbf{Complexity Analysis}: Pareto frontier identification and efficiency scoring balancing performance with operational cost
    \item \textbf{Automated Selection}: Business constraint-aware selection with configurable weighting of performance, simplicity, and efficiency
    \item \textbf{Performance Monitoring}: Degradation detection with automated retraining triggers and alerting
    \item \textbf{Model Registry}: Production-ready versioning, stage management, and artifact tracking
    \item \textbf{A/B Testing}: Statistical framework for production model comparison with power analysis
\end{itemize}

Systematic model selection transforms ML development from trial-and-error into an engineering discipline. By formalizing business constraints, applying statistical rigor, and monitoring production performance, teams can confidently deploy models that deliver sustained business value.
