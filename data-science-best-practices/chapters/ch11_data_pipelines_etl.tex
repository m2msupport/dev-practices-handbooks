\chapter{Data Pipeline Engineering: Event-Driven Architectures}

\section{Introduction}

Modern enterprise data systems process billions of events daily—user clicks, transaction completions, sensor readings, service logs. Traditional batch ETL pipelines, running hourly or daily, cannot meet the latency requirements of real-time ML applications. A fraud detection model that receives transaction features 6 hours after the purchase is worthless. A recommendation system that updates daily loses revenue to competitors updating in real-time. The difference between batch and event-driven architectures is the difference between reactive and proactive business intelligence.

Event-driven data pipelines are the nervous system of real-time ML infrastructure. They transform discrete business events into continuous feature streams, enabling models to make predictions on fresh data with sub-second latency. However, building reliable event-driven systems is fundamentally different from batch processing. Out-of-order events, network partitions, schema evolution, and exactly-once semantics introduce complexity that trivializes batch ETL challenges.

\subsection{The Real-Time Revenue Dashboard Problem}

Consider an e-commerce platform tracking real-time revenue metrics across 15 geographic regions, 50 product categories, and 5 customer segments (750 aggregations). Business stakeholders demand dashboards updating every 30 seconds, showing:

\begin{itemize}
    \item Current hour revenue vs. same hour last week
    \item Top 10 products by revenue in last 15 minutes
    \item Customer lifetime value trends (real-time)
    \item Conversion funnel metrics with <1 minute latency
    \item Anomaly detection on revenue drops >10\% in any segment
\end{itemize}

A traditional approach might use:
\begin{itemize}
    \item Batch ETL job running every 5 minutes, querying transactional database
    \item Computing all 750 aggregations from scratch each run
    \item Writing results to analytics database
    \item Dashboard polling database every 30 seconds
\end{itemize}

\textbf{Problems with this approach:}
\begin{itemize}
    \item \textbf{Query Load}: 12 full-table scans per hour devastate database performance
    \item \textbf{Latency}: 5-minute batches + processing time = 6-8 minute lag minimum
    \item \textbf{Cost}: Computing 750 aggregations from scratch every 5 minutes wastes 95\%+ compute
    \item \textbf{Scalability}: Query time grows linearly with transaction volume
    \item \textbf{Reliability}: Database connection failures cascade to all downstream systems
\end{itemize}

\textbf{Event-driven solution:}
\begin{itemize}
    \item Transaction events published to Kafka immediately after completion
    \item Stream processor maintains rolling aggregations in memory windows
    \item Only incremental updates computed (new events added, expired events removed)
    \item Results published to materialized views for dashboard queries
    \item Latency: <500ms from transaction to dashboard update
    \item Cost: 10x reduction in compute vs. batch approach
\end{itemize}

This chapter provides the engineering frameworks to build such systems.

\subsection{Why Event-Driven Pipelines Matter}

Event-driven architectures provide fundamental advantages for ML systems:

\begin{itemize}
    \item \textbf{Latency Reduction}: Process events as they occur rather than waiting for batch windows
    \item \textbf{Resource Efficiency}: Incremental processing avoids redundant computation
    \item \textbf{Decoupling}: Producers and consumers operate independently via message queues
    \item \textbf{Scalability}: Horizontal scaling through partitioning and parallel processing
    \item \textbf{Resilience}: Message persistence enables recovery from failures
    \item \textbf{Real-Time ML}: Features computed on-demand for immediate predictions
\end{itemize}

However, event-driven systems introduce new failure modes:
\begin{itemize}
    \item \textbf{Out-of-Order Events}: Network delays cause events to arrive non-chronologically
    \item \textbf{Duplicate Events}: Retry logic can produce multiple copies of same event
    \item \textbf{Schema Evolution}: Event format changes require backward compatibility
    \item \textbf{Exactly-Once Semantics}: Ensuring each event processed once is non-trivial
    \item \textbf{State Management}: Maintaining accurate state across partitions and restarts
\end{itemize}

\subsection{The Cost of Poor Event Pipeline Design}

Industry data shows:
\begin{itemize}
    \item \textbf{70\% of event-driven systems} experience data loss or duplication in first 6 months
    \item \textbf{Out-of-order processing} causes 40\% of streaming ML feature bugs
    \item \textbf{Schema evolution failures} require emergency rollbacks in 25\% of deployments
    \item \textbf{Partition hotspots} cause 60\% degradation in throughput
    \item \textbf{State management errors} cost \$500K+ annually in data quality incidents
\end{itemize}

\subsection{Chapter Overview}

This chapter provides production-grade event-driven pipeline frameworks:

\begin{enumerate}
    \item \textbf{Event-Driven Architecture Fundamentals}: Publish-subscribe patterns and message brokers
    \item \textbf{Apache Kafka Integration}: Production deployment with partitioning and replication
    \item \textbf{Stream Processing}: Real-time feature computation with windowing
    \item \textbf{Message Ordering and Partitioning}: Strategies for scalability and correctness
    \item \textbf{Event Schema Management}: Evolution strategies with backward compatibility
    \item \textbf{Exactly-Once Semantics}: Idempotency and deduplication patterns
    \item \textbf{State Management}: Maintaining consistency across partitions
    \item \textbf{Monitoring and Observability}: Tracking lag, throughput, and data quality
\end{enumerate}

\section{Event-Driven Architecture Fundamentals}

Event-driven architectures decouple data producers from consumers through asynchronous message passing.

\subsection{Core Concepts}

\subsubsection{Events}

An \textbf{event} represents a discrete occurrence in a system:
\begin{itemize}
    \item \textbf{Fact-Based}: Events describe what happened, not commands to execute
    \item \textbf{Immutable}: Once created, events never change
    \item \textbf{Timestamped}: Include occurrence time for ordering and windowing
    \item \textbf{Self-Contained}: Carry all necessary context for processing
\end{itemize}

\textbf{Example events:}
\begin{itemize}
    \item UserRegistered(user\_id, email, timestamp, source)
    \item PurchaseCompleted(order\_id, user\_id, amount, items, timestamp)
    \item ModelPredictionMade(model\_id, input\_features, prediction, confidence, timestamp)
\end{itemize}

\subsubsection{Event Streams}

An \textbf{event stream} is an unbounded sequence of events:
\begin{itemize}
    \item \textbf{Ordered}: Events have logical ordering (by timestamp or sequence number)
    \item \textbf{Append-Only}: New events added to end, old events never modified
    \item \textbf{Durable}: Events persist beyond processing for replay
    \item \textbf{Partitioned}: Stream divided into parallel partitions for scale
\end{itemize}

\subsubsection{Publish-Subscribe Pattern}

\textbf{Producers} publish events to topics without knowing consumers:
\begin{itemize}
    \item Decoupling: Producers and consumers evolve independently
    \item Fan-out: Multiple consumers process same event stream
    \item Buffering: Message queue absorbs traffic spikes
\end{itemize}

\textbf{Consumers} subscribe to topics and process events:
\begin{itemize}
    \item Independent: Each consumer maintains own processing state
    \item Scalable: Multiple consumer instances process partitions in parallel
    \item Resilient: Failed consumers resume from last checkpoint
\end{itemize}

\subsection{Apache Kafka: Production Message Broker}

Apache Kafka is the industry-standard distributed message broker for event-driven systems.

\subsubsection{Kafka Architecture}

\textbf{Core components:}
\begin{itemize}
    \item \textbf{Topics}: Logical channels for event streams (e.g., "user-events", "transactions")
    \item \textbf{Partitions}: Topics split into ordered, immutable partitions for parallelism
    \item \textbf{Brokers}: Servers that store and serve partition data
    \item \textbf{Producers}: Applications that publish events to topics
    \item \textbf{Consumers}: Applications that subscribe to topics and process events
    \item \textbf{Consumer Groups}: Coordinated set of consumers for parallel processing
\end{itemize}

\textbf{Kafka guarantees:}
\begin{itemize}
    \item Messages within a partition are strictly ordered
    \item Messages persist for configurable retention period (days/weeks)
    \item At-least-once delivery (with producer retries)
    \item Exactly-once delivery (with idempotent producers and transactional consumers)
\end{itemize}

\subsection{EventDrivenPipeline: Production Framework}

\begin{lstlisting}[language=Python, caption={Event-Driven Pipeline Base Class}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
from enum import Enum
import logging
import json
import traceback

logger = logging.getLogger(__name__)

class EventType(Enum):
    """Standard event types for data pipelines."""
    DATA_INGESTED = "data.ingested"
    DATA_TRANSFORMED = "data.transformed"
    DATA_VALIDATED = "data.validated"
    FEATURE_COMPUTED = "feature.computed"
    MODEL_PREDICTION = "model.prediction"
    PIPELINE_FAILED = "pipeline.failed"
    SCHEMA_EVOLVED = "schema.evolved"

@dataclass
class Event:
    """
    Immutable event representing pipeline occurrence.

    Attributes:
        event_id: Unique event identifier
        event_type: Type of event
        timestamp: When event occurred (UTC)
        source: Service/component that generated event
        data: Event payload
        metadata: Additional context (trace_id, user_id, etc.)
        schema_version: Event schema version for evolution
    """
    event_id: str
    event_type: EventType
    timestamp: datetime
    source: str
    data: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)
    schema_version: str = "1.0"

    def to_dict(self) -> Dict[str, Any]:
        """Serialize event to dictionary for Kafka."""
        return {
            'event_id': self.event_id,
            'event_type': self.event_type.value,
            'timestamp': self.timestamp.isoformat(),
            'source': self.source,
            'data': self.data,
            'metadata': self.metadata,
            'schema_version': self.schema_version
        }

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> 'Event':
        """Deserialize event from dictionary."""
        return cls(
            event_id=d['event_id'],
            event_type=EventType(d['event_type']),
            timestamp=datetime.fromisoformat(d['timestamp']),
            source=d['source'],
            data=d['data'],
            metadata=d.get('metadata', {}),
            schema_version=d.get('schema_version', '1.0')
        )

class EventHandler(ABC):
    """
    Abstract base class for event handlers.

    Handlers process events and optionally emit new events.
    """

    @abstractmethod
    def handle(self, event: Event) -> Optional[List[Event]]:
        """
        Process event and optionally emit downstream events.

        Args:
            event: Event to process

        Returns:
            List of events to emit, or None
        """
        pass

    @abstractmethod
    def can_handle(self, event: Event) -> bool:
        """
        Check if handler can process this event type.

        Args:
            event: Event to check

        Returns:
            True if handler supports this event
        """
        pass

class EventDrivenPipeline:
    """
    Event-driven data pipeline with Kafka integration.

    Consumes events from input topic, routes to handlers,
    publishes output events to downstream topics.

    Example:
        >>> pipeline = EventDrivenPipeline(
        ...     name="feature_pipeline",
        ...     input_topic="raw_events",
        ...     output_topic="features"
        ... )
        >>> pipeline.register_handler(FeatureComputeHandler())
        >>> pipeline.start()
    """

    def __init__(
        self,
        name: str,
        input_topic: str,
        output_topic: str,
        kafka_config: Optional[Dict[str, Any]] = None,
        enable_dlq: bool = True,
        max_retries: int = 3
    ):
        """
        Initialize event-driven pipeline.

        Args:
            name: Pipeline identifier
            input_topic: Kafka topic to consume from
            output_topic: Kafka topic to produce to
            kafka_config: Kafka connection configuration
            enable_dlq: Enable dead-letter queue for failed events
            max_retries: Maximum retry attempts per event
        """
        self.name = name
        self.input_topic = input_topic
        self.output_topic = output_topic
        self.kafka_config = kafka_config or self._default_kafka_config()
        self.enable_dlq = enable_dlq
        self.max_retries = max_retries

        # Event handlers
        self.handlers: List[EventHandler] = []

        # Metrics
        self.events_processed = 0
        self.events_failed = 0
        self.events_retried = 0

        # State
        self.running = False

        logger.info(
            f"Initialized EventDrivenPipeline: {name} "
            f"({input_topic} -> {output_topic})"
        )

    def _default_kafka_config(self) -> Dict[str, Any]:
        """Get default Kafka configuration."""
        return {
            'bootstrap_servers': ['localhost:9092'],
            'group_id': f"{self.name}_consumer_group",
            'auto_offset_reset': 'latest',
            'enable_auto_commit': False,  # Manual commit for exactly-once
            'max_poll_records': 500,
            'session_timeout_ms': 30000
        }

    def register_handler(self, handler: EventHandler):
        """
        Register event handler.

        Args:
            handler: EventHandler instance
        """
        self.handlers.append(handler)
        logger.info(f"Registered handler: {handler.__class__.__name__}")

    def start(self):
        """Start consuming and processing events."""
        from kafka import KafkaConsumer, KafkaProducer

        self.running = True

        # Initialize Kafka consumer
        consumer = KafkaConsumer(
            self.input_topic,
            bootstrap_servers=self.kafka_config['bootstrap_servers'],
            group_id=self.kafka_config['group_id'],
            auto_offset_reset=self.kafka_config['auto_offset_reset'],
            enable_auto_commit=self.kafka_config['enable_auto_commit'],
            max_poll_records=self.kafka_config['max_poll_records'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

        # Initialize Kafka producer
        producer = KafkaProducer(
            bootstrap_servers=self.kafka_config['bootstrap_servers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # Wait for all replicas
            retries=3
        )

        # Dead-letter queue producer
        dlq_producer = None
        if self.enable_dlq:
            dlq_producer = KafkaProducer(
                bootstrap_servers=self.kafka_config['bootstrap_servers'],
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )

        logger.info(f"Starting event processing: {self.input_topic}")

        try:
            for message in consumer:
                if not self.running:
                    break

                try:
                    # Deserialize event
                    event_dict = message.value
                    event = Event.from_dict(event_dict)

                    # Process event
                    output_events = self._process_event(event)

                    # Publish output events
                    if output_events:
                        for output_event in output_events:
                            producer.send(
                                self.output_topic,
                                value=output_event.to_dict()
                            )

                    # Commit offset (exactly-once)
                    consumer.commit()

                    self.events_processed += 1

                    if self.events_processed % 100 == 0:
                        logger.info(
                            f"Processed {self.events_processed} events, "
                            f"failed: {self.events_failed}"
                        )

                except Exception as e:
                    logger.error(f"Error processing event: {e}")
                    logger.error(traceback.format_exc())

                    self.events_failed += 1

                    # Send to dead-letter queue
                    if dlq_producer:
                        dlq_event = {
                            'original_event': message.value,
                            'error': str(e),
                            'traceback': traceback.format_exc(),
                            'timestamp': datetime.now().isoformat()
                        }
                        dlq_producer.send(
                            f"{self.input_topic}_dlq",
                            value=dlq_event
                        )

                    # Still commit offset to avoid reprocessing
                    consumer.commit()

        except KeyboardInterrupt:
            logger.info("Shutting down pipeline")

        finally:
            self.running = False
            consumer.close()
            producer.close()
            if dlq_producer:
                dlq_producer.close()

    def _process_event(self, event: Event) -> Optional[List[Event]]:
        """
        Route event to appropriate handler.

        Args:
            event: Event to process

        Returns:
            List of output events
        """
        output_events = []

        for handler in self.handlers:
            if handler.can_handle(event):
                try:
                    result = handler.handle(event)
                    if result:
                        output_events.extend(result)
                except Exception as e:
                    logger.error(
                        f"Handler {handler.__class__.__name__} failed: {e}"
                    )
                    raise

        return output_events if output_events else None

    def stop(self):
        """Stop pipeline processing."""
        self.running = False
        logger.info("Pipeline stopped")

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get pipeline metrics.

        Returns:
            Dictionary of metrics
        """
        return {
            'events_processed': self.events_processed,
            'events_failed': self.events_failed,
            'events_retried': self.events_retried,
            'success_rate': (
                (self.events_processed - self.events_failed) /
                max(self.events_processed, 1)
            )
        }
\end{lstlisting}

\subsection{Event Schema Management}

Schema evolution is critical for maintaining backward compatibility as systems evolve.

\begin{lstlisting}[language=Python, caption={Event Schema Registry}]
from typing import Dict, Type
from dataclasses import dataclass, field
import json
from enum import Enum

class SchemaCompatibility(Enum):
    """Schema evolution compatibility modes."""
    BACKWARD = "backward"  # New schema can read old data
    FORWARD = "forward"    # Old schema can read new data
    FULL = "full"          # Both backward and forward compatible
    NONE = "none"          # No compatibility checking

@dataclass
class EventSchema:
    """
    Event schema definition with versioning.

    Attributes:
        name: Schema name (e.g., "PurchaseCompleted")
        version: Schema version (semantic versioning)
        fields: Field definitions with types
        required_fields: List of required field names
        compatibility: Compatibility mode for evolution
    """
    name: str
    version: str
    fields: Dict[str, type]
    required_fields: List[str] = field(default_factory=list)
    compatibility: SchemaCompatibility = SchemaCompatibility.BACKWARD

    def validate(self, data: Dict[str, Any]) -> bool:
        """
        Validate data against schema.

        Args:
            data: Data to validate

        Returns:
            True if valid

        Raises:
            ValueError: If validation fails
        """
        # Check required fields
        missing_fields = set(self.required_fields) - set(data.keys())
        if missing_fields:
            raise ValueError(f"Missing required fields: {missing_fields}")

        # Check field types
        for field_name, field_value in data.items():
            if field_name in self.fields:
                expected_type = self.fields[field_name]
                if not isinstance(field_value, expected_type):
                    raise ValueError(
                        f"Field {field_name} has type {type(field_value)}, "
                        f"expected {expected_type}"
                    )

        return True

    def is_compatible_with(self, other: 'EventSchema') -> bool:
        """
        Check compatibility with another schema version.

        Args:
            other: Other schema version

        Returns:
            True if compatible
        """
        if self.compatibility == SchemaCompatibility.NONE:
            return True

        if self.compatibility in [SchemaCompatibility.BACKWARD, SchemaCompatibility.FULL]:
            # New schema must be able to read old data
            # All required fields in new schema must exist in old schema
            new_required = set(self.required_fields)
            old_fields = set(other.fields.keys())

            if not new_required.issubset(old_fields):
                return False

        if self.compatibility in [SchemaCompatibility.FORWARD, SchemaCompatibility.FULL]:
            # Old schema must be able to read new data
            # All required fields in old schema must exist in new schema
            old_required = set(other.required_fields)
            new_fields = set(self.fields.keys())

            if not old_required.issubset(new_fields):
                return False

        return True

class SchemaRegistry:
    """
    Registry for event schemas with versioning and validation.

    Supports schema evolution with compatibility checking.

    Example:
        >>> registry = SchemaRegistry()
        >>> schema_v1 = EventSchema(
        ...     name="PurchaseCompleted",
        ...     version="1.0",
        ...     fields={'order_id': str, 'amount': float},
        ...     required_fields=['order_id', 'amount']
        ... )
        >>> registry.register(schema_v1)
        >>> registry.validate("PurchaseCompleted", "1.0", data)
    """

    def __init__(self):
        """Initialize schema registry."""
        self.schemas: Dict[str, Dict[str, EventSchema]] = {}
        logger.info("Initialized SchemaRegistry")

    def register(self, schema: EventSchema):
        """
        Register schema version.

        Args:
            schema: Schema to register

        Raises:
            ValueError: If incompatible with existing schemas
        """
        if schema.name not in self.schemas:
            self.schemas[schema.name] = {}

        # Check compatibility with existing versions
        for version, existing_schema in self.schemas[schema.name].items():
            if not schema.is_compatible_with(existing_schema):
                raise ValueError(
                    f"Schema {schema.name} v{schema.version} is not "
                    f"compatible with v{version}"
                )

        self.schemas[schema.name][schema.version] = schema

        logger.info(
            f"Registered schema: {schema.name} v{schema.version} "
            f"({schema.compatibility.value})"
        )

    def get_schema(self, name: str, version: str) -> EventSchema:
        """
        Get schema by name and version.

        Args:
            name: Schema name
            version: Schema version

        Returns:
            EventSchema

        Raises:
            KeyError: If schema not found
        """
        if name not in self.schemas:
            raise KeyError(f"Schema {name} not found")

        if version not in self.schemas[name]:
            raise KeyError(f"Schema {name} v{version} not found")

        return self.schemas[name][version]

    def validate(self, name: str, version: str, data: Dict[str, Any]) -> bool:
        """
        Validate data against schema.

        Args:
            name: Schema name
            version: Schema version
            data: Data to validate

        Returns:
            True if valid
        """
        schema = self.get_schema(name, version)
        return schema.validate(data)

    def get_latest_version(self, name: str) -> str:
        """
        Get latest version of schema.

        Args:
            name: Schema name

        Returns:
            Latest version string
        """
        if name not in self.schemas:
            raise KeyError(f"Schema {name} not found")

        # Sort versions semantically
        versions = sorted(
            self.schemas[name].keys(),
            key=lambda v: [int(x) for x in v.split('.')]
        )

        return versions[-1]
\end{lstlisting}

\section{Stream Processing with Windowing}

Stream processing computes aggregations over unbounded event streams using time windows.

\subsection{StreamProcessor: Real-Time Aggregations}

\begin{lstlisting}[language=Python, caption={Stream Processor with Windowing}]
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque
import threading
import queue

@dataclass
class WindowConfig:
    """
    Time window configuration.

    Attributes:
        window_type: "tumbling", "sliding", or "session"
        window_size: Window duration
        slide_interval: Slide interval for sliding windows
        allowed_lateness: Grace period for late events
    """
    window_type: str  # tumbling, sliding, session
    window_size: timedelta
    slide_interval: Optional[timedelta] = None
    allowed_lateness: timedelta = timedelta(minutes=5)

    def __post_init__(self):
        """Validate configuration."""
        if self.window_type == "sliding" and not self.slide_interval:
            raise ValueError("Sliding windows require slide_interval")

class StreamProcessor:
    """
    Real-time stream processor with windowing and aggregations.

    Maintains in-memory state for active windows, evicts old data,
    and computes incremental aggregations.

    Example:
        >>> processor = StreamProcessor(
        ...     window_config=WindowConfig(
        ...         window_type="sliding",
        ...         window_size=timedelta(minutes=5),
        ...         slide_interval=timedelta(minutes=1)
        ...     )
        ... )
        >>> processor.add_aggregation("sum", sum_func)
        >>> processor.process_event(event)
        >>> features = processor.get_window_features()
    """

    def __init__(
        self,
        window_config: WindowConfig,
        partition_key: Optional[str] = None
    ):
        """
        Initialize stream processor.

        Args:
            window_config: Window configuration
            partition_key: Key for partitioning events (e.g., "user_id")
        """
        self.window_config = window_config
        self.partition_key = partition_key

        # Window storage: {window_key: [events]}
        self.windows: Dict[str, deque] = {}

        # Aggregation functions: {name: func}
        self.aggregations: Dict[str, Callable[[List[Event]], Any]] = {}

        # Metrics
        self.events_processed = 0
        self.events_late = 0
        self.windows_closed = 0

        logger.info(
            f"Initialized StreamProcessor: "
            f"{window_config.window_type} windows of {window_config.window_size}"
        )

    def add_aggregation(
        self,
        name: str,
        aggregation_func: Callable[[List[Event]], Any]
    ):
        """
        Register aggregation function.

        Args:
            name: Aggregation name
            aggregation_func: Function that takes event list and returns result
        """
        self.aggregations[name] = aggregation_func
        logger.info(f"Registered aggregation: {name}")

    def process_event(self, event: Event):
        """
        Process event and update windows.

        Args:
            event: Event to process
        """
        # Get window keys for this event
        window_keys = self._get_window_keys(event.timestamp)

        # Check if event is too late
        if self._is_late(event.timestamp):
            self.events_late += 1
            logger.warning(
                f"Late event: {event.event_id} "
                f"(timestamp={event.timestamp})"
            )
            return

        # Add to appropriate windows
        for window_key in window_keys:
            if window_key not in self.windows:
                self.windows[window_key] = deque()

            self.windows[window_key].append(event)

        # Evict old windows
        self._evict_old_windows(event.timestamp)

        self.events_processed += 1

    def _get_window_keys(self, timestamp: datetime) -> List[str]:
        """
        Get window keys for event timestamp.

        Args:
            timestamp: Event timestamp

        Returns:
            List of window keys this event belongs to
        """
        if self.window_config.window_type == "tumbling":
            # Single window based on time bucket
            window_start = self._align_to_window(timestamp)
            return [window_start.isoformat()]

        elif self.window_config.window_type == "sliding":
            # Multiple overlapping windows
            keys = []
            window_size = self.window_config.window_size
            slide = self.window_config.slide_interval

            # Find all windows this event belongs to
            current = self._align_to_window(timestamp)

            # Look back to include all relevant windows
            lookback_windows = int(window_size / slide) + 1

            for i in range(lookback_windows):
                window_start = current - (slide * i)
                window_end = window_start + window_size

                if window_start <= timestamp < window_end:
                    keys.append(window_start.isoformat())

            return keys

        else:  # session windows
            # Session windows group events with gaps < session_gap
            # Simplified implementation
            return [f"session_{timestamp.strftime('%Y%m%d_%H%M')}"]

    def _align_to_window(self, timestamp: datetime) -> datetime:
        """
        Align timestamp to window boundary.

        Args:
            timestamp: Timestamp to align

        Returns:
            Window-aligned timestamp
        """
        if self.window_config.window_type == "tumbling":
            interval = self.window_config.window_size
        else:  # sliding
            interval = self.window_config.slide_interval

        epoch = datetime(1970, 1, 1, tzinfo=timestamp.tzinfo)
        seconds_since_epoch = (timestamp - epoch).total_seconds()
        interval_seconds = interval.total_seconds()

        bucket = int(seconds_since_epoch // interval_seconds)
        aligned = epoch + timedelta(seconds=bucket * interval_seconds)

        return aligned

    def _is_late(self, timestamp: datetime) -> bool:
        """
        Check if event is too late.

        Args:
            timestamp: Event timestamp

        Returns:
            True if event is beyond allowed lateness
        """
        now = datetime.now(tz=timestamp.tzinfo)
        max_delay = self.window_config.window_size + self.window_config.allowed_lateness

        return (now - timestamp) > max_delay

    def _evict_old_windows(self, current_time: datetime):
        """
        Remove expired windows.

        Args:
            current_time: Current processing time
        """
        grace_period = self.window_config.allowed_lateness
        expired_keys = []

        for window_key in self.windows:
            try:
                window_start = datetime.fromisoformat(window_key)
                window_end = window_start + self.window_config.window_size

                if current_time > window_end + grace_period:
                    expired_keys.append(window_key)
            except ValueError:
                # Session window key format
                continue

        for key in expired_keys:
            del self.windows[key]
            self.windows_closed += 1

    def get_window_features(
        self,
        window_key: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Compute features for window.

        Args:
            window_key: Window to compute (latest if None)

        Returns:
            Dictionary of computed features
        """
        if window_key is None:
            if not self.windows:
                return {}
            window_key = max(self.windows.keys())

        if window_key not in self.windows:
            return {}

        events = list(self.windows[window_key])

        if not events:
            return {}

        # Compute all aggregations
        features = {'window_key': window_key, 'event_count': len(events)}

        for agg_name, agg_func in self.aggregations.items():
            try:
                features[agg_name] = agg_func(events)
            except Exception as e:
                logger.error(f"Aggregation {agg_name} failed: {e}")
                features[agg_name] = None

        return features

    def get_all_window_features(self) -> List[Dict[str, Any]]:
        """
        Get features for all active windows.

        Returns:
            List of feature dictionaries
        """
        return [
            self.get_window_features(window_key)
            for window_key in self.windows.keys()
        ]
\end{lstlisting}

\section{Message Ordering and Partitioning Strategies}

Partitioning enables horizontal scaling while maintaining ordering guarantees.

\subsection{MessageRouter: Intelligent Partitioning}

\begin{lstlisting}[language=Python, caption={Message Router with Partitioning Logic}]
from typing import Dict, List, Optional, Callable
import hashlib

class PartitionStrategy(Enum):
    """Partitioning strategies for message routing."""
    ROUND_ROBIN = "round_robin"
    HASH = "hash"
    CUSTOM = "custom"
    STICKY = "sticky"

class MessageRouter:
    """
    Route messages to partitions with ordering guarantees.

    Ensures events with same partition key go to same partition,
    maintaining ordering within key while enabling parallelism.

    Example:
        >>> router = MessageRouter(
        ...     num_partitions=16,
        ...     strategy=PartitionStrategy.HASH,
        ...     partition_key="user_id"
        ... )
        >>> partition = router.route(event)
    """

    def __init__(
        self,
        num_partitions: int,
        strategy: PartitionStrategy = PartitionStrategy.HASH,
        partition_key: Optional[str] = None,
        custom_partitioner: Optional[Callable] = None
    ):
        """
        Initialize message router.

        Args:
            num_partitions: Total number of partitions
            strategy: Partitioning strategy
            partition_key: Event field to use for partitioning
            custom_partitioner: Custom partition function
        """
        self.num_partitions = num_partitions
        self.strategy = strategy
        self.partition_key = partition_key
        self.custom_partitioner = custom_partitioner

        # Round-robin state
        self.round_robin_counter = 0

        # Metrics
        self.partition_counts: Dict[int, int] = {
            i: 0 for i in range(num_partitions)
        }

        logger.info(
            f"Initialized MessageRouter: {num_partitions} partitions, "
            f"strategy={strategy.value}"
        )

    def route(self, event: Event) -> int:
        """
        Determine partition for event.

        Args:
            event: Event to route

        Returns:
            Partition number (0 to num_partitions-1)
        """
        if self.strategy == PartitionStrategy.ROUND_ROBIN:
            partition = self._round_robin()

        elif self.strategy == PartitionStrategy.HASH:
            partition = self._hash_partition(event)

        elif self.strategy == PartitionStrategy.STICKY:
            partition = self._sticky_partition(event)

        elif self.strategy == PartitionStrategy.CUSTOM:
            if not self.custom_partitioner:
                raise ValueError("Custom strategy requires custom_partitioner")
            partition = self.custom_partitioner(event, self.num_partitions)

        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

        # Track partition distribution
        self.partition_counts[partition] += 1

        return partition

    def _round_robin(self) -> int:
        """Round-robin partitioning."""
        partition = self.round_robin_counter % self.num_partitions
        self.round_robin_counter += 1
        return partition

    def _hash_partition(self, event: Event) -> int:
        """
        Hash-based partitioning.

        Uses partition key to ensure same key -> same partition.
        """
        if not self.partition_key:
            # Hash event_id if no partition key specified
            key_value = event.event_id
        else:
            # Extract partition key from event data
            key_value = str(event.data.get(self.partition_key, event.event_id))

        # Consistent hashing
        hash_value = int(hashlib.md5(key_value.encode()).hexdigest(), 16)
        partition = hash_value % self.num_partitions

        return partition

    def _sticky_partition(self, event: Event) -> int:
        """
        Sticky partitioning for load balancing.

        Batches messages to same partition until threshold,
        then switches to reduce connection overhead.
        """
        # Simplified: use hash partitioning
        return self._hash_partition(event)

    def get_partition_distribution(self) -> Dict[str, Any]:
        """
        Get partition distribution metrics.

        Returns:
            Distribution statistics
        """
        total = sum(self.partition_counts.values())

        if total == 0:
            return {'partitions': {}, 'balance': 1.0}

        # Calculate balance metric (std dev from uniform)
        expected = total / self.num_partitions
        variance = sum(
            (count - expected) ** 2
            for count in self.partition_counts.values()
        ) / self.num_partitions

        std_dev = variance ** 0.5
        balance = 1.0 - (std_dev / expected if expected > 0 else 0)

        return {
            'partitions': dict(self.partition_counts),
            'total_messages': total,
            'expected_per_partition': expected,
            'balance': balance,  # 1.0 = perfect balance, 0.0 = all in one partition
            'std_dev': std_dev
        }
\end{lstlisting}

\subsection{Partition Key Selection Guidelines}

\textbf{Good partition keys:}
\begin{itemize}
    \item \textbf{High cardinality}: Many unique values (user\_id, order\_id)
    \item \textbf{Even distribution}: Values spread uniformly across range
    \item \textbf{Stable}: Value doesn't change for same logical entity
    \item \textbf{Business-aligned}: Matches query patterns (aggregate by user\_id -> partition by user\_id)
\end{itemize}

\textbf{Bad partition keys:}
\begin{itemize}
    \item \textbf{Low cardinality}: Few unique values (country, boolean flags)
    \item \textbf{Skewed distribution}: Some values much more common (80/20 rule)
    \item \textbf{Sequential}: Auto-incrementing IDs cause hotspots
    \item \textbf{Timestamp-based}: Recent timestamps get all traffic
\end{itemize}

\textbf{Example partition key strategies:}

\begin{lstlisting}[language=Python, caption={Partition Key Strategies}]
# Example: E-commerce event partitioning

# BAD: Partition by product_category (low cardinality)
# Result: "Electronics" partition gets 60% of traffic
router_bad = MessageRouter(
    num_partitions=16,
    strategy=PartitionStrategy.HASH,
    partition_key="product_category"  # Only ~50 categories
)

# GOOD: Partition by user_id (high cardinality, even distribution)
# Result: Balanced load across partitions
router_good = MessageRouter(
    num_partitions=16,
    strategy=PartitionStrategy.HASH,
    partition_key="user_id"  # Millions of users
)

# BETTER: Custom partitioning for complex requirements
def custom_partitioner(event: Event, num_partitions: int) -> int:
    """
    Custom partitioning for mixed workload.

    - VIP users: Always partition 0 (dedicated processing)
    - Regular users: Hash-based on user_id
    - Anonymous: Round-robin
    """
    user_id = event.data.get('user_id')

    if not user_id:
        # Anonymous - distribute evenly
        return hash(event.event_id) % num_partitions

    if event.data.get('is_vip'):
        # VIP users - dedicated partition
        return 0

    # Regular users - hash partition
    return int(hashlib.md5(user_id.encode()).hexdigest(), 16) % num_partitions

router_custom = MessageRouter(
    num_partitions=16,
    strategy=PartitionStrategy.CUSTOM,
    custom_partitioner=custom_partitioner
)
\end{lstlisting}

\section{Real-World Scenario: The Real-Time Revenue Dashboard}

\subsection{The Problem}

An e-commerce company needs real-time revenue visibility across:
\begin{itemize}
    \item 15 geographic regions
    \item 50 product categories
    \item 5 customer segments (VIP, Premium, Standard, Trial, Anonymous)
    \item 3 time windows (last 15 minutes, last hour, last 24 hours)
\end{itemize}

Total: 15 × 50 × 5 × 3 = 11,250 real-time aggregations

\textbf{Business requirements:}
\begin{itemize}
    \item Dashboard updates every 30 seconds
    \item <1 minute latency from transaction to dashboard
    \item 99.9\% accuracy (no missing or duplicate transactions)
    \item Alert if any segment revenue drops >15\% vs. previous period
    \item Support 10,000 transactions/second at peak
\end{itemize}

\textbf{Initial batch implementation problems:}
\begin{itemize}
    \item Query execution time: 45 seconds for full aggregation
    \item Database CPU spiking to 100\% during queries
    \item 3-5 minute latency from transaction to dashboard
    \item Cost: \$15K/month in database compute for repeated full scans
\end{itemize}

\subsection{Event-Driven Solution}

\begin{lstlisting}[language=Python, caption={Real-Time Revenue Dashboard Implementation}]
import uuid
from decimal import Decimal

# Define event schema
purchase_schema = EventSchema(
    name="PurchaseCompleted",
    version="1.0",
    fields={
        'order_id': str,
        'user_id': str,
        'amount': float,
        'region': str,
        'category': str,
        'customer_segment': str,
        'timestamp': str
    },
    required_fields=['order_id', 'user_id', 'amount', 'timestamp'],
    compatibility=SchemaCompatibility.BACKWARD
)

# Register schema
registry = SchemaRegistry()
registry.register(purchase_schema)

# Create event handlers
class RevenueAggregationHandler(EventHandler):
    """Aggregate revenue by dimensions in real-time."""

    def __init__(self, output_topic: str):
        self.output_topic = output_topic

        # Stream processors for different windows
        self.processors = {
            '15min': StreamProcessor(
                window_config=WindowConfig(
                    window_type="tumbling",
                    window_size=timedelta(minutes=15)
                )
            ),
            '1hour': StreamProcessor(
                window_config=WindowConfig(
                    window_type="tumbling",
                    window_size=timedelta(hours=1)
                )
            ),
            '24hour': StreamProcessor(
                window_config=WindowConfig(
                    window_type="tumbling",
                    window_size=timedelta(hours=24)
                )
            )
        }

        # Register aggregations for each processor
        for processor in self.processors.values():
            # Total revenue
            processor.add_aggregation(
                'total_revenue',
                lambda events: sum(e.data['amount'] for e in events)
            )

            # Transaction count
            processor.add_aggregation(
                'transaction_count',
                lambda events: len(events)
            )

            # Average order value
            processor.add_aggregation(
                'avg_order_value',
                lambda events: (
                    sum(e.data['amount'] for e in events) / len(events)
                    if events else 0
                )
            )

            # Revenue by region
            processor.add_aggregation(
                'revenue_by_region',
                lambda events: self._aggregate_by_dimension(events, 'region')
            )

            # Revenue by category
            processor.add_aggregation(
                'revenue_by_category',
                lambda events: self._aggregate_by_dimension(events, 'category')
            )

            # Revenue by segment
            processor.add_aggregation(
                'revenue_by_segment',
                lambda events: self._aggregate_by_dimension(events, 'customer_segment')
            )

    def _aggregate_by_dimension(
        self,
        events: List[Event],
        dimension: str
    ) -> Dict[str, float]:
        """Aggregate revenue by dimension."""
        aggregates = {}

        for event in events:
            dim_value = event.data.get(dimension, 'Unknown')
            amount = event.data['amount']
            aggregates[dim_value] = aggregates.get(dim_value, 0) + amount

        return aggregates

    def can_handle(self, event: Event) -> bool:
        """Handle PurchaseCompleted events."""
        return event.event_type == EventType.DATA_INGESTED and \
               event.data.get('event_name') == 'PurchaseCompleted'

    def handle(self, event: Event) -> Optional[List[Event]]:
        """Process purchase event and compute aggregations."""
        # Validate event schema
        registry.validate(
            'PurchaseCompleted',
            event.schema_version,
            event.data
        )

        # Process through all window processors
        for window_name, processor in self.processors.items():
            processor.process_event(event)

        # Emit feature events
        output_events = []

        for window_name, processor in self.processors.items():
            features = processor.get_window_features()

            if features:
                feature_event = Event(
                    event_id=str(uuid.uuid4()),
                    event_type=EventType.FEATURE_COMPUTED,
                    timestamp=datetime.now(),
                    source='revenue_aggregation_handler',
                    data={
                        'window': window_name,
                        'features': features
                    },
                    metadata={'parent_event_id': event.event_id}
                )

                output_events.append(feature_event)

        return output_events

class AnomalyDetectionHandler(EventHandler):
    """Detect revenue anomalies in real-time."""

    def __init__(self, alert_threshold: float = 0.15):
        self.alert_threshold = alert_threshold
        self.baseline_revenue: Dict[str, float] = {}

    def can_handle(self, event: Event) -> bool:
        """Handle feature computation events."""
        return event.event_type == EventType.FEATURE_COMPUTED

    def handle(self, event: Event) -> Optional[List[Event]]:
        """Check for anomalies in revenue."""
        features = event.data.get('features', {})
        window = event.data.get('window')

        # Only monitor 15-minute windows for real-time alerts
        if window != '15min':
            return None

        current_revenue = features.get('total_revenue', 0)

        # Check against baseline
        baseline_key = f"{window}_baseline"

        if baseline_key in self.baseline_revenue:
            baseline = self.baseline_revenue[baseline_key]

            if baseline > 0:
                change = (current_revenue - baseline) / baseline

                if abs(change) > self.alert_threshold:
                    # Revenue anomaly detected!
                    alert_event = Event(
                        event_id=str(uuid.uuid4()),
                        event_type=EventType.PIPELINE_FAILED,  # Reuse for alerts
                        timestamp=datetime.now(),
                        source='anomaly_detection_handler',
                        data={
                            'alert_type': 'revenue_anomaly',
                            'window': window,
                            'current_revenue': current_revenue,
                            'baseline_revenue': baseline,
                            'change_percent': change * 100,
                            'severity': 'critical' if abs(change) > 0.25 else 'warning'
                        }
                    )

                    logger.warning(
                        f"Revenue anomaly detected: {change:.1%} change "
                        f"in {window} window"
                    )

                    return [alert_event]

        # Update baseline
        self.baseline_revenue[baseline_key] = current_revenue

        return None

# Setup pipeline
pipeline = EventDrivenPipeline(
    name="revenue_dashboard",
    input_topic="purchase_events",
    output_topic="revenue_features",
    kafka_config={
        'bootstrap_servers': ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092'],
        'group_id': 'revenue_dashboard_consumer',
        'auto_offset_reset': 'latest',
        'enable_auto_commit': False,
        'max_poll_records': 1000
    }
)

# Register handlers
pipeline.register_handler(RevenueAggregationHandler('revenue_features'))
pipeline.register_handler(AnomalyDetectionHandler(alert_threshold=0.15))

# Setup message router for input partitioning
router = MessageRouter(
    num_partitions=16,
    strategy=PartitionStrategy.HASH,
    partition_key='user_id'  # Maintain user ordering
)

# Simulate purchase events
def generate_purchase_event(
    user_id: str,
    amount: float,
    region: str,
    category: str,
    segment: str
) -> Event:
    """Generate purchase event."""
    return Event(
        event_id=str(uuid.uuid4()),
        event_type=EventType.DATA_INGESTED,
        timestamp=datetime.now(),
        source='transaction_service',
        data={
            'event_name': 'PurchaseCompleted',
            'order_id': str(uuid.uuid4()),
            'user_id': user_id,
            'amount': amount,
            'region': region,
            'category': category,
            'customer_segment': segment
        },
        schema_version='1.0'
    )

# Example usage
if __name__ == '__main__':
    # Start pipeline in background thread
    import threading

    pipeline_thread = threading.Thread(target=pipeline.start, daemon=True)
    pipeline_thread.start()

    # Simulate incoming purchases
    regions = ['US-East', 'US-West', 'EU', 'Asia', 'LatAm']
    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']
    segments = ['VIP', 'Premium', 'Standard', 'Trial', 'Anonymous']

    import random
    import time

    for i in range(1000):
        event = generate_purchase_event(
            user_id=f"user_{random.randint(1, 10000)}",
            amount=random.uniform(10, 500),
            region=random.choice(regions),
            category=random.choice(categories),
            segment=random.choice(segments)
        )

        # Route to partition
        partition = router.route(event)

        logger.info(
            f"Generated purchase: ${event.data['amount']:.2f} "
            f"-> partition {partition}"
        )

        time.sleep(0.01)  # 100 events/second

    # Check router distribution
    distribution = router.get_partition_distribution()
    logger.info(f"Partition distribution: balance={distribution['balance']:.2%}")

    # Get pipeline metrics
    metrics = pipeline.get_metrics()
    logger.info(f"Pipeline metrics: {metrics}")
\end{lstlisting}

\subsection{Outcome}

With event-driven architecture:
\begin{itemize}
    \item \textbf{Latency}: Reduced from 3-5 minutes to <500ms
    \item \textbf{Cost}: Reduced from \$15K/month to \$2K/month (87\% reduction)
    \item \textbf{Scalability}: Handles 10K transactions/second at peak (10x headroom)
    \item \textbf{Accuracy}: 99.99\% (exactly-once semantics with Kafka)
    \item \textbf{Reliability}: Zero dashboard downtime in 6 months
    \item \textbf{Alerting}: Revenue anomalies detected in <1 minute (vs. hours)
\end{itemize}

\textbf{Key engineering decisions:}
\begin{itemize}
    \item \textbf{Kafka partitioning by user\_id}: Maintains ordering for user-level features
    \item \textbf{Tumbling windows}: Simpler than sliding for business requirements
    \item \textbf{In-memory aggregation}: Sub-second latency without database queries
    \item \textbf{Dead-letter queue}: Failed events preserved for debugging
    \item \textbf{Schema registry}: Enables safe schema evolution
\end{itemize}

\section{Production Kafka Deployment}

\subsection{Kafka Producer Best Practices}

\begin{lstlisting}[language=Python, caption={Production Kafka Producer Configuration}]
from kafka import KafkaProducer
import json

def create_production_producer(
    bootstrap_servers: List[str],
    enable_idempotence: bool = True
) -> KafkaProducer:
    """
    Create production-grade Kafka producer.

    Args:
        bootstrap_servers: Kafka broker addresses
        enable_idempotence: Enable exactly-once semantics

    Returns:
        Configured KafkaProducer
    """
    producer = KafkaProducer(
        # Connection
        bootstrap_servers=bootstrap_servers,

        # Serialization
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        key_serializer=lambda k: k.encode('utf-8') if k else None,

        # Reliability
        acks='all',  # Wait for all replicas to acknowledge
        enable_idempotence=enable_idempotence,  # Exactly-once semantics
        max_in_flight_requests_per_connection=5,  # Pipeline up to 5 batches

        # Retries
        retries=3,
        retry_backoff_ms=100,

        # Batching for throughput
        batch_size=32768,  # 32KB batches
        linger_ms=10,  # Wait up to 10ms to fill batch

        # Compression
        compression_type='lz4',  # Fast compression

        # Timeouts
        request_timeout_ms=30000,

        # Buffer
        buffer_memory=33554432,  # 32MB buffer
    )

    return producer

# Usage example
producer = create_production_producer(
    bootstrap_servers=['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092']
)

# Send with callback
def on_send_success(record_metadata):
    logger.info(
        f"Message delivered: topic={record_metadata.topic}, "
        f"partition={record_metadata.partition}, "
        f"offset={record_metadata.offset}"
    )

def on_send_error(excp):
    logger.error(f"Message delivery failed: {excp}")

# Asynchronous send with callback
future = producer.send(
    'purchase_events',
    value=event.to_dict(),
    key=event.data.get('user_id')  # Partition by user_id
)

future.add_callback(on_send_success)
future.add_errback(on_send_error)

# Flush to ensure delivery
producer.flush()
\end{lstlisting}

\subsection{Kafka Consumer Best Practices}

\begin{lstlisting}[language=Python, caption={Production Kafka Consumer Configuration}]
from kafka import KafkaConsumer

def create_production_consumer(
    topics: List[str],
    group_id: str,
    bootstrap_servers: List[str]
) -> KafkaConsumer:
    """
    Create production-grade Kafka consumer.

    Args:
        topics: Topics to subscribe to
        group_id: Consumer group identifier
        bootstrap_servers: Kafka broker addresses

    Returns:
        Configured KafkaConsumer
    """
    consumer = KafkaConsumer(
        *topics,

        # Connection
        bootstrap_servers=bootstrap_servers,
        group_id=group_id,

        # Deserialization
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        key_deserializer=lambda k: k.decode('utf-8') if k else None,

        # Offset management
        auto_offset_reset='earliest',  # Start from beginning if no offset
        enable_auto_commit=False,  # Manual commit for exactly-once

        # Fetching
        max_poll_records=500,  # Process up to 500 messages per poll
        max_poll_interval_ms=300000,  # 5 minutes max between polls

        # Session
        session_timeout_ms=30000,  # 30 seconds
        heartbeat_interval_ms=10000,  # 10 seconds

        # Performance
        fetch_min_bytes=1024,  # Minimum 1KB per fetch
        fetch_max_wait_ms=500,  # Wait up to 500ms for min bytes
    )

    return consumer

# Usage example
consumer = create_production_consumer(
    topics=['purchase_events'],
    group_id='revenue_dashboard_consumer',
    bootstrap_servers=['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092']
)

# Consume with manual offset management
try:
    for message in consumer:
        try:
            # Process message
            event_dict = message.value
            event = Event.from_dict(event_dict)

            # Process through pipeline
            output_events = pipeline._process_event(event)

            # Commit offset only after successful processing
            consumer.commit()

        except Exception as e:
            logger.error(f"Error processing message: {e}")
            # Don't commit - message will be reprocessed

except KeyboardInterrupt:
    logger.info("Shutting down consumer")
finally:
    consumer.close()
\end{lstlisting}

\section{Error Handling and Fault Tolerance}

Event-driven data pipelines operate in hostile environments: networks partition, services crash, dependencies timeout, messages arrive corrupted. Unlike batch ETL where failures trigger human intervention, event-driven systems must recover automatically—processing millions of events daily with minimal supervision. The difference between naive error handling and production-grade fault tolerance is the difference between 99\% uptime and 99.99\% uptime, between occasional data loss and guaranteed processing.

Production event pipelines require defense-in-depth: retry strategies for transient failures, circuit breakers for cascading failures, dead letter queues for poison messages, and exponential backoff to prevent thundering herds. However, incorrect error handling creates new failure modes: infinite retry loops waste resources, premature circuit breaker trips cause false outages, and poorly configured backoff strategies amplify traffic spikes.

\subsection{The Cascading Failure Crisis}

Consider a real-time fraud detection system processing 50,000 transactions per second:

\textbf{Architecture:}
\begin{itemize}
    \item Kafka ingests transaction events
    \item Stream processor enriches with customer features from PostgreSQL database
    \item ML service scores transactions for fraud probability
    \item High-risk transactions routed to manual review queue
    \item All transactions logged to data warehouse
\end{itemize}

\textbf{Friday 14:32 - The trigger:}
\begin{itemize}
    \item PostgreSQL primary database experiences disk I/O saturation (routine batch job ran during peak hours)
    \item Database query latency increases from 5ms to 2000ms
    \item Stream processor timeouts escalate from 0.1\% to 15\%
\end{itemize}

\textbf{14:35 - First cascade:}
\begin{itemize}
    \item Stream processors retry failed enrichment calls immediately (no backoff)
    \item Retry storm amplifies database load: 50K TPS becomes 150K TPS query load
    \item Database connection pool exhausted (100 connections saturated)
    \item All enrichment calls now fail with "connection pool exhausted"
\end{itemize}

\textbf{14:37 - Second cascade:}
\begin{itemize}
    \item Stream processors have no circuit breaker—continue hammering database
    \item Kafka consumer lag increases: 0 seconds → 30 seconds → 5 minutes
    \item ML service receives backlog of stale events (fraud patterns drift in 5 minutes)
    \item False positive rate doubles: 2\% → 4\% (manual review team overwhelmed)
\end{itemize}

\textbf{14:40 - Third cascade:}
\begin{itemize}
    \item Failed events have no dead letter queue—processors retry indefinitely
    \item Memory usage escalates as retry queues grow unbounded
    \item Stream processors start OOM crashing and restarting
    \item Kafka rebalances trigger on every crash (processing halts during rebalance)
    \item Consumer lag: 5 minutes → 30 minutes → 2 hours
\end{itemize}

\textbf{14:45 - Total system failure:}
\begin{itemize}
    \item All 50 stream processor instances in crash-restart loop
    \item Fraud detection completely offline
    \item Manual review queue backlog: 2 hours worth of transactions
    \item Estimated fraud loss during outage: \$2.3M
    \item Customer service overwhelmed with false decline complaints
\end{itemize}

\textbf{Root cause analysis:}
\begin{itemize}
    \item \textbf{No retry backoff}: Immediate retries amplified database load 3x
    \item \textbf{No circuit breaker}: System continued attacking failed dependency
    \item \textbf{No dead letter queue}: Poison messages caused infinite retry loops
    \item \textbf{No bulkhead isolation}: Database failure cascaded to entire pipeline
    \item \textbf{No graceful degradation}: System had no fallback for missing enrichment
\end{itemize}

\textbf{Cost of poor error handling:}
\begin{itemize}
    \item Direct fraud losses: \$2.3M
    \item Customer service costs: \$180K (overtime for complaint handling)
    \item Revenue loss from false declines: \$450K (estimated)
    \item Engineering time for incident response: 120 hours
    \item Regulatory reporting requirements: SOX incident report filed
\end{itemize}

This scenario demonstrates why production event pipelines require comprehensive fault tolerance.

\subsection{Error Classification and Handling Decision Trees}

Not all errors require the same handling strategy. Production systems must classify errors and apply appropriate recovery patterns.

\subsubsection{Error Categories}

\textbf{Transient Errors} (retry with backoff):
\begin{itemize}
    \item Network timeouts and connection resets
    \item Service temporarily unavailable (503)
    \item Database connection pool temporarily exhausted
    \item Rate limiting (429 Too Many Requests)
    \item Temporary disk full / out of memory
\end{itemize}

\textbf{Persistent Errors} (dead letter queue):
\begin{itemize}
    \item Malformed message payload (schema validation failure)
    \item Missing required fields
    \item Invalid data types (string where integer expected)
    \item Business logic validation failures
    \item Referential integrity violations
\end{itemize}

\textbf{Dependency Failures} (circuit breaker):
\begin{itemize}
    \item External API consistently failing (>50\% error rate)
    \item Database primary offline (failover required)
    \item Service deployment in progress
    \item Cascading failures across service mesh
\end{itemize}

\textbf{Catastrophic Errors} (immediate alert + halt):
\begin{itemize}
    \item Data corruption detected (checksum mismatch)
    \item Security violations (authentication failure, unauthorized access)
    \item Critical configuration errors
    \item Kafka cluster unreachable
    \item Unrecoverable state corruption
\end{itemize}

\subsubsection{Error Handling Decision Tree}

\begin{lstlisting}[style=python, caption={Error Classification Logic}]
from enum import Enum
from typing import Optional, Type
import time

class ErrorCategory(Enum):
    """Error category classification."""
    TRANSIENT = "transient"  # Retry with backoff
    PERSISTENT = "persistent"  # Dead letter queue
    DEPENDENCY = "dependency"  # Circuit breaker
    CATASTROPHIC = "catastrophic"  # Alert and halt

class ErrorClassifier:
    """
    Classifies errors to determine appropriate handling strategy.

    Production consideration: Error classification drives recovery
    behavior. Misclassifying transient errors as persistent sends
    recoverable messages to dead letter queue. Misclassifying
    persistent as transient creates infinite retry loops.
    """

    # Transient error indicators
    TRANSIENT_EXCEPTIONS = (
        ConnectionError,
        TimeoutError,
        OSError,  # Includes network errors
    )

    TRANSIENT_HTTP_CODES = {408, 429, 502, 503, 504}

    # Persistent error indicators
    PERSISTENT_EXCEPTIONS = (
        ValueError,
        TypeError,
        KeyError,
        AttributeError,
    )

    PERSISTENT_HTTP_CODES = {400, 404, 422}

    # Dependency failure indicators
    DEPENDENCY_HTTP_CODES = {500, 501}

    @classmethod
    def classify_exception(
        cls,
        exception: Exception,
        context: Optional[dict] = None
    ) -> ErrorCategory:
        """
        Classify exception into error category.

        Args:
            exception: The exception to classify
            context: Additional context (HTTP status, retry count, etc.)

        Returns:
            ErrorCategory indicating handling strategy
        """
        context = context or {}

        # Check for catastrophic errors
        if cls._is_catastrophic(exception, context):
            return ErrorCategory.CATASTROPHIC

        # Check for dependency failures
        if cls._is_dependency_failure(exception, context):
            return ErrorCategory.DEPENDENCY

        # Check for transient errors
        if cls._is_transient(exception, context):
            return ErrorCategory.TRANSIENT

        # Default to persistent (dead letter queue)
        return ErrorCategory.PERSISTENT

    @classmethod
    def _is_catastrophic(cls, exception: Exception, context: dict) -> bool:
        """Check if error is catastrophic."""
        # Security violations
        if "unauthorized" in str(exception).lower():
            return True

        # Data corruption
        if "checksum" in str(exception).lower():
            return True

        # Critical infrastructure
        if "kafka cluster unreachable" in str(exception).lower():
            return True

        return False

    @classmethod
    def _is_dependency_failure(cls, exception: Exception, context: dict) -> bool:
        """Check if error indicates dependency failure."""
        # High error rate from context
        if context.get('error_rate', 0) > 0.5:
            return True

        # 5xx errors from services
        http_status = context.get('http_status')
        if http_status in cls.DEPENDENCY_HTTP_CODES:
            return True

        return False

    @classmethod
    def _is_transient(cls, exception: Exception, context: dict) -> bool:
        """Check if error is transient."""
        # Exception type
        if isinstance(exception, cls.TRANSIENT_EXCEPTIONS):
            return True

        # HTTP status codes
        http_status = context.get('http_status')
        if http_status in cls.TRANSIENT_HTTP_CODES:
            return True

        # Rate limiting
        if "rate limit" in str(exception).lower():
            return True

        return False

# Example usage
try:
    response = external_api.call()
except requests.exceptions.Timeout as e:
    category = ErrorClassifier.classify_exception(
        e,
        context={'http_status': 408, 'retry_count': 2}
    )
    # category == ErrorCategory.TRANSIENT -> retry with backoff
\end{lstlisting}

\subsection{RetryStrategy: Exponential Backoff with Jitter}

Retry strategies must balance recovery speed with system protection. Immediate retries amplify load on struggling services. Exponential backoff provides breathing room, but synchronized retries create thundering herd problems. Jitter randomizes retry timing to distribute load.

\begin{lstlisting}[style=python, caption={Production Retry Strategy}]
import random
import time
from dataclasses import dataclass, field
from typing import Callable, Optional, Any
import logging

logger = logging.getLogger(__name__)

@dataclass
class RetryConfig:
    """Configuration for retry behavior."""
    max_attempts: int = 3
    base_delay_ms: int = 100  # Initial delay
    max_delay_ms: int = 30000  # Cap at 30 seconds
    exponential_base: float = 2.0  # Delay multiplier
    jitter: bool = True  # Add randomization
    jitter_ratio: float = 0.3  # +/- 30% randomization

class RetryStrategy:
    """
    Implements exponential backoff with jitter for fault tolerance.

    Production considerations:
    1. Exponential backoff: Delay increases exponentially (100ms, 200ms, 400ms)
    2. Max delay cap: Prevents excessive wait times
    3. Jitter: Randomizes retry timing to prevent thundering herd
    4. Attempt tracking: Monitors retry attempts for alerting

    Example progression (base=100ms, exponent=2.0, jitter=0.3):
    - Attempt 1: 100ms * (1 +/- 0.3) = 70-130ms
    - Attempt 2: 200ms * (1 +/- 0.3) = 140-260ms
    - Attempt 3: 400ms * (1 +/- 0.3) = 280-520ms
    - Attempt 4: 800ms * (1 +/- 0.3) = 560-1040ms
    """

    def __init__(self, config: Optional[RetryConfig] = None):
        self.config = config or RetryConfig()
        self.attempt_count = 0

    def execute(
        self,
        operation: Callable,
        *args,
        error_classifier: Optional[ErrorClassifier] = None,
        **kwargs
    ) -> Any:
        """
        Execute operation with retry logic.

        Args:
            operation: Callable to execute
            *args: Positional arguments for operation
            error_classifier: Optional error classifier
            **kwargs: Keyword arguments for operation

        Returns:
            Result of successful operation

        Raises:
            Exception: If all retry attempts exhausted
        """
        last_exception = None
        error_classifier = error_classifier or ErrorClassifier()

        for attempt in range(1, self.config.max_attempts + 1):
            self.attempt_count = attempt

            try:
                result = operation(*args, **kwargs)

                # Success - reset attempt counter
                if attempt > 1:
                    logger.info(
                        f"Operation succeeded on attempt {attempt}"
                    )

                return result

            except Exception as e:
                last_exception = e

                # Classify error
                category = error_classifier.classify_exception(e)

                # Don't retry persistent or catastrophic errors
                if category in [ErrorCategory.PERSISTENT, ErrorCategory.CATASTROPHIC]:
                    logger.warning(
                        f"Non-retryable error ({category.value}): {e}"
                    )
                    raise

                # Check if more attempts available
                if attempt >= self.config.max_attempts:
                    logger.error(
                        f"All {self.config.max_attempts} retry attempts exhausted"
                    )
                    raise

                # Calculate backoff delay
                delay_ms = self._calculate_delay(attempt)

                logger.warning(
                    f"Attempt {attempt} failed: {e}. "
                    f"Retrying in {delay_ms}ms..."
                )

                time.sleep(delay_ms / 1000.0)

        # Should never reach here, but satisfy type checker
        raise last_exception

    def _calculate_delay(self, attempt: int) -> int:
        """
        Calculate retry delay with exponential backoff and jitter.

        Args:
            attempt: Current attempt number (1-indexed)

        Returns:
            Delay in milliseconds
        """
        # Exponential backoff: base * (exponential_base ^ (attempt - 1))
        delay = self.config.base_delay_ms * (
            self.config.exponential_base ** (attempt - 1)
        )

        # Apply maximum delay cap
        delay = min(delay, self.config.max_delay_ms)

        # Add jitter if enabled
        if self.config.jitter:
            jitter_range = delay * self.config.jitter_ratio
            jitter = random.uniform(-jitter_range, jitter_range)
            delay = delay + jitter

        return int(delay)

    def get_metrics(self) -> dict:
        """Get retry metrics for monitoring."""
        return {
            'total_attempts': self.attempt_count,
            'max_attempts': self.config.max_attempts,
            'config': {
                'base_delay_ms': self.config.base_delay_ms,
                'max_delay_ms': self.config.max_delay_ms,
                'exponential_base': self.config.exponential_base,
                'jitter': self.config.jitter
            }
        }

# Example usage
def unreliable_api_call(data: dict) -> dict:
    """Simulates API call that may fail transiently."""
    # Implementation that may raise ConnectionError, TimeoutError, etc.
    pass

retry_config = RetryConfig(
    max_attempts=5,
    base_delay_ms=100,
    max_delay_ms=10000,
    exponential_base=2.0,
    jitter=True
)

retry_strategy = RetryStrategy(retry_config)

try:
    result = retry_strategy.execute(
        unreliable_api_call,
        data={'transaction_id': '12345'}
    )
    logger.info(f"API call succeeded: {result}")
except Exception as e:
    logger.error(f"API call failed after all retries: {e}")
\end{lstlisting}

\subsection{CircuitBreaker: Dependency Protection}

Circuit breakers prevent cascading failures by detecting failing dependencies and temporarily stopping requests. When error rate exceeds threshold, circuit "opens" and fails fast instead of waiting for timeouts. After cooldown period, circuit "half-opens" to test recovery.

\textbf{States:}
\begin{itemize}
    \item \textbf{CLOSED}: Normal operation, requests pass through
    \item \textbf{OPEN}: Failure threshold exceeded, fail fast without calling dependency
    \item \textbf{HALF\_OPEN}: Testing if dependency recovered, allow limited requests
\end{itemize}

\begin{lstlisting}[style=python, caption={Circuit Breaker Pattern}]
from enum import Enum
from datetime import datetime, timedelta
from typing import Callable, Any, Optional
from dataclasses import dataclass, field
import threading

class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"  # Normal operation
    OPEN = "open"  # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing recovery

@dataclass
class CircuitBreakerConfig:
    """Circuit breaker configuration."""
    failure_threshold: int = 5  # Failures before opening
    success_threshold: int = 2  # Successes to close from half-open
    timeout_seconds: int = 60  # Time before trying half-open
    window_size: int = 10  # Rolling window for error rate
    error_rate_threshold: float = 0.5  # 50% error rate trips breaker

class CircuitBreakerOpenError(Exception):
    """Raised when circuit breaker is open."""
    pass

class CircuitBreaker:
    """
    Implements circuit breaker pattern for dependency protection.

    Production considerations:
    1. Prevents cascading failures by failing fast when dependency down
    2. Automatic recovery testing via half-open state
    3. Rolling window prevents stale failure data
    4. Thread-safe for concurrent access
    5. Metrics for monitoring circuit state transitions

    Typical thresholds:
    - High-criticality: failure_threshold=3, timeout=30s (aggressive)
    - Medium-criticality: failure_threshold=5, timeout=60s (balanced)
    - Low-criticality: failure_threshold=10, timeout=120s (conservative)
    """

    def __init__(
        self,
        name: str,
        config: Optional[CircuitBreakerConfig] = None
    ):
        self.name = name
        self.config = config or CircuitBreakerConfig()

        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.last_state_change: datetime = datetime.now()

        # Rolling window for error rate calculation
        self.recent_results: list[bool] = []  # True=success, False=failure

        # Thread safety
        self.lock = threading.Lock()

        # Metrics
        self.total_calls = 0
        self.total_failures = 0
        self.total_rejections = 0

    def call(self, operation: Callable, *args, **kwargs) -> Any:
        """
        Execute operation through circuit breaker.

        Args:
            operation: Callable to execute
            *args: Positional arguments
            **kwargs: Keyword arguments

        Returns:
            Result of operation

        Raises:
            CircuitBreakerOpenError: If circuit is open
            Exception: If operation fails
        """
        with self.lock:
            self.total_calls += 1

            # Check if circuit is open
            if self.state == CircuitState.OPEN:
                # Check if timeout expired (try half-open)
                if self._should_attempt_reset():
                    self.state = CircuitState.HALF_OPEN
                    self.success_count = 0
                    logger.info(
                        f"Circuit breaker '{self.name}' entering HALF_OPEN state"
                    )
                else:
                    self.total_rejections += 1
                    raise CircuitBreakerOpenError(
                        f"Circuit breaker '{self.name}' is OPEN"
                    )

        # Execute operation
        try:
            result = operation(*args, **kwargs)
            self._record_success()
            return result

        except Exception as e:
            self._record_failure()
            raise

    def _record_success(self):
        """Record successful operation."""
        with self.lock:
            self.recent_results.append(True)
            self._trim_window()

            if self.state == CircuitState.HALF_OPEN:
                self.success_count += 1

                # Enough successes to close circuit
                if self.success_count >= self.config.success_threshold:
                    self._transition_to_closed()

            elif self.state == CircuitState.CLOSED:
                # Reset failure count on success
                self.failure_count = 0

    def _record_failure(self):
        """Record failed operation."""
        with self.lock:
            self.total_failures += 1
            self.recent_results.append(False)
            self._trim_window()
            self.last_failure_time = datetime.now()

            if self.state == CircuitState.HALF_OPEN:
                # Failure in half-open -> back to open
                self._transition_to_open()

            elif self.state == CircuitState.CLOSED:
                self.failure_count += 1

                # Check failure threshold
                if self.failure_count >= self.config.failure_threshold:
                    self._transition_to_open()

                # Check error rate
                error_rate = self._calculate_error_rate()
                if error_rate >= self.config.error_rate_threshold:
                    self._transition_to_open()

    def _should_attempt_reset(self) -> bool:
        """Check if enough time passed to attempt reset."""
        if self.last_failure_time is None:
            return True

        timeout_delta = timedelta(seconds=self.config.timeout_seconds)
        return datetime.now() - self.last_failure_time >= timeout_delta

    def _calculate_error_rate(self) -> float:
        """Calculate error rate from recent results."""
        if not self.recent_results:
            return 0.0

        failures = sum(1 for result in self.recent_results if not result)
        return failures / len(self.recent_results)

    def _trim_window(self):
        """Trim rolling window to configured size."""
        if len(self.recent_results) > self.config.window_size:
            self.recent_results = self.recent_results[-self.config.window_size:]

    def _transition_to_open(self):
        """Transition to OPEN state."""
        if self.state != CircuitState.OPEN:
            logger.warning(
                f"Circuit breaker '{self.name}' transitioning to OPEN. "
                f"Failure count: {self.failure_count}, "
                f"Error rate: {self._calculate_error_rate():.2%}"
            )
            self.state = CircuitState.OPEN
            self.last_state_change = datetime.now()

    def _transition_to_closed(self):
        """Transition to CLOSED state."""
        logger.info(
            f"Circuit breaker '{self.name}' transitioning to CLOSED. "
            f"Success count: {self.success_count}"
        )
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_state_change = datetime.now()

    def get_metrics(self) -> dict:
        """Get circuit breaker metrics."""
        with self.lock:
            return {
                'name': self.name,
                'state': self.state.value,
                'total_calls': self.total_calls,
                'total_failures': self.total_failures,
                'total_rejections': self.total_rejections,
                'failure_count': self.failure_count,
                'success_count': self.success_count,
                'error_rate': self._calculate_error_rate(),
                'last_state_change': self.last_state_change.isoformat(),
                'time_in_current_state': (
                    datetime.now() - self.last_state_change
                ).total_seconds()
            }

# Example usage
db_circuit_breaker = CircuitBreaker(
    name="postgresql_enrichment",
    config=CircuitBreakerConfig(
        failure_threshold=5,
        success_threshold=2,
        timeout_seconds=60,
        error_rate_threshold=0.5
    )
)

def enrich_with_customer_features(transaction: dict) -> dict:
    """Enrich transaction with customer features from database."""
    try:
        return db_circuit_breaker.call(
            database.get_customer_features,
            customer_id=transaction['customer_id']
        )
    except CircuitBreakerOpenError:
        # Circuit open - use cached/default features
        logger.warning("Using cached features due to circuit breaker")
        return get_cached_features(transaction['customer_id'])
    except Exception as e:
        logger.error(f"Enrichment failed: {e}")
        raise
\end{lstlisting}

\subsection{DeadLetterQueue: Failed Message Management}

Dead letter queues (DLQ) capture messages that cannot be processed successfully, preventing infinite retry loops while preserving failed messages for debugging and recovery.

\begin{lstlisting}[style=python, caption={Dead Letter Queue Implementation}]
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, Any, Dict
import json
from kafka import KafkaProducer
import logging

logger = logging.getLogger(__name__)

@dataclass
class DeadLetterMessage:
    """Message sent to dead letter queue."""
    original_topic: str
    original_partition: int
    original_offset: int
    original_message: dict
    error_type: str
    error_message: str
    error_category: str
    retry_count: int
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict:
        """Convert to dictionary for serialization."""
        return {
            'original_topic': self.original_topic,
            'original_partition': self.original_partition,
            'original_offset': self.original_offset,
            'original_message': self.original_message,
            'error_type': self.error_type,
            'error_message': self.error_message,
            'error_category': self.error_category,
            'retry_count': self.retry_count,
            'timestamp': self.timestamp.isoformat(),
            'metadata': self.metadata
        }

@dataclass
class DLQConfig:
    """Dead letter queue configuration."""
    enabled: bool = True
    topic_suffix: str = ".dlq"  # Append to original topic name
    max_message_size: int = 1048576  # 1MB
    include_stack_trace: bool = True
    retention_days: int = 30

class DeadLetterQueue:
    """
    Manages dead letter queue for failed message processing.

    Production considerations:
    1. Preserves failed messages for debugging and recovery
    2. Captures error context (exception, retry count, timestamps)
    3. Prevents infinite retry loops
    4. Enables batch reprocessing after fixes deployed
    5. Monitors DLQ size for alerting

    DLQ naming convention: {original_topic}.dlq
    Example: purchase_events -> purchase_events.dlq
    """

    def __init__(
        self,
        kafka_config: dict,
        config: Optional[DLQConfig] = None
    ):
        self.config = config or DLQConfig()

        if self.config.enabled:
            self.producer = KafkaProducer(
                bootstrap_servers=kafka_config['bootstrap_servers'],
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                # DLQ-specific config
                acks='all',  # Ensure DLQ messages persisted
                retries=3,  # Retry DLQ sends
                max_in_flight_requests_per_connection=1  # Preserve order
            )
        else:
            self.producer = None

        # Metrics
        self.messages_sent = 0
        self.messages_failed = 0
        self.errors_by_category: Dict[str, int] = {}

    def send(
        self,
        original_topic: str,
        original_partition: int,
        original_offset: int,
        message: dict,
        exception: Exception,
        retry_count: int,
        context: Optional[dict] = None
    ):
        """
        Send message to dead letter queue.

        Args:
            original_topic: Topic message came from
            original_partition: Partition message came from
            original_offset: Offset of original message
            message: Original message content
            exception: Exception that caused failure
            retry_count: Number of retry attempts made
            context: Additional context for debugging
        """
        if not self.config.enabled:
            logger.warning("DLQ disabled, dropping failed message")
            return

        # Classify error
        error_category = ErrorClassifier.classify_exception(
            exception,
            context=context or {}
        )

        # Create DLQ message
        dlq_message = DeadLetterMessage(
            original_topic=original_topic,
            original_partition=original_partition,
            original_offset=original_offset,
            original_message=message,
            error_type=type(exception).__name__,
            error_message=str(exception),
            error_category=error_category.value,
            retry_count=retry_count,
            metadata=context or {}
        )

        # Include stack trace if configured
        if self.config.include_stack_trace:
            import traceback
            dlq_message.metadata['stack_trace'] = traceback.format_exc()

        # Determine DLQ topic
        dlq_topic = f"{original_topic}{self.config.topic_suffix}"

        # Send to DLQ
        try:
            future = self.producer.send(
                dlq_topic,
                value=dlq_message.to_dict(),
                key=str(original_offset)
            )

            # Wait for confirmation
            record_metadata = future.get(timeout=10)

            self.messages_sent += 1
            self.errors_by_category[error_category.value] = \
                self.errors_by_category.get(error_category.value, 0) + 1

            logger.info(
                f"Sent message to DLQ: {dlq_topic}, "
                f"partition={record_metadata.partition}, "
                f"offset={record_metadata.offset}, "
                f"error_category={error_category.value}"
            )

        except Exception as e:
            self.messages_failed += 1
            logger.error(f"Failed to send message to DLQ: {e}")
            # Critical: DLQ send failed - message lost unless logged
            logger.error(
                f"LOST MESSAGE: topic={original_topic}, "
                f"partition={original_partition}, "
                f"offset={original_offset}, "
                f"message={json.dumps(message)}"
            )

    def get_metrics(self) -> dict:
        """Get DLQ metrics for monitoring."""
        return {
            'messages_sent': self.messages_sent,
            'messages_failed': self.messages_failed,
            'errors_by_category': self.errors_by_category,
            'config': {
                'enabled': self.config.enabled,
                'retention_days': self.config.retention_days
            }
        }

    def close(self):
        """Close DLQ producer."""
        if self.producer:
            self.producer.close()

# Example usage
dlq = DeadLetterQueue(
    kafka_config={'bootstrap_servers': ['kafka:9092']},
    config=DLQConfig(
        enabled=True,
        topic_suffix=".dlq",
        include_stack_trace=True,
        retention_days=30
    )
)

# In message processing loop
try:
    process_message(message)
except Exception as e:
    category = ErrorClassifier.classify_exception(e)

    if category == ErrorCategory.PERSISTENT:
        # Send to DLQ instead of retrying
        dlq.send(
            original_topic='purchase_events',
            original_partition=message.partition,
            original_offset=message.offset,
            message=message.value,
            exception=e,
            retry_count=3,
            context={'error_category': category.value}
        )
\end{lstlisting}

\subsection{ErrorHandler: Integrated Fault Tolerance}

The \texttt{ErrorHandler} integrates retry strategies, circuit breakers, and dead letter queues into a unified fault tolerance framework.

\begin{lstlisting}[style=python, caption={Comprehensive Error Handler}]
from typing import Callable, Optional, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ErrorHandlerConfig:
    """Configuration for error handler."""
    retry_config: RetryConfig = field(default_factory=RetryConfig)
    circuit_config: CircuitBreakerConfig = field(
        default_factory=CircuitBreakerConfig
    )
    dlq_config: DLQConfig = field(default_factory=DLQConfig)
    enable_circuit_breaker: bool = True
    enable_retry: bool = True
    enable_dlq: bool = True

class ErrorHandler:
    """
    Comprehensive error handling framework.

    Integrates three fault tolerance patterns:
    1. Retry strategy with exponential backoff (transient errors)
    2. Circuit breaker (dependency failures)
    3. Dead letter queue (persistent errors)

    Decision flow:
    1. Circuit breaker checks if dependency healthy
    2. Retry strategy handles transient failures
    3. DLQ captures persistent failures after retry exhaustion

    Production usage:
    - Wrap all external dependency calls
    - Configure thresholds based on SLAs
    - Monitor metrics for circuit state and DLQ size
    """

    def __init__(
        self,
        name: str,
        kafka_config: dict,
        config: Optional[ErrorHandlerConfig] = None
    ):
        self.name = name
        self.config = config or ErrorHandlerConfig()

        # Initialize components
        self.circuit_breaker: Optional[CircuitBreaker] = None
        if self.config.enable_circuit_breaker:
            self.circuit_breaker = CircuitBreaker(
                name=name,
                config=self.config.circuit_config
            )

        self.retry_strategy: Optional[RetryStrategy] = None
        if self.config.enable_retry:
            self.retry_strategy = RetryStrategy(
                config=self.config.retry_config
            )

        self.dlq: Optional[DeadLetterQueue] = None
        if self.config.enable_dlq:
            self.dlq = DeadLetterQueue(
                kafka_config=kafka_config,
                config=self.config.dlq_config
            )

        self.error_classifier = ErrorClassifier()

    def execute(
        self,
        operation: Callable,
        *args,
        message_context: Optional[dict] = None,
        **kwargs
    ) -> Any:
        """
        Execute operation with comprehensive error handling.

        Args:
            operation: Callable to execute
            *args: Positional arguments
            message_context: Kafka message context for DLQ
            **kwargs: Keyword arguments

        Returns:
            Result of operation

        Raises:
            Exception: If operation fails after all recovery attempts
        """
        message_context = message_context or {}

        # Define operation with circuit breaker
        def protected_operation():
            if self.circuit_breaker:
                return self.circuit_breaker.call(operation, *args, **kwargs)
            else:
                return operation(*args, **kwargs)

        # Execute with retry strategy
        try:
            if self.retry_strategy:
                return self.retry_strategy.execute(
                    protected_operation,
                    error_classifier=self.error_classifier
                )
            else:
                return protected_operation()

        except CircuitBreakerOpenError as e:
            # Circuit open - don't send to DLQ, this is expected
            logger.warning(f"Circuit breaker open for {self.name}")
            raise

        except Exception as e:
            # Classify error
            category = self.error_classifier.classify_exception(e)

            # Send persistent errors to DLQ
            if category == ErrorCategory.PERSISTENT and self.dlq:
                self.dlq.send(
                    original_topic=message_context.get('topic', 'unknown'),
                    original_partition=message_context.get('partition', -1),
                    original_offset=message_context.get('offset', -1),
                    message=message_context.get('message', {}),
                    exception=e,
                    retry_count=self.retry_strategy.attempt_count if self.retry_strategy else 0,
                    context={'error_category': category.value}
                )

                logger.info(
                    f"Message sent to DLQ after {self.retry_strategy.attempt_count} retries"
                )

                # Don't re-raise for persistent errors sent to DLQ
                return None

            # Re-raise for catastrophic or unhandled errors
            raise

    def get_metrics(self) -> dict:
        """Get comprehensive error handling metrics."""
        metrics = {
            'name': self.name,
            'components': {
                'circuit_breaker': self.config.enable_circuit_breaker,
                'retry_strategy': self.config.enable_retry,
                'dead_letter_queue': self.config.enable_dlq
            }
        }

        if self.circuit_breaker:
            metrics['circuit_breaker'] = self.circuit_breaker.get_metrics()

        if self.retry_strategy:
            metrics['retry_strategy'] = self.retry_strategy.get_metrics()

        if self.dlq:
            metrics['dlq'] = self.dlq.get_metrics()

        return metrics

# Example: Protecting database enrichment
error_handler = ErrorHandler(
    name="customer_enrichment",
    kafka_config={'bootstrap_servers': ['kafka:9092']},
    config=ErrorHandlerConfig(
        retry_config=RetryConfig(
            max_attempts=3,
            base_delay_ms=100,
            exponential_base=2.0
        ),
        circuit_config=CircuitBreakerConfig(
            failure_threshold=5,
            timeout_seconds=60
        ),
        dlq_config=DLQConfig(
            enabled=True,
            retention_days=7
        )
    )
)

# In stream processor
for message in consumer:
    try:
        event = Event.from_dict(message.value)

        # Protected enrichment call
        enriched_data = error_handler.execute(
            database.get_customer_features,
            customer_id=event.customer_id,
            message_context={
                'topic': message.topic,
                'partition': message.partition,
                'offset': message.offset,
                'message': message.value
            }
        )

        if enriched_data:
            # Process enriched event
            process_event(event, enriched_data)
        else:
            # Sent to DLQ or circuit open - use fallback
            process_event_with_defaults(event)

    except Exception as e:
        logger.error(f"Fatal error processing message: {e}")
        # Decide whether to halt or continue
\end{lstlisting}

\subsection{Production Monitoring and Alerting}

Error handling systems must be monitored to detect degradation before cascading failures:

\begin{lstlisting}[style=python, caption={Error Handling Metrics}]
from dataclasses import dataclass
from typing import Dict
from prometheus_client import Counter, Gauge, Histogram
import time

# Prometheus metrics
circuit_breaker_state = Gauge(
    'circuit_breaker_state',
    'Circuit breaker state (0=closed, 1=half_open, 2=open)',
    ['name']
)

circuit_breaker_failures = Counter(
    'circuit_breaker_failures_total',
    'Total circuit breaker failures',
    ['name']
)

circuit_breaker_rejections = Counter(
    'circuit_breaker_rejections_total',
    'Total circuit breaker rejections',
    ['name']
)

retry_attempts = Histogram(
    'retry_attempts',
    'Number of retry attempts per operation',
    ['operation']
)

dlq_messages = Counter(
    'dlq_messages_total',
    'Messages sent to dead letter queue',
    ['topic', 'error_category']
)

dlq_send_failures = Counter(
    'dlq_send_failures_total',
    'Failed attempts to send to DLQ',
    ['topic']
)

@dataclass
class ErrorMetrics:
    """Aggregated error handling metrics."""
    circuit_breaker_states: Dict[str, str]
    total_circuit_rejections: int
    total_dlq_messages: int
    dlq_messages_by_category: Dict[str, int]
    avg_retry_attempts: float

    def should_alert(self) -> list[str]:
        """Determine if alerting required."""
        alerts = []

        # Alert if any circuit breaker open
        open_circuits = [
            name for name, state in self.circuit_breaker_states.items()
            if state == 'open'
        ]
        if open_circuits:
            alerts.append(
                f"Circuit breakers OPEN: {', '.join(open_circuits)}"
            )

        # Alert if DLQ accumulating messages rapidly
        if self.total_dlq_messages > 1000:
            alerts.append(
                f"High DLQ volume: {self.total_dlq_messages} messages"
            )

        # Alert if excessive retries
        if self.avg_retry_attempts > 2.0:
            alerts.append(
                f"High retry rate: avg {self.avg_retry_attempts:.1f} attempts"
            )

        return alerts
\end{lstlisting}

\textbf{Key monitoring metrics:}
\begin{itemize}
    \item \textbf{Circuit breaker state}: Open circuits indicate dependency outages
    \item \textbf{DLQ message rate}: Increasing rate indicates data quality issues
    \item \textbf{Retry attempt distribution}: High retries indicate infrastructure instability
    \item \textbf{Error category distribution}: Shift to catastrophic errors requires immediate action
\end{itemize}

\textbf{Alert thresholds:}
\begin{itemize}
    \item Circuit breaker open >5 minutes → Page on-call
    \item DLQ message rate >100/min → Warning
    \item DLQ message rate >1000/min → Page on-call
    \item Average retry attempts >3 → Warning
    \item Any catastrophic errors → Page on-call immediately
\end{itemize}

\section{Data Quality Validation and Graceful Degradation}

Error handling ensures systems recover from failures, but data quality validation ensures recovered systems process \textit{correct} data. A pipeline that successfully processes 1 million corrupt events is worse than one that fails fast on the first invalid record. Production event-driven systems require quality gates that automatically halt pipelines on validation failures, graceful degradation strategies for partial outages, and fallback mechanisms when primary data sources fail.

The tension between availability and correctness is fundamental: strict quality gates reduce availability (pipelines halt on any anomaly), while lenient validation risks propagating bad data through downstream systems. Production systems must balance these concerns through tiered validation: critical invariants trigger immediate halts, statistical anomalies trigger alerts, and minor issues log warnings while continuing processing.

\subsection{The Data Quality Catastrophe}

Consider a real-time recommendation system serving 100 million daily users:

\textbf{Architecture:}
\begin{itemize}
    \item User click events stream through Kafka (500K events/second)
    \item Stream processor computes real-time engagement features
    \item Features feed ML model predicting product recommendations
    \item Recommendations displayed on homepage and product pages
    \item Business metrics: \$50M daily GMV (Gross Merchandise Value)
\end{itemize}

\textbf{Monday 09:15 - Silent data corruption:}
\begin{itemize}
    \item Mobile app deployment introduces bug: click timestamps in milliseconds instead of seconds
    \item Example: \texttt{timestamp: 1699876543000} (year 53844) instead of \texttt{1699876543} (Nov 2023)
    \item Stream processor has no timestamp validation—accepts future dates
    \item Feature computation calculates "time since last click" as negative values
    \item Engagement features corrupted: \texttt{minutes\_since\_last\_click: -1.7e12}
\end{itemize}

\textbf{09:20 - Corruption propagates:}
\begin{itemize}
    \item ML model trained on features in range [0, 1440] (max 24 hours)
    \item Receives features with values like \texttt{-1.7e12, -8.5e11, -3.2e13}
    \item Model has no input validation—processes corrupt features
    \item Predictions become random (model outside training distribution)
    \item Recommendation quality collapses: CTR drops from 8\% to 1.2\%
\end{itemize}

\textbf{09:30 - Business impact escalates:}
\begin{itemize}
    \item Homepage shows irrelevant recommendations (winter coats in summer, baby products to singles)
    \item Customer engagement plummets: session duration -60\%, bounce rate +40\%
    \item Revenue impact: GMV down 35\% (\$2.9M/hour loss rate)
    \item Customer support tickets surge: "Why am I seeing these products?"
    \item No automatic quality gates triggered—system "working normally"
\end{itemize}

\textbf{09:45 - Downstream cascade:}
\begin{itemize}
    \item Corrupt features written to feature store
    \item Batch model training job starts at 10:00 using corrupt data
    \item Training detects NaN values (from overflow), crashes
    \item ML engineers investigate, discover 30 minutes of corrupt features
    \item All downstream features tainted—requires full reprocessing
\end{itemize}

\textbf{10:30 - Discovery and response:}
\begin{itemize}
    \item Business analyst notices GMV anomaly, alerts engineering
    \item Engineers discover mobile app timestamp bug
    \item Emergency fix deployed: app update + server-side timestamp override
    \item But damage done: 75 minutes of bad recommendations
    \item Feature store cleanup required: 45M corrupt feature records
\end{itemize}

\textbf{Root cause analysis:}
\begin{itemize}
    \item \textbf{No timestamp validation}: Stream processor accepted obviously invalid timestamps
    \item \textbf{No feature range validation}: ML model accepted features 12 orders of magnitude out of range
    \item \textbf{No quality gates}: No automated halting on statistical anomalies
    \item \textbf{No fallback strategy}: System had no graceful degradation to cached features
    \item \textbf{No business metric monitoring}: 30-minute delay detecting revenue impact
\end{itemize}

\textbf{Total impact:}
\begin{itemize}
    \item Revenue loss: \$3.6M (75 minutes at \$2.9M/hour)
    \item Feature store cleanup: 12 hours engineering time
    \item Model retraining delay: 24 hours (missed daily refresh)
    \item Customer trust damage: -15\% customer satisfaction score
    \item Regulatory reporting: Data quality incident report required
\end{itemize}

\textbf{What quality gates would have prevented this:}
\begin{itemize}
    \item \textbf{Timestamp range validation}: Reject events with timestamps >1 hour in future or >1 year in past
    \item \textbf{Feature range validation}: Halt pipeline if features exceed [min\_value, max\_value] from schema
    \item \textbf{Statistical anomaly detection}: Alert if feature distribution shifts >3 standard deviations
    \item \textbf{Business metric monitoring}: Automatic alert on CTR drop >20\%
    \item \textbf{Graceful degradation}: Fallback to cached features when input quality degrades
\end{itemize}

This scenario demonstrates why production pipelines require comprehensive data quality validation.

\subsection{Data Quality Validation Framework}

Quality validation must operate at multiple levels: schema validation (structure), semantic validation (business rules), and statistical validation (distribution).

\subsubsection{Quality Validation Rules}

\textbf{Schema Validation:}
\begin{itemize}
    \item Required fields present
    \item Data types correct
    \item Field formats valid (e.g., email regex, phone number pattern)
\end{itemize}

\textbf{Semantic Validation:}
\begin{itemize}
    \item Business invariants (e.g., price > 0, quantity > 0)
    \item Referential integrity (e.g., product\_id exists in catalog)
    \item Temporal constraints (e.g., order\_date <= ship\_date)
\end{itemize}

\textbf{Statistical Validation:}
\begin{itemize}
    \item Value range checks (e.g., age in [0, 120])
    \item Distribution monitoring (e.g., mean, stddev, percentiles)
    \item Anomaly detection (e.g., sudden spike in null values)
\end{itemize}

\begin{lstlisting}[style=python, caption={Quality Validation Rules}]
from dataclasses import dataclass, field
from typing import Optional, Callable, Any, List, Dict
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
import logging

logger = logging.getLogger(__name__)

class ValidationSeverity(Enum):
    """Severity level for validation failures."""
    CRITICAL = "critical"  # Halt pipeline immediately
    ERROR = "error"  # Log error, send to DLQ
    WARNING = "warning"  # Log warning, continue processing
    INFO = "info"  # Log info only

class ValidationType(Enum):
    """Type of validation check."""
    SCHEMA = "schema"
    SEMANTIC = "semantic"
    STATISTICAL = "statistical"
    BUSINESS = "business"

@dataclass
class ValidationRule:
    """
    A single validation rule.

    Production considerations:
    1. Severity determines pipeline behavior (halt vs. warn)
    2. Rule should be stateless and deterministic
    3. Error messages must be actionable for debugging
    """
    name: str
    validation_type: ValidationType
    severity: ValidationSeverity
    check: Callable[[Any], bool]
    error_message: str
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ValidationResult:
    """Result of validation check."""
    rule_name: str
    passed: bool
    severity: ValidationSeverity
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class QualityValidator:
    """
    Validates data quality using configurable rules.

    Production usage:
    1. Define rules for each event type
    2. Apply rules in stream processor before feature computation
    3. Route to DLQ or halt based on severity
    4. Monitor validation failure rates
    """

    def __init__(self, rules: Optional[List[ValidationRule]] = None):
        self.rules = rules or []

        # Metrics
        self.validations_performed = 0
        self.validations_failed = 0
        self.failures_by_rule: Dict[str, int] = {}
        self.failures_by_severity: Dict[str, int] = {}

    def add_rule(self, rule: ValidationRule):
        """Add validation rule."""
        self.rules.append(rule)

    def validate(self, data: dict) -> List[ValidationResult]:
        """
        Validate data against all rules.

        Args:
            data: Event data to validate

        Returns:
            List of validation results
        """
        self.validations_performed += 1
        results = []

        for rule in self.rules:
            try:
                passed = rule.check(data)

                result = ValidationResult(
                    rule_name=rule.name,
                    passed=passed,
                    severity=rule.severity,
                    error_message=None if passed else rule.error_message,
                    metadata=rule.metadata
                )

                if not passed:
                    self.validations_failed += 1
                    self.failures_by_rule[rule.name] = \
                        self.failures_by_rule.get(rule.name, 0) + 1
                    self.failures_by_severity[rule.severity.value] = \
                        self.failures_by_severity.get(rule.severity.value, 0) + 1

                    logger.warning(
                        f"Validation failed: {rule.name} ({rule.severity.value}): "
                        f"{rule.error_message}"
                    )

                results.append(result)

            except Exception as e:
                # Validation check itself failed
                logger.error(f"Validation rule {rule.name} raised exception: {e}")
                results.append(ValidationResult(
                    rule_name=rule.name,
                    passed=False,
                    severity=ValidationSeverity.ERROR,
                    error_message=f"Validation check failed: {e}"
                ))

        return results

    def has_critical_failures(self, results: List[ValidationResult]) -> bool:
        """Check if any critical validations failed."""
        return any(
            not r.passed and r.severity == ValidationSeverity.CRITICAL
            for r in results
        )

    def get_metrics(self) -> dict:
        """Get validation metrics."""
        return {
            'validations_performed': self.validations_performed,
            'validations_failed': self.validations_failed,
            'failure_rate': (
                self.validations_failed / self.validations_performed
                if self.validations_performed > 0 else 0
            ),
            'failures_by_rule': self.failures_by_rule,
            'failures_by_severity': self.failures_by_severity
        }

# Example: Validation rules for purchase events
def create_purchase_event_validator() -> QualityValidator:
    """Create validator for purchase events."""
    validator = QualityValidator()

    # Schema validation: Required fields
    validator.add_rule(ValidationRule(
        name="required_fields",
        validation_type=ValidationType.SCHEMA,
        severity=ValidationSeverity.CRITICAL,
        check=lambda d: all(
            field in d for field in ['transaction_id', 'customer_id', 'amount', 'timestamp']
        ),
        error_message="Missing required fields"
    ))

    # Schema validation: Data types
    validator.add_rule(ValidationRule(
        name="amount_type",
        validation_type=ValidationType.SCHEMA,
        severity=ValidationSeverity.CRITICAL,
        check=lambda d: isinstance(d.get('amount'), (int, float)),
        error_message="Amount must be numeric"
    ))

    # Semantic validation: Business rules
    validator.add_rule(ValidationRule(
        name="amount_positive",
        validation_type=ValidationType.SEMANTIC,
        severity=ValidationSeverity.CRITICAL,
        check=lambda d: d.get('amount', 0) > 0,
        error_message="Amount must be positive"
    ))

    validator.add_rule(ValidationRule(
        name="amount_reasonable",
        validation_type=ValidationType.SEMANTIC,
        severity=ValidationSeverity.WARNING,
        check=lambda d: d.get('amount', 0) < 100000,  # $100K threshold
        error_message="Amount exceeds typical transaction size",
        metadata={'threshold': 100000}
    ))

    # Temporal validation: Timestamp sanity
    validator.add_rule(ValidationRule(
        name="timestamp_valid",
        validation_type=ValidationType.SEMANTIC,
        severity=ValidationSeverity.CRITICAL,
        check=lambda d: _validate_timestamp(d.get('timestamp')),
        error_message="Timestamp outside valid range (>1 hour future or >1 year past)"
    ))

    # Referential integrity (example - requires external lookup)
    validator.add_rule(ValidationRule(
        name="customer_exists",
        validation_type=ValidationType.SEMANTIC,
        severity=ValidationSeverity.ERROR,
        check=lambda d: _customer_exists(d.get('customer_id')),
        error_message="Customer ID not found in system"
    ))

    return validator

def _validate_timestamp(timestamp: Any) -> bool:
    """Validate timestamp is within reasonable range."""
    try:
        if isinstance(timestamp, str):
            ts = datetime.fromisoformat(timestamp)
        elif isinstance(timestamp, (int, float)):
            # Check if milliseconds (>10 digits) vs seconds
            if timestamp > 1e10:  # Likely milliseconds
                timestamp = timestamp / 1000
            ts = datetime.fromtimestamp(timestamp)
        else:
            return False

        now = datetime.now()
        one_hour_future = now + timedelta(hours=1)
        one_year_past = now - timedelta(days=365)

        return one_year_past <= ts <= one_hour_future

    except (ValueError, OSError):
        return False

def _customer_exists(customer_id: Any) -> bool:
    """Check if customer exists (placeholder - would query database)."""
    # Production: Query customer database/cache
    return customer_id is not None

# Example usage
validator = create_purchase_event_validator()

event = {
    'transaction_id': 'tx_12345',
    'customer_id': 'cust_789',
    'amount': 49.99,
    'timestamp': 1699876543  # Valid timestamp
}

results = validator.validate(event)

if validator.has_critical_failures(results):
    logger.error("Critical validation failures - halting pipeline")
    # Send to DLQ
else:
    # Process event
    pass
\end{lstlisting}

\subsection{DataQualityGate: Automatic Pipeline Control}

Quality gates monitor validation metrics and automatically halt pipelines when failure rates exceed thresholds.

\begin{lstlisting}[style=python, caption={Data Quality Gate Implementation}]
from dataclasses import dataclass, field
from typing import Optional, List, Dict
from datetime import datetime, timedelta
from collections import deque
import threading

@dataclass
class QualityGateConfig:
    """Configuration for quality gate."""
    # Failure rate thresholds
    critical_failure_rate: float = 0.01  # 1% critical failures halts pipeline
    error_failure_rate: float = 0.05  # 5% error rate triggers alert
    warning_failure_rate: float = 0.10  # 10% warning rate triggers alert

    # Window size for rate calculation
    window_size: int = 1000  # Last 1000 events
    min_samples: int = 100  # Minimum samples before enforcement

    # Circuit breaker style behavior
    halt_duration_seconds: int = 300  # 5 minutes halt before retry

    # Statistical anomaly detection
    enable_anomaly_detection: bool = True
    anomaly_std_threshold: float = 3.0  # 3 standard deviations

class PipelineHaltException(Exception):
    """Raised when quality gate halts pipeline."""
    pass

class DataQualityGate:
    """
    Monitors data quality and controls pipeline execution.

    Production considerations:
    1. Halts pipeline on critical failure rate threshold
    2. Rolling window prevents stale metrics
    3. Minimum sample requirement prevents false alarms on startup
    4. Circuit breaker pattern: halt for cooldown, then retry
    5. Statistical anomaly detection for distribution shifts

    Typical configurations:
    - High-risk pipelines: critical_rate=0.001 (0.1%), halt immediately
    - Medium-risk: critical_rate=0.01 (1%), alert and investigate
    - Low-risk: critical_rate=0.05 (5%), log warnings
    """

    def __init__(self, config: Optional[QualityGateConfig] = None):
        self.config = config or QualityGateConfig()

        # State
        self.is_halted = False
        self.halt_time: Optional[datetime] = None
        self.halt_reason: Optional[str] = None

        # Rolling window of validation results
        self.recent_results: deque = deque(maxlen=self.config.window_size)

        # Statistical tracking
        self.baseline_metrics: Optional[Dict[str, float]] = None

        # Thread safety
        self.lock = threading.Lock()

        # Metrics
        self.total_events = 0
        self.total_halts = 0
        self.total_critical_failures = 0
        self.total_error_failures = 0
        self.total_warning_failures = 0

    def check_quality(self, validation_results: List[ValidationResult]):
        """
        Check quality gate based on validation results.

        Args:
            validation_results: Results from QualityValidator

        Raises:
            PipelineHaltException: If quality gate triggered
        """
        with self.lock:
            self.total_events += 1

            # Check if currently halted
            if self.is_halted:
                if self._should_resume():
                    logger.info("Quality gate resuming pipeline after cooldown")
                    self.is_halted = False
                    self.halt_time = None
                    self.halt_reason = None
                else:
                    raise PipelineHaltException(
                        f"Pipeline halted by quality gate: {self.halt_reason}. "
                        f"Halted at: {self.halt_time}"
                    )

            # Track results
            self.recent_results.append(validation_results)

            # Count failures by severity
            critical_failures = sum(
                1 for r in validation_results
                if not r.passed and r.severity == ValidationSeverity.CRITICAL
            )
            error_failures = sum(
                1 for r in validation_results
                if not r.passed and r.severity == ValidationSeverity.ERROR
            )
            warning_failures = sum(
                1 for r in validation_results
                if not r.passed and r.severity == ValidationSeverity.WARNING
            )

            self.total_critical_failures += critical_failures
            self.total_error_failures += error_failures
            self.total_warning_failures += warning_failures

            # Check minimum samples
            if len(self.recent_results) < self.config.min_samples:
                return  # Not enough data yet

            # Calculate failure rates
            rates = self._calculate_failure_rates()

            # Check thresholds
            if rates['critical'] >= self.config.critical_failure_rate:
                self._trigger_halt(
                    f"Critical failure rate {rates['critical']:.2%} exceeds "
                    f"threshold {self.config.critical_failure_rate:.2%}"
                )

            if rates['error'] >= self.config.error_failure_rate:
                logger.error(
                    f"Error failure rate {rates['error']:.2%} exceeds "
                    f"threshold {self.config.error_failure_rate:.2%}"
                )

            if rates['warning'] >= self.config.warning_failure_rate:
                logger.warning(
                    f"Warning failure rate {rates['warning']:.2%} exceeds "
                    f"threshold {self.config.warning_failure_rate:.2%}"
                )

            # Statistical anomaly detection
            if self.config.enable_anomaly_detection:
                self._check_anomalies(rates)

    def _calculate_failure_rates(self) -> Dict[str, float]:
        """Calculate failure rates from recent results."""
        total = len(self.recent_results)
        if total == 0:
            return {'critical': 0, 'error': 0, 'warning': 0}

        critical_count = 0
        error_count = 0
        warning_count = 0

        for results in self.recent_results:
            for r in results:
                if not r.passed:
                    if r.severity == ValidationSeverity.CRITICAL:
                        critical_count += 1
                    elif r.severity == ValidationSeverity.ERROR:
                        error_count += 1
                    elif r.severity == ValidationSeverity.WARNING:
                        warning_count += 1

        return {
            'critical': critical_count / total,
            'error': error_count / total,
            'warning': warning_count / total
        }

    def _check_anomalies(self, current_rates: Dict[str, float]):
        """Check for statistical anomalies in failure rates."""
        # Establish baseline if not set
        if self.baseline_metrics is None and len(self.recent_results) >= 100:
            self.baseline_metrics = current_rates.copy()
            return

        if self.baseline_metrics is None:
            return

        # Check for significant deviation from baseline
        for severity, current_rate in current_rates.items():
            baseline_rate = self.baseline_metrics.get(severity, 0)

            # Simple anomaly: rate increased by more than threshold * baseline
            if current_rate > baseline_rate * (1 + self.config.anomaly_std_threshold):
                logger.warning(
                    f"Anomaly detected: {severity} failure rate {current_rate:.2%} "
                    f"significantly higher than baseline {baseline_rate:.2%}"
                )

    def _trigger_halt(self, reason: str):
        """Trigger pipeline halt."""
        self.is_halted = True
        self.halt_time = datetime.now()
        self.halt_reason = reason
        self.total_halts += 1

        logger.critical(f"QUALITY GATE HALTED PIPELINE: {reason}")

        raise PipelineHaltException(reason)

    def _should_resume(self) -> bool:
        """Check if enough time passed to resume pipeline."""
        if self.halt_time is None:
            return True

        elapsed = (datetime.now() - self.halt_time).total_seconds()
        return elapsed >= self.config.halt_duration_seconds

    def manual_resume(self):
        """Manually resume pipeline (e.g., after fixing data source)."""
        with self.lock:
            self.is_halted = False
            self.halt_time = None
            self.halt_reason = None
            logger.info("Quality gate manually resumed")

    def get_metrics(self) -> dict:
        """Get quality gate metrics."""
        with self.lock:
            rates = self._calculate_failure_rates()
            return {
                'is_halted': self.is_halted,
                'halt_reason': self.halt_reason,
                'total_events': self.total_events,
                'total_halts': self.total_halts,
                'total_critical_failures': self.total_critical_failures,
                'total_error_failures': self.total_error_failures,
                'total_warning_failures': self.total_warning_failures,
                'current_failure_rates': rates,
                'baseline_metrics': self.baseline_metrics,
                'window_size': len(self.recent_results)
            }

# Example usage
quality_gate = DataQualityGate(
    config=QualityGateConfig(
        critical_failure_rate=0.01,  # Halt at 1% critical failures
        error_failure_rate=0.05,  # Alert at 5% errors
        window_size=1000,
        min_samples=100
    )
)

validator = create_purchase_event_validator()

# In stream processing loop
for message in consumer:
    try:
        event = Event.from_dict(message.value)

        # Validate event
        validation_results = validator.validate(event.__dict__)

        # Check quality gate
        quality_gate.check_quality(validation_results)

        # Process event
        process_event(event)

    except PipelineHaltException as e:
        logger.critical(f"Pipeline halted by quality gate: {e}")
        # Alert on-call engineer
        # Halt processing until issue resolved
        break
    except Exception as e:
        logger.error(f"Error processing event: {e}")
\end{lstlisting}

\subsection{Graceful Degradation with Fallback Strategies}

When primary data sources fail or quality degrades, systems should degrade gracefully rather than halt completely. Fallback strategies provide approximate data maintaining partial functionality.

\begin{lstlisting}[style=python, caption={Fallback Manager for Graceful Degradation}]
from typing import Optional, Callable, Any, List
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class FallbackStrategy(Enum):
    """Fallback strategy types."""
    CACHED_VALUE = "cached"  # Use last known good value
    DEFAULT_VALUE = "default"  # Use configured default
    HISTORICAL_AVERAGE = "historical_avg"  # Use historical average
    APPROXIMATE = "approximate"  # Use approximation algorithm
    SKIP = "skip"  # Skip enrichment, continue with partial data

@dataclass
class FallbackConfig:
    """Configuration for fallback behavior."""
    strategy: FallbackStrategy
    cache_ttl_seconds: int = 3600  # 1 hour cache validity
    default_value: Any = None
    enable_staleness_warning: bool = True
    max_staleness_seconds: int = 300  # 5 minutes max staleness

@dataclass
class FallbackResult:
    """Result of fallback operation."""
    success: bool
    value: Any
    strategy_used: FallbackStrategy
    is_stale: bool = False
    staleness_seconds: Optional[float] = None
    error: Optional[str] = None

class FallbackManager:
    """
    Manages fallback strategies for graceful degradation.

    Production considerations:
    1. Maintains cache of recent successful values
    2. Configurable staleness tolerance
    3. Tracks fallback usage for monitoring
    4. Prioritizes fallback strategies by data quality

    Fallback hierarchy (best to worst):
    1. Recent cached value (<5 min old)
    2. Historical average (last 24 hours)
    3. Stale cached value (>5 min old)
    4. Default value
    5. Skip enrichment
    """

    def __init__(self, name: str):
        self.name = name

        # Cache: key -> (value, timestamp)
        self.cache: Dict[str, tuple[Any, datetime]] = {}

        # Historical values for averaging
        self.historical_values: Dict[str, List[tuple[Any, datetime]]] = {}

        # Metrics
        self.total_fallbacks = 0
        self.fallbacks_by_strategy: Dict[str, int] = {}
        self.cache_hits = 0
        self.cache_misses = 0

    def execute_with_fallback(
        self,
        key: str,
        primary_operation: Callable[[], Any],
        config: FallbackConfig
    ) -> FallbackResult:
        """
        Execute operation with fallback on failure.

        Args:
            key: Cache key for this operation
            primary_operation: Primary data fetch operation
            config: Fallback configuration

        Returns:
            FallbackResult with value and metadata
        """
        # Try primary operation
        try:
            value = primary_operation()

            # Success - update cache and historical values
            self._update_cache(key, value)
            self._update_historical(key, value)

            return FallbackResult(
                success=True,
                value=value,
                strategy_used=FallbackStrategy.CACHED_VALUE,  # For next time
                is_stale=False
            )

        except Exception as e:
            logger.warning(
                f"Primary operation failed for {self.name}:{key}: {e}. "
                f"Attempting fallback strategy: {config.strategy.value}"
            )

            # Primary failed - use fallback
            self.total_fallbacks += 1
            self.fallbacks_by_strategy[config.strategy.value] = \
                self.fallbacks_by_strategy.get(config.strategy.value, 0) + 1

            return self._execute_fallback(key, config, error=str(e))

    def _execute_fallback(
        self,
        key: str,
        config: FallbackConfig,
        error: str
    ) -> FallbackResult:
        """Execute configured fallback strategy."""

        if config.strategy == FallbackStrategy.CACHED_VALUE:
            return self._fallback_cached(key, config, error)

        elif config.strategy == FallbackStrategy.HISTORICAL_AVERAGE:
            return self._fallback_historical_avg(key, config, error)

        elif config.strategy == FallbackStrategy.DEFAULT_VALUE:
            return FallbackResult(
                success=True,
                value=config.default_value,
                strategy_used=FallbackStrategy.DEFAULT_VALUE,
                is_stale=True,
                error=error
            )

        elif config.strategy == FallbackStrategy.SKIP:
            return FallbackResult(
                success=False,
                value=None,
                strategy_used=FallbackStrategy.SKIP,
                error=error
            )

        else:
            logger.error(f"Unknown fallback strategy: {config.strategy}")
            return FallbackResult(
                success=False,
                value=None,
                strategy_used=config.strategy,
                error=f"Unknown strategy: {config.strategy}"
            )

    def _fallback_cached(
        self,
        key: str,
        config: FallbackConfig,
        error: str
    ) -> FallbackResult:
        """Fallback to cached value."""
        if key not in self.cache:
            self.cache_misses += 1
            logger.warning(f"Cache miss for {self.name}:{key}")
            # No cache - try next best strategy
            return self._execute_fallback(
                key,
                FallbackConfig(strategy=FallbackStrategy.HISTORICAL_AVERAGE),
                error
            )

        value, timestamp = self.cache[key]
        staleness = (datetime.now() - timestamp).total_seconds()

        # Check if cache too stale
        if staleness > config.cache_ttl_seconds:
            logger.warning(
                f"Cached value for {self.name}:{key} is stale "
                f"({staleness:.0f}s > {config.cache_ttl_seconds}s)"
            )
            # Try historical average instead
            return self._execute_fallback(
                key,
                FallbackConfig(strategy=FallbackStrategy.HISTORICAL_AVERAGE),
                error
            )

        self.cache_hits += 1
        is_stale = staleness > config.max_staleness_seconds

        if is_stale and config.enable_staleness_warning:
            logger.warning(
                f"Using stale cached value for {self.name}:{key} "
                f"(age: {staleness:.0f}s)"
            )

        return FallbackResult(
            success=True,
            value=value,
            strategy_used=FallbackStrategy.CACHED_VALUE,
            is_stale=is_stale,
            staleness_seconds=staleness,
            error=error
        )

    def _fallback_historical_avg(
        self,
        key: str,
        config: FallbackConfig,
        error: str
    ) -> FallbackResult:
        """Fallback to historical average."""
        if key not in self.historical_values or not self.historical_values[key]:
            logger.warning(f"No historical values for {self.name}:{key}")
            # No history - use default
            return self._execute_fallback(
                key,
                FallbackConfig(
                    strategy=FallbackStrategy.DEFAULT_VALUE,
                    default_value=config.default_value
                ),
                error
            )

        # Get recent historical values (last 24 hours)
        cutoff = datetime.now() - timedelta(hours=24)
        recent_values = [
            value for value, timestamp in self.historical_values[key]
            if timestamp >= cutoff
        ]

        if not recent_values:
            logger.warning(f"No recent historical values for {self.name}:{key}")
            return self._execute_fallback(
                key,
                FallbackConfig(
                    strategy=FallbackStrategy.DEFAULT_VALUE,
                    default_value=config.default_value
                ),
                error
            )

        # Compute average (assuming numeric values)
        try:
            avg_value = sum(recent_values) / len(recent_values)
            logger.info(
                f"Using historical average for {self.name}:{key}: "
                f"{avg_value} (n={len(recent_values)})"
            )

            return FallbackResult(
                success=True,
                value=avg_value,
                strategy_used=FallbackStrategy.HISTORICAL_AVERAGE,
                is_stale=True,
                error=error
            )

        except (TypeError, ZeroDivisionError) as e:
            logger.error(f"Failed to compute historical average: {e}")
            return self._execute_fallback(
                key,
                FallbackConfig(
                    strategy=FallbackStrategy.DEFAULT_VALUE,
                    default_value=config.default_value
                ),
                error
            )

    def _update_cache(self, key: str, value: Any):
        """Update cache with successful value."""
        self.cache[key] = (value, datetime.now())

    def _update_historical(self, key: str, value: Any):
        """Update historical values."""
        if key not in self.historical_values:
            self.historical_values[key] = []

        self.historical_values[key].append((value, datetime.now()))

        # Trim old values (keep last 1000)
        if len(self.historical_values[key]) > 1000:
            self.historical_values[key] = self.historical_values[key][-1000:]

    def get_metrics(self) -> dict:
        """Get fallback metrics."""
        return {
            'name': self.name,
            'total_fallbacks': self.total_fallbacks,
            'fallbacks_by_strategy': self.fallbacks_by_strategy,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'cache_hit_rate': (
                self.cache_hits / (self.cache_hits + self.cache_misses)
                if (self.cache_hits + self.cache_misses) > 0 else 0
            ),
            'cache_size': len(self.cache),
            'historical_size': sum(
                len(values) for values in self.historical_values.values()
            )
        }

# Example usage
fallback_manager = FallbackManager(name="customer_features")

def get_customer_features(customer_id: str) -> dict:
    """Fetch customer features from database (may fail)."""
    # This may raise exception if database down
    return database.get_features(customer_id)

# In stream processor with graceful degradation
for message in consumer:
    event = Event.from_dict(message.value)

    # Try to enrich with fallback
    result = fallback_manager.execute_with_fallback(
        key=event.customer_id,
        primary_operation=lambda: get_customer_features(event.customer_id),
        config=FallbackConfig(
            strategy=FallbackStrategy.CACHED_VALUE,
            cache_ttl_seconds=3600,
            max_staleness_seconds=300
        )
    )

    if result.success:
        if result.is_stale:
            logger.warning(
                f"Using stale features (strategy: {result.strategy_used.value})"
            )
        features = result.value
    else:
        logger.error("All fallback strategies failed - using defaults")
        features = get_default_features()

    # Process with features (fresh, stale, or default)
    process_event(event, features)
\end{lstlisting}

\subsection{DataApproximator: Emergency Operations}

When all fallback strategies fail, data approximation provides "good enough" values to maintain critical business functions.

\begin{lstlisting}[style=python, caption={Data Approximation Strategies}]
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ApproximationStrategy:
    """Strategy for approximating missing data."""
    name: str
    description: str
    confidence_score: float  # 0-1, how confident in approximation

class DataApproximator:
    """
    Provides data approximations during outages.

    Production use cases:
    1. Customer features unavailable -> use segment averages
    2. Real-time inventory unavailable -> use yesterday's snapshot
    3. Pricing service down -> use cached catalog prices
    4. ML model unavailable -> use simple heuristic rules

    Key principle: Approximate data with known confidence
    is better than no data (graceful degradation).
    """

    @staticmethod
    def approximate_customer_features(
        customer_id: str,
        segment: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Approximate customer features using segment averages.

        When customer database unavailable, use:
        1. Segment-level averages (if segment known)
        2. Global population averages (if segment unknown)

        Returns features with confidence scores.
        """
        # Segment-based approximations
        segment_averages = {
            'premium': {
                'lifetime_value': 2500.0,
                'avg_order_value': 150.0,
                'purchase_frequency_days': 15,
                'confidence': 0.7
            },
            'regular': {
                'lifetime_value': 500.0,
                'avg_order_value': 50.0,
                'purchase_frequency_days': 45,
                'confidence': 0.6
            },
            'new': {
                'lifetime_value': 100.0,
                'avg_order_value': 35.0,
                'purchase_frequency_days': 90,
                'confidence': 0.5
            }
        }

        # Global averages (fallback)
        global_averages = {
            'lifetime_value': 800.0,
            'avg_order_value': 65.0,
            'purchase_frequency_days': 50,
            'confidence': 0.4
        }

        if segment and segment in segment_averages:
            features = segment_averages[segment]
            logger.info(
                f"Using segment '{segment}' averages for customer {customer_id} "
                f"(confidence: {features['confidence']})"
            )
        else:
            features = global_averages
            logger.warning(
                f"Using global averages for customer {customer_id} "
                f"(confidence: {features['confidence']})"
            )

        return features

    @staticmethod
    def approximate_inventory(
        product_id: str,
        historical_inventory: Optional[Dict[str, int]] = None
    ) -> int:
        """
        Approximate inventory when real-time unavailable.

        Strategies:
        1. Use yesterday's end-of-day snapshot
        2. Use average inventory level
        3. Conservative estimate (assume low stock)
        """
        if historical_inventory and product_id in historical_inventory:
            # Use yesterday's value
            inventory = historical_inventory[product_id]
            logger.info(
                f"Using historical inventory for {product_id}: {inventory} units"
            )
            return inventory

        # Conservative estimate (prevent overselling)
        logger.warning(
            f"No historical inventory for {product_id}, using conservative estimate"
        )
        return 10  # Assume low but available

    @staticmethod
    def approximate_fraud_score(
        transaction: Dict[str, Any],
        simple_rules: bool = True
    ) -> float:
        """
        Approximate fraud score when ML model unavailable.

        Simple heuristic rules:
        1. High amounts (>$1000) -> higher risk
        2. International transactions -> higher risk
        3. Velocity (multiple transactions) -> higher risk

        Returns score 0-1 (1 = highest fraud risk).
        """
        score = 0.0

        amount = transaction.get('amount', 0)
        if amount > 1000:
            score += 0.3
        elif amount > 5000:
            score += 0.5

        if transaction.get('country') != transaction.get('customer_country'):
            score += 0.2

        # Simple velocity check (would need recent transaction cache)
        if transaction.get('recent_transaction_count', 0) > 5:
            score += 0.3

        # Cap at 1.0
        score = min(score, 1.0)

        logger.info(
            f"Heuristic fraud score for transaction {transaction.get('id')}: "
            f"{score:.2f}"
        )

        return score

    @staticmethod
    def get_approximation_confidence(
        approximation_type: str,
        data_availability: float
    ) -> float:
        """
        Calculate confidence in approximation.

        Args:
            approximation_type: Type of approximation
            data_availability: Fraction of primary data available (0-1)

        Returns:
            Confidence score 0-1
        """
        # Base confidence by approximation type
        base_confidence = {
            'cached_recent': 0.9,  # <1 hour old
            'cached_stale': 0.6,  # >1 hour old
            'segment_average': 0.7,
            'global_average': 0.4,
            'heuristic_rules': 0.5,
            'default_value': 0.2
        }

        base = base_confidence.get(approximation_type, 0.3)

        # Adjust based on data availability
        confidence = base * (0.5 + 0.5 * data_availability)

        return confidence

# Example: Using approximations in production
def enrich_event_with_approximation(event: dict) -> dict:
    """Enrich event with approximations when primary sources fail."""

    try:
        # Try primary source
        features = database.get_customer_features(event['customer_id'])
        event['features'] = features
        event['approximated'] = False
        event['confidence'] = 1.0

    except Exception as e:
        logger.warning(f"Primary features unavailable: {e}. Using approximation.")

        # Fall back to approximation
        features = DataApproximator.approximate_customer_features(
            customer_id=event['customer_id'],
            segment=event.get('customer_segment')
        )

        event['features'] = {
            k: v for k, v in features.items() if k != 'confidence'
        }
        event['approximated'] = True
        event['confidence'] = features['confidence']

        # Log approximation for audit trail
        logger.info(
            f"Event {event['id']} enriched with approximated features "
            f"(confidence: {event['confidence']})"
        )

    return event
\end{lstlisting}

\subsection{Quality Metrics and Monitoring}

Production systems must monitor data quality metrics to detect degradation early:

\begin{lstlisting}[style=python, caption={Quality Monitoring Dashboard}]
from dataclasses import dataclass
from typing import Dict, List
from prometheus_client import Counter, Gauge, Histogram

# Prometheus metrics
validation_failures = Counter(
    'validation_failures_total',
    'Total validation failures',
    ['rule_name', 'severity']
)

quality_gate_halts = Counter(
    'quality_gate_halts_total',
    'Total quality gate halts',
    ['reason']
)

fallback_operations = Counter(
    'fallback_operations_total',
    'Total fallback operations',
    ['strategy']
)

approximation_confidence = Histogram(
    'approximation_confidence',
    'Confidence scores for approximated data',
    ['approximation_type']
)

data_staleness_seconds = Histogram(
    'data_staleness_seconds',
    'Age of cached/fallback data in seconds',
    buckets=[10, 30, 60, 300, 600, 1800, 3600]  # 10s to 1h
)

@dataclass
class QualityMetrics:
    """Aggregated data quality metrics."""
    validation_failure_rate: float
    quality_gate_halt_count: int
    fallback_usage_rate: float
    approximation_usage_rate: float
    avg_data_staleness_seconds: float
    confidence_score: float  # Overall confidence in data quality

    def is_healthy(self) -> bool:
        """Check if data quality is healthy."""
        return (
            self.validation_failure_rate < 0.05 and  # <5% failures
            self.quality_gate_halt_count == 0 and
            self.fallback_usage_rate < 0.10 and  # <10% fallbacks
            self.avg_data_staleness_seconds < 300 and  # <5 min stale
            self.confidence_score > 0.8  # >80% confidence
        )

    def get_alerts(self) -> List[str]:
        """Get alerts based on thresholds."""
        alerts = []

        if self.validation_failure_rate > 0.10:
            alerts.append(
                f"HIGH validation failure rate: {self.validation_failure_rate:.1%}"
            )

        if self.quality_gate_halt_count > 0:
            alerts.append(
                f"Quality gate halted pipeline {self.quality_gate_halt_count} times"
            )

        if self.fallback_usage_rate > 0.20:
            alerts.append(
                f"HIGH fallback usage: {self.fallback_usage_rate:.1%} of operations"
            )

        if self.avg_data_staleness_seconds > 600:
            alerts.append(
                f"HIGH data staleness: {self.avg_data_staleness_seconds:.0f}s average"
            )

        if self.confidence_score < 0.6:
            alerts.append(
                f"LOW data confidence: {self.confidence_score:.1%}"
            )

        return alerts
\end{lstlisting}

\textbf{Key quality metrics to monitor:}
\begin{itemize}
    \item \textbf{Validation failure rate}: Percentage of events failing validation
    \item \textbf{Quality gate halts}: Number of automatic pipeline halts
    \item \textbf{Fallback usage rate}: Percentage of operations using fallback
    \item \textbf{Data staleness}: Age of cached/fallback data
    \item \textbf{Approximation confidence}: Average confidence in approximated data
    \item \textbf{Schema evolution failures}: Breaking changes in event schemas
\end{itemize}

\textbf{Alert thresholds:}
\begin{itemize}
    \item Validation failure rate >5\% → Warning
    \item Validation failure rate >10\% → Page on-call
    \item Any quality gate halt → Page on-call immediately
    \item Fallback usage >20\% → Warning (primary source degraded)
    \item Average confidence <60\% → Warning (data quality compromised)
    \item Data staleness >10 minutes → Warning
\end{itemize}

\section{Disaster Recovery and Automated Backup}

Fault tolerance handles transient failures, but disasters require complete system restoration. Production event-driven pipelines process billions of dollars in transactions daily—a catastrophic failure without backup means permanent data loss and business continuity failure. Unlike batch systems where data lives in databases, event-driven systems must backup streaming state, Kafka topics, feature stores, and processing checkpoints across multiple regions.

The difference between recovery time objective (RTO) and actual recovery time is often the difference between controlled failover and business-ending outage. A pipeline with 1-hour RTO that actually takes 6 hours to restore fails regulatory requirements, breaks SLAs, and loses customer trust. Production DR systems require automated backup procedures, cross-region replication, validated restore processes, and quarterly disaster recovery drills.

\subsection{The Midnight Pipeline Failure}

Consider a high-frequency trading platform processing market data and executing trades:

\textbf{Architecture:}
\begin{itemize}
    \item Kafka cluster ingesting 2M market events/second (NYSE, NASDAQ, crypto exchanges)
    \item Stream processor computing trading signals (moving averages, momentum indicators)
    \item Trading engine executing algorithmic orders (\$500M daily trading volume)
    \item Feature store maintaining 10TB of historical price data
    \item Critical SLA: <100ms end-to-end latency, 99.99\% uptime
\end{itemize}

\textbf{Business criticality:}
\begin{itemize}
    \item Market hours: 9:30 AM - 4:00 PM EST (6.5 hours)
    \item Pre-market and after-hours trading: 4:00 AM - 8:00 PM EST (16 hours)
    \item Every minute of downtime during market hours costs \$50K in missed opportunities
    \item Regulatory requirement: All trades auditable for 7 years
    \item RTO: 15 minutes, RPO: 0 (zero data loss)
\end{itemize}

\textbf{Friday 23:45 - Hardware failure:}
\begin{itemize}
    \item Primary datacenter (US-East) loses entire Kafka cluster (hardware failure in storage array)
    \item Kafka replication to secondary region exists, but...
    \item Stream processor state not backed up (processing 2GB of in-memory windowed aggregations)
    \item Feature store last backup: 6 hours old (daily backup at 18:00)
    \item On-call engineer paged: "Kafka cluster unreachable"
\end{itemize}

\textbf{23:50 - Initial assessment:}
\begin{itemize}
    \item Engineer confirms: Primary Kafka cluster dead (RAID controller failure, 12 disk array destroyed)
    \item Secondary Kafka cluster in US-West has replicated topics, BUT:
    \item Consumer group offsets not replicated (would restart from beginning or end)
    \item Stream processor state lost (6 hours of aggregated trading signals)
    \item Feature store in US-West is 6 hours stale
    \item Critical decision: Asian markets open in 5 hours (4:00 AM EST)
\end{itemize}

\textbf{00:15 - Recovery attempt begins:}
\begin{itemize}
    \item Attempt 1: Failover to US-West Kafka cluster
    \item Problem: Consumer offsets missing, would reprocess 6 hours of data (360M events)
    \item Reprocessing time estimate: 4 hours (not enough time before market open)
    \item Attempt 2: Restore feature store from backup
    \item Problem: 6-hour-old backup missing critical evening trading data
    \item Using stale features would generate invalid trading signals
\end{itemize}

\textbf{01:30 - Escalation:}
\begin{itemize}
    \item CTO paged: "Cannot meet 4:00 AM market open deadline"
    \item Business impact assessment:
        \begin{itemize}
            \item Asian market hours: 4:00 AM - 8:00 AM (4 hours offline)
            \item European market hours: 9:00 AM - 11:30 AM (2.5 hours offline)
            \item If not fixed by 9:30 AM: Miss US market open (\$3M+ lost revenue)
        \end{itemize}
    \item Decision: Manual rebuild of feature store from archived trades
\end{itemize}

\textbf{02:00 - Manual recovery:}
\begin{itemize}
    \item Engineers manually rebuild 6 hours of missing feature store data
    \item Querying trade archive database (not optimized for bulk reads)
    \item Recomputing 6 hours of streaming aggregations in batch mode
    \item Progress: 1 hour of data processed per 45 minutes of engineering time
    \item Estimate: 4.5 hours to completion → 6:30 AM (misses Asian market open)
\end{itemize}

\textbf{04:00 - Asian market opens without trading:}
\begin{itemize}
    \item Platform still offline (50\% through manual rebuild)
    \item Missed trading opportunities: \$800K (estimated)
    \item Client notifications: "Technical difficulties, trading suspended"
    \item Regulatory reporting: Exchange notifications required
\end{itemize}

\textbf{06:45 - System restored:}
\begin{itemize}
    \item Feature store rebuilt and validated
    \item Stream processors restarted with recovered state
    \item End-to-end testing: 15 minutes
    \item System online: 7:00 AM EST
    \item Total downtime: 7 hours 15 minutes
\end{itemize}

\textbf{Final impact:}
\begin{itemize}
    \item Revenue loss: \$2.1M (missed Asian and early European trading)
    \item Regulatory fines: \$150K (delayed exchange notifications)
    \item Engineering costs: \$45K (overnight emergency response, 6 engineers × 8 hours)
    \item Client trust: 3 institutional clients suspended trading (investigating alternatives)
    \item SLA breach: 99.99\% uptime requirement violated (quarterly penalties: \$500K)
\end{itemize}

\textbf{Root cause analysis:}
\begin{itemize}
    \item \textbf{No automated failover}: Manual intervention required for region switching
    \item \textbf{Consumer offset replication missing}: Couldn't resume from last processed event
    \item \textbf{Stream processor state not backed up}: Lost 6 hours of in-memory aggregations
    \item \textbf{Backup frequency too low}: 6-hour gap exceeded RPO requirement (0 data loss)
    \item \textbf{No DR drills}: Never tested actual recovery time (assumed 15 min, actual 7+ hours)
    \item \textbf{No automated restore procedures}: Manual rebuild took 4.5 hours
\end{itemize}

\textbf{What proper DR would have prevented:}
\begin{itemize}
    \item \textbf{Continuous replication}: Consumer offsets replicated every 30 seconds
    \item \textbf{Stream processor state snapshots}: Checkpoints every 5 minutes to S3
    \item \textbf{Incremental feature store backups}: Every 15 minutes (RPO: 15 min)
    \item \textbf{Automated failover}: Detect failure and switch regions in <5 minutes
    \item \textbf{Validated restore procedures}: Tested monthly, guaranteed 15-minute RTO
    \item \textbf{Cross-region read replicas}: Feature store replicated real-time (lag <1 min)
\end{itemize}

With proper DR: Total downtime would have been 12 minutes (within 15-minute RTO), zero trading impact.

This scenario demonstrates why production pipelines require comprehensive disaster recovery planning.

\subsection{Recovery Time and Recovery Point Objectives}

RTO (Recovery Time Objective) and RPO (Recovery Point Objective) drive disaster recovery architecture:

\textbf{RTO (Recovery Time Objective):}
\begin{itemize}
    \item Maximum acceptable downtime before system must be restored
    \item Determines automation requirements and infrastructure investment
    \item Tier 1 (Critical): RTO <15 minutes → Requires automated failover, hot standby
    \item Tier 2 (Important): RTO <1 hour → Semi-automated recovery, warm standby
    \item Tier 3 (Standard): RTO <4 hours → Manual procedures acceptable, cold standby
\end{itemize}

\textbf{RPO (Recovery Point Objective):}
\begin{itemize}
    \item Maximum acceptable data loss measured in time
    \item Determines backup frequency and replication strategy
    \item RPO = 0: Zero data loss → Synchronous replication, continuous backup
    \item RPO <5 min: Near-zero loss → Asynchronous replication, frequent snapshots
    \item RPO <1 hour: Minimal loss → Periodic backups, batch replication
\end{itemize}

\textbf{Cost vs. Recovery Trade-offs:}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Tier} & \textbf{RTO/RPO} & \textbf{Strategy} & \textbf{Cost Multiplier} \\
\hline
Critical & RTO <15 min, RPO 0 & Active-active, sync replication & 3-4x \\
Important & RTO <1 hour, RPO <5 min & Active-passive, async replication & 2-3x \\
Standard & RTO <4 hours, RPO <1 hour & Periodic backups & 1.5x \\
\hline
\end{tabular}
\end{center}

\subsection{BackupScheduler: Automated Backup with Retention}

Automated backups must run continuously without human intervention, with configurable retention policies to balance storage costs and compliance requirements.

\begin{lstlisting}[style=python, caption={Backup Scheduler with Retention Policies}]
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Callable
from datetime import datetime, timedelta
from enum import Enum
import threading
import time
import logging
import json

logger = logging.getLogger(__name__)

class BackupType(Enum):
    """Type of backup."""
    FULL = "full"  # Complete snapshot
    INCREMENTAL = "incremental"  # Only changes since last backup
    DIFFERENTIAL = "differential"  # Changes since last full backup

class BackupStatus(Enum):
    """Status of backup operation."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class RetentionPolicy:
    """
    Backup retention policy.

    Production examples:
    - Trading platform: 7 years (regulatory)
    - E-commerce: 90 days (business continuity)
    - Analytics: 30 days (cost optimization)
    """
    # Retention periods
    hourly_retention_count: int = 24  # Keep 24 hourly backups
    daily_retention_count: int = 7  # Keep 7 daily backups
    weekly_retention_count: int = 4  # Keep 4 weekly backups
    monthly_retention_count: int = 12  # Keep 12 monthly backups
    yearly_retention_count: int = 7  # Keep 7 yearly backups

    # Regulatory compliance
    compliance_retention_days: Optional[int] = None  # e.g., 2555 days (7 years)

@dataclass
class BackupMetadata:
    """Metadata for a backup."""
    backup_id: str
    backup_type: BackupType
    status: BackupStatus
    timestamp: datetime
    size_bytes: int
    location: str
    checksum: str
    retention_until: datetime
    metadata: Dict[str, any] = field(default_factory=dict)

@dataclass
class BackupConfig:
    """Configuration for backup operations."""
    # Backup frequency
    full_backup_interval_hours: int = 24  # Daily full backups
    incremental_backup_interval_minutes: int = 15  # Every 15 minutes
    differential_backup_interval_hours: int = 6  # Every 6 hours

    # Storage
    backup_location: str = "s3://backups"
    compression_enabled: bool = True
    encryption_enabled: bool = True

    # Retention
    retention_policy: RetentionPolicy = field(default_factory=RetentionPolicy)

    # Replication
    cross_region_replication: bool = True
    replication_regions: List[str] = field(default_factory=lambda: ["us-west-2", "eu-west-1"])

class BackupScheduler:
    """
    Manages automated backup scheduling and retention.

    Production considerations:
    1. Non-blocking execution (runs in background thread)
    2. Automatic retry on transient failures
    3. Retention policy enforcement (delete old backups)
    4. Cross-region replication for disaster recovery
    5. Backup validation (checksum verification)

    Backup strategy:
    - Full backup: Complete snapshot (daily)
    - Incremental: Only changes since last backup (every 15 min)
    - Differential: Changes since last full (every 6 hours)
    """

    def __init__(
        self,
        name: str,
        backup_function: Callable[[BackupType], BackupMetadata],
        config: Optional[BackupConfig] = None
    ):
        self.name = name
        self.backup_function = backup_function
        self.config = config or BackupConfig()

        # State
        self.running = False
        self.thread: Optional[threading.Thread] = None
        self.last_full_backup: Optional[datetime] = None
        self.last_incremental_backup: Optional[datetime] = None
        self.last_differential_backup: Optional[datetime] = None

        # Backup history
        self.backup_history: List[BackupMetadata] = []

        # Metrics
        self.total_backups = 0
        self.total_failures = 0
        self.total_bytes_backed_up = 0

    def start(self):
        """Start backup scheduler."""
        if self.running:
            logger.warning(f"Backup scheduler {self.name} already running")
            return

        self.running = True
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        logger.info(f"Backup scheduler {self.name} started")

    def stop(self):
        """Stop backup scheduler."""
        self.running = False
        if self.thread:
            self.thread.join(timeout=30)
        logger.info(f"Backup scheduler {self.name} stopped")

    def _run(self):
        """Main scheduler loop."""
        while self.running:
            try:
                # Check if backups are due
                self._check_and_execute_backups()

                # Enforce retention policy
                self._enforce_retention_policy()

                # Sleep for 1 minute
                time.sleep(60)

            except Exception as e:
                logger.error(f"Error in backup scheduler loop: {e}")
                time.sleep(60)

    def _check_and_execute_backups(self):
        """Check if any backups are due and execute them."""
        now = datetime.now()

        # Full backup
        if self._is_full_backup_due(now):
            self._execute_backup(BackupType.FULL)
            self.last_full_backup = now

        # Differential backup
        elif self._is_differential_backup_due(now):
            self._execute_backup(BackupType.DIFFERENTIAL)
            self.last_differential_backup = now

        # Incremental backup
        elif self._is_incremental_backup_due(now):
            self._execute_backup(BackupType.INCREMENTAL)
            self.last_incremental_backup = now

    def _is_full_backup_due(self, now: datetime) -> bool:
        """Check if full backup is due."""
        if self.last_full_backup is None:
            return True

        hours_since_last = (now - self.last_full_backup).total_seconds() / 3600
        return hours_since_last >= self.config.full_backup_interval_hours

    def _is_differential_backup_due(self, now: datetime) -> bool:
        """Check if differential backup is due."""
        if self.last_differential_backup is None:
            return False

        hours_since_last = (now - self.last_differential_backup).total_seconds() / 3600
        return hours_since_last >= self.config.differential_backup_interval_hours

    def _is_incremental_backup_due(self, now: datetime) -> bool:
        """Check if incremental backup is due."""
        if self.last_incremental_backup is None:
            return False

        minutes_since_last = (now - self.last_incremental_backup).total_seconds() / 60
        return minutes_since_last >= self.config.incremental_backup_interval_minutes

    def _execute_backup(self, backup_type: BackupType):
        """Execute backup operation."""
        logger.info(f"Starting {backup_type.value} backup for {self.name}")

        try:
            # Execute backup
            metadata = self.backup_function(backup_type)

            # Validate backup
            if not self._validate_backup(metadata):
                raise ValueError("Backup validation failed")

            # Calculate retention period
            retention_until = self._calculate_retention(metadata.timestamp, backup_type)
            metadata.retention_until = retention_until

            # Store metadata
            self.backup_history.append(metadata)

            # Update metrics
            self.total_backups += 1
            self.total_bytes_backed_up += metadata.size_bytes

            logger.info(
                f"Completed {backup_type.value} backup: {metadata.backup_id}, "
                f"size={metadata.size_bytes / 1e9:.2f}GB, "
                f"retention_until={retention_until}"
            )

            # Replicate to other regions if configured
            if self.config.cross_region_replication:
                self._replicate_backup(metadata)

        except Exception as e:
            self.total_failures += 1
            logger.error(f"Backup failed: {e}")

    def _validate_backup(self, metadata: BackupMetadata) -> bool:
        """Validate backup integrity."""
        # Check size
        if metadata.size_bytes == 0:
            logger.error("Backup has zero size")
            return False

        # Check checksum exists
        if not metadata.checksum:
            logger.error("Backup missing checksum")
            return False

        # Additional validation would go here
        # (e.g., verify checksum, test restore small sample)

        return True

    def _calculate_retention(
        self,
        backup_time: datetime,
        backup_type: BackupType
    ) -> datetime:
        """Calculate retention expiration based on policy."""
        policy = self.config.retention_policy

        # Compliance retention overrides everything
        if policy.compliance_retention_days:
            return backup_time + timedelta(days=policy.compliance_retention_days)

        # Determine retention based on backup frequency
        if backup_type == BackupType.FULL:
            # Full backups kept longer
            return backup_time + timedelta(days=policy.monthly_retention_count * 30)
        elif backup_type == BackupType.DIFFERENTIAL:
            return backup_time + timedelta(days=policy.weekly_retention_count * 7)
        else:  # INCREMENTAL
            return backup_time + timedelta(days=policy.daily_retention_count)

    def _enforce_retention_policy(self):
        """Delete backups exceeding retention period."""
        now = datetime.now()
        expired_backups = [
            b for b in self.backup_history
            if b.retention_until < now
        ]

        for backup in expired_backups:
            try:
                self._delete_backup(backup)
                self.backup_history.remove(backup)
                logger.info(f"Deleted expired backup: {backup.backup_id}")
            except Exception as e:
                logger.error(f"Failed to delete backup {backup.backup_id}: {e}")

    def _delete_backup(self, metadata: BackupMetadata):
        """Delete backup from storage."""
        # Implementation would delete from S3, GCS, etc.
        logger.info(f"Deleting backup {metadata.backup_id} from {metadata.location}")

    def _replicate_backup(self, metadata: BackupMetadata):
        """Replicate backup to other regions."""
        for region in self.config.replication_regions:
            try:
                logger.info(f"Replicating backup {metadata.backup_id} to {region}")
                # Implementation would copy to other region
            except Exception as e:
                logger.error(f"Replication to {region} failed: {e}")

    def get_metrics(self) -> dict:
        """Get backup metrics."""
        return {
            'name': self.name,
            'running': self.running,
            'total_backups': self.total_backups,
            'total_failures': self.total_failures,
            'failure_rate': (
                self.total_failures / self.total_backups
                if self.total_backups > 0 else 0
            ),
            'total_bytes_backed_up': self.total_bytes_backed_up,
            'backup_count': len(self.backup_history),
            'last_full_backup': self.last_full_backup.isoformat() if self.last_full_backup else None,
            'last_incremental_backup': self.last_incremental_backup.isoformat() if self.last_incremental_backup else None
        }

# Example: Kafka topic backup
def backup_kafka_topics(backup_type: BackupType) -> BackupMetadata:
    """Backup Kafka topics to S3."""
    import uuid
    import hashlib

    backup_id = f"kafka-{backup_type.value}-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

    # Implementation would:
    # 1. Export Kafka topics using kafka-connect or custom consumer
    # 2. Compress and encrypt data
    # 3. Upload to S3
    # 4. Calculate checksum

    # Simulated metadata
    data = f"backup-data-{backup_id}".encode()
    checksum = hashlib.sha256(data).hexdigest()

    return BackupMetadata(
        backup_id=backup_id,
        backup_type=backup_type,
        status=BackupStatus.COMPLETED,
        timestamp=datetime.now(),
        size_bytes=len(data) * 1000000,  # Simulated size
        location=f"s3://backups/kafka/{backup_id}.tar.gz.enc",
        checksum=checksum,
        retention_until=datetime.now() + timedelta(days=30)
    )

# Usage
scheduler = BackupScheduler(
    name="kafka_topics",
    backup_function=backup_kafka_topics,
    config=BackupConfig(
        full_backup_interval_hours=24,
        incremental_backup_interval_minutes=15,
        cross_region_replication=True,
        replication_regions=["us-west-2", "eu-west-1"]
    )
)

scheduler.start()
\end{lstlisting}

\subsection{RestoreManager: Validated Restore Procedures}

Backups are worthless if restore procedures are untested. Production restore managers must validate data integrity and provide point-in-time recovery.

\begin{lstlisting}[style=python, caption={Restore Manager with Validation}]
from typing import Optional, Callable
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class RestoreConfig:
    """Configuration for restore operations."""
    # Validation
    validate_checksum: bool = True
    validate_data_integrity: bool = True
    validate_sample_size: int = 1000  # Rows to validate

    # Recovery point
    point_in_time_recovery: bool = True

    # Safety
    dry_run: bool = False  # Test restore without applying
    require_confirmation: bool = True

class RestoreResult:
    """Result of restore operation."""
    def __init__(self):
        self.success: bool = False
        self.restored_backup_id: Optional[str] = None
        self.restored_timestamp: Optional[datetime] = None
        self.validation_passed: bool = False
        self.records_restored: int = 0
        self.duration_seconds: float = 0
        self.errors: List[str] = []

class RestoreManager:
    """
    Manages restore operations with validation.

    Production considerations:
    1. Point-in-time recovery (restore to specific timestamp)
    2. Checksum validation before restore
    3. Data integrity checks after restore
    4. Dry-run mode for testing
    5. Automatic rollback on validation failure

    Restore process:
    1. Identify backup to restore
    2. Validate backup integrity (checksum)
    3. Extract and decrypt backup
    4. Restore data to target system
    5. Validate restored data
    6. Update system state (consumer offsets, etc.)
    """

    def __init__(
        self,
        backup_history: List[BackupMetadata],
        config: Optional[RestoreConfig] = None
    ):
        self.backup_history = backup_history
        self.config = config or RestoreConfig()

        # Metrics
        self.total_restores = 0
        self.total_failures = 0

    def restore_latest(
        self,
        restore_function: Callable[[BackupMetadata], RestoreResult]
    ) -> RestoreResult:
        """Restore from most recent backup."""
        if not self.backup_history:
            result = RestoreResult()
            result.errors.append("No backups available")
            return result

        latest_backup = max(self.backup_history, key=lambda b: b.timestamp)
        return self.restore_from_backup(latest_backup, restore_function)

    def restore_point_in_time(
        self,
        target_time: datetime,
        restore_function: Callable[[BackupMetadata], RestoreResult]
    ) -> RestoreResult:
        """
        Restore to specific point in time.

        Strategy:
        1. Find last full backup before target time
        2. Apply incremental backups up to target time
        """
        # Find full backup before target time
        full_backups = [
            b for b in self.backup_history
            if b.backup_type == BackupType.FULL and b.timestamp <= target_time
        ]

        if not full_backups:
            result = RestoreResult()
            result.errors.append(f"No full backup before {target_time}")
            return result

        base_backup = max(full_backups, key=lambda b: b.timestamp)

        logger.info(
            f"Restoring from full backup {base_backup.backup_id} "
            f"at {base_backup.timestamp}"
        )

        # Restore base backup
        result = self.restore_from_backup(base_backup, restore_function)
        if not result.success:
            return result

        # Apply incremental backups
        incremental_backups = [
            b for b in self.backup_history
            if b.backup_type == BackupType.INCREMENTAL
            and base_backup.timestamp < b.timestamp <= target_time
        ]

        incremental_backups.sort(key=lambda b: b.timestamp)

        for inc_backup in incremental_backups:
            logger.info(f"Applying incremental backup {inc_backup.backup_id}")
            inc_result = self.restore_from_backup(inc_backup, restore_function)

            if not inc_result.success:
                result.success = False
                result.errors.extend(inc_result.errors)
                return result

            result.records_restored += inc_result.records_restored

        return result

    def restore_from_backup(
        self,
        backup: BackupMetadata,
        restore_function: Callable[[BackupMetadata], RestoreResult]
    ) -> RestoreResult:
        """Restore from specific backup."""
        import time

        logger.info(f"Starting restore from backup {backup.backup_id}")

        start_time = time.time()
        result = RestoreResult()
        result.restored_backup_id = backup.backup_id
        result.restored_timestamp = backup.timestamp

        try:
            # Step 1: Validate backup checksum
            if self.config.validate_checksum:
                if not self._validate_checksum(backup):
                    result.errors.append("Checksum validation failed")
                    return result

            # Step 2: Dry run check
            if self.config.dry_run:
                logger.info("Dry run mode - skipping actual restore")
                result.success = True
                result.validation_passed = True
                return result

            # Step 3: Confirmation check
            if self.config.require_confirmation:
                logger.warning(
                    f"Restore will overwrite current data with backup from "
                    f"{backup.timestamp}. Proceeding..."
                )

            # Step 4: Execute restore
            result = restore_function(backup)

            # Step 5: Validate restored data
            if self.config.validate_data_integrity and result.success:
                result.validation_passed = self._validate_restored_data(backup, result)

                if not result.validation_passed:
                    result.success = False
                    result.errors.append("Data integrity validation failed")
                    logger.error("Restore validation failed - consider rollback")

            # Update metrics
            if result.success:
                self.total_restores += 1
            else:
                self.total_failures += 1

            result.duration_seconds = time.time() - start_time

            logger.info(
                f"Restore {'succeeded' if result.success else 'failed'}: "
                f"{result.records_restored} records in {result.duration_seconds:.1f}s"
            )

            return result

        except Exception as e:
            result.success = False
            result.errors.append(f"Restore exception: {e}")
            result.duration_seconds = time.time() - start_time
            self.total_failures += 1
            logger.error(f"Restore failed: {e}")
            return result

    def _validate_checksum(self, backup: BackupMetadata) -> bool:
        """Validate backup checksum before restore."""
        logger.info(f"Validating checksum for backup {backup.backup_id}")

        # Implementation would:
        # 1. Download backup file
        # 2. Calculate checksum
        # 3. Compare with stored checksum

        # Simulated validation
        return True

    def _validate_restored_data(
        self,
        backup: BackupMetadata,
        result: RestoreResult
    ) -> bool:
        """Validate restored data integrity."""
        logger.info("Validating restored data integrity")

        # Implementation would:
        # 1. Sample random records
        # 2. Verify data types and constraints
        # 3. Check record counts match expected
        # 4. Validate business logic invariants

        # Simulated validation
        if result.records_restored == 0:
            logger.error("No records restored")
            return False

        logger.info(f"Validated {self.config.validate_sample_size} sample records")
        return True

    def get_metrics(self) -> dict:
        """Get restore metrics."""
        return {
            'total_restores': self.total_restores,
            'total_failures': self.total_failures,
            'success_rate': (
                (self.total_restores - self.total_failures) / self.total_restores
                if self.total_restores > 0 else 0
            )
        }

# Example: Kafka topic restore
def restore_kafka_topics(backup: BackupMetadata) -> RestoreResult:
    """Restore Kafka topics from backup."""
    result = RestoreResult()

    try:
        logger.info(f"Restoring Kafka topics from {backup.location}")

        # Implementation would:
        # 1. Download backup from S3
        # 2. Decrypt and decompress
        # 3. Create topics if not exist
        # 4. Produce messages to topics
        # 5. Restore consumer group offsets

        # Simulated restore
        result.success = True
        result.records_restored = 1000000  # 1M messages
        result.validation_passed = True

        return result

    except Exception as e:
        result.success = False
        result.errors.append(str(e))
        return result

# Usage
restore_manager = RestoreManager(
    backup_history=scheduler.backup_history,
    config=RestoreConfig(
        validate_checksum=True,
        validate_data_integrity=True,
        dry_run=False
    )
)

# Restore latest
result = restore_manager.restore_latest(restore_kafka_topics)

# Or restore to specific time
target_time = datetime.now() - timedelta(hours=2)
result = restore_manager.restore_point_in_time(target_time, restore_kafka_topics)
\end{lstlisting}

\subsection{DisasterRecoveryManager: Automated Failover}

Complete disaster recovery requires automated detection and failover to secondary regions.

\begin{lstlisting}[style=python, caption={Disaster Recovery Manager}]
from enum import Enum
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
import time
import threading
import logging

logger = logging.getLogger(__name__)

class FailoverStatus(Enum):
    """Status of failover operation."""
    ACTIVE_PRIMARY = "active_primary"  # Normal operation on primary
    FAILOVER_IN_PROGRESS = "failover_in_progress"  # Switching to secondary
    ACTIVE_SECONDARY = "active_secondary"  # Running on secondary
    FAILBACK_IN_PROGRESS = "failback_in_progress"  # Returning to primary
    DEGRADED = "degraded"  # Partial functionality

@dataclass
class RegionConfig:
    """Configuration for a region."""
    name: str
    is_primary: bool
    kafka_bootstrap_servers: List[str]
    feature_store_endpoint: str
    processing_capacity: float  # 0-1, capacity as fraction of full load
    health_check_endpoint: str

@dataclass
class DRConfig:
    """Disaster recovery configuration."""
    # RTO/RPO
    rto_minutes: int = 15  # Recovery time objective
    rpo_minutes: int = 5  # Recovery point objective

    # Health monitoring
    health_check_interval_seconds: int = 30
    failure_threshold: int = 3  # Failures before triggering failover

    # Failover
    auto_failover_enabled: bool = True
    auto_failback_enabled: bool = False  # Require manual failback
    failover_validation_required: bool = True

@dataclass
class FailoverEvent:
    """Record of failover event."""
    timestamp: datetime
    from_region: str
    to_region: str
    trigger_reason: str
    duration_seconds: float
    success: bool
    records_lost: int = 0  # Data loss (violates RPO if > 0)

class DisasterRecoveryManager:
    """
    Manages disaster recovery and automated failover.

    Production capabilities:
    1. Continuous health monitoring across regions
    2. Automatic failover on primary region failure
    3. Point-in-time consistency across regions
    4. RTO/RPO tracking and alerting
    5. Failover drill automation

    Architecture patterns:
    - Active-passive: Primary region serves traffic, secondary on standby
    - Active-active: Both regions serve traffic (requires conflict resolution)
    - Multi-region: >2 regions for global distribution
    """

    def __init__(
        self,
        regions: List[RegionConfig],
        config: Optional[DRConfig] = None
    ):
        self.regions = {r.name: r for r in regions}
        self.config = config or DRConfig()

        # State
        self.active_region: str = next(r.name for r in regions if r.is_primary)
        self.failover_status = FailoverStatus.ACTIVE_PRIMARY

        # Health monitoring
        self.region_health: Dict[str, bool] = {r.name: True for r in regions}
        self.consecutive_failures: Dict[str, int] = {r.name: 0 for r in regions}

        # Monitoring thread
        self.monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None

        # Failover history
        self.failover_history: List[FailoverEvent] = []

        # Metrics
        self.total_failovers = 0
        self.total_failbacks = 0

    def start_monitoring(self):
        """Start health monitoring and auto-failover."""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("Disaster recovery monitoring started")

    def stop_monitoring(self):
        """Stop health monitoring."""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=30)
        logger.info("Disaster recovery monitoring stopped")

    def _monitor_loop(self):
        """Continuous health monitoring loop."""
        while self.monitoring:
            try:
                self._check_region_health()

                # Check if failover needed
                if self._should_trigger_failover():
                    if self.config.auto_failover_enabled:
                        self._execute_failover()
                    else:
                        logger.critical(
                            "Failover required but auto-failover disabled. "
                            "Manual intervention needed!"
                        )

                time.sleep(self.config.health_check_interval_seconds)

            except Exception as e:
                logger.error(f"Error in DR monitoring loop: {e}")
                time.sleep(self.config.health_check_interval_seconds)

    def _check_region_health(self):
        """Check health of all regions."""
        for region_name, region in self.regions.items():
            try:
                healthy = self._perform_health_check(region)

                if healthy:
                    self.region_health[region_name] = True
                    self.consecutive_failures[region_name] = 0
                else:
                    self.consecutive_failures[region_name] += 1

                    if self.consecutive_failures[region_name] >= self.config.failure_threshold:
                        self.region_health[region_name] = False
                        logger.error(
                            f"Region {region_name} marked unhealthy after "
                            f"{self.consecutive_failures[region_name]} failures"
                        )

            except Exception as e:
                logger.error(f"Health check failed for {region_name}: {e}")
                self.consecutive_failures[region_name] += 1

    def _perform_health_check(self, region: RegionConfig) -> bool:
        """Perform health check on region."""
        # Implementation would:
        # 1. Check Kafka cluster reachability
        # 2. Verify feature store responsiveness
        # 3. Test processing pipeline health
        # 4. Measure end-to-end latency

        # Simulated health check
        logger.debug(f"Health check: {region.name}")
        return True

    def _should_trigger_failover(self) -> bool:
        """Determine if failover should be triggered."""
        # Check if active region is unhealthy
        if not self.region_health.get(self.active_region, True):
            logger.warning(f"Active region {self.active_region} is unhealthy")
            return True

        return False

    def _execute_failover(self):
        """Execute automated failover to secondary region."""
        if self.failover_status != FailoverStatus.ACTIVE_PRIMARY:
            logger.warning(f"Cannot failover from state: {self.failover_status}")
            return

        # Select target region (first healthy non-active region)
        target_region = self._select_target_region()
        if not target_region:
            logger.critical("No healthy region available for failover!")
            return

        logger.critical(
            f"EXECUTING FAILOVER: {self.active_region} -> {target_region}"
        )

        self.failover_status = FailoverStatus.FAILOVER_IN_PROGRESS
        start_time = time.time()

        event = FailoverEvent(
            timestamp=datetime.now(),
            from_region=self.active_region,
            to_region=target_region,
            trigger_reason="Automated failover: primary region unhealthy",
            duration_seconds=0,
            success=False
        )

        try:
            # Step 1: Stop writes to primary region (if possible)
            logger.info("Step 1: Stopping writes to primary region")

            # Step 2: Ensure secondary region is caught up
            logger.info("Step 2: Verifying secondary region replication lag")
            replication_lag_seconds = self._check_replication_lag(target_region)

            if replication_lag_seconds > self.config.rpo_minutes * 60:
                logger.error(
                    f"Replication lag {replication_lag_seconds}s exceeds "
                    f"RPO {self.config.rpo_minutes * 60}s"
                )
                # Continue anyway - availability over consistency

            # Step 3: Promote secondary to primary
            logger.info("Step 3: Promoting secondary region to primary")
            self._promote_region(target_region)

            # Step 4: Update DNS / load balancer
            logger.info("Step 4: Updating traffic routing")
            self._update_routing(target_region)

            # Step 5: Validate failover
            if self.config.failover_validation_required:
                logger.info("Step 5: Validating failover")
                if not self._validate_failover(target_region):
                    raise Exception("Failover validation failed")

            # Success
            self.active_region = target_region
            self.failover_status = FailoverStatus.ACTIVE_SECONDARY
            event.success = True
            event.duration_seconds = time.time() - start_time

            self.total_failovers += 1

            logger.info(
                f"FAILOVER COMPLETED: Now active on {target_region} "
                f"(duration: {event.duration_seconds:.1f}s)"
            )

            # Check RTO compliance
            if event.duration_seconds > self.config.rto_minutes * 60:
                logger.error(
                    f"RTO VIOLATED: Failover took {event.duration_seconds:.1f}s, "
                    f"exceeds RTO {self.config.rto_minutes * 60}s"
                )

        except Exception as e:
            event.success = False
            event.duration_seconds = time.time() - start_time
            logger.critical(f"FAILOVER FAILED: {e}")
            self.failover_status = FailoverStatus.DEGRADED

        finally:
            self.failover_history.append(event)

    def _select_target_region(self) -> Optional[str]:
        """Select best region for failover."""
        healthy_regions = [
            name for name, healthy in self.region_health.items()
            if healthy and name != self.active_region
        ]

        if not healthy_regions:
            return None

        # Select region with highest capacity
        return max(
            healthy_regions,
            key=lambda r: self.regions[r].processing_capacity
        )

    def _check_replication_lag(self, target_region: str) -> float:
        """Check replication lag to target region."""
        # Implementation would:
        # 1. Query Kafka consumer lag
        # 2. Check feature store replication timestamp
        # 3. Verify checkpoint synchronization

        # Simulated lag check
        return 2.5  # 2.5 seconds lag

    def _promote_region(self, region_name: str):
        """Promote region to primary."""
        logger.info(f"Promoting {region_name} to primary")
        # Implementation would:
        # 1. Enable writes on secondary Kafka cluster
        # 2. Update feature store to read-write mode
        # 3. Start processing pipelines

    def _update_routing(self, region_name: str):
        """Update traffic routing to new region."""
        logger.info(f"Routing traffic to {region_name}")
        # Implementation would:
        # 1. Update DNS records
        # 2. Reconfigure load balancers
        # 3. Update client configurations

    def _validate_failover(self, region_name: str) -> bool:
        """Validate failover completed successfully."""
        logger.info(f"Validating failover to {region_name}")

        # Check region health
        if not self._perform_health_check(self.regions[region_name]):
            logger.error("Health check failed after failover")
            return False

        # Check processing pipeline
        # Implementation would verify end-to-end processing

        return True

    def execute_failover_drill(self) -> FailoverEvent:
        """Execute failover drill for testing."""
        logger.info("EXECUTING FAILOVER DRILL (simulation)")

        # Simulate failover without actually switching
        event = FailoverEvent(
            timestamp=datetime.now(),
            from_region=self.active_region,
            to_region="drill-simulation",
            trigger_reason="Scheduled DR drill",
            duration_seconds=0,
            success=False
        )

        # Test would validate:
        # 1. Backup restore procedures
        # 2. Region promotion steps
        # 3. Traffic routing updates
        # 4. End-to-end processing validation

        event.success = True
        event.duration_seconds = 45.0  # Simulated duration

        logger.info(f"Failover drill completed in {event.duration_seconds}s")

        return event

    def get_metrics(self) -> dict:
        """Get disaster recovery metrics."""
        return {
            'active_region': self.active_region,
            'failover_status': self.failover_status.value,
            'region_health': self.region_health,
            'total_failovers': self.total_failovers,
            'total_failbacks': self.total_failbacks,
            'failover_history_count': len(self.failover_history),
            'avg_failover_duration': (
                sum(e.duration_seconds for e in self.failover_history) / len(self.failover_history)
                if self.failover_history else 0
            ),
            'rto_compliance': sum(
                1 for e in self.failover_history
                if e.duration_seconds <= self.config.rto_minutes * 60
            ) / len(self.failover_history) if self.failover_history else 1.0
        }

# Example usage
regions = [
    RegionConfig(
        name="us-east-1",
        is_primary=True,
        kafka_bootstrap_servers=["kafka-east-1:9092", "kafka-east-2:9092"],
        feature_store_endpoint="https://features-east.example.com",
        processing_capacity=1.0,
        health_check_endpoint="https://health-east.example.com"
    ),
    RegionConfig(
        name="us-west-2",
        is_primary=False,
        kafka_bootstrap_servers=["kafka-west-1:9092", "kafka-west-2:9092"],
        feature_store_endpoint="https://features-west.example.com",
        processing_capacity=1.0,
        health_check_endpoint="https://health-west.example.com"
    )
]

dr_manager = DisasterRecoveryManager(
    regions=regions,
    config=DRConfig(
        rto_minutes=15,
        rpo_minutes=5,
        auto_failover_enabled=True,
        health_check_interval_seconds=30
    )
)

dr_manager.start_monitoring()
\end{lstlisting}

\subsection{RecoveryTester: Automated DR Validation}

Untested disaster recovery plans fail in production. Automated testing validates RTO/RPO compliance.

\begin{lstlisting}[style=python, caption={Disaster Recovery Testing Framework}]
from typing import List, Callable
from dataclasses import dataclass
import time
import logging

logger = logging.getLogger(__name__)

@dataclass
class DRTestResult:
    """Result of DR test."""
    test_name: str
    success: bool
    duration_seconds: float
    rto_compliant: bool
    rpo_compliant: bool
    errors: List[str] = field(default_factory=list)
    metrics: Dict[str, any] = field(default_factory=dict)

class RecoveryTester:
    """
    Automated disaster recovery testing.

    Production test scenarios:
    1. Full region failover test
    2. Backup and restore validation
    3. Data integrity verification
    4. Performance under degraded conditions
    5. Failback procedures

    Testing frequency:
    - Smoke tests: Daily (quick validation)
    - Full DR drill: Monthly
    - Cross-region failover: Quarterly
    - Chaos engineering: Continuous (limited scope)
    """

    def __init__(
        self,
        dr_manager: DisasterRecoveryManager,
        backup_scheduler: BackupScheduler,
        restore_manager: RestoreManager,
        rto_minutes: int,
        rpo_minutes: int
    ):
        self.dr_manager = dr_manager
        self.backup_scheduler = backup_scheduler
        self.restore_manager = restore_manager
        self.rto_minutes = rto_minutes
        self.rpo_minutes = rpo_minutes

        # Test history
        self.test_history: List[DRTestResult] = []

    def run_full_dr_drill(self) -> DRTestResult:
        """
        Execute complete disaster recovery drill.

        Simulates full region failure and validates recovery.
        """
        logger.info("===== STARTING FULL DR DRILL =====")

        result = DRTestResult(
            test_name="full_dr_drill",
            success=False,
            duration_seconds=0,
            rto_compliant=False,
            rpo_compliant=False
        )

        start_time = time.time()

        try:
            # Test 1: Backup validation
            logger.info("Test 1: Validating backups exist")
            if not self.backup_scheduler.backup_history:
                result.errors.append("No backups available")
                return result

            # Test 2: Simulate region failure
            logger.info("Test 2: Simulating primary region failure")
            original_region = self.dr_manager.active_region
            original_health = self.dr_manager.region_health[original_region]

            # Temporarily mark primary as unhealthy
            self.dr_manager.region_health[original_region] = False

            # Test 3: Trigger failover
            logger.info("Test 3: Triggering failover")
            failover_start = time.time()
            self.dr_manager._execute_failover()
            failover_duration = time.time() - failover_start

            # Check RTO compliance
            result.rto_compliant = failover_duration <= self.rto_minutes * 60
            result.metrics['failover_duration_seconds'] = failover_duration

            if not result.rto_compliant:
                result.errors.append(
                    f"RTO violated: {failover_duration:.1f}s > {self.rto_minutes * 60}s"
                )

            # Test 4: Validate secondary region operational
            logger.info("Test 4: Validating secondary region")
            if self.dr_manager.failover_status != FailoverStatus.ACTIVE_SECONDARY:
                result.errors.append("Failover did not complete successfully")
                return result

            # Test 5: Perform restore test
            logger.info("Test 5: Testing backup restore")
            restore_result = self.restore_manager.restore_latest(
                lambda b: self._test_restore_function(b)
            )

            if not restore_result.success:
                result.errors.append("Restore test failed")
                result.errors.extend(restore_result.errors)

            # Test 6: Data integrity check
            logger.info("Test 6: Validating data integrity")
            data_valid = self._validate_data_integrity()
            if not data_valid:
                result.errors.append("Data integrity validation failed")

            # Test 7: RPO validation
            logger.info("Test 7: Checking RPO compliance")
            data_loss_minutes = self._measure_data_loss()
            result.rpo_compliant = data_loss_minutes <= self.rpo_minutes
            result.metrics['data_loss_minutes'] = data_loss_minutes

            if not result.rpo_compliant:
                result.errors.append(
                    f"RPO violated: {data_loss_minutes} min data loss > "
                    f"{self.rpo_minutes} min"
                )

            # Success if no errors
            result.success = len(result.errors) == 0

            # Cleanup: Restore original region health
            self.dr_manager.region_health[original_region] = original_health

        except Exception as e:
            result.errors.append(f"DR drill exception: {e}")
            logger.error(f"DR drill failed: {e}")

        finally:
            result.duration_seconds = time.time() - start_time
            self.test_history.append(result)

            logger.info(
                f"===== DR DRILL COMPLETED: "
                f"{'SUCCESS' if result.success else 'FAILED'} "
                f"in {result.duration_seconds:.1f}s ====="
            )

        return result

    def run_backup_validation_test(self) -> DRTestResult:
        """Validate all backups are restorable."""
        logger.info("Running backup validation test")

        result = DRTestResult(
            test_name="backup_validation",
            success=False,
            duration_seconds=0,
            rto_compliant=True,  # Not applicable
            rpo_compliant=True  # Not applicable
        )

        start_time = time.time()

        try:
            backups_tested = 0
            backups_valid = 0

            for backup in self.backup_scheduler.backup_history[-10:]:  # Test last 10
                logger.info(f"Validating backup {backup.backup_id}")

                # Test checksum
                if not self.restore_manager._validate_checksum(backup):
                    result.errors.append(f"Backup {backup.backup_id} checksum invalid")
                    continue

                backups_tested += 1
                backups_valid += 1

            result.metrics['backups_tested'] = backups_tested
            result.metrics['backups_valid'] = backups_valid
            result.success = backups_valid == backups_tested

        except Exception as e:
            result.errors.append(f"Backup validation exception: {e}")

        finally:
            result.duration_seconds = time.time() - start_time
            self.test_history.append(result)

        return result

    def _test_restore_function(self, backup: BackupMetadata) -> RestoreResult:
        """Test restore function (non-destructive)."""
        result = RestoreResult()
        result.success = True
        result.records_restored = 1000
        result.validation_passed = True
        return result

    def _validate_data_integrity(self) -> bool:
        """Validate data integrity after failover."""
        # Implementation would:
        # 1. Query sample records
        # 2. Verify data constraints
        # 3. Check business logic invariants
        return True

    def _measure_data_loss(self) -> float:
        """Measure data loss in minutes."""
        # Implementation would:
        # 1. Compare last processed timestamp with current time
        # 2. Query replication lag
        # 3. Calculate gap in minutes
        return 2.5  # 2.5 minutes of data loss

    def generate_dr_report(self) -> str:
        """Generate disaster recovery readiness report."""
        report = []
        report.append("=" * 60)
        report.append("DISASTER RECOVERY READINESS REPORT")
        report.append("=" * 60)
        report.append("")

        # Summary
        total_tests = len(self.test_history)
        passed_tests = sum(1 for t in self.test_history if t.success)
        report.append(f"Total DR Tests: {total_tests}")
        report.append(f"Passed: {passed_tests}")
        report.append(f"Failed: {total_tests - passed_tests}")
        report.append(f"Success Rate: {passed_tests / total_tests * 100:.1f}%")
        report.append("")

        # RTO/RPO Compliance
        rto_compliant = sum(1 for t in self.test_history if t.rto_compliant)
        rpo_compliant = sum(1 for t in self.test_history if t.rpo_compliant)
        report.append(f"RTO Compliance: {rto_compliant / total_tests * 100:.1f}%")
        report.append(f"RPO Compliance: {rpo_compliant / total_tests * 100:.1f}%")
        report.append("")

        # Recent test results
        report.append("Recent Test Results:")
        for test in self.test_history[-5:]:
            status = "PASS" if test.success else "FAIL"
            report.append(
                f"  {test.timestamp.strftime('%Y-%m-%d %H:%M')} - "
                f"{test.test_name}: {status} ({test.duration_seconds:.1f}s)"
            )

        report.append("=" * 60)

        return "\n".join(report)

# Example usage
tester = RecoveryTester(
    dr_manager=dr_manager,
    backup_scheduler=scheduler,
    restore_manager=restore_manager,
    rto_minutes=15,
    rpo_minutes=5
)

# Run monthly DR drill
result = tester.run_full_dr_drill()

# Generate report
print(tester.generate_dr_report())
\end{lstlisting}

\subsection{Production DR Metrics and Monitoring}

Monitor disaster recovery readiness to ensure compliance with RTO/RPO objectives:

\begin{lstlisting}[style=python, caption={DR Monitoring Metrics}]
from prometheus_client import Counter, Gauge, Histogram

# Backup metrics
backup_operations_total = Counter(
    'backup_operations_total',
    'Total backup operations',
    ['backup_type', 'status']
)

backup_size_bytes = Histogram(
    'backup_size_bytes',
    'Backup size in bytes',
    ['backup_type']
)

backup_duration_seconds = Histogram(
    'backup_duration_seconds',
    'Backup duration',
    ['backup_type']
)

# Restore metrics
restore_operations_total = Counter(
    'restore_operations_total',
    'Total restore operations',
    ['status']
)

restore_duration_seconds = Histogram(
    'restore_duration_seconds',
    'Restore duration in seconds'
)

# Failover metrics
failover_events_total = Counter(
    'failover_events_total',
    'Total failover events',
    ['from_region', 'to_region', 'success']
)

failover_duration_seconds = Histogram(
    'failover_duration_seconds',
    'Failover duration in seconds'
)

rto_compliance = Gauge(
    'rto_compliance_ratio',
    'Ratio of failovers meeting RTO'
)

rpo_compliance = Gauge(
    'rpo_compliance_ratio',
    'Ratio of recoveries meeting RPO'
)

replication_lag_seconds = Gauge(
    'replication_lag_seconds',
    'Replication lag to secondary region',
    ['region']
)
\end{lstlisting}

\textbf{Key DR metrics to monitor:}
\begin{itemize}
    \item \textbf{Backup success rate}: Percentage of successful backups (target: >99.9\%)
    \item \textbf{Backup freshness}: Time since last successful backup (alert if >RPO)
    \item \textbf{Restore test frequency}: Days since last restore test (alert if >30 days)
    \item \textbf{Replication lag}: Lag to secondary region (alert if >RPO)
    \item \textbf{RTO compliance}: Percentage of drills meeting RTO (target: 100\%)
    \item \textbf{RPO compliance}: Percentage of recoveries meeting RPO (target: 100\%)
\end{itemize}

\textbf{Alert thresholds:}
\begin{itemize}
    \item Backup failure → Page on-call immediately
    \item Backup age >RPO → Critical alert
    \item Replication lag >RPO → Warning (approaching RPO violation)
    \item Restore test not run in 30 days → Warning (DR readiness unknown)
    \item DR drill RTO violation → Post-mortem required
    \item RPO violated during recovery → Regulatory reporting may be required
\end{itemize}

\section{Performance Optimization and Parallel Processing}

Event-driven pipelines process millions of events per second, but naive implementations bottleneck at 10K events/second. The difference between 10K and 1M events/second throughput is the difference between handling regional e-commerce and operating at Amazon scale. Production pipelines require parallel processing with dynamic resource allocation, intelligent data partitioning, and automatic load balancing to scale with data growth.

Pipeline performance degrades nonlinearly with data volume: doubling throughput often requires 4x resources without optimization. Intelligent partitioning converts O(n²) operations to O(n), dynamic worker allocation eliminates idle resources, and load balancing prevents hotspot bottlenecks. However, poor parallelization creates new problems: excessive thread overhead, partition skew, and coordination bottlenecks can reduce throughput below single-threaded performance.

\subsection{The Scaling Wall}

Consider a ride-sharing platform processing real-time location updates and trip events:

\textbf{Initial system (Year 1):}
\begin{itemize}
    \item Traffic: 50K events/second (1M active drivers, 5M riders)
    \item Architecture: Single Kafka partition per city, 10 cities
    \item Processing: Single-threaded consumer per partition
    \item Latency: <100ms event-to-dashboard
    \item Infrastructure: 10 stream processor instances (1 per city)
    \item Cost: \$5K/month
\end{itemize}

\textbf{Year 2 - 10x growth:}
\begin{itemize}
    \item Traffic: 500K events/second (10M drivers, 50M riders, 50 cities)
    \item Problem: Single-threaded processing can't keep up
    \item Consumer lag: 0 seconds → 30 seconds → 5 minutes → Growing unbounded
    \item Attempted fix: Add more partitions (100 partitions, 100 consumers)
    \item Result: Lag reduced but latency still high (rebalancing overhead)
    \item Infrastructure: 100 instances
    \item Cost: \$50K/month (10x, linear with growth)
\end{itemize}

\textbf{Year 3 - 100x growth:}
\begin{itemize}
    \item Traffic: 5M events/second (100M drivers, 500M riders, 200 cities)
    \item Consumer lag: Back to growing unbounded
    \item Problem 1: \textbf{Partition hotspots} (New York City = 40\% of traffic, single partition)
    \item Problem 2: \textbf{Inefficient serialization} (JSON parsing = 60\% CPU time)
    \item Problem 3: \textbf{Memory bottleneck} (Each consumer needs 4GB for windowed aggregations)
    \item Problem 4: \textbf{Network saturation} (Cross-region data transfer = 2Gbps per consumer)
    \item Infrastructure: 1000 instances attempted
    \item Cost: \$500K/month (100x, unsustainable)
    \item Business impact: Dashboards 10+ minutes stale, customer complaints about ETAs
\end{itemize}

\textbf{The scaling wall:}
\begin{itemize}
    \item \textbf{Hit Year 3}: Can't scale linearly anymore
    \item \textbf{Adding instances doesn't help}: Partition hotspots limit throughput
    \item \textbf{Rebalancing takes minutes}: Every deployment causes 5-minute lag spike
    \item \textbf{Cost growing faster than revenue}: 10x traffic → 100x cost
    \item \textbf{Engineering team paralyzed}: 80\% time fighting performance fires
\end{itemize}

\textbf{Root cause analysis:}
\begin{itemize}
    \item \textbf{No dynamic parallelism}: Fixed 1 thread per partition
    \item \textbf{Poor partitioning strategy}: Geographic partitioning creates 100:1 skew (NYC vs. small cities)
    \item \textbf{No load balancing}: Hot partitions overwhelmed, cold partitions idle
    \item \textbf{No performance monitoring}: Bottlenecks discovered only during incidents
    \item \textbf{Monolithic processing}: Can't scale compute independently from I/O
\end{itemize}

\textbf{Performance optimization solution:}
\begin{itemize}
    \item \textbf{Parallel processing}: Multiple workers per partition, dynamic allocation
    \item \textbf{Smart partitioning}: Hybrid key (city + driver\_id hash) balances load
    \item \textbf{Load balancing}: Work-stealing algorithm redistributes from hot to cold workers
    \item \textbf{Binary serialization}: Protobuf reduces parsing CPU 90\%
    \item \textbf{Incremental aggregation}: Stream windowing instead of full recomputation
\end{itemize}

\textbf{Results after optimization:}
\begin{itemize}
    \item Throughput: 5M events/second → 20M events/second (4x headroom)
    \item Latency: 10 minutes → 50ms (200x improvement)
    \item Infrastructure: 1000 instances → 100 instances (10x reduction)
    \item Cost: \$500K/month → \$50K/month (10x reduction)
    \item Scalability: Linear scaling restored (can handle 100x more growth)
\end{itemize}

This scenario demonstrates why production pipelines require sophisticated parallel processing and partitioning.

\subsection{Parallel Processing Fundamentals}

Parallel processing multiplies throughput by distributing work across multiple workers. Key concepts:

\textbf{Parallelism Models:}
\begin{itemize}
    \item \textbf{Task parallelism}: Different operations on different data (map-reduce)
    \item \textbf{Data parallelism}: Same operation on different data partitions
    \item \textbf{Pipeline parallelism}: Different stages process different events concurrently
\end{itemize}

\textbf{Parallelism Overhead:}
\begin{itemize}
    \item \textbf{Thread creation}: 1-10ms per thread
    \item \textbf{Context switching}: 1-100μs per switch
    \item \textbf{Synchronization}: 100ns-1μs per lock acquisition
    \item \textbf{Communication}: Variable (shared memory < IPC < network)
\end{itemize}

\textbf{Optimal Worker Count:}
\begin{equation}
\text{Workers}_{\text{optimal}} = \frac{\text{CPU cores} \times (1 + \frac{\text{Wait time}}{\text{Compute time}})}{\text{Context switch overhead}}
\end{equation}

For I/O-bound tasks (wait time >> compute time), optimal workers = 2-4x CPU cores.
For CPU-bound tasks (wait time ≈ 0), optimal workers = CPU cores.

\subsection{ParallelProcessor: Dynamic Worker Allocation}

Production parallel processing must adapt worker count dynamically based on load and resource availability.

\begin{lstlisting}[style=python, caption={Parallel Processor with Dynamic Allocation}]
from dataclasses import dataclass, field
from typing import List, Callable, Any, Optional, Dict
from concurrent.futures import ThreadPoolExecutor, Future
from queue import Queue, Empty
import threading
import time
import logging
import psutil

logger = logging.getLogger(__name__)

@dataclass
class ParallelConfig:
    """Configuration for parallel processing."""
    # Worker allocation
    min_workers: int = 2
    max_workers: int = 32
    initial_workers: int = 4

    # Auto-scaling
    auto_scale_enabled: bool = True
    scale_up_threshold: float = 0.8  # 80% utilization
    scale_down_threshold: float = 0.3  # 30% utilization
    scale_check_interval_seconds: int = 10

    # Resource limits
    max_cpu_percent: float = 80.0  # Max CPU utilization
    max_memory_mb: int = 4096  # Max memory per worker

    # Queue management
    queue_size: int = 10000
    batch_size: int = 100  # Process in batches

@dataclass
class WorkerMetrics:
    """Metrics for a worker thread."""
    worker_id: int
    tasks_processed: int = 0
    tasks_failed: int = 0
    total_processing_time: float = 0.0
    idle_time: float = 0.0
    current_task_start: Optional[float] = None

class ParallelProcessor:
    """
    Parallel processor with dynamic worker allocation.

    Production features:
    1. Auto-scaling based on queue depth and CPU utilization
    2. Graceful degradation under resource constraints
    3. Work-stealing from idle workers
    4. Batch processing for efficiency
    5. Per-worker metrics for monitoring

    Performance characteristics:
    - Throughput: Linear scaling up to CPU core count
    - Latency: Queue depth × batch time
    - Overhead: ~5% for coordination
    """

    def __init__(
        self,
        name: str,
        process_function: Callable[[Any], Any],
        config: Optional[ParallelConfig] = None
    ):
        self.name = name
        self.process_function = process_function
        self.config = config or ParallelConfig()

        # Worker pool
        self.executor: Optional[ThreadPoolExecutor] = None
        self.current_workers = 0

        # Work queue
        self.work_queue: Queue = Queue(maxsize=self.config.queue_size)
        self.running = False

        # Worker metrics
        self.worker_metrics: Dict[int, WorkerMetrics] = {}
        self.next_worker_id = 0

        # Auto-scaling thread
        self.autoscale_thread: Optional[threading.Thread] = None

        # Global metrics
        self.total_tasks_submitted = 0
        self.total_tasks_completed = 0
        self.total_tasks_failed = 0

    def start(self):
        """Start parallel processor."""
        if self.running:
            return

        self.running = True

        # Create initial worker pool
        self._scale_workers(self.config.initial_workers)

        # Start auto-scaling thread
        if self.config.auto_scale_enabled:
            self.autoscale_thread = threading.Thread(
                target=self._autoscale_loop,
                daemon=True
            )
            self.autoscale_thread.start()

        logger.info(
            f"Parallel processor '{self.name}' started with "
            f"{self.current_workers} workers"
        )

    def stop(self):
        """Stop parallel processor."""
        self.running = False

        if self.executor:
            self.executor.shutdown(wait=True)

        logger.info(f"Parallel processor '{self.name}' stopped")

    def submit(self, task: Any) -> bool:
        """
        Submit task for processing.

        Args:
            task: Task to process

        Returns:
            True if submitted, False if queue full
        """
        try:
            self.work_queue.put(task, block=False)
            self.total_tasks_submitted += 1
            return True
        except:
            logger.warning(f"Work queue full, dropping task")
            return False

    def submit_batch(self, tasks: List[Any]) -> int:
        """Submit batch of tasks. Returns number submitted."""
        submitted = 0
        for task in tasks:
            if self.submit(task):
                submitted += 1
            else:
                break
        return submitted

    def _scale_workers(self, target_workers: int):
        """Scale worker pool to target size."""
        target_workers = max(
            self.config.min_workers,
            min(target_workers, self.config.max_workers)
        )

        if target_workers == self.current_workers:
            return

        logger.info(
            f"Scaling workers: {self.current_workers} → {target_workers}"
        )

        # Shutdown existing executor
        if self.executor:
            self.executor.shutdown(wait=False)

        # Create new executor with target workers
        self.executor = ThreadPoolExecutor(
            max_workers=target_workers,
            thread_name_prefix=f"{self.name}-worker"
        )

        # Submit worker tasks
        for i in range(target_workers):
            worker_id = self.next_worker_id
            self.next_worker_id += 1

            self.worker_metrics[worker_id] = WorkerMetrics(worker_id=worker_id)
            self.executor.submit(self._worker_loop, worker_id)

        self.current_workers = target_workers

    def _worker_loop(self, worker_id: int):
        """Main loop for worker thread."""
        metrics = self.worker_metrics[worker_id]

        logger.debug(f"Worker {worker_id} started")

        while self.running:
            try:
                # Get batch of tasks
                batch = []
                batch_start = time.time()

                # Try to get full batch
                for _ in range(self.config.batch_size):
                    try:
                        task = self.work_queue.get(timeout=0.1)
                        batch.append(task)
                    except Empty:
                        break

                if not batch:
                    # No work available - idle
                    idle_start = time.time()
                    time.sleep(0.1)
                    metrics.idle_time += time.time() - idle_start
                    continue

                # Process batch
                metrics.current_task_start = time.time()

                for task in batch:
                    try:
                        self.process_function(task)
                        metrics.tasks_processed += 1
                        self.total_tasks_completed += 1

                    except Exception as e:
                        metrics.tasks_failed += 1
                        self.total_tasks_failed += 1
                        logger.error(f"Task processing failed: {e}")

                processing_time = time.time() - metrics.current_task_start
                metrics.total_processing_time += processing_time
                metrics.current_task_start = None

            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}")

        logger.debug(f"Worker {worker_id} stopped")

    def _autoscale_loop(self):
        """Auto-scaling loop."""
        while self.running:
            try:
                time.sleep(self.config.scale_check_interval_seconds)

                # Calculate metrics
                queue_utilization = self.work_queue.qsize() / self.config.queue_size
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024

                # Determine scaling action
                target_workers = self.current_workers

                # Scale up if queue backing up or high CPU but queue has work
                if queue_utilization > self.config.scale_up_threshold:
                    target_workers = min(
                        self.current_workers + 2,
                        self.config.max_workers
                    )
                    logger.info(
                        f"Scaling up: queue utilization {queue_utilization:.1%}"
                    )

                # Scale down if queue empty and low CPU
                elif (queue_utilization < self.config.scale_down_threshold
                      and cpu_percent < 50):
                    target_workers = max(
                        self.current_workers - 1,
                        self.config.min_workers
                    )
                    logger.info(
                        f"Scaling down: queue utilization {queue_utilization:.1%}, "
                        f"CPU {cpu_percent:.1f}%"
                    )

                # Check resource limits
                if cpu_percent > self.config.max_cpu_percent:
                    logger.warning(
                        f"CPU limit exceeded: {cpu_percent:.1f}% > "
                        f"{self.config.max_cpu_percent}%"
                    )

                if memory_mb > self.config.max_memory_mb:
                    logger.warning(
                        f"Memory limit exceeded: {memory_mb:.0f}MB > "
                        f"{self.config.max_memory_mb}MB"
                    )
                    # Scale down to reduce memory
                    target_workers = max(
                        self.current_workers - 2,
                        self.config.min_workers
                    )

                # Apply scaling
                if target_workers != self.current_workers:
                    self._scale_workers(target_workers)

            except Exception as e:
                logger.error(f"Auto-scaling error: {e}")

    def get_metrics(self) -> dict:
        """Get parallel processor metrics."""
        # Calculate aggregate worker metrics
        total_processed = sum(m.tasks_processed for m in self.worker_metrics.values())
        total_failed = sum(m.tasks_failed for m in self.worker_metrics.values())
        total_processing_time = sum(
            m.total_processing_time for m in self.worker_metrics.values()
        )
        total_idle_time = sum(m.idle_time for m in self.worker_metrics.values())

        avg_task_time = (
            total_processing_time / total_processed
            if total_processed > 0 else 0
        )

        worker_utilization = (
            total_processing_time / (total_processing_time + total_idle_time)
            if (total_processing_time + total_idle_time) > 0 else 0
        )

        return {
            'name': self.name,
            'current_workers': self.current_workers,
            'queue_size': self.work_queue.qsize(),
            'queue_utilization': self.work_queue.qsize() / self.config.queue_size,
            'total_tasks_submitted': self.total_tasks_submitted,
            'total_tasks_completed': self.total_tasks_completed,
            'total_tasks_failed': self.total_tasks_failed,
            'success_rate': (
                self.total_tasks_completed / self.total_tasks_submitted
                if self.total_tasks_submitted > 0 else 0
            ),
            'avg_task_time_seconds': avg_task_time,
            'worker_utilization': worker_utilization,
            'throughput_per_second': (
                total_processed / total_processing_time
                if total_processing_time > 0 else 0
            )
        }

# Example usage
def process_event(event: dict):
    """Process a single event."""
    # Simulate processing
    time.sleep(0.001)  # 1ms processing time

processor = ParallelProcessor(
    name="event_processor",
    process_function=process_event,
    config=ParallelConfig(
        min_workers=2,
        max_workers=16,
        initial_workers=4,
        auto_scale_enabled=True
    )
)

processor.start()

# Submit events
events = [{'id': i} for i in range(10000)]
processor.submit_batch(events)

# Monitor
time.sleep(5)
metrics = processor.get_metrics()
logger.info(f"Throughput: {metrics['throughput_per_second']:.0f} events/sec")

processor.stop()
\end{lstlisting}

\subsection{DataPartitioner: Optimal Partitioning Strategies}

Intelligent data partitioning is critical for parallel processing performance. Poor partitioning creates hotspots and load imbalance.

\begin{lstlisting}[style=python, caption={Data Partitioning Strategies}]
from typing import Any, List, Callable, Optional
from dataclasses import dataclass
from enum import Enum
import hashlib
import logging

logger = logging.getLogger(__name__)

class PartitionStrategy(Enum):
    """Partitioning strategy types."""
    HASH = "hash"  # Hash-based (uniform distribution)
    RANGE = "range"  # Range-based (sorted data)
    ROUND_ROBIN = "round_robin"  # Cyclic assignment
    COMPOSITE = "composite"  # Multiple keys
    CONSISTENT_HASH = "consistent_hash"  # Consistent hashing (minimal rebalancing)

@dataclass
class PartitionStats:
    """Statistics for partition load balancing."""
    partition_id: int
    event_count: int = 0
    bytes_processed: int = 0
    avg_processing_time_ms: float = 0.0

class DataPartitioner:
    """
    Intelligent data partitioning for parallel processing.

    Production partitioning strategies:
    1. Hash partitioning: Uniform distribution (good for unknown data)
    2. Range partitioning: Sorted access patterns (good for time-series)
    3. Composite partitioning: Multiple keys (good for hierarchical data)
    4. Consistent hashing: Minimal rebalancing (good for dynamic clusters)

    Partitioning goals:
    - Uniform distribution (avoid hotspots)
    - Maintain related data locality
    - Minimize cross-partition queries
    - Enable parallel processing
    """

    def __init__(
        self,
        num_partitions: int,
        strategy: PartitionStrategy = PartitionStrategy.HASH
    ):
        self.num_partitions = num_partitions
        self.strategy = strategy

        # Partition statistics
        self.partition_stats: Dict[int, PartitionStats] = {
            i: PartitionStats(partition_id=i)
            for i in range(num_partitions)
        }

        # Round-robin state
        self.round_robin_counter = 0

        # Consistent hashing ring
        self.consistent_hash_ring: List[tuple[int, int]] = []  # (hash, partition)
        if strategy == PartitionStrategy.CONSISTENT_HASH:
            self._build_consistent_hash_ring()

    def partition(
        self,
        key: Any,
        num_replicas: int = 1
    ) -> List[int]:
        """
        Determine partition(s) for key.

        Args:
            key: Partition key (can be single value or composite)
            num_replicas: Number of replicas (for fault tolerance)

        Returns:
            List of partition IDs
        """
        if self.strategy == PartitionStrategy.HASH:
            return self._hash_partition(key, num_replicas)

        elif self.strategy == PartitionStrategy.RANGE:
            return self._range_partition(key, num_replicas)

        elif self.strategy == PartitionStrategy.ROUND_ROBIN:
            return self._round_robin_partition(num_replicas)

        elif self.strategy == PartitionStrategy.COMPOSITE:
            return self._composite_partition(key, num_replicas)

        elif self.strategy == PartitionStrategy.CONSISTENT_HASH:
            return self._consistent_hash_partition(key, num_replicas)

        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    def _hash_partition(self, key: Any, num_replicas: int) -> List[int]:
        """Hash-based partitioning for uniform distribution."""
        # Convert key to string and hash
        key_str = str(key)
        hash_value = int(hashlib.md5(key_str.encode()).hexdigest(), 16)

        # Primary partition
        primary = hash_value % self.num_partitions
        partitions = [primary]

        # Add replicas (offset by hash)
        for i in range(1, num_replicas):
            replica = (primary + i) % self.num_partitions
            partitions.append(replica)

        return partitions

    def _range_partition(self, key: Any, num_replicas: int) -> List[int]:
        """
        Range-based partitioning for sorted data.

        Example: Time-series data partitioned by timestamp
        """
        # Assume key is numeric (timestamp, ID, etc.)
        try:
            key_value = float(key)

            # Simple range partitioning (equal ranges)
            # Production would use configurable range boundaries
            partition = int((key_value % 1000000) / (1000000 / self.num_partitions))
            partition = min(partition, self.num_partitions - 1)

            partitions = [partition]

            # Replicas in adjacent partitions
            for i in range(1, num_replicas):
                replica = (partition + i) % self.num_partitions
                partitions.append(replica)

            return partitions

        except (ValueError, TypeError):
            # Fall back to hash if not numeric
            return self._hash_partition(key, num_replicas)

    def _round_robin_partition(self, num_replicas: int) -> List[int]:
        """Round-robin partitioning for uniform distribution."""
        primary = self.round_robin_counter % self.num_partitions
        self.round_robin_counter += 1

        partitions = [primary]

        for i in range(1, num_replicas):
            replica = (primary + i) % self.num_partitions
            partitions.append(replica)

        return partitions

    def _composite_partition(self, key: Any, num_replicas: int) -> List[int]:
        """
        Composite partitioning using multiple key components.

        Example: (city, driver_id) where city provides locality,
        driver_id provides distribution within city.
        """
        if isinstance(key, (list, tuple)) and len(key) >= 2:
            # Use first component for coarse partitioning
            coarse_key = key[0]
            coarse_hash = int(hashlib.md5(str(coarse_key).encode()).hexdigest(), 16)
            coarse_partition = coarse_hash % (self.num_partitions // 4)  # 25% buckets

            # Use second component for fine partitioning within coarse bucket
            fine_key = key[1]
            fine_hash = int(hashlib.md5(str(fine_key).encode()).hexdigest(), 16)
            fine_offset = fine_hash % 4  # 4 partitions per coarse bucket

            primary = coarse_partition * 4 + fine_offset
            primary = min(primary, self.num_partitions - 1)

            partitions = [primary]

            for i in range(1, num_replicas):
                replica = (primary + i) % self.num_partitions
                partitions.append(replica)

            return partitions
        else:
            # Fall back to hash if not composite
            return self._hash_partition(key, num_replicas)

    def _build_consistent_hash_ring(self, virtual_nodes_per_partition: int = 100):
        """Build consistent hashing ring."""
        self.consistent_hash_ring = []

        for partition_id in range(self.num_partitions):
            # Create virtual nodes for each partition
            for i in range(virtual_nodes_per_partition):
                node_key = f"{partition_id}-{i}"
                hash_value = int(hashlib.md5(node_key.encode()).hexdigest(), 16)
                self.consistent_hash_ring.append((hash_value, partition_id))

        # Sort ring by hash value
        self.consistent_hash_ring.sort(key=lambda x: x[0])

    def _consistent_hash_partition(self, key: Any, num_replicas: int) -> List[int]:
        """
        Consistent hashing for minimal rebalancing.

        When adding/removing partitions, only K/N keys move
        (K = total keys, N = number of partitions).
        """
        if not self.consistent_hash_ring:
            self._build_consistent_hash_ring()

        # Hash the key
        key_str = str(key)
        hash_value = int(hashlib.md5(key_str.encode()).hexdigest(), 16)

        # Find position in ring (binary search)
        left, right = 0, len(self.consistent_hash_ring)
        while left < right:
            mid = (left + right) // 2
            if self.consistent_hash_ring[mid][0] < hash_value:
                left = mid + 1
            else:
                right = mid

        # Get partitions (with wraparound)
        partitions = []
        seen = set()

        for i in range(len(self.consistent_hash_ring)):
            idx = (left + i) % len(self.consistent_hash_ring)
            _, partition_id = self.consistent_hash_ring[idx]

            if partition_id not in seen:
                partitions.append(partition_id)
                seen.add(partition_id)

                if len(partitions) >= num_replicas:
                    break

        return partitions

    def record_event(self, partition_id: int, processing_time_ms: float, size_bytes: int):
        """Record event for partition statistics."""
        stats = self.partition_stats[partition_id]
        stats.event_count += 1
        stats.bytes_processed += size_bytes

        # Update moving average
        alpha = 0.1  # Smoothing factor
        stats.avg_processing_time_ms = (
            alpha * processing_time_ms +
            (1 - alpha) * stats.avg_processing_time_ms
        )

    def get_partition_balance(self) -> dict:
        """Calculate partition load balance metrics."""
        counts = [s.event_count for s in self.partition_stats.values()]

        if not counts or sum(counts) == 0:
            return {
                'balance_factor': 1.0,
                'max_skew': 0.0,
                'hotspot_partitions': []
            }

        avg_count = sum(counts) / len(counts)
        max_count = max(counts)
        min_count = min(counts)

        # Balance factor: 1.0 = perfect, <0.5 = severe imbalance
        balance_factor = min_count / max_count if max_count > 0 else 1.0

        # Max skew: How much hottest partition exceeds average
        max_skew = (max_count - avg_count) / avg_count if avg_count > 0 else 0.0

        # Identify hotspot partitions (>2x average)
        hotspot_threshold = avg_count * 2
        hotspot_partitions = [
            s.partition_id for s in self.partition_stats.values()
            if s.event_count > hotspot_threshold
        ]

        return {
            'balance_factor': balance_factor,
            'max_skew': max_skew,
            'hotspot_partitions': hotspot_partitions,
            'partition_stats': {
                'min': min_count,
                'max': max_count,
                'avg': avg_count,
                'total': sum(counts)
            }
        }

# Example: Comparing partitioning strategies
def compare_partitioning_strategies():
    """Compare different partitioning strategies."""
    num_partitions = 10
    num_events = 10000

    strategies = [
        PartitionStrategy.HASH,
        PartitionStrategy.ROUND_ROBIN,
        PartitionStrategy.CONSISTENT_HASH
    ]

    # Simulate events (some with geographic skew like NYC)
    events = []
    for i in range(num_events):
        if i < 4000:  # 40% from "NYC"
            city = "NYC"
        elif i < 6000:  # 20% from "LA"
            city = "LA"
        else:  # 40% distributed across other cities
            city = f"City{i % 20}"

        events.append({
            'driver_id': i,
            'city': city,
            'composite_key': (city, i)
        })

    results = {}

    for strategy in strategies:
        partitioner = DataPartitioner(num_partitions, strategy)

        # Partition events
        for event in events:
            if strategy == PartitionStrategy.COMPOSITE:
                key = event['composite_key']
            else:
                key = event['driver_id']

            partitions = partitioner.partition(key)
            partitioner.record_event(partitions[0], 1.0, 100)

        # Get balance metrics
        balance = partitioner.get_partition_balance()
        results[strategy.value] = balance

        logger.info(
            f"{strategy.value}: balance={balance['balance_factor']:.2f}, "
            f"skew={balance['max_skew']:.2f}, "
            f"hotspots={len(balance['hotspot_partitions'])}"
        )

    return results
\end{lstlisting}

\subsection{LoadBalancer: Work Distribution}

Load balancers distribute work across workers to maximize throughput and minimize latency.

\begin{lstlisting}[style=python, caption={Load Balancer with Work Stealing}]
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from queue import Queue, Empty
from collections import deque
import threading
import time
import logging

logger = logging.getLogger(__name__)

@dataclass
class WorkerQueue:
    """Work queue for a single worker."""
    worker_id: int
    queue: deque = field(default_factory=deque)
    tasks_processed: int = 0
    current_load: int = 0  # Number of tasks in queue
    is_busy: bool = False

class LoadBalancer:
    """
    Load balancer with work-stealing algorithm.

    Production strategies:
    1. Least-loaded: Assign to worker with smallest queue
    2. Round-robin: Cyclic assignment (simple, no coordination)
    3. Work-stealing: Idle workers steal from busy workers
    4. Locality-aware: Prefer workers with related data cached

    Performance characteristics:
    - Least-loaded: Best balance, requires coordination
    - Round-robin: Minimal overhead, may cause imbalance
    - Work-stealing: Self-balancing, handles heterogeneous tasks
    """

    def __init__(self, num_workers: int):
        self.num_workers = num_workers

        # Worker queues
        self.worker_queues: Dict[int, WorkerQueue] = {
            i: WorkerQueue(worker_id=i)
            for i in range(num_workers)
        }

        # Coordination
        self.lock = threading.Lock()

        # Metrics
        self.total_tasks_assigned = 0
        self.steal_attempts = 0
        self.steal_successes = 0

    def assign_task(self, task: Any, strategy: str = "least_loaded") -> int:
        """
        Assign task to worker.

        Args:
            task: Task to assign
            strategy: Assignment strategy

        Returns:
            Worker ID assigned to
        """
        with self.lock:
            self.total_tasks_assigned += 1

            if strategy == "least_loaded":
                return self._assign_least_loaded(task)
            elif strategy == "round_robin":
                return self._assign_round_robin(task)
            elif strategy == "locality_aware":
                return self._assign_locality_aware(task)
            else:
                raise ValueError(f"Unknown strategy: {strategy}")

    def _assign_least_loaded(self, task: Any) -> int:
        """Assign to worker with smallest queue."""
        # Find worker with minimum load
        min_worker = min(
            self.worker_queues.values(),
            key=lambda w: w.current_load
        )

        min_worker.queue.append(task)
        min_worker.current_load += 1

        return min_worker.worker_id

    def _assign_round_robin(self, task: Any) -> int:
        """Assign in round-robin fashion."""
        worker_id = self.total_tasks_assigned % self.num_workers
        worker = self.worker_queues[worker_id]

        worker.queue.append(task)
        worker.current_load += 1

        return worker_id

    def _assign_locality_aware(self, task: Any) -> int:
        """
        Assign based on data locality.

        Example: Route events for same customer_id to same worker
        for cache locality.
        """
        # Extract locality key from task
        locality_key = self._extract_locality_key(task)

        # Hash to worker
        if locality_key:
            import hashlib
            hash_value = int(hashlib.md5(str(locality_key).encode()).hexdigest(), 16)
            worker_id = hash_value % self.num_workers
        else:
            # Fall back to least-loaded
            return self._assign_least_loaded(task)

        worker = self.worker_queues[worker_id]
        worker.queue.append(task)
        worker.current_load += 1

        return worker_id

    def _extract_locality_key(self, task: Any) -> Optional[Any]:
        """Extract locality key from task."""
        # Implementation depends on task structure
        if isinstance(task, dict):
            # Try common locality keys
            for key in ['customer_id', 'user_id', 'session_id', 'partition']:
                if key in task:
                    return task[key]
        return None

    def get_task(self, worker_id: int, enable_stealing: bool = True) -> Optional[Any]:
        """
        Get next task for worker.

        Args:
            worker_id: Worker requesting task
            enable_stealing: Allow stealing from other workers

        Returns:
            Task or None if no work available
        """
        worker = self.worker_queues[worker_id]

        # Try own queue first
        with self.lock:
            if worker.queue:
                task = worker.queue.popleft()
                worker.current_load -= 1
                worker.tasks_processed += 1
                worker.is_busy = True
                return task

        # No work in own queue - try stealing
        if enable_stealing:
            stolen_task = self._steal_work(worker_id)
            if stolen_task:
                return stolen_task

        # No work available
        with self.lock:
            worker.is_busy = False

        return None

    def _steal_work(self, thief_worker_id: int) -> Optional[Any]:
        """
        Steal work from busiest worker.

        Work-stealing algorithm:
        1. Find busiest worker (largest queue)
        2. Steal half of their queue
        3. Update load balancing metrics
        """
        self.steal_attempts += 1

        with self.lock:
            # Find busiest worker (excluding self)
            busiest_worker = max(
                (w for w in self.worker_queues.values() if w.worker_id != thief_worker_id),
                key=lambda w: w.current_load,
                default=None
            )

            if not busiest_worker or busiest_worker.current_load < 2:
                # Not worth stealing
                return None

            # Steal half of busiest worker's queue
            steal_count = busiest_worker.current_load // 2

            if steal_count == 0:
                return None

            thief_worker = self.worker_queues[thief_worker_id]
            stolen_tasks = []

            for _ in range(steal_count):
                if busiest_worker.queue:
                    task = busiest_worker.queue.pop()  # Take from end
                    stolen_tasks.append(task)
                    busiest_worker.current_load -= 1

            # Add stolen tasks to thief's queue
            thief_worker.queue.extend(stolen_tasks[1:])  # Keep first for immediate return
            thief_worker.current_load += len(stolen_tasks) - 1

            if stolen_tasks:
                self.steal_successes += 1
                logger.debug(
                    f"Worker {thief_worker_id} stole {len(stolen_tasks)} tasks "
                    f"from worker {busiest_worker.worker_id}"
                )
                return stolen_tasks[0]

        return None

    def mark_task_complete(self, worker_id: int):
        """Mark current task as complete for worker."""
        with self.lock:
            worker = self.worker_queues[worker_id]
            worker.is_busy = False

    def get_metrics(self) -> dict:
        """Get load balancer metrics."""
        with self.lock:
            loads = [w.current_load for w in self.worker_queues.values()]
            tasks_processed = [w.tasks_processed for w in self.worker_queues.values()]

            avg_load = sum(loads) / len(loads) if loads else 0
            max_load = max(loads) if loads else 0
            min_load = min(loads) if loads else 0

            load_imbalance = (
                (max_load - min_load) / avg_load
                if avg_load > 0 else 0
            )

            return {
                'total_tasks_assigned': self.total_tasks_assigned,
                'total_tasks_processed': sum(tasks_processed),
                'steal_attempts': self.steal_attempts,
                'steal_successes': self.steal_successes,
                'steal_success_rate': (
                    self.steal_successes / self.steal_attempts
                    if self.steal_attempts > 0 else 0
                ),
                'current_load': {
                    'avg': avg_load,
                    'max': max_load,
                    'min': min_load,
                    'imbalance': load_imbalance
                },
                'worker_stats': {
                    w.worker_id: {
                        'current_load': w.current_load,
                        'tasks_processed': w.tasks_processed,
                        'is_busy': w.is_busy
                    }
                    for w in self.worker_queues.values()
                }
            }

# Example usage
load_balancer = LoadBalancer(num_workers=4)

# Assign tasks
for i in range(100):
    task = {'id': i, 'customer_id': i % 20}
    worker_id = load_balancer.assign_task(task, strategy="least_loaded")

# Workers retrieve tasks with work-stealing
worker_id = 0
task = load_balancer.get_task(worker_id, enable_stealing=True)

if task:
    # Process task
    load_balancer.mark_task_complete(worker_id)

# Monitor balance
metrics = load_balancer.get_metrics()
logger.info(f"Load imbalance: {metrics['current_load']['imbalance']:.2%}")
\end{lstlisting}

\subsection{PerformanceMonitor: Bottleneck Detection}

Production pipelines require continuous performance monitoring to detect bottlenecks before they impact users.

\begin{lstlisting}[style=python, caption={Performance Monitor with Bottleneck Detection}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import deque
import time
import threading
import logging
import psutil

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """Performance metrics snapshot."""
    timestamp: datetime
    throughput_events_per_sec: float
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float
    cpu_percent: float
    memory_mb: float
    queue_depth: int
    worker_utilization: float

@dataclass
class Bottleneck:
    """Identified performance bottleneck."""
    type: str  # "cpu", "memory", "io", "queue", "serialization"
    severity: str  # "critical", "warning", "info"
    description: str
    metrics: Dict[str, Any]
    recommendations: List[str]

class PerformanceMonitor:
    """
    Monitor pipeline performance and detect bottlenecks.

    Production capabilities:
    1. Real-time throughput and latency tracking
    2. Resource utilization monitoring (CPU, memory, I/O)
    3. Automatic bottleneck detection
    4. Performance trend analysis
    5. Actionable optimization recommendations

    Bottleneck detection algorithms:
    - CPU bound: High CPU (>80%), low queue depth
    - Memory bound: High memory (>80%), frequent GC
    - I/O bound: Low CPU, high latency, network/disk saturation
    - Queue bound: Growing queue depth despite available resources
    - Serialization bound: High CPU in deserialization, low throughput
    """

    def __init__(
        self,
        name: str,
        check_interval_seconds: int = 5,
        history_size: int = 1000
    ):
        self.name = name
        self.check_interval_seconds = check_interval_seconds
        self.history_size = history_size

        # Metrics history
        self.metrics_history: deque = deque(maxlen=history_size)

        # Latency samples (for percentile calculation)
        self.latency_samples: deque = deque(maxlen=10000)

        # Event counters
        self.events_processed = 0
        self.last_events_processed = 0
        self.last_check_time = time.time()

        # Monitoring thread
        self.running = False
        self.monitor_thread: Optional[threading.Thread] = None

        # Detected bottlenecks
        self.current_bottlenecks: List[Bottleneck] = []

        # Lock for thread safety
        self.lock = threading.Lock()

    def start(self):
        """Start performance monitoring."""
        if self.running:
            return

        self.running = True
        self.last_check_time = time.time()

        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            daemon=True
        )
        self.monitor_thread.start()

        logger.info(f"Performance monitor '{self.name}' started")

    def stop(self):
        """Stop performance monitoring."""
        self.running = False

        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)

        logger.info(f"Performance monitor '{self.name}' stopped")

    def record_event(self, latency_ms: float):
        """Record event processing for metrics."""
        with self.lock:
            self.events_processed += 1
            self.latency_samples.append(latency_ms)

    def record_batch(self, count: int, total_latency_ms: float):
        """Record batch processing for efficiency."""
        with self.lock:
            self.events_processed += count
            avg_latency = total_latency_ms / count if count > 0 else 0
            # Add average latency for batch
            self.latency_samples.append(avg_latency)

    def _monitor_loop(self):
        """Main monitoring loop."""
        while self.running:
            try:
                time.sleep(self.check_interval_seconds)

                # Collect metrics
                metrics = self._collect_metrics()

                # Store in history
                with self.lock:
                    self.metrics_history.append(metrics)

                # Detect bottlenecks
                bottlenecks = self._detect_bottlenecks(metrics)

                with self.lock:
                    self.current_bottlenecks = bottlenecks

                # Log bottlenecks
                for bottleneck in bottlenecks:
                    if bottleneck.severity == "critical":
                        logger.error(
                            f"CRITICAL bottleneck detected: {bottleneck.description}"
                        )
                    elif bottleneck.severity == "warning":
                        logger.warning(
                            f"Performance warning: {bottleneck.description}"
                        )

            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")

    def _collect_metrics(self) -> PerformanceMetrics:
        """Collect current performance metrics."""
        now = time.time()

        with self.lock:
            # Calculate throughput
            events_delta = self.events_processed - self.last_events_processed
            time_delta = now - self.last_check_time

            throughput = events_delta / time_delta if time_delta > 0 else 0

            # Update counters
            self.last_events_processed = self.events_processed
            self.last_check_time = now

            # Calculate latency percentiles
            latency_p50, latency_p95, latency_p99 = self._calculate_percentiles(
                list(self.latency_samples)
            )

        # System resources
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024

        # Queue depth (would be provided by processor)
        queue_depth = 0  # Placeholder

        # Worker utilization (would be provided by processor)
        worker_utilization = 0.0  # Placeholder

        return PerformanceMetrics(
            timestamp=datetime.now(),
            throughput_events_per_sec=throughput,
            latency_p50_ms=latency_p50,
            latency_p95_ms=latency_p95,
            latency_p99_ms=latency_p99,
            cpu_percent=cpu_percent,
            memory_mb=memory_mb,
            queue_depth=queue_depth,
            worker_utilization=worker_utilization
        )

    def _calculate_percentiles(self, samples: List[float]) -> tuple[float, float, float]:
        """Calculate latency percentiles."""
        if not samples:
            return 0.0, 0.0, 0.0

        sorted_samples = sorted(samples)
        n = len(sorted_samples)

        p50_idx = int(n * 0.50)
        p95_idx = int(n * 0.95)
        p99_idx = int(n * 0.99)

        p50 = sorted_samples[min(p50_idx, n - 1)]
        p95 = sorted_samples[min(p95_idx, n - 1)]
        p99 = sorted_samples[min(p99_idx, n - 1)]

        return p50, p95, p99

    def _detect_bottlenecks(self, metrics: PerformanceMetrics) -> List[Bottleneck]:
        """Detect performance bottlenecks."""
        bottlenecks = []

        # CPU bottleneck: High CPU with low throughput
        if metrics.cpu_percent > 80:
            severity = "critical" if metrics.cpu_percent > 90 else "warning"
            bottlenecks.append(Bottleneck(
                type="cpu",
                severity=severity,
                description=f"High CPU utilization: {metrics.cpu_percent:.1f}%",
                metrics={'cpu_percent': metrics.cpu_percent},
                recommendations=[
                    "Profile CPU-intensive operations",
                    "Consider parallel processing",
                    "Optimize serialization/deserialization",
                    "Scale horizontally (add workers)"
                ]
            ))

        # Memory bottleneck
        if metrics.memory_mb > 3072:  # >3GB
            severity = "critical" if metrics.memory_mb > 4096 else "warning"
            bottlenecks.append(Bottleneck(
                type="memory",
                severity=severity,
                description=f"High memory usage: {metrics.memory_mb:.0f}MB",
                metrics={'memory_mb': metrics.memory_mb},
                recommendations=[
                    "Check for memory leaks",
                    "Reduce window sizes for aggregations",
                    "Implement batch processing",
                    "Increase instance memory or scale out"
                ]
            ))

        # Latency bottleneck: High tail latencies
        if metrics.latency_p99_ms > 1000:  # >1 second
            severity = "critical" if metrics.latency_p99_ms > 5000 else "warning"
            bottlenecks.append(Bottleneck(
                type="latency",
                severity=severity,
                description=(
                    f"High tail latency: p99={metrics.latency_p99_ms:.0f}ms, "
                    f"p95={metrics.latency_p95_ms:.0f}ms"
                ),
                metrics={
                    'latency_p50_ms': metrics.latency_p50_ms,
                    'latency_p95_ms': metrics.latency_p95_ms,
                    'latency_p99_ms': metrics.latency_p99_ms
                },
                recommendations=[
                    "Profile slow operations",
                    "Check external dependency latencies",
                    "Implement caching for frequent lookups",
                    "Add timeouts to prevent stragglers"
                ]
            ))

        # Queue bottleneck: Growing queue depth
        if metrics.queue_depth > 5000:
            severity = "critical" if metrics.queue_depth > 10000 else "warning"
            bottlenecks.append(Bottleneck(
                type="queue",
                severity=severity,
                description=f"High queue depth: {metrics.queue_depth} events",
                metrics={'queue_depth': metrics.queue_depth},
                recommendations=[
                    "Scale up workers",
                    "Check for downstream bottlenecks",
                    "Implement backpressure",
                    "Increase processing parallelism"
                ]
            ))

        # Throughput degradation: Check trend
        if len(self.metrics_history) >= 10:
            throughput_trend = self._calculate_throughput_trend()

            if throughput_trend < -0.2:  # >20% degradation
                bottlenecks.append(Bottleneck(
                    type="throughput_degradation",
                    severity="warning",
                    description=f"Throughput degrading: {throughput_trend:.1%} trend",
                    metrics={'throughput_trend': throughput_trend},
                    recommendations=[
                        "Investigate recent changes",
                        "Check resource utilization trends",
                        "Look for data volume increases",
                        "Review recent deployments"
                    ]
                ))

        return bottlenecks

    def _calculate_throughput_trend(self) -> float:
        """
        Calculate throughput trend over recent history.

        Returns:
            Trend coefficient: positive = improving, negative = degrading
        """
        with self.lock:
            if len(self.metrics_history) < 10:
                return 0.0

            recent_metrics = list(self.metrics_history)[-10:]

        throughputs = [m.throughput_events_per_sec for m in recent_metrics]

        # Simple linear regression slope
        n = len(throughputs)
        x_mean = (n - 1) / 2
        y_mean = sum(throughputs) / n

        numerator = sum((i - x_mean) * (y - y_mean) for i, y in enumerate(throughputs))
        denominator = sum((i - x_mean) ** 2 for i in range(n))

        slope = numerator / denominator if denominator != 0 else 0

        # Normalize by average throughput
        trend = slope / y_mean if y_mean > 0 else 0

        return trend

    def generate_performance_report(self) -> str:
        """Generate performance analysis report."""
        report = []
        report.append("=" * 60)
        report.append(f"PERFORMANCE REPORT: {self.name}")
        report.append("=" * 60)
        report.append("")

        # Current metrics
        current = self.metrics_history[-1] if self.metrics_history else None
        if current:
            report.append("Current Performance:")
            report.append(f"  Throughput: {current.throughput_events_per_sec:.0f} events/sec")
            report.append(f"  Latency p50: {current.latency_p50_ms:.1f}ms")
            report.append(f"  Latency p95: {current.latency_p95_ms:.1f}ms")
            report.append(f"  Latency p99: {current.latency_p99_ms:.1f}ms")
            report.append(f"  CPU: {current.cpu_percent:.1f}%")
            report.append(f"  Memory: {current.memory_mb:.0f}MB")
            report.append("")

        # Bottlenecks
        if self.current_bottlenecks:
            report.append("Detected Bottlenecks:")
            for bottleneck in self.current_bottlenecks:
                report.append(f"  [{bottleneck.severity.upper()}] {bottleneck.type}:")
                report.append(f"    {bottleneck.description}")
                report.append(f"    Recommendations:")
                for rec in bottleneck.recommendations:
                    report.append(f"      - {rec}")
                report.append("")
        else:
            report.append("No bottlenecks detected")
            report.append("")

        report.append("=" * 60)
        return "\n".join(report)

# Example usage
monitor = PerformanceMonitor(name="kafka_processor", check_interval_seconds=5)
monitor.start()

# Simulate processing
for i in range(1000):
    monitor.record_event(latency_ms=random.uniform(1, 100))

time.sleep(10)
print(monitor.generate_performance_report())
monitor.stop()
\end{lstlisting}

\textbf{Key performance metrics to monitor:}
\begin{itemize}
    \item \textbf{Throughput}: Events processed per second (target: matches input rate)
    \item \textbf{Latency percentiles}: p50, p95, p99 processing time (target: <100ms p99)
    \item \textbf{Worker utilization}: Percentage of time workers busy (target: 70-90\%)
    \item \textbf{Queue depth}: Number of events waiting (target: <1000, alert if growing)
    \item \textbf{Partition balance}: Load distribution across partitions (target: >0.8)
    \item \textbf{CPU/Memory usage}: Resource utilization (target: <80\%)
\end{itemize}

\textbf{Performance optimization checklist:}
\begin{itemize}
    \item \textbf{Parallelism}: Dynamic worker allocation based on load
    \item \textbf{Partitioning}: Balanced distribution avoiding hotspots
    \item \textbf{Serialization}: Binary formats (Protobuf/Avro) over JSON
    \item \textbf{Batching}: Process events in batches (100-1000) for efficiency
    \item \textbf{Caching}: Cache frequently accessed reference data
    \item \textbf{Indexing}: Optimize database queries with proper indexes
    \item \textbf{Compression}: Compress network payloads (especially cross-region)
    \item \textbf{Resource limits}: Set CPU/memory limits to prevent resource starvation
\end{itemize}

\textbf{Alert thresholds:}
\begin{itemize}
    \item Throughput <50\% of expected → Warning (investigate degradation)
    \item Latency p99 >1 second → Warning (check bottlenecks)
    \item Latency p99 >5 seconds → Critical (page on-call)
    \item Queue depth growing >5 minutes → Warning (scaling needed)
    \item Worker utilization <30\% → Info (consider scaling down)
    \item Worker utilization >95\% → Warning (saturated, scale up)
    \item Partition balance <0.5 → Critical (severe hotspot)
    \item CPU >80\% sustained → Warning (approaching saturation)
\end{itemize}

\section{Caching and Data Compression}

Event-driven pipelines repeatedly access reference data: customer profiles, product catalogs, feature vectors, lookup tables. Fetching this data from databases on every event creates latency spikes and database bottlenecks. Production pipelines use intelligent caching to reduce database load by 90\%, cutting latency from 50ms to 500μs. However, naive caching creates new failure modes: cache stampedes overwhelm databases during invalidation, stale data causes incorrect results, and unbounded caches exhaust memory.

Data compression reduces network bandwidth and storage costs by 70-90\%, but poor compression choices hurt performance. JSON consumes 10x more bandwidth than Protobuf for the same data. Compressing already-compressed data wastes CPU. Production systems require intelligent compression selection based on data type, access patterns, and performance requirements.

\subsection{The Cache Stampede}

Consider a real-time personalization engine serving product recommendations:

\textbf{Architecture:}
\begin{itemize}
    \item 10,000 stream processor instances
    \item Each instance caches user preference models (1M users, 100KB per model)
    \item Cache hit rate: 95\% (19 of 20 requests cached)
    \item Database: PostgreSQL with 1000 connection limit
    \item Traffic: 500K requests/second
\end{itemize}

\textbf{Normal operation:}
\begin{itemize}
    \item Cache hit rate 95\% → 25K database queries/second (well within capacity)
    \item Average latency: 2ms (500μs cache hit, 50ms cache miss)
    \item Database CPU: 40\%
    \item System healthy
\end{itemize}

\textbf{Tuesday 14:00 - Deployment trigger:}
\begin{itemize}
    \item Engineering deploys new preference model version
    \item Deployment strategy: Rolling restart of all 10,000 instances
    \item Each restart clears in-memory cache (no persistent cache)
    \item Restart duration: 30 seconds per instance
    \item Total deployment: 30 minutes
\end{itemize}

\textbf{14:00 - Cache stampede begins:}
\begin{itemize}
    \item First 1000 instances restart simultaneously (batch deployment)
    \item All caches empty → 100\% cache miss rate for those instances
    \item Database load spike: 25K/sec → 75K/sec queries (+200\%)
    \item Database CPU: 40\% → 85\%
    \item Query latency: 50ms → 200ms
\end{itemize}

\textbf{14:05 - Second wave compounds problem:}
\begin{itemize}
    \item Next 1000 instances restart
    \item Database now serving: 2000 cold instances + 8000 warm instances
    \item Query load: 75K/sec → 125K/sec
    \item Database connection pool saturated (1000 connections)
    \item Connection wait time: 0ms → 500ms
    \item Total latency: 50ms → 700ms
\end{itemize}

\textbf{14:10 - Cascading failure:}
\begin{itemize}
    \item Slow cache fills cause request timeouts
    \item Timeout triggers retry logic
    \item Retry amplifies database load: 125K/sec → 250K/sec
    \item Database connections exhausted
    \item Connection errors: "FATAL: sorry, too many clients already"
    \item Personalization requests failing: 0\% → 30\% → 60\%
\end{itemize}

\textbf{14:15 - Total system failure:}
\begin{itemize}
    \item Database CPU: 100\%, swapping to disk
    \item Query latency: 700ms → 5 seconds → 30 seconds
    \item All 10,000 instances in cache-miss mode (can't fill cache during overload)
    \item Personalization system offline
    \item Fallback to default recommendations (no personalization)
    \item Customer engagement drops 40\%
\end{itemize}

\textbf{14:30 - Emergency response:}
\begin{itemize}
    \item Halt deployment immediately
    \item Scale database: Add 5 read replicas (takes 15 minutes)
    \item Implement connection throttling on stream processors
    \item Allow cache warmup before serving traffic
\end{itemize}

\textbf{15:00 - System recovery:}
\begin{itemize}
    \item Read replicas online, distributing load
    \item Caches gradually warming (controlled rate)
    \item Cache hit rate: 60\% → 80\% → 95\% (over 30 minutes)
    \item System fully recovered
    \item Total outage: 45 minutes
\end{itemize}

\textbf{Business impact:}
\begin{itemize}
    \item Revenue loss: \$1.8M (45 minutes at \$2.4M/hour during peak hours)
    \item Customer experience: 40\% engagement drop, 25\% bounce rate increase
    \item Engineering costs: \$30K (emergency response, database scaling)
    \item Reputation damage: Social media complaints, support tickets spike
\end{itemize}

\textbf{Root cause analysis:}
\begin{itemize}
    \item \textbf{No cache persistence}: Restarts cleared all cached data
    \item \textbf{Synchronous deployment}: 1000 instances restarted simultaneously
    \item \textbf{No cache warmup}: Instances immediately served traffic after restart
    \item \textbf{No stampede protection}: No request coalescing, jitter, or circuit breakers
    \item \textbf{No connection pooling limits}: Each instance could open unlimited connections
\end{itemize}

\textbf{Proper caching would have prevented this:}
\begin{itemize}
    \item \textbf{Persistent cache}: Redis/Memcached survives instance restarts
    \item \textbf{Cache warmup}: Preload hot keys before serving traffic
    \item \textbf{Request coalescing}: Single database query for concurrent identical requests
    \item \textbf{Probabilistic early refresh}: Refresh cache before expiration (jitter)
    \item \textbf{Circuit breaker}: Stop cache fills when database struggling
    \item \textbf{Gradual deployment}: Rolling restart with 1-2\% batch size
\end{itemize}

With proper caching: No stampede, deployment completes normally, zero user impact.

This scenario demonstrates why production pipelines require sophisticated cache management.

\subsection{CacheManager: Intelligent Eviction}

Production cache management requires eviction policies that balance hit rate, memory usage, and access patterns.

\begin{lstlisting}[style=python, caption={Cache Manager with Intelligent Eviction}]
from dataclasses import dataclass, field
from typing import Any, Optional, Dict, Callable
from datetime import datetime, timedelta
from collections import OrderedDict
import threading
import time
import hashlib
import logging

logger = logging.getLogger(__name__)

class EvictionPolicy(Enum):
    """Cache eviction policies."""
    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    TTL = "ttl"  # Time To Live
    SIZE_AWARE = "size_aware"  # Evict largest items first
    COST_AWARE = "cost_aware"  # Evict items with lowest cost/benefit ratio

@dataclass
class CacheEntry:
    """Entry in cache with metadata."""
    key: str
    value: Any
    size_bytes: int
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    fetch_cost_ms: float = 0.0  # Cost to fetch if evicted
    ttl_seconds: Optional[int] = None

    def is_expired(self) -> bool:
        """Check if entry has expired."""
        if self.ttl_seconds is None:
            return False
        age = (datetime.now() - self.created_at).total_seconds()
        return age > self.ttl_seconds

@dataclass
class CacheConfig:
    """Configuration for cache."""
    max_size_bytes: int = 1024 * 1024 * 1024  # 1GB default
    max_entries: int = 1000000  # 1M entries
    eviction_policy: EvictionPolicy = EvictionPolicy.LRU
    default_ttl_seconds: Optional[int] = 3600  # 1 hour
    enable_metrics: bool = True
    enable_stampede_protection: bool = True

class CacheManager:
    """
    Intelligent cache with multiple eviction policies.

    Production features:
    1. Multiple eviction policies (LRU, LFU, TTL, size-aware, cost-aware)
    2. Stampede protection (request coalescing)
    3. Probabilistic early refresh
    4. Size and entry count limits
    5. Comprehensive metrics

    Cache eviction policies:
    - LRU: Best for recency-biased workloads (news, trending items)
    - LFU: Best for frequency-biased workloads (popular products)
    - TTL: Best for time-sensitive data (prices, inventory)
    - Size-aware: Best for mixed-size data (images, documents)
    - Cost-aware: Best when refetch costs vary (simple queries vs ML inference)
    """

    def __init__(self, config: Optional[CacheConfig] = None):
        self.config = config or CacheConfig()

        # Cache storage
        self.cache: Dict[str, CacheEntry] = {}
        if self.config.eviction_policy == EvictionPolicy.LRU:
            self.cache = OrderedDict()  # Maintains insertion order

        # Thread safety
        self.lock = threading.RLock()

        # Stampede protection: Track in-flight requests
        self.in_flight: Dict[str, threading.Event] = {}

        # Metrics
        self.hits = 0
        self.misses = 0
        self.evictions = 0
        self.current_size_bytes = 0
        self.stampede_protections = 0

    def get(
        self,
        key: str,
        fetch_function: Optional[Callable[[], Any]] = None,
        ttl_seconds: Optional[int] = None
    ) -> Optional[Any]:
        """
        Get value from cache or fetch if missing.

        Args:
            key: Cache key
            fetch_function: Function to fetch value on cache miss
            ttl_seconds: TTL for this entry (overrides default)

        Returns:
            Cached or fetched value, or None if not found and no fetch function
        """
        with self.lock:
            # Check cache
            if key in self.cache:
                entry = self.cache[key]

                # Check expiration
                if entry.is_expired():
                    logger.debug(f"Cache entry expired: {key}")
                    del self.cache[key]
                    self.current_size_bytes -= entry.size_bytes
                else:
                    # Cache hit
                    self.hits += 1
                    entry.access_count += 1
                    entry.last_accessed = datetime.now()

                    # LRU: Move to end
                    if self.config.eviction_policy == EvictionPolicy.LRU:
                        self.cache.move_to_end(key)

                    return entry.value

            # Cache miss
            self.misses += 1

        # Fetch if function provided
        if fetch_function:
            return self._fetch_and_cache(key, fetch_function, ttl_seconds)

        return None

    def _fetch_and_cache(
        self,
        key: str,
        fetch_function: Callable[[], Any],
        ttl_seconds: Optional[int]
    ) -> Any:
        """
        Fetch value and add to cache with stampede protection.

        Stampede protection: If multiple requests for same key arrive
        simultaneously, only one fetches while others wait.
        """
        if self.config.enable_stampede_protection:
            # Check if another thread is already fetching this key
            with self.lock:
                if key in self.in_flight:
                    # Wait for in-flight request to complete
                    event = self.in_flight[key]
                    self.stampede_protections += 1

            if key in self.in_flight:
                logger.debug(f"Stampede protection: waiting for {key}")
                event.wait(timeout=30)  # Wait up to 30 seconds

                # Check cache again (might be populated now)
                with self.lock:
                    if key in self.cache:
                        return self.cache[key].value

            # Mark as in-flight
            event = threading.Event()
            with self.lock:
                self.in_flight[key] = event

            try:
                # Fetch value
                start_time = time.time()
                value = fetch_function()
                fetch_cost_ms = (time.time() - start_time) * 1000

                # Cache the value
                self.put(
                    key,
                    value,
                    ttl_seconds=ttl_seconds,
                    fetch_cost_ms=fetch_cost_ms
                )

                return value

            finally:
                # Signal completion
                event.set()
                with self.lock:
                    del self.in_flight[key]
        else:
            # No stampede protection
            start_time = time.time()
            value = fetch_function()
            fetch_cost_ms = (time.time() - start_time) * 1000

            self.put(key, value, ttl_seconds=ttl_seconds, fetch_cost_ms=fetch_cost_ms)
            return value

    def put(
        self,
        key: str,
        value: Any,
        ttl_seconds: Optional[int] = None,
        fetch_cost_ms: float = 0.0
    ):
        """Add entry to cache."""
        with self.lock:
            # Calculate entry size
            size_bytes = self._estimate_size(value)

            # Check if eviction needed
            while self._needs_eviction(size_bytes):
                self._evict_one()

            # Create entry
            entry = CacheEntry(
                key=key,
                value=value,
                size_bytes=size_bytes,
                created_at=datetime.now(),
                last_accessed=datetime.now(),
                access_count=1,
                fetch_cost_ms=fetch_cost_ms,
                ttl_seconds=ttl_seconds or self.config.default_ttl_seconds
            )

            # Add to cache
            if key in self.cache:
                # Update existing
                old_entry = self.cache[key]
                self.current_size_bytes -= old_entry.size_bytes

            self.cache[key] = entry
            self.current_size_bytes += size_bytes

            if self.config.eviction_policy == EvictionPolicy.LRU:
                self.cache.move_to_end(key)

    def _needs_eviction(self, new_entry_size: int) -> bool:
        """Check if eviction needed before adding new entry."""
        # Check size limit
        if self.current_size_bytes + new_entry_size > self.config.max_size_bytes:
            return True

        # Check entry count limit
        if len(self.cache) >= self.config.max_entries:
            return True

        return False

    def _evict_one(self):
        """Evict one entry based on policy."""
        if not self.cache:
            return

        if self.config.eviction_policy == EvictionPolicy.LRU:
            # Evict least recently used (first in OrderedDict)
            key, entry = self.cache.popitem(last=False)

        elif self.config.eviction_policy == EvictionPolicy.LFU:
            # Evict least frequently used
            key = min(self.cache.keys(), key=lambda k: self.cache[k].access_count)
            entry = self.cache.pop(key)

        elif self.config.eviction_policy == EvictionPolicy.TTL:
            # Evict oldest entry
            key = min(self.cache.keys(), key=lambda k: self.cache[k].created_at)
            entry = self.cache.pop(key)

        elif self.config.eviction_policy == EvictionPolicy.SIZE_AWARE:
            # Evict largest entry
            key = max(self.cache.keys(), key=lambda k: self.cache[k].size_bytes)
            entry = self.cache.pop(key)

        elif self.config.eviction_policy == EvictionPolicy.COST_AWARE:
            # Evict entry with lowest cost/benefit ratio
            # Benefit = access_count * fetch_cost_ms
            # Cost = size_bytes
            def cost_benefit_ratio(k):
                e = self.cache[k]
                benefit = e.access_count * e.fetch_cost_ms
                return e.size_bytes / benefit if benefit > 0 else float('inf')

            key = max(self.cache.keys(), key=cost_benefit_ratio)
            entry = self.cache.pop(key)

        else:
            # Default to LRU behavior
            key = next(iter(self.cache))
            entry = self.cache.pop(key)

        self.current_size_bytes -= entry.size_bytes
        self.evictions += 1

        logger.debug(
            f"Evicted: {key} (policy={self.config.eviction_policy.value}, "
            f"size={entry.size_bytes}, accesses={entry.access_count})"
        )

    def _estimate_size(self, value: Any) -> int:
        """Estimate memory size of value."""
        import sys
        return sys.getsizeof(value)

    def invalidate(self, key: str):
        """Invalidate cache entry."""
        with self.lock:
            if key in self.cache:
                entry = self.cache.pop(key)
                self.current_size_bytes -= entry.size_bytes

    def clear(self):
        """Clear entire cache."""
        with self.lock:
            self.cache.clear()
            self.current_size_bytes = 0

    def get_metrics(self) -> dict:
        """Get cache metrics."""
        with self.lock:
            total_requests = self.hits + self.misses
            hit_rate = self.hits / total_requests if total_requests > 0 else 0

            return {
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': hit_rate,
                'evictions': self.evictions,
                'stampede_protections': self.stampede_protections,
                'current_entries': len(self.cache),
                'current_size_bytes': self.current_size_bytes,
                'size_utilization': (
                    self.current_size_bytes / self.config.max_size_bytes
                ),
                'entry_utilization': (
                    len(self.cache) / self.config.max_entries
                )
            }

# Example usage
def fetch_user_preferences(user_id: str) -> dict:
    """Expensive database query."""
    time.sleep(0.05)  # 50ms query
    return {'user_id': user_id, 'preferences': ['electronics', 'books']}

cache = CacheManager(
    config=CacheConfig(
        max_size_bytes=100 * 1024 * 1024,  # 100MB
        eviction_policy=EvictionPolicy.LRU,
        default_ttl_seconds=3600,
        enable_stampede_protection=True
    )
)

# Get with automatic fetch
user_id = "user_12345"
prefs = cache.get(
    key=f"user_prefs:{user_id}",
    fetch_function=lambda: fetch_user_preferences(user_id),
    ttl_seconds=1800  # 30 minutes
)

# Check metrics
metrics = cache.get_metrics()
logger.info(f"Cache hit rate: {metrics['hit_rate']:.1%}")
\end{lstlisting}

\subsection{CompressionOptimizer: Algorithm Selection}

Different data types require different compression algorithms. Production systems must select compression based on data characteristics and performance requirements.

\begin{lstlisting}[style=python, caption={Compression Optimizer for Data Types}]
from enum import Enum
from typing import Any, Optional, Dict
import zlib
import gzip
import bz2
import lz4.frame
import snappy
import json
import pickle
import time
import logging

logger = logging.getLogger(__name__)

class CompressionAlgorithm(Enum):
    """Compression algorithms."""
    NONE = "none"
    GZIP = "gzip"  # Good compression, moderate speed
    ZLIB = "zlib"  # Similar to gzip, slightly faster
    BZ2 = "bz2"  # Best compression, slowest
    LZ4 = "lz4"  # Fastest, moderate compression
    SNAPPY = "snappy"  # Very fast, moderate compression

class SerializationFormat(Enum):
    """Serialization formats."""
    JSON = "json"  # Human-readable, verbose
    PICKLE = "pickle"  # Python-specific, compact
    PROTOBUF = "protobuf"  # Language-agnostic, very compact
    AVRO = "avro"  # Schema evolution support
    MSGPACK = "msgpack"  # Compact, fast

@dataclass
class CompressionResult:
    """Result of compression operation."""
    original_size: int
    compressed_size: int
    compression_ratio: float
    compression_time_ms: float
    algorithm: CompressionAlgorithm

class CompressionOptimizer:
    """
    Intelligent compression algorithm selection.

    Compression algorithm characteristics:
    - GZIP: 60-70% compression, 10-50 MB/s compression, 100-200 MB/s decompression
    - BZ2: 70-80% compression, 5-10 MB/s compression, 20-40 MB/s decompression
    - LZ4: 40-50% compression, 300-500 MB/s compression, 1000-2000 MB/s decompression
    - Snappy: 40-50% compression, 250-500 MB/s compression, 500-1000 MB/s decompression

    Selection guidelines:
    - Text data (logs, JSON): GZIP (good compression)
    - Binary data (Protobuf): LZ4 or Snappy (speed)
    - Cold storage: BZ2 (best compression)
    - Real-time streaming: LZ4 (fastest decompression)
    - Already compressed (images, video): NONE (wastes CPU)
    """

    def __init__(self):
        # Metrics
        self.compressions_by_algorithm: Dict[str, int] = {}
        self.total_bytes_in = 0
        self.total_bytes_out = 0

    def compress(
        self,
        data: bytes,
        algorithm: Optional[CompressionAlgorithm] = None,
        level: int = 6
    ) -> tuple[bytes, CompressionResult]:
        """
        Compress data with specified algorithm.

        Args:
            data: Raw bytes to compress
            algorithm: Compression algorithm (auto-select if None)
            level: Compression level (1-9, higher = better compression, slower)

        Returns:
            Compressed bytes and compression result
        """
        if algorithm is None:
            algorithm = self._select_algorithm(data)

        start_time = time.time()
        original_size = len(data)

        # Compress based on algorithm
        if algorithm == CompressionAlgorithm.NONE:
            compressed = data

        elif algorithm == CompressionAlgorithm.GZIP:
            compressed = gzip.compress(data, compresslevel=level)

        elif algorithm == CompressionAlgorithm.ZLIB:
            compressed = zlib.compress(data, level=level)

        elif algorithm == CompressionAlgorithm.BZ2:
            compressed = bz2.compress(data, compresslevel=level)

        elif algorithm == CompressionAlgorithm.LZ4:
            compressed = lz4.frame.compress(data, compression_level=level)

        elif algorithm == CompressionAlgorithm.SNAPPY:
            compressed = snappy.compress(data)

        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

        compression_time_ms = (time.time() - start_time) * 1000
        compressed_size = len(compressed)
        compression_ratio = compressed_size / original_size if original_size > 0 else 1.0

        # Update metrics
        self.compressions_by_algorithm[algorithm.value] = \
            self.compressions_by_algorithm.get(algorithm.value, 0) + 1
        self.total_bytes_in += original_size
        self.total_bytes_out += compressed_size

        result = CompressionResult(
            original_size=original_size,
            compressed_size=compressed_size,
            compression_ratio=compression_ratio,
            compression_time_ms=compression_time_ms,
            algorithm=algorithm
        )

        logger.debug(
            f"Compressed {original_size} → {compressed_size} bytes "
            f"({compression_ratio:.1%} ratio) using {algorithm.value} "
            f"in {compression_time_ms:.2f}ms"
        )

        return compressed, result

    def decompress(
        self,
        data: bytes,
        algorithm: CompressionAlgorithm
    ) -> bytes:
        """Decompress data."""
        if algorithm == CompressionAlgorithm.NONE:
            return data
        elif algorithm == CompressionAlgorithm.GZIP:
            return gzip.decompress(data)
        elif algorithm == CompressionAlgorithm.ZLIB:
            return zlib.decompress(data)
        elif algorithm == CompressionAlgorithm.BZ2:
            return bz2.decompress(data)
        elif algorithm == CompressionAlgorithm.LZ4:
            return lz4.frame.decompress(data)
        elif algorithm == CompressionAlgorithm.SNAPPY:
            return snappy.decompress(data)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm}")

    def _select_algorithm(self, data: bytes) -> CompressionAlgorithm:
        """
        Auto-select compression algorithm based on data characteristics.

        Heuristics:
        - Small data (<1KB): NONE (compression overhead not worth it)
        - Already compressed (high entropy): NONE
        - Text-like (low entropy, high compressibility): GZIP
        - Binary (moderate entropy): LZ4
        """
        size = len(data)

        # Small data: Don't compress
        if size < 1024:
            return CompressionAlgorithm.NONE

        # Check entropy (simple heuristic: unique bytes / total bytes)
        unique_bytes = len(set(data[:min(1000, size)]))
        entropy_ratio = unique_bytes / min(1000, size)

        # High entropy (already compressed): Don't compress
        if entropy_ratio > 0.8:
            logger.debug(f"High entropy ({entropy_ratio:.2f}), skipping compression")
            return CompressionAlgorithm.NONE

        # Low entropy (text-like): Use GZIP for good compression
        if entropy_ratio < 0.3:
            return CompressionAlgorithm.GZIP

        # Medium entropy: Use LZ4 for speed
        return CompressionAlgorithm.LZ4

    def benchmark_algorithms(
        self,
        data: bytes
    ) -> Dict[str, CompressionResult]:
        """Benchmark all algorithms on sample data."""
        results = {}

        for algorithm in CompressionAlgorithm:
            if algorithm == CompressionAlgorithm.NONE:
                continue

            try:
                _, result = self.compress(data, algorithm=algorithm)
                results[algorithm.value] = result
            except Exception as e:
                logger.error(f"Benchmark failed for {algorithm.value}: {e}")

        return results

    def get_metrics(self) -> dict:
        """Get compression metrics."""
        overall_ratio = (
            self.total_bytes_out / self.total_bytes_in
            if self.total_bytes_in > 0 else 1.0
        )

        return {
            'total_bytes_in': self.total_bytes_in,
            'total_bytes_out': self.total_bytes_out,
            'overall_compression_ratio': overall_ratio,
            'compression_savings_pct': (1 - overall_ratio) * 100,
            'compressions_by_algorithm': self.compressions_by_algorithm
        }

# Serialization optimization
class SerializationOptimizer:
    """
    Optimize serialization format selection.

    Format comparison for typical event data:
    - JSON: 1000 bytes, human-readable, slow (50-100 MB/s)
    - Pickle: 600 bytes, Python-only, fast (200-500 MB/s)
    - Protobuf: 100 bytes, schema required, very fast (500-1000 MB/s)
    - Avro: 120 bytes, schema evolution, fast (300-600 MB/s)
    - MsgPack: 400 bytes, schema-less, fast (200-400 MB/s)
    """

    @staticmethod
    def serialize(
        data: dict,
        format: SerializationFormat = SerializationFormat.JSON
    ) -> bytes:
        """Serialize data to bytes."""
        if format == SerializationFormat.JSON:
            return json.dumps(data).encode('utf-8')

        elif format == SerializationFormat.PICKLE:
            return pickle.dumps(data)

        elif format == SerializationFormat.PROTOBUF:
            # Requires protobuf schema - placeholder
            raise NotImplementedError("Protobuf requires schema definition")

        elif format == SerializationFormat.AVRO:
            # Requires avro schema - placeholder
            raise NotImplementedError("Avro requires schema definition")

        elif format == SerializationFormat.MSGPACK:
            import msgpack
            return msgpack.packb(data)

        else:
            raise ValueError(f"Unknown format: {format}")

    @staticmethod
    def compare_formats(data: dict) -> Dict[str, dict]:
        """Compare serialization formats."""
        results = {}

        for fmt in [SerializationFormat.JSON, SerializationFormat.PICKLE]:
            try:
                start = time.time()
                serialized = SerializationOptimizer.serialize(data, fmt)
                serialize_time_ms = (time.time() - start) * 1000

                results[fmt.value] = {
                    'size_bytes': len(serialized),
                    'serialize_time_ms': serialize_time_ms,
                    'throughput_mb_per_sec': (
                        len(serialized) / 1024 / 1024 / (serialize_time_ms / 1000)
                    )
                }
            except Exception as e:
                logger.error(f"Format {fmt.value} failed: {e}")

        return results

# Example usage
compressor = CompressionOptimizer()

# Text data (JSON event)
event = {'user_id': '12345', 'action': 'purchase', 'amount': 99.99}
event_json = json.dumps(event).encode('utf-8')

compressed, result = compressor.compress(event_json)
logger.info(
    f"JSON compression: {result.original_size} → {result.compressed_size} bytes "
    f"({result.compression_ratio:.1%}), algorithm={result.algorithm.value}"
)

# Benchmark algorithms
sample_data = event_json * 100  # 100 events
benchmark_results = compressor.benchmark_algorithms(sample_data)

for algo, result in benchmark_results.items():
    logger.info(
        f"{algo}: {result.compression_ratio:.1%} ratio, "
        f"{result.compression_time_ms:.2f}ms"
    )
\end{lstlisting}

\textbf{Compression algorithm selection guide:}
\begin{itemize}
    \item \textbf{Real-time streaming}: LZ4 or Snappy (fastest decompression)
    \item \textbf{Log files}: GZIP (good compression, standard format)
    \item \textbf{Cold storage}: BZ2 (maximum compression)
    \item \textbf{Cross-region transfer}: GZIP or BZ2 (minimize network cost)
    \item \textbf{Binary formats (Protobuf)}: LZ4 (already compact)
    \item \textbf{Images/video}: NONE (already compressed)
\end{itemize}

\textbf{Serialization format selection guide:}
\begin{itemize}
    \item \textbf{Development/debugging}: JSON (human-readable)
    \item \textbf{Production event streaming}: Protobuf or Avro (10x smaller than JSON)
    \item \textbf{Schema evolution required}: Avro (handles schema changes)
    \item \textbf{Language interoperability}: Protobuf or Avro (not Pickle)
    \item \textbf{Fastest processing}: Protobuf (500-1000 MB/s)
\end{itemize}

\textbf{Combined optimization example:}
\begin{itemize}
    \item JSON + GZIP: 1000 bytes → 250 bytes (75\% savings)
    \item Protobuf + LZ4: 1000 bytes → 70 bytes (93\% savings, 10x faster)
    \item Best practice: Protobuf for serialization, LZ4 for compression
\end{itemize}

\textbf{Production caching and compression metrics:}
\begin{itemize}
    \item \textbf{Cache hit rate}: Target >90\% for reference data
    \item \textbf{Cache latency}: p99 <1ms (vs 10-50ms database query)
    \item \textbf{Compression ratio}: Text 60-70\%, binary 30-40\%
    \item \textbf{Compression throughput}: >100 MB/s for real-time streaming
    \item \textbf{Bandwidth savings}: 70-90\% for cross-region traffic
\end{itemize}

\textbf{Alert thresholds:}
\begin{itemize}
    \item Cache hit rate <80\% → Warning (check TTL, eviction policy)
    \item Cache memory >90\% → Warning (increase limit or tune eviction)
    \item Stampede protections >10\% requests → Warning (deployment issue)
    \item Compression ratio <30\% → Info (check if data already compressed)
    \item Compression time >10ms → Warning (use faster algorithm)
\end{itemize}

\subsection{Query Optimization and Index Management}

Even with efficient caching and compression, poorly optimized queries and missing indexes can devastate pipeline performance. A single full table scan on billions of rows can cost thousands of dollars and hours of execution time. Smart query optimization and index management are essential for cost-effective, performant data pipelines.

\subsubsection{Query Optimization with Cost Analysis}

\begin{lstlisting}[style=python, caption=QueryOptimizer with Cost-Based Analysis]
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
from enum import Enum
import sqlparse
from sqlparse.sql import Identifier, Where
from sqlparse.tokens import Keyword, DML
import logging
import time

class QueryType(Enum):
    """Types of queries with different optimization strategies."""
    SELECT = "select"
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    AGGREGATE = "aggregate"
    JOIN = "join"

class OptimizationIssue(Enum):
    """Common query performance issues."""
    FULL_TABLE_SCAN = "full_table_scan"
    MISSING_INDEX = "missing_index"
    INEFFICIENT_JOIN = "inefficient_join"
    SELECT_STAR = "select_star"
    IMPLICIT_CONVERSION = "implicit_conversion"
    CARTESIAN_PRODUCT = "cartesian_product"
    SUBQUERY_IN_WHERE = "subquery_in_where"
    NO_LIMIT = "no_limit"
    FUNCTION_ON_INDEXED_COLUMN = "function_on_indexed_column"

@dataclass
class QueryPlan:
    """Execution plan analysis."""
    estimated_cost: float
    estimated_rows: int
    scan_type: str  # "seq_scan", "index_scan", "bitmap_scan"
    indexes_used: List[str]
    tables_accessed: List[str]
    join_type: Optional[str]  # "nested_loop", "hash_join", "merge_join"
    execution_time_ms: float

@dataclass
class OptimizationRecommendation:
    """Recommendation for query improvement."""
    issue: OptimizationIssue
    severity: str  # "critical", "warning", "info"
    description: str
    suggested_fix: str
    estimated_improvement: str  # "10x faster", "90% cost reduction"

class QueryOptimizer:
    """Analyzes queries and recommends optimizations."""

    def __init__(self, connection, index_manager):
        self.connection = connection
        self.index_manager = index_manager
        self.logger = logging.getLogger(__name__)

        # Cost thresholds
        self.high_cost_threshold = 1000.0
        self.high_rows_threshold = 1000000

        # Track query performance
        self.query_history: List[Dict] = []

    def analyze_query(self, query: str,
                     params: Optional[Dict] = None) -> List[OptimizationRecommendation]:
        """Analyze query and return optimization recommendations."""
        recommendations = []

        # Parse query
        parsed = sqlparse.parse(query)[0]
        query_type = self._identify_query_type(parsed)

        # Get execution plan
        plan = self._get_execution_plan(query, params)

        # Check for common issues
        recommendations.extend(self._check_table_scans(plan))
        recommendations.extend(self._check_missing_indexes(query, plan))
        recommendations.extend(self._check_select_star(parsed))
        recommendations.extend(self._check_inefficient_joins(plan))
        recommendations.extend(self._check_subqueries(parsed))
        recommendations.extend(self._check_limit_clause(parsed, query_type))
        recommendations.extend(self._check_function_on_index(query, plan))

        # Log high-cost queries
        if plan.estimated_cost > self.high_cost_threshold:
            self.logger.warning(
                f"High-cost query detected: cost={plan.estimated_cost:.2f}, "
                f"rows={plan.estimated_rows}, time={plan.execution_time_ms:.2f}ms"
            )

        return recommendations

    def _get_execution_plan(self, query: str,
                           params: Optional[Dict]) -> QueryPlan:
        """Get query execution plan from database."""
        # Execute EXPLAIN ANALYZE
        explain_query = f"EXPLAIN (ANALYZE, FORMAT JSON) {query}"

        start_time = time.time()
        cursor = self.connection.cursor()
        cursor.execute(explain_query, params or {})
        plan_json = cursor.fetchone()[0]
        execution_time = (time.time() - start_time) * 1000

        # Parse plan (simplified for PostgreSQL)
        root_plan = plan_json[0]['Plan']

        return QueryPlan(
            estimated_cost=root_plan.get('Total Cost', 0),
            estimated_rows=root_plan.get('Plan Rows', 0),
            scan_type=root_plan.get('Node Type', 'Unknown'),
            indexes_used=self._extract_indexes(root_plan),
            tables_accessed=self._extract_tables(root_plan),
            join_type=root_plan.get('Join Type'),
            execution_time_ms=execution_time
        )

    def _check_table_scans(self, plan: QueryPlan) -> List[OptimizationRecommendation]:
        """Check for expensive full table scans."""
        recommendations = []

        if plan.scan_type == 'Seq Scan' and plan.estimated_rows > self.high_rows_threshold:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.FULL_TABLE_SCAN,
                severity="critical",
                description=f"Full table scan on {plan.estimated_rows:,} rows",
                suggested_fix="Add index on WHERE clause columns or use partitioning",
                estimated_improvement="10-100x faster with index"
            ))

        return recommendations

    def _check_missing_indexes(self, query: str,
                              plan: QueryPlan) -> List[OptimizationRecommendation]:
        """Check for missing indexes on WHERE columns."""
        recommendations = []

        # Extract WHERE clause columns
        where_columns = self._extract_where_columns(query)

        for table in plan.tables_accessed:
            table_indexes = self.index_manager.get_table_indexes(table)
            indexed_columns = {idx['column'] for idx in table_indexes}

            # Find columns without indexes
            missing = where_columns - indexed_columns

            if missing:
                recommendations.append(OptimizationRecommendation(
                    issue=OptimizationIssue.MISSING_INDEX,
                    severity="warning",
                    description=f"Missing indexes on {table} for columns: {', '.join(missing)}",
                    suggested_fix=f"CREATE INDEX idx_{table}_{'_'.join(missing)} ON {table}({', '.join(missing)})",
                    estimated_improvement="5-50x faster for filtered queries"
                ))

        return recommendations

    def _check_select_star(self, parsed) -> List[OptimizationRecommendation]:
        """Check for SELECT * antipattern."""
        recommendations = []

        query_str = str(parsed).upper()
        if 'SELECT *' in query_str:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.SELECT_STAR,
                severity="info",
                description="Using SELECT * retrieves unnecessary columns",
                suggested_fix="Specify only required columns: SELECT col1, col2, col3",
                estimated_improvement="30-70% reduction in data transfer"
            ))

        return recommendations

    def _check_inefficient_joins(self, plan: QueryPlan) -> List[OptimizationRecommendation]:
        """Check for inefficient join strategies."""
        recommendations = []

        if plan.join_type == 'Nested Loop' and plan.estimated_rows > 10000:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.INEFFICIENT_JOIN,
                severity="warning",
                description=f"Nested loop join on {plan.estimated_rows:,} rows",
                suggested_fix="Add indexes on join columns to enable hash/merge join",
                estimated_improvement="10-100x faster with proper indexes"
            ))

        # Check for Cartesian product (cross join)
        if not plan.join_type and len(plan.tables_accessed) > 1:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.CARTESIAN_PRODUCT,
                severity="critical",
                description="Possible Cartesian product (missing JOIN condition)",
                suggested_fix="Add explicit JOIN conditions between tables",
                estimated_improvement="Prevents exponential row explosion"
            ))

        return recommendations

    def _check_subqueries(self, parsed) -> List[OptimizationRecommendation]:
        """Check for subqueries in WHERE clause."""
        recommendations = []

        query_str = str(parsed).upper()
        if 'WHERE' in query_str and 'SELECT' in query_str.split('WHERE')[1]:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.SUBQUERY_IN_WHERE,
                severity="warning",
                description="Subquery in WHERE clause may execute per row",
                suggested_fix="Rewrite as JOIN or use CTE (WITH clause)",
                estimated_improvement="5-10x faster with JOIN"
            ))

        return recommendations

    def _check_limit_clause(self, parsed,
                           query_type: QueryType) -> List[OptimizationRecommendation]:
        """Check for missing LIMIT on large result sets."""
        recommendations = []

        query_str = str(parsed).upper()
        if query_type == QueryType.SELECT and 'LIMIT' not in query_str:
            recommendations.append(OptimizationRecommendation(
                issue=OptimizationIssue.NO_LIMIT,
                severity="info",
                description="No LIMIT clause on SELECT query",
                suggested_fix="Add LIMIT clause if you don't need all rows",
                estimated_improvement="Faster execution and reduced memory"
            ))

        return recommendations

    def _check_function_on_index(self, query: str,
                                plan: QueryPlan) -> List[OptimizationRecommendation]:
        """Check for functions applied to indexed columns."""
        recommendations = []

        # Check for common function patterns that break index usage
        functions = ['UPPER(', 'LOWER(', 'DATE(', 'YEAR(', 'CAST(']
        where_clause = query.split('WHERE', 1)[-1] if 'WHERE' in query.upper() else ''

        for func in functions:
            if func in where_clause.upper():
                recommendations.append(OptimizationRecommendation(
                    issue=OptimizationIssue.FUNCTION_ON_INDEXED_COLUMN,
                    severity="warning",
                    description=f"Function {func} on column prevents index usage",
                    suggested_fix="Use functional index or move function to comparison value",
                    estimated_improvement="10x faster by enabling index scan"
                ))
                break

        return recommendations

    def optimize_query(self, query: str) -> str:
        """Automatically optimize query where possible."""
        optimized = query

        # Remove SELECT *
        if 'SELECT *' in optimized.upper():
            self.logger.info("Manual optimization needed: Replace SELECT * with specific columns")

        # Add LIMIT if missing (conservative)
        if 'LIMIT' not in optimized.upper() and 'SELECT' in optimized.upper():
            # Don't auto-add LIMIT, just recommend
            self.logger.info("Consider adding LIMIT clause for testing")

        return optimized

    def _identify_query_type(self, parsed) -> QueryType:
        """Identify the type of query."""
        first_token = str(parsed.tokens[0]).upper()

        if 'SELECT' in first_token:
            query_str = str(parsed).upper()
            if 'JOIN' in query_str:
                return QueryType.JOIN
            elif any(agg in query_str for agg in ['COUNT', 'SUM', 'AVG', 'MAX', 'MIN']):
                return QueryType.AGGREGATE
            return QueryType.SELECT
        elif 'INSERT' in first_token:
            return QueryType.INSERT
        elif 'UPDATE' in first_token:
            return QueryType.UPDATE
        elif 'DELETE' in first_token:
            return QueryType.DELETE

        return QueryType.SELECT

    def _extract_where_columns(self, query: str) -> Set[str]:
        """Extract column names from WHERE clause."""
        # Simplified extraction (production would use proper SQL parsing)
        where_clause = query.split('WHERE', 1)[-1] if 'WHERE' in query.upper() else ''
        columns = set()

        # Extract column names before comparison operators
        for part in where_clause.split():
            if any(op in part for op in ['=', '>', '<', 'IN']):
                column = part.split('=')[0].split('>')[0].split('<')[0]
                column = column.strip().split('.')[-1]  # Remove table prefix
                if column and column[0].isalpha():
                    columns.add(column.lower())

        return columns

    def _extract_indexes(self, plan: Dict) -> List[str]:
        """Extract indexes used from execution plan."""
        indexes = []
        if 'Index Name' in plan:
            indexes.append(plan['Index Name'])
        if 'Plans' in plan:
            for subplan in plan['Plans']:
                indexes.extend(self._extract_indexes(subplan))
        return indexes

    def _extract_tables(self, plan: Dict) -> List[str]:
        """Extract tables accessed from execution plan."""
        tables = []
        if 'Relation Name' in plan:
            tables.append(plan['Relation Name'])
        if 'Plans' in plan:
            for subplan in plan['Plans']:
                tables.extend(self._extract_tables(subplan))
        return list(set(tables))

# Example usage
optimizer = QueryOptimizer(connection, index_manager)

# Analyze problematic query
query = """
SELECT *
FROM orders o
JOIN customers c ON UPPER(o.customer_email) = UPPER(c.email)
WHERE YEAR(o.order_date) = 2024
  AND o.status IN (SELECT status FROM active_statuses)
"""

recommendations = optimizer.analyze_query(query)

for rec in recommendations:
    print(f"{rec.severity.upper()}: {rec.description}")
    print(f"Fix: {rec.suggested_fix}")
    print(f"Impact: {rec.estimated_improvement}\n")

# Output:
# CRITICAL: Full table scan on 50,000,000 rows
# Fix: Add index on WHERE clause columns or use partitioning
# Impact: 10-100x faster with index
#
# WARNING: Function UPPER( on column prevents index usage
# Fix: Use functional index or move function to comparison value
# Impact: 10x faster by enabling index scan
#
# WARNING: Subquery in WHERE clause may execute per row
# Fix: Rewrite as JOIN or use CTE (WITH clause)
# Impact: 5-10x faster with JOIN
#
# INFO: Using SELECT * retrieves unnecessary columns
# Fix: Specify only required columns: SELECT col1, col2, col3
# Impact: 30-70% reduction in data transfer
\end{lstlisting}

\subsubsection{Index Management for Performance Tuning}

\begin{lstlisting}[style=python, caption=IndexManager with Smart Recommendations]
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
from enum import Enum
import logging
from datetime import datetime, timedelta

class IndexType(Enum):
    """Types of database indexes."""
    BTREE = "btree"  # Default, balanced tree
    HASH = "hash"  # Equality lookups only
    GIN = "gin"  # Generalized Inverted Index (arrays, JSON)
    GIST = "gist"  # Geometric/spatial data
    BRIN = "brin"  # Block Range Index (very large tables)
    PARTIAL = "partial"  # Index on subset of rows

@dataclass
class IndexStats:
    """Statistics about index usage and performance."""
    index_name: str
    table_name: str
    column_names: List[str]
    index_type: IndexType
    size_bytes: int
    num_rows: int
    num_scans: int  # Times index was used
    tuples_read: int  # Rows read via index
    tuples_fetched: int  # Rows actually fetched
    last_used: Optional[datetime]
    is_unique: bool
    is_primary: bool

@dataclass
class IndexRecommendation:
    """Recommendation for index creation or modification."""
    action: str  # "create", "drop", "rebuild"
    table_name: str
    column_names: List[str]
    index_type: IndexType
    reason: str
    estimated_benefit: str
    sql: str

class IndexManager:
    """Manages database indexes for optimal performance."""

    def __init__(self, connection):
        self.connection = connection
        self.logger = logging.getLogger(__name__)

        # Thresholds for recommendations
        self.unused_index_days = 30
        self.min_scan_threshold = 10
        self.bloat_threshold = 0.3  # 30% wasted space
        self.large_table_threshold = 1000000  # 1M rows

    def get_table_indexes(self, table_name: str) -> List[Dict]:
        """Get all indexes for a table."""
        cursor = self.connection.cursor()

        # PostgreSQL specific query
        cursor.execute("""
            SELECT
                indexname,
                indexdef
            FROM pg_indexes
            WHERE tablename = %s
        """, (table_name,))

        indexes = []
        for row in cursor.fetchall():
            # Parse index definition to extract columns
            columns = self._parse_index_columns(row[1])
            indexes.append({
                'name': row[0],
                'column': columns[0] if columns else None,
                'columns': columns
            })

        return indexes

    def collect_index_statistics(self) -> List[IndexStats]:
        """Collect usage statistics for all indexes."""
        cursor = self.connection.cursor()

        # Get index usage stats (PostgreSQL)
        cursor.execute("""
            SELECT
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes
            ORDER BY idx_scan ASC
        """)

        stats = []
        for row in cursor.fetchall():
            # Get additional index info
            index_info = self._get_index_info(row[2])

            stats.append(IndexStats(
                index_name=row[2],
                table_name=row[1],
                column_names=index_info['columns'],
                index_type=IndexType(index_info['type']),
                size_bytes=index_info['size'],
                num_rows=index_info['num_rows'],
                num_scans=row[3],
                tuples_read=row[4],
                tuples_fetched=row[5],
                last_used=None,  # Would need additional tracking
                is_unique=index_info['is_unique'],
                is_primary=index_info['is_primary']
            ))

        return stats

    def find_unused_indexes(self) -> List[IndexStats]:
        """Find indexes that are never or rarely used."""
        all_stats = self.collect_index_statistics()

        unused = []
        for stat in all_stats:
            # Skip primary keys and unique constraints
            if stat.is_primary or stat.is_unique:
                continue

            # Flag as unused if very few scans
            if stat.num_scans < self.min_scan_threshold:
                unused.append(stat)
                self.logger.warning(
                    f"Unused index: {stat.index_name} on {stat.table_name} "
                    f"(scans: {stat.num_scans}, size: {stat.size_bytes / 1024 / 1024:.2f}MB)"
                )

        return unused

    def find_duplicate_indexes(self) -> List[tuple]:
        """Find duplicate or redundant indexes."""
        all_stats = self.collect_index_statistics()

        # Group by table
        by_table = {}
        for stat in all_stats:
            if stat.table_name not in by_table:
                by_table[stat.table_name] = []
            by_table[stat.table_name].append(stat)

        duplicates = []
        for table, indexes in by_table.items():
            # Check for exact duplicates
            for i, idx1 in enumerate(indexes):
                for idx2 in indexes[i+1:]:
                    # Same columns = duplicate
                    if idx1.column_names == idx2.column_names:
                        duplicates.append((idx1, idx2))
                        self.logger.warning(
                            f"Duplicate indexes: {idx1.index_name} and {idx2.index_name} "
                            f"on {table}({', '.join(idx1.column_names)})"
                        )

                    # Check for redundancy (covered by composite index)
                    elif self._is_redundant(idx1, idx2):
                        duplicates.append((idx1, idx2))
                        self.logger.info(
                            f"Redundant index: {idx1.index_name} covered by {idx2.index_name}"
                        )

        return duplicates

    def recommend_indexes(self, query_log: List[str]) -> List[IndexRecommendation]:
        """Analyze query patterns and recommend indexes."""
        recommendations = []

        # Track column usage in WHERE clauses
        column_usage = {}

        for query in query_log:
            # Extract columns from WHERE clauses
            columns = self._extract_query_columns(query)
            table = self._extract_table_name(query)

            if not table:
                continue

            for col in columns:
                key = f"{table}.{col}"
                column_usage[key] = column_usage.get(key, 0) + 1

        # Recommend indexes for frequently queried columns
        for col_key, count in column_usage.items():
            if count > 100:  # Threshold: used in 100+ queries
                table, column = col_key.split('.')

                # Check if index exists
                existing = self.get_table_indexes(table)
                if not any(column in idx['columns'] for idx in existing):
                    recommendations.append(IndexRecommendation(
                        action="create",
                        table_name=table,
                        column_names=[column],
                        index_type=IndexType.BTREE,
                        reason=f"Column used in {count} queries",
                        estimated_benefit="5-50x faster for filtered queries",
                        sql=f"CREATE INDEX idx_{table}_{column} ON {table}({column})"
                    ))

        return recommendations

    def find_bloated_indexes(self) -> List[IndexStats]:
        """Find indexes with significant bloat."""
        cursor = self.connection.cursor()

        # PostgreSQL bloat estimation
        cursor.execute("""
            SELECT
                schemaname,
                tablename,
                indexname,
                pg_size_pretty(pg_relation_size(indexrelid)) as size,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes
            WHERE idx_scan > 0
        """)

        bloated = []
        all_stats = self.collect_index_statistics()

        for stat in all_stats:
            # Simple heuristic: if tuples_fetched << tuples_read, possible bloat
            if stat.tuples_read > 0:
                efficiency = stat.tuples_fetched / stat.tuples_read
                if efficiency < (1 - self.bloat_threshold):
                    bloated.append(stat)
                    self.logger.warning(
                        f"Bloated index: {stat.index_name} "
                        f"(efficiency: {efficiency*100:.1f}%)"
                    )

        return bloated

    def rebuild_index(self, index_name: str) -> None:
        """Rebuild index to remove bloat."""
        self.logger.info(f"Rebuilding index: {index_name}")

        cursor = self.connection.cursor()
        cursor.execute(f"REINDEX INDEX CONCURRENTLY {index_name}")
        self.connection.commit()

        self.logger.info(f"Index rebuilt: {index_name}")

    def create_composite_index(self, table: str,
                              columns: List[str],
                              index_type: IndexType = IndexType.BTREE) -> str:
        """Create composite index on multiple columns."""
        index_name = f"idx_{table}_{'_'.join(columns)}"

        sql = f"""
        CREATE INDEX {index_name}
        ON {table}({', '.join(columns)})
        USING {index_type.value}
        """

        cursor = self.connection.cursor()
        cursor.execute(sql)
        self.connection.commit()

        self.logger.info(f"Created composite index: {index_name}")
        return index_name

    def create_partial_index(self, table: str, column: str,
                           condition: str) -> str:
        """Create partial index for subset of rows."""
        index_name = f"idx_{table}_{column}_partial"

        sql = f"""
        CREATE INDEX {index_name}
        ON {table}({column})
        WHERE {condition}
        """

        cursor = self.connection.cursor()
        cursor.execute(sql)
        self.connection.commit()

        self.logger.info(
            f"Created partial index: {index_name} "
            f"with condition: {condition}"
        )
        return index_name

    def drop_index(self, index_name: str, cascade: bool = False) -> None:
        """Drop an index."""
        cascade_clause = "CASCADE" if cascade else ""

        cursor = self.connection.cursor()
        cursor.execute(f"DROP INDEX {index_name} {cascade_clause}")
        self.connection.commit()

        self.logger.info(f"Dropped index: {index_name}")

    def _get_index_info(self, index_name: str) -> Dict:
        """Get detailed information about an index."""
        cursor = self.connection.cursor()

        cursor.execute("""
            SELECT
                i.indisunique,
                i.indisprimary,
                pg_relation_size(i.indexrelid),
                a.attname,
                am.amname
            FROM pg_index i
            JOIN pg_class c ON c.oid = i.indexrelid
            JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
            JOIN pg_am am ON am.oid = c.relam
            WHERE c.relname = %s
        """, (index_name,))

        rows = cursor.fetchall()
        if not rows:
            return {
                'columns': [],
                'type': 'btree',
                'size': 0,
                'num_rows': 0,
                'is_unique': False,
                'is_primary': False
            }

        return {
            'columns': [row[3] for row in rows],
            'type': rows[0][4],
            'size': rows[0][2],
            'num_rows': 0,  # Would need table stats
            'is_unique': rows[0][0],
            'is_primary': rows[0][1]
        }

    def _parse_index_columns(self, index_def: str) -> List[str]:
        """Parse column names from index definition."""
        # Extract columns from CREATE INDEX statement
        # Example: "CREATE INDEX idx_name ON table (col1, col2)"
        start = index_def.find('(')
        end = index_def.find(')')

        if start == -1 or end == -1:
            return []

        columns_str = index_def[start+1:end]
        return [col.strip() for col in columns_str.split(',')]

    def _is_redundant(self, idx1: IndexStats, idx2: IndexStats) -> bool:
        """Check if idx1 is redundant given idx2."""
        # Single column index is redundant if it's the first column of composite
        if len(idx1.column_names) == 1 and len(idx2.column_names) > 1:
            return idx1.column_names[0] == idx2.column_names[0]

        return False

    def _extract_query_columns(self, query: str) -> List[str]:
        """Extract column names from query WHERE clause."""
        where_clause = query.split('WHERE', 1)[-1] if 'WHERE' in query.upper() else ''

        columns = []
        for part in where_clause.split():
            if any(op in part for op in ['=', '>', '<', 'IN']):
                column = part.split('=')[0].split('>')[0].split('<')[0]
                column = column.strip().split('.')[-1]
                if column and column[0].isalpha():
                    columns.append(column.lower())

        return columns

    def _extract_table_name(self, query: str) -> Optional[str]:
        """Extract table name from query."""
        # Simplified extraction
        if 'FROM' in query.upper():
            from_part = query.upper().split('FROM')[1].split('WHERE')[0]
            table = from_part.strip().split()[0]
            return table.lower()
        return None

# Example usage
index_manager = IndexManager(connection)

# Find unused indexes
unused = index_manager.find_unused_indexes()
print(f"Found {len(unused)} unused indexes")
for idx in unused:
    print(f"  {idx.index_name}: {idx.size_bytes / 1024 / 1024:.2f}MB, {idx.num_scans} scans")

# Find duplicates
duplicates = index_manager.find_duplicate_indexes()
print(f"\nFound {len(duplicates)} duplicate/redundant indexes")

# Create composite index for common query pattern
index_manager.create_composite_index(
    table="orders",
    columns=["customer_id", "order_date"],
    index_type=IndexType.BTREE
)

# Create partial index for active records only
index_manager.create_partial_index(
    table="orders",
    column="status",
    condition="status = 'active'"
)

# Rebuild bloated indexes
bloated = index_manager.find_bloated_indexes()
for idx in bloated:
    index_manager.rebuild_index(idx.index_name)
\end{lstlisting}

\textbf{Index Strategy Decision Framework:}

\begin{itemize}
    \item \textbf{B-Tree indexes} (default): Range queries, sorting, equality lookups
    \begin{itemize}
        \item Use for: WHERE status = 'active', ORDER BY created\_at, price BETWEEN 10 AND 100
        \item Performance: $O(\log n)$ lookups, excellent for sorted access
    \end{itemize}

    \item \textbf{Hash indexes}: Equality lookups only, no range queries
    \begin{itemize}
        \item Use for: WHERE user\_id = 12345 (exact match only)
        \item Performance: $O(1)$ lookups, but limited functionality
        \item Note: PostgreSQL now supports WAL logging for hash indexes
    \end{itemize}

    \item \textbf{GIN indexes}: Array/JSON data, full-text search
    \begin{itemize}
        \item Use for: WHERE tags @> ARRAY['python', 'data'], JSONB queries
        \item Performance: Fast for containment queries, larger size
    \end{itemize}

    \item \textbf{BRIN indexes}: Very large tables with correlated data
    \begin{itemize}
        \item Use for: Time-series data (WHERE timestamp > '2024-01-01')
        \item Performance: 100x smaller than B-Tree, good for sequential scans
        \item Best for: Billions of rows with natural ordering
    \end{itemize}

    \item \textbf{Partial indexes}: Index subset of rows
    \begin{itemize}
        \item Use for: WHERE status = 'active' (if only 5\% of rows active)
        \item Performance: 95\% smaller index, faster maintenance
    \end{itemize}

    \item \textbf{Composite indexes}: Multiple columns, column order matters
    \begin{itemize}
        \item Use for: WHERE customer\_id = X AND order\_date > Y
        \item Rule: Put equality columns first, range columns last
        \item Index (a, b, c) works for: (a), (a,b), (a,b,c) but NOT (b,c)
    \end{itemize}
\end{itemize}

\textbf{Production Metrics and Alerts:}

\begin{itemize}
    \item Index scan ratio >90\% → Good (indexes being used)
    \item Sequential scan ratio >50\% → Warning (add indexes)
    \item Index size >10x table size → Warning (too many indexes)
    \item Unused index for >30 days → Info (consider dropping)
    \item Index bloat >30\% → Warning (rebuild index)
    \item Query cost >10,000 → Alert (needs optimization)
    \item Query execution >10s → Critical (investigate immediately)
\end{itemize}

\subsection{Enterprise-Grade Pipeline Implementations}

Production data pipelines require sophisticated orchestration, state management, and monitoring capabilities. Simple ETL scripts fail at enterprise scale where pipelines must handle failures gracefully, guarantee exactly-once delivery, manage complex dependencies, and provide comprehensive observability.

This section presents four core production-ready components that form the foundation of enterprise data infrastructure:

\begin{enumerate}
    \item \textbf{DataPipeline}: Full lifecycle management with checkpointing and recovery
    \item \textbf{StreamProcessor}: Exactly-once semantics with distributed state management
    \item \textbf{PipelineOrchestrator}: DAG-based dependency resolution and parallel execution
    \item \textbf{PipelineMonitor}: Real-time metrics, alerting, and performance tracking
\end{enumerate}

\subsubsection{DataPipeline: Enterprise Lifecycle Management}

\begin{lstlisting}[style=python, caption=DataPipeline with Full Lifecycle Management]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable, Any
from enum import Enum
from datetime import datetime, timedelta
import logging
import json
import time
from pathlib import Path
import hashlib
import pickle

class PipelineStatus(Enum):
    """Pipeline execution states."""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    RETRYING = "retrying"
    CANCELLED = "cancelled"

class CheckpointStrategy(Enum):
    """Checkpointing strategies."""
    NONE = "none"  # No checkpointing
    PERIODIC = "periodic"  # Time-based checkpoints
    RECORD_COUNT = "record_count"  # Every N records
    STAGE = "stage"  # After each stage
    ADAPTIVE = "adaptive"  # Based on data volume

@dataclass
class PipelineStage:
    """Individual pipeline stage definition."""
    name: str
    function: Callable[[Any], Any]
    required_resources: Dict[str, float] = field(default_factory=dict)  # CPU, memory
    timeout_seconds: int = 3600
    retry_count: int = 3
    skip_on_failure: bool = False
    checkpoint_enabled: bool = True

@dataclass
class PipelineCheckpoint:
    """Checkpoint state for recovery."""
    pipeline_id: str
    stage_name: str
    offset: int  # Records processed
    timestamp: datetime
    state_data: Dict[str, Any]
    checksum: str  # Data integrity verification

@dataclass
class PipelineMetrics:
    """Pipeline execution metrics."""
    pipeline_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    records_processed: int = 0
    records_failed: int = 0
    bytes_processed: int = 0
    current_stage: Optional[str] = None
    stage_metrics: Dict[str, Dict] = field(default_factory=dict)
    error_count: int = 0
    retry_count: int = 0

class DataPipeline:
    """Enterprise-grade data pipeline with full lifecycle management."""

    def __init__(self, pipeline_id: str, config: Dict[str, Any]):
        self.pipeline_id = pipeline_id
        self.config = config
        self.logger = logging.getLogger(f"pipeline.{pipeline_id}")

        # Pipeline configuration
        self.stages: List[PipelineStage] = []
        self.checkpoint_strategy = CheckpointStrategy(
            config.get('checkpoint_strategy', 'stage')
        )
        self.checkpoint_interval = config.get('checkpoint_interval', 300)  # 5 min
        self.checkpoint_dir = Path(config.get('checkpoint_dir', '/tmp/checkpoints'))

        # State management
        self.status = PipelineStatus.PENDING
        self.metrics = PipelineMetrics(
            pipeline_id=pipeline_id,
            start_time=datetime.now()
        )
        self.current_checkpoint: Optional[PipelineCheckpoint] = None

        # Execution control
        self.cancelled = False
        self.last_checkpoint_time = time.time()

        # Create checkpoint directory
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def add_stage(self, stage: PipelineStage) -> None:
        """Add a stage to the pipeline."""
        self.stages.append(stage)
        self.logger.info(f"Added stage: {stage.name}")

    def execute(self, input_data: Any,
                resume_from_checkpoint: bool = True) -> Any:
        """Execute the pipeline with full lifecycle management."""
        try:
            self.status = PipelineStatus.RUNNING
            self.metrics.start_time = datetime.now()

            self.logger.info(
                f"Pipeline {self.pipeline_id} starting execution "
                f"({len(self.stages)} stages)"
            )

            # Try to resume from checkpoint
            start_stage_idx = 0
            current_data = input_data

            if resume_from_checkpoint:
                checkpoint = self._load_latest_checkpoint()
                if checkpoint:
                    self.logger.info(
                        f"Resuming from checkpoint: stage={checkpoint.stage_name}, "
                        f"offset={checkpoint.offset}"
                    )
                    start_stage_idx = self._get_stage_index(checkpoint.stage_name)
                    current_data = checkpoint.state_data.get('intermediate_data', input_data)
                    self.metrics.records_processed = checkpoint.offset

            # Execute stages
            for idx in range(start_stage_idx, len(self.stages)):
                if self.cancelled:
                    self.status = PipelineStatus.CANCELLED
                    self.logger.warning("Pipeline cancelled by user")
                    break

                stage = self.stages[idx]
                current_data = self._execute_stage(stage, current_data, idx)

            # Pipeline completed successfully
            if not self.cancelled:
                self.status = PipelineStatus.SUCCESS
                self.metrics.end_time = datetime.now()
                duration = (self.metrics.end_time - self.metrics.start_time).total_seconds()

                self.logger.info(
                    f"Pipeline {self.pipeline_id} completed successfully. "
                    f"Duration: {duration:.2f}s, "
                    f"Records: {self.metrics.records_processed}, "
                    f"Errors: {self.metrics.error_count}"
                )

                # Clean up checkpoints on success
                self._cleanup_checkpoints()

            return current_data

        except Exception as e:
            self.status = PipelineStatus.FAILED
            self.metrics.end_time = datetime.now()
            self.logger.error(f"Pipeline {self.pipeline_id} failed: {str(e)}", exc_info=True)

            # Save checkpoint for recovery
            if self.metrics.current_stage:
                self._save_checkpoint(
                    stage_name=self.metrics.current_stage,
                    offset=self.metrics.records_processed,
                    state_data={'error': str(e), 'intermediate_data': current_data}
                )

            raise

    def _execute_stage(self, stage: PipelineStage,
                      input_data: Any, stage_idx: int) -> Any:
        """Execute a single pipeline stage with error handling."""
        self.metrics.current_stage = stage.name

        self.logger.info(
            f"Executing stage [{stage_idx+1}/{len(self.stages)}]: {stage.name}"
        )

        stage_start = time.time()
        attempt = 0

        while attempt <= stage.retry_count:
            try:
                # Execute stage function
                output_data = stage.function(input_data)

                # Update metrics
                stage_duration = time.time() - stage_start
                self.metrics.stage_metrics[stage.name] = {
                    'duration': stage_duration,
                    'success': True,
                    'attempts': attempt + 1
                }

                self.logger.info(
                    f"Stage {stage.name} completed in {stage_duration:.2f}s "
                    f"(attempt {attempt + 1})"
                )

                # Checkpoint if enabled
                if stage.checkpoint_enabled:
                    self._maybe_checkpoint(stage.name, output_data)

                return output_data

            except Exception as e:
                attempt += 1
                self.metrics.error_count += 1

                if attempt <= stage.retry_count:
                    self.logger.warning(
                        f"Stage {stage.name} failed (attempt {attempt}/{stage.retry_count}): {str(e)}"
                    )
                    self.metrics.retry_count += 1
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    # Max retries exceeded
                    self.metrics.stage_metrics[stage.name] = {
                        'duration': time.time() - stage_start,
                        'success': False,
                        'attempts': attempt,
                        'error': str(e)
                    }

                    if stage.skip_on_failure:
                        self.logger.error(
                            f"Stage {stage.name} failed but marked as skippable: {str(e)}"
                        )
                        return input_data  # Return input unchanged
                    else:
                        self.logger.error(
                            f"Stage {stage.name} failed after {attempt} attempts: {str(e)}"
                        )
                        raise

    def _maybe_checkpoint(self, stage_name: str, data: Any) -> None:
        """Conditionally save checkpoint based on strategy."""
        should_checkpoint = False

        if self.checkpoint_strategy == CheckpointStrategy.STAGE:
            should_checkpoint = True
        elif self.checkpoint_strategy == CheckpointStrategy.PERIODIC:
            if time.time() - self.last_checkpoint_time > self.checkpoint_interval:
                should_checkpoint = True
        elif self.checkpoint_strategy == CheckpointStrategy.RECORD_COUNT:
            # Checkpoint every 10,000 records
            if self.metrics.records_processed % 10000 == 0:
                should_checkpoint = True
        elif self.checkpoint_strategy == CheckpointStrategy.ADAPTIVE:
            # Checkpoint based on data size (every 100MB)
            if self.metrics.bytes_processed > 100 * 1024 * 1024:
                should_checkpoint = True

        if should_checkpoint:
            self._save_checkpoint(
                stage_name=stage_name,
                offset=self.metrics.records_processed,
                state_data={'intermediate_data': data}
            )
            self.last_checkpoint_time = time.time()
            self.metrics.bytes_processed = 0  # Reset for adaptive

    def _save_checkpoint(self, stage_name: str,
                        offset: int, state_data: Dict) -> None:
        """Save checkpoint to disk."""
        checkpoint = PipelineCheckpoint(
            pipeline_id=self.pipeline_id,
            stage_name=stage_name,
            offset=offset,
            timestamp=datetime.now(),
            state_data=state_data,
            checksum=self._compute_checksum(state_data)
        )

        checkpoint_path = self.checkpoint_dir / f"{self.pipeline_id}_latest.pkl"

        try:
            with open(checkpoint_path, 'wb') as f:
                pickle.dump(checkpoint, f)

            self.logger.info(
                f"Checkpoint saved: stage={stage_name}, offset={offset}"
            )
            self.current_checkpoint = checkpoint

        except Exception as e:
            self.logger.error(f"Failed to save checkpoint: {str(e)}")

    def _load_latest_checkpoint(self) -> Optional[PipelineCheckpoint]:
        """Load the latest checkpoint from disk."""
        checkpoint_path = self.checkpoint_dir / f"{self.pipeline_id}_latest.pkl"

        if not checkpoint_path.exists():
            return None

        try:
            with open(checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)

            # Verify checksum
            if checkpoint.checksum != self._compute_checksum(checkpoint.state_data):
                self.logger.warning("Checkpoint checksum mismatch, ignoring")
                return None

            self.logger.info(f"Loaded checkpoint from {checkpoint.timestamp}")
            return checkpoint

        except Exception as e:
            self.logger.error(f"Failed to load checkpoint: {str(e)}")
            return None

    def _cleanup_checkpoints(self) -> None:
        """Clean up checkpoints after successful completion."""
        checkpoint_path = self.checkpoint_dir / f"{self.pipeline_id}_latest.pkl"
        if checkpoint_path.exists():
            checkpoint_path.unlink()
            self.logger.info("Checkpoint cleaned up")

    def _compute_checksum(self, data: Dict) -> str:
        """Compute checksum for data integrity."""
        data_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.sha256(data_str.encode()).hexdigest()

    def _get_stage_index(self, stage_name: str) -> int:
        """Get the index of a stage by name."""
        for idx, stage in enumerate(self.stages):
            if stage.name == stage_name:
                return idx
        return 0

    def cancel(self) -> None:
        """Cancel the pipeline execution."""
        self.cancelled = True
        self.logger.warning("Pipeline cancellation requested")

    def get_metrics(self) -> PipelineMetrics:
        """Get current pipeline metrics."""
        return self.metrics

    def get_status(self) -> PipelineStatus:
        """Get current pipeline status."""
        return self.status

# Example usage
pipeline = DataPipeline(
    pipeline_id="customer_etl_v1",
    config={
        'checkpoint_strategy': 'stage',
        'checkpoint_dir': '/data/checkpoints'
    }
)

# Define stages
pipeline.add_stage(PipelineStage(
    name="extract_customers",
    function=lambda x: extract_from_database(x),
    timeout_seconds=1800,
    retry_count=3
))

pipeline.add_stage(PipelineStage(
    name="transform_data",
    function=lambda x: transform_customer_data(x),
    timeout_seconds=3600,
    retry_count=2
))

pipeline.add_stage(PipelineStage(
    name="load_to_warehouse",
    function=lambda x: load_to_snowflake(x),
    timeout_seconds=1800,
    retry_count=5
))

# Execute with automatic recovery
try:
    result = pipeline.execute(
        input_data={'start_date': '2024-01-01'},
        resume_from_checkpoint=True
    )
    print(f"Pipeline completed: {pipeline.metrics.records_processed} records")
except Exception as e:
    print(f"Pipeline failed: {e}")
    metrics = pipeline.get_metrics()
    print(f"Failed at stage: {metrics.current_stage}")
\end{lstlisting}

\subsubsection{StreamProcessor: Exactly-Once Semantics}

\begin{lstlisting}[style=python, caption=StreamProcessor with Exactly-Once Guarantees]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable, Any, Set
from enum import Enum
import logging
from datetime import datetime, timedelta
from collections import defaultdict
import threading
import time
from kafka import KafkaConsumer, KafkaProducer
from kafka.structs import TopicPartition

class ProcessingGuarantee(Enum):
    """Processing guarantee levels."""
    AT_MOST_ONCE = "at_most_once"  # Fast, may lose data
    AT_LEAST_ONCE = "at_least_once"  # May duplicate
    EXACTLY_ONCE = "exactly_once"  # Slowest, no duplicates

@dataclass
class StreamState:
    """State for a stream processing task."""
    key: str
    value: Any
    last_updated: datetime
    version: int = 1

@dataclass
class ProcessingOffset:
    """Offset tracking for exactly-once processing."""
    topic: str
    partition: int
    offset: int
    timestamp: datetime

class StateBackend:
    """Distributed state backend for stream processing."""

    def __init__(self, backend_type: str = "rocksdb"):
        self.backend_type = backend_type
        self.state: Dict[str, StreamState] = {}
        self.lock = threading.RLock()
        self.logger = logging.getLogger(__name__)

    def get(self, key: str) -> Optional[StreamState]:
        """Get state by key."""
        with self.lock:
            return self.state.get(key)

    def put(self, key: str, value: Any) -> None:
        """Put state with versioning."""
        with self.lock:
            existing = self.state.get(key)
            version = existing.version + 1 if existing else 1

            self.state[key] = StreamState(
                key=key,
                value=value,
                last_updated=datetime.now(),
                version=version
            )

    def delete(self, key: str) -> None:
        """Delete state by key."""
        with self.lock:
            self.state.pop(key, None)

    def checkpoint(self) -> Dict[str, StreamState]:
        """Create checkpoint of entire state."""
        with self.lock:
            return self.state.copy()

    def restore(self, checkpoint: Dict[str, StreamState]) -> None:
        """Restore state from checkpoint."""
        with self.lock:
            self.state = checkpoint.copy()
            self.logger.info(f"Restored state: {len(self.state)} keys")

class DeduplicationCache:
    """Cache for deduplicating processed records."""

    def __init__(self, ttl_seconds: int = 3600, max_size: int = 100000):
        self.ttl_seconds = ttl_seconds
        self.max_size = max_size
        self.cache: Dict[str, datetime] = {}
        self.lock = threading.RLock()

    def is_duplicate(self, record_id: str) -> bool:
        """Check if record has been processed."""
        with self.lock:
            if record_id in self.cache:
                # Check TTL
                if datetime.now() - self.cache[record_id] < timedelta(seconds=self.ttl_seconds):
                    return True
                else:
                    # Expired, remove
                    del self.cache[record_id]
            return False

    def mark_processed(self, record_id: str) -> None:
        """Mark record as processed."""
        with self.lock:
            # Evict oldest if at capacity
            if len(self.cache) >= self.max_size:
                oldest_key = min(self.cache, key=self.cache.get)
                del self.cache[oldest_key]

            self.cache[record_id] = datetime.now()

    def cleanup_expired(self) -> None:
        """Remove expired entries."""
        with self.lock:
            now = datetime.now()
            expired = [
                k for k, v in self.cache.items()
                if now - v >= timedelta(seconds=self.ttl_seconds)
            ]
            for key in expired:
                del self.cache[key]

class StreamProcessor:
    """Stream processor with exactly-once semantics."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Processing guarantee
        self.guarantee = ProcessingGuarantee(
            config.get('processing_guarantee', 'exactly_once')
        )

        # Kafka setup
        self.consumer = KafkaConsumer(
            *config['input_topics'],
            bootstrap_servers=config['bootstrap_servers'],
            group_id=config['consumer_group'],
            enable_auto_commit=False,  # Manual commit for exactly-once
            isolation_level='read_committed',  # For transactional reads
            auto_offset_reset='earliest'
        )

        self.producer = KafkaProducer(
            bootstrap_servers=config['bootstrap_servers'],
            transactional_id=config.get('transactional_id'),  # Enable transactions
            enable_idempotence=True,  # Exactly-once to broker
            acks='all',  # Wait for all replicas
            retries=10
        )

        # Initialize transactions
        if self.guarantee == ProcessingGuarantee.EXACTLY_ONCE:
            self.producer.init_transactions()

        # State management
        self.state_backend = StateBackend()
        self.dedup_cache = DeduplicationCache()

        # Offset tracking
        self.processed_offsets: Dict[TopicPartition, ProcessingOffset] = {}

        # Processing function
        self.process_function: Optional[Callable] = None

        # Control
        self.running = False
        self.checkpoint_interval = config.get('checkpoint_interval', 60)  # 60s
        self.last_checkpoint = time.time()

    def set_process_function(self, func: Callable[[Any], Any]) -> None:
        """Set the processing function."""
        self.process_function = func

    def start(self) -> None:
        """Start stream processing."""
        if not self.process_function:
            raise ValueError("Process function not set")

        self.running = True
        self.logger.info(
            f"Stream processor starting with {self.guarantee.value} semantics"
        )

        try:
            while self.running:
                # Poll for records
                records = self.consumer.poll(timeout_ms=1000, max_records=500)

                if not records:
                    continue

                # Process batch with exactly-once semantics
                if self.guarantee == ProcessingGuarantee.EXACTLY_ONCE:
                    self._process_batch_exactly_once(records)
                elif self.guarantee == ProcessingGuarantee.AT_LEAST_ONCE:
                    self._process_batch_at_least_once(records)
                else:
                    self._process_batch_at_most_once(records)

                # Periodic checkpoint
                if time.time() - self.last_checkpoint > self.checkpoint_interval:
                    self._checkpoint_state()
                    self.last_checkpoint = time.time()

        except Exception as e:
            self.logger.error(f"Stream processing failed: {str(e)}", exc_info=True)
            raise
        finally:
            self.stop()

    def _process_batch_exactly_once(self, records: Dict) -> None:
        """Process batch with exactly-once semantics using transactions."""
        # Begin transaction
        self.producer.begin_transaction()

        try:
            processed_count = 0
            offsets_to_commit = {}

            for topic_partition, messages in records.items():
                for message in messages:
                    # Generate unique record ID
                    record_id = f"{topic_partition.topic}:{topic_partition.partition}:{message.offset}"

                    # Deduplication check
                    if self.dedup_cache.is_duplicate(record_id):
                        self.logger.debug(f"Skipping duplicate: {record_id}")
                        continue

                    # Process record
                    try:
                        result = self.process_function(message.value)

                        # Send result (within transaction)
                        if result:
                            self.producer.send(
                                self.config['output_topic'],
                                value=result,
                                key=message.key
                            )

                        # Mark as processed
                        self.dedup_cache.mark_processed(record_id)
                        processed_count += 1

                        # Track offset
                        offsets_to_commit[topic_partition] = message.offset + 1

                    except Exception as e:
                        self.logger.error(
                            f"Failed to process record {record_id}: {str(e)}"
                        )
                        # Abort transaction and retry
                        self.producer.abort_transaction()
                        raise

            # Commit offsets to transaction
            if offsets_to_commit:
                self.consumer.commit(offsets=offsets_to_commit)

            # Commit transaction (atomic: all or nothing)
            self.producer.commit_transaction()

            self.logger.info(
                f"Batch processed with exactly-once: {processed_count} records"
            )

        except Exception as e:
            # Abort on any failure
            self.producer.abort_transaction()
            self.logger.error(f"Transaction aborted: {str(e)}")
            raise

    def _process_batch_at_least_once(self, records: Dict) -> None:
        """Process batch with at-least-once semantics."""
        processed_count = 0

        for topic_partition, messages in records.items():
            for message in messages:
                try:
                    # Process record
                    result = self.process_function(message.value)

                    # Send result
                    if result:
                        self.producer.send(
                            self.config['output_topic'],
                            value=result,
                            key=message.key
                        )

                    processed_count += 1

                except Exception as e:
                    self.logger.error(f"Failed to process record: {str(e)}")
                    # Continue processing other records

        # Flush producer
        self.producer.flush()

        # Commit offsets after processing
        self.consumer.commit()

        self.logger.info(
            f"Batch processed with at-least-once: {processed_count} records"
        )

    def _process_batch_at_most_once(self, records: Dict) -> None:
        """Process batch with at-most-once semantics."""
        # Commit offsets BEFORE processing
        self.consumer.commit()

        processed_count = 0

        for topic_partition, messages in records.items():
            for message in messages:
                try:
                    result = self.process_function(message.value)

                    if result:
                        self.producer.send(
                            self.config['output_topic'],
                            value=result,
                            key=message.key
                        )

                    processed_count += 1

                except Exception as e:
                    self.logger.warning(
                        f"Failed to process record (lost): {str(e)}"
                    )
                    # Record is lost if processing fails

        self.producer.flush()

        self.logger.info(
            f"Batch processed with at-most-once: {processed_count} records"
        )

    def _checkpoint_state(self) -> None:
        """Checkpoint state for recovery."""
        checkpoint = self.state_backend.checkpoint()
        self.logger.info(f"State checkpointed: {len(checkpoint)} keys")

        # Cleanup deduplication cache
        self.dedup_cache.cleanup_expired()

    def stop(self) -> None:
        """Stop stream processing."""
        self.running = False
        self.consumer.close()
        self.producer.close()
        self.logger.info("Stream processor stopped")

# Example usage
processor = StreamProcessor(config={
    'input_topics': ['raw_events'],
    'output_topic': 'processed_events',
    'bootstrap_servers': ['localhost:9092'],
    'consumer_group': 'event_processor_v1',
    'transactional_id': 'event_processor_tx',
    'processing_guarantee': 'exactly_once',
    'checkpoint_interval': 60
})

# Define processing logic
def process_event(event):
    # Transform event
    transformed = {
        'event_id': event['id'],
        'timestamp': datetime.now().isoformat(),
        'processed_value': event['value'] * 2
    }
    return transformed

processor.set_process_function(process_event)

# Start processing (blocks until stopped)
processor.start()
\end{lstlisting}

\subsubsection{PipelineOrchestrator: DAG-Based Execution}

\begin{lstlisting}[style=python, caption=PipelineOrchestrator with Dependency Management]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable, Any, Set
from enum import Enum
from datetime import datetime, timedelta
import logging
from collections import defaultdict, deque
import threading
from concurrent.futures import ThreadPoolExecutor, Future
import time

class TaskStatus(Enum):
    """Task execution status."""
    PENDING = "pending"
    WAITING = "waiting"  # Waiting for dependencies
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"

class ScheduleType(Enum):
    """Task scheduling types."""
    MANUAL = "manual"
    CRON = "cron"
    INTERVAL = "interval"
    EVENT = "event"

@dataclass
class Task:
    """Pipeline task definition."""
    task_id: str
    function: Callable[[Any], Any]
    dependencies: List[str] = field(default_factory=list)
    retry_count: int = 3
    timeout_seconds: int = 3600
    trigger_rule: str = "all_success"  # all_success, one_success, none_failed
    resources: Dict[str, float] = field(default_factory=dict)

@dataclass
class TaskExecution:
    """Task execution instance."""
    task_id: str
    status: TaskStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Any = None
    error: Optional[str] = None
    attempt: int = 0

@dataclass
class DAGRun:
    """DAG execution instance."""
    dag_id: str
    run_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: TaskStatus = TaskStatus.PENDING
    task_executions: Dict[str, TaskExecution] = field(default_factory=dict)

class PipelineOrchestrator:
    """DAG-based pipeline orchestrator with dependency management."""

    def __init__(self, dag_id: str, config: Dict[str, Any]):
        self.dag_id = dag_id
        self.config = config
        self.logger = logging.getLogger(f"orchestrator.{dag_id}")

        # DAG definition
        self.tasks: Dict[str, Task] = {}
        self.task_graph: Dict[str, Set[str]] = defaultdict(set)  # task -> dependents

        # Execution state
        self.current_run: Optional[DAGRun] = None
        self.run_history: List[DAGRun] = []

        # Concurrency control
        self.max_parallel_tasks = config.get('max_parallel_tasks', 10)
        self.executor = ThreadPoolExecutor(max_workers=self.max_parallel_tasks)
        self.running_tasks: Dict[str, Future] = {}

        # Resource management
        self.available_resources = {
            'cpu': config.get('max_cpu', 16.0),
            'memory_gb': config.get('max_memory_gb', 64.0)
        }
        self.allocated_resources: Dict[str, Dict[str, float]] = {}
        self.resource_lock = threading.RLock()

        # Scheduling
        self.schedule_type = ScheduleType(config.get('schedule_type', 'manual'))
        self.schedule_interval = config.get('schedule_interval', 3600)  # 1 hour

    def add_task(self, task: Task) -> None:
        """Add a task to the DAG."""
        self.tasks[task.task_id] = task

        # Build dependency graph
        for dep in task.dependencies:
            self.task_graph[dep].add(task.task_id)

        self.logger.info(
            f"Added task: {task.task_id} "
            f"(dependencies: {len(task.dependencies)})"
        )

    def validate_dag(self) -> bool:
        """Validate DAG for cycles and missing dependencies."""
        # Check for missing dependencies
        for task_id, task in self.tasks.items():
            for dep in task.dependencies:
                if dep not in self.tasks:
                    self.logger.error(
                        f"Task {task_id} has missing dependency: {dep}"
                    )
                    return False

        # Check for cycles using DFS
        visited = set()
        rec_stack = set()

        def has_cycle(node: str) -> bool:
            visited.add(node)
            rec_stack.add(node)

            for neighbor in self.task_graph[node]:
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    self.logger.error(f"Cycle detected involving task: {node}")
                    return True

            rec_stack.remove(node)
            return False

        for task_id in self.tasks:
            if task_id not in visited:
                if has_cycle(task_id):
                    return False

        self.logger.info("DAG validation passed")
        return True

    def execute(self, context: Optional[Dict] = None) -> DAGRun:
        """Execute the DAG."""
        if not self.validate_dag():
            raise ValueError("DAG validation failed")

        # Create DAG run
        run_id = f"{self.dag_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.current_run = DAGRun(
            dag_id=self.dag_id,
            run_id=run_id,
            start_time=datetime.now(),
            status=TaskStatus.RUNNING
        )

        # Initialize task executions
        for task_id in self.tasks:
            self.current_run.task_executions[task_id] = TaskExecution(
                task_id=task_id,
                status=TaskStatus.PENDING
            )

        self.logger.info(f"DAG run started: {run_id} ({len(self.tasks)} tasks)")

        try:
            # Execute tasks in topological order with parallelism
            self._execute_dag_parallel(context or {})

            # Check final status
            all_success = all(
                exec.status == TaskStatus.SUCCESS
                for exec in self.current_run.task_executions.values()
            )

            self.current_run.status = (
                TaskStatus.SUCCESS if all_success else TaskStatus.FAILED
            )

        except Exception as e:
            self.logger.error(f"DAG execution failed: {str(e)}", exc_info=True)
            self.current_run.status = TaskStatus.FAILED
            raise
        finally:
            self.current_run.end_time = datetime.now()
            duration = (self.current_run.end_time - self.current_run.start_time).total_seconds()

            self.logger.info(
                f"DAG run completed: {run_id} "
                f"(status: {self.current_run.status.value}, "
                f"duration: {duration:.2f}s)"
            )

            self.run_history.append(self.current_run)

        return self.current_run

    def _execute_dag_parallel(self, context: Dict) -> None:
        """Execute DAG tasks in parallel respecting dependencies."""
        # Track task completion
        completed_tasks: Set[str] = set()
        failed_tasks: Set[str] = set()

        while len(completed_tasks) + len(failed_tasks) < len(self.tasks):
            # Find tasks ready to run
            ready_tasks = self._get_ready_tasks(completed_tasks, failed_tasks)

            # Submit ready tasks
            for task_id in ready_tasks:
                if self._can_allocate_resources(task_id):
                    self._submit_task(task_id, context)
                else:
                    self.logger.debug(
                        f"Task {task_id} waiting for resources"
                    )

            # Wait for at least one task to complete
            if self.running_tasks:
                # Check running tasks
                completed_now = []
                for task_id, future in list(self.running_tasks.items()):
                    if future.done():
                        try:
                            result = future.result()
                            self._mark_task_success(task_id, result)
                            completed_tasks.add(task_id)
                        except Exception as e:
                            self._mark_task_failed(task_id, str(e))
                            failed_tasks.add(task_id)

                        completed_now.append(task_id)
                        self._release_resources(task_id)

                # Remove completed tasks
                for task_id in completed_now:
                    del self.running_tasks[task_id]

            # Avoid busy waiting
            if not ready_tasks and self.running_tasks:
                time.sleep(0.1)
            elif not ready_tasks and not self.running_tasks:
                # Deadlock or all failed
                break

    def _get_ready_tasks(self, completed: Set[str],
                        failed: Set[str]) -> List[str]:
        """Get tasks ready to run based on dependencies."""
        ready = []

        for task_id, task in self.tasks.items():
            exec_status = self.current_run.task_executions[task_id].status

            # Skip if already processed
            if exec_status != TaskStatus.PENDING:
                continue

            # Check if dependencies are satisfied
            if self._check_trigger_rule(task, completed, failed):
                ready.append(task_id)

        return ready

    def _check_trigger_rule(self, task: Task,
                           completed: Set[str],
                           failed: Set[str]) -> bool:
        """Check if task trigger rule is satisfied."""
        if not task.dependencies:
            return True

        deps_completed = all(dep in completed for dep in task.dependencies)
        deps_failed = any(dep in failed for dep in task.dependencies)

        if task.trigger_rule == "all_success":
            return deps_completed and not deps_failed
        elif task.trigger_rule == "one_success":
            return any(dep in completed for dep in task.dependencies)
        elif task.trigger_rule == "none_failed":
            return not deps_failed and (deps_completed or not task.dependencies)

        return False

    def _can_allocate_resources(self, task_id: str) -> bool:
        """Check if resources are available for task."""
        task = self.tasks[task_id]

        with self.resource_lock:
            for resource, required in task.resources.items():
                allocated = sum(
                    r.get(resource, 0)
                    for r in self.allocated_resources.values()
                )
                if allocated + required > self.available_resources.get(resource, float('inf')):
                    return False

            # Allocate resources
            self.allocated_resources[task_id] = task.resources.copy()
            return True

    def _release_resources(self, task_id: str) -> None:
        """Release resources allocated to task."""
        with self.resource_lock:
            self.allocated_resources.pop(task_id, None)

    def _submit_task(self, task_id: str, context: Dict) -> None:
        """Submit task for execution."""
        task = self.tasks[task_id]
        exec = self.current_run.task_executions[task_id]

        exec.status = TaskStatus.RUNNING
        exec.start_time = datetime.now()

        self.logger.info(f"Submitting task: {task_id}")

        # Submit to executor
        future = self.executor.submit(self._execute_task, task, context)
        self.running_tasks[task_id] = future

    def _execute_task(self, task: Task, context: Dict) -> Any:
        """Execute a single task with retries."""
        attempt = 0

        while attempt <= task.retry_count:
            try:
                # Execute task function
                result = task.function(context)
                return result

            except Exception as e:
                attempt += 1
                if attempt <= task.retry_count:
                    self.logger.warning(
                        f"Task {task.task_id} failed "
                        f"(attempt {attempt}/{task.retry_count}): {str(e)}"
                    )
                    time.sleep(2 ** attempt)
                else:
                    raise

    def _mark_task_success(self, task_id: str, result: Any) -> None:
        """Mark task as successful."""
        exec = self.current_run.task_executions[task_id]
        exec.status = TaskStatus.SUCCESS
        exec.end_time = datetime.now()
        exec.result = result

        duration = (exec.end_time - exec.start_time).total_seconds()
        self.logger.info(f"Task {task_id} succeeded in {duration:.2f}s")

    def _mark_task_failed(self, task_id: str, error: str) -> None:
        """Mark task as failed."""
        exec = self.current_run.task_executions[task_id]
        exec.status = TaskStatus.FAILED
        exec.end_time = datetime.now()
        exec.error = error

        self.logger.error(f"Task {task_id} failed: {error}")

    def get_task_status(self, task_id: str) -> Optional[TaskExecution]:
        """Get status of a specific task."""
        if self.current_run:
            return self.current_run.task_executions.get(task_id)
        return None

    def shutdown(self) -> None:
        """Shutdown the orchestrator."""
        self.executor.shutdown(wait=True)
        self.logger.info("Orchestrator shutdown")

# Example usage
orchestrator = PipelineOrchestrator(
    dag_id="daily_etl",
    config={
        'max_parallel_tasks': 5,
        'max_cpu': 16.0,
        'max_memory_gb': 64.0
    }
)

# Define tasks
orchestrator.add_task(Task(
    task_id="extract_orders",
    function=lambda ctx: extract_orders(ctx['start_date']),
    dependencies=[],
    resources={'cpu': 2.0, 'memory_gb': 4.0}
))

orchestrator.add_task(Task(
    task_id="extract_customers",
    function=lambda ctx: extract_customers(ctx['start_date']),
    dependencies=[],
    resources={'cpu': 2.0, 'memory_gb': 4.0}
))

orchestrator.add_task(Task(
    task_id="join_data",
    function=lambda ctx: join_orders_customers(),
    dependencies=["extract_orders", "extract_customers"],
    resources={'cpu': 4.0, 'memory_gb': 8.0}
))

orchestrator.add_task(Task(
    task_id="aggregate_metrics",
    function=lambda ctx: compute_metrics(),
    dependencies=["join_data"],
    resources={'cpu': 2.0, 'memory_gb': 4.0}
))

orchestrator.add_task(Task(
    task_id="load_warehouse",
    function=lambda ctx: load_to_warehouse(),
    dependencies=["aggregate_metrics"],
    resources={'cpu': 1.0, 'memory_gb': 2.0}
))

# Execute DAG
dag_run = orchestrator.execute(context={'start_date': '2024-01-01'})
print(f"DAG run: {dag_run.status.value}")

# Check individual task statuses
for task_id, execution in dag_run.task_executions.items():
    print(f"  {task_id}: {execution.status.value}")
\end{lstlisting}

\subsubsection{PipelineMonitor: Comprehensive Observability}

\begin{lstlisting}[style=python, caption=PipelineMonitor with Performance Tracking]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable, Any
from enum import Enum
from datetime import datetime, timedelta
import logging
from collections import deque, defaultdict
import threading
import time
from prometheus_client import Counter, Gauge, Histogram, Summary
import json

class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

@dataclass
class Alert:
    """Pipeline alert."""
    alert_id: str
    pipeline_id: str
    severity: AlertSeverity
    message: str
    timestamp: datetime
    metric_name: str
    metric_value: float
    threshold: float

@dataclass
class PerformanceMetrics:
    """Performance metrics snapshot."""
    timestamp: datetime
    throughput_rps: float  # Records per second
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float
    error_rate: float
    cpu_usage_percent: float
    memory_usage_mb: float
    active_tasks: int

class MetricsAggregator:
    """Aggregates metrics over time windows."""

    def __init__(self, window_size: int = 300):  # 5 min window
        self.window_size = window_size
        self.values: deque = deque(maxlen=window_size)
        self.lock = threading.RLock()

    def add(self, value: float) -> None:
        """Add a value to the window."""
        with self.lock:
            self.values.append((time.time(), value))

    def get_percentile(self, percentile: float) -> float:
        """Calculate percentile from window."""
        with self.lock:
            if not self.values:
                return 0.0

            # Filter recent values
            cutoff = time.time() - self.window_size
            recent = [v for t, v in self.values if t > cutoff]

            if not recent:
                return 0.0

            sorted_values = sorted(recent)
            index = int(len(sorted_values) * percentile / 100)
            return sorted_values[min(index, len(sorted_values) - 1)]

    def get_average(self) -> float:
        """Calculate average from window."""
        with self.lock:
            if not self.values:
                return 0.0

            cutoff = time.time() - self.window_size
            recent = [v for t, v in self.values if t > cutoff]
            return sum(recent) / len(recent) if recent else 0.0

class PipelineMonitor:
    """Comprehensive pipeline monitoring and alerting."""

    def __init__(self, pipeline_id: str, config: Dict[str, Any]):
        self.pipeline_id = pipeline_id
        self.config = config
        self.logger = logging.getLogger(f"monitor.{pipeline_id}")

        # Prometheus metrics
        self.records_processed = Counter(
            'pipeline_records_processed_total',
            'Total records processed',
            ['pipeline_id', 'stage']
        )

        self.records_failed = Counter(
            'pipeline_records_failed_total',
            'Total records failed',
            ['pipeline_id', 'stage', 'error_type']
        )

        self.processing_latency = Histogram(
            'pipeline_processing_latency_seconds',
            'Processing latency in seconds',
            ['pipeline_id', 'stage'],
            buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 30.0]
        )

        self.active_tasks_gauge = Gauge(
            'pipeline_active_tasks',
            'Number of active tasks',
            ['pipeline_id']
        )

        self.throughput_gauge = Gauge(
            'pipeline_throughput_rps',
            'Records per second throughput',
            ['pipeline_id']
        )

        # Custom metrics aggregation
        self.latency_aggregator = MetricsAggregator(window_size=300)
        self.throughput_window = deque(maxlen=60)  # 60 seconds

        # Alert management
        self.alerts: List[Alert] = []
        self.alert_callbacks: List[Callable[[Alert], None]] = []
        self.alert_thresholds = config.get('alert_thresholds', {
            'error_rate_percent': 5.0,
            'latency_p99_ms': 5000.0,
            'throughput_rps': 100.0,
            'cpu_usage_percent': 90.0,
            'memory_usage_mb': 8000.0
        })

        # Performance tracking
        self.performance_history: List[PerformanceMetrics] = []
        self.last_metrics_time = time.time()
        self.records_since_last = 0

        # Monitoring thread
        self.monitoring_thread: Optional[threading.Thread] = None
        self.running = False

    def start_monitoring(self) -> None:
        """Start background monitoring thread."""
        self.running = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True
        )
        self.monitoring_thread.start()
        self.logger.info("Monitoring started")

    def stop_monitoring(self) -> None:
        """Stop monitoring thread."""
        self.running = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        self.logger.info("Monitoring stopped")

    def record_processing(self, stage: str, latency_seconds: float,
                         success: bool = True,
                         error_type: Optional[str] = None) -> None:
        """Record a processing event."""
        # Update Prometheus metrics
        if success:
            self.records_processed.labels(
                pipeline_id=self.pipeline_id,
                stage=stage
            ).inc()
        else:
            self.records_failed.labels(
                pipeline_id=self.pipeline_id,
                stage=stage,
                error_type=error_type or "unknown"
            ).inc()

        self.processing_latency.labels(
            pipeline_id=self.pipeline_id,
            stage=stage
        ).observe(latency_seconds)

        # Update aggregators
        self.latency_aggregator.add(latency_seconds * 1000)  # Convert to ms
        self.records_since_last += 1

    def update_active_tasks(self, count: int) -> None:
        """Update active tasks count."""
        self.active_tasks_gauge.labels(
            pipeline_id=self.pipeline_id
        ).set(count)

    def _monitoring_loop(self) -> None:
        """Background monitoring loop."""
        while self.running:
            try:
                # Collect metrics
                metrics = self._collect_metrics()

                # Store performance snapshot
                self.performance_history.append(metrics)

                # Keep only last 24 hours
                cutoff = datetime.now() - timedelta(hours=24)
                self.performance_history = [
                    m for m in self.performance_history
                    if m.timestamp > cutoff
                ]

                # Check thresholds and generate alerts
                self._check_alerts(metrics)

                # Sleep until next collection
                time.sleep(10)  # 10 second intervals

            except Exception as e:
                self.logger.error(
                    f"Monitoring loop error: {str(e)}",
                    exc_info=True
                )

    def _collect_metrics(self) -> PerformanceMetrics:
        """Collect current metrics snapshot."""
        now = time.time()
        elapsed = now - self.last_metrics_time

        # Calculate throughput
        throughput = self.records_since_last / elapsed if elapsed > 0 else 0.0

        # Update throughput gauge
        self.throughput_gauge.labels(
            pipeline_id=self.pipeline_id
        ).set(throughput)

        # Calculate latencies
        p50 = self.latency_aggregator.get_percentile(50)
        p95 = self.latency_aggregator.get_percentile(95)
        p99 = self.latency_aggregator.get_percentile(99)

        # Create snapshot
        metrics = PerformanceMetrics(
            timestamp=datetime.now(),
            throughput_rps=throughput,
            latency_p50_ms=p50,
            latency_p95_ms=p95,
            latency_p99_ms=p99,
            error_rate=0.0,  # Would calculate from Prometheus
            cpu_usage_percent=0.0,  # Would get from system
            memory_usage_mb=0.0,  # Would get from system
            active_tasks=0  # Would get from orchestrator
        )

        # Reset counters
        self.last_metrics_time = now
        self.records_since_last = 0

        return metrics

    def _check_alerts(self, metrics: PerformanceMetrics) -> None:
        """Check metrics against thresholds and generate alerts."""
        # Check error rate
        if metrics.error_rate > self.alert_thresholds['error_rate_percent']:
            self._create_alert(
                severity=AlertSeverity.CRITICAL,
                message=f"Error rate {metrics.error_rate:.2f}% exceeds threshold",
                metric_name="error_rate",
                metric_value=metrics.error_rate,
                threshold=self.alert_thresholds['error_rate_percent']
            )

        # Check latency p99
        if metrics.latency_p99_ms > self.alert_thresholds['latency_p99_ms']:
            self._create_alert(
                severity=AlertSeverity.WARNING,
                message=f"P99 latency {metrics.latency_p99_ms:.2f}ms exceeds threshold",
                metric_name="latency_p99",
                metric_value=metrics.latency_p99_ms,
                threshold=self.alert_thresholds['latency_p99_ms']
            )

        # Check throughput (low throughput alert)
        if metrics.throughput_rps < self.alert_thresholds['throughput_rps']:
            self._create_alert(
                severity=AlertSeverity.WARNING,
                message=f"Throughput {metrics.throughput_rps:.2f} RPS below threshold",
                metric_name="throughput",
                metric_value=metrics.throughput_rps,
                threshold=self.alert_thresholds['throughput_rps']
            )

    def _create_alert(self, severity: AlertSeverity,
                     message: str, metric_name: str,
                     metric_value: float, threshold: float) -> None:
        """Create and dispatch alert."""
        alert = Alert(
            alert_id=f"{self.pipeline_id}_{metric_name}_{int(time.time())}",
            pipeline_id=self.pipeline_id,
            severity=severity,
            message=message,
            timestamp=datetime.now(),
            metric_name=metric_name,
            metric_value=metric_value,
            threshold=threshold
        )

        self.alerts.append(alert)

        self.logger.warning(
            f"ALERT [{severity.value.upper()}]: {message}"
        )

        # Trigger callbacks
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                self.logger.error(f"Alert callback failed: {str(e)}")

    def register_alert_callback(self, callback: Callable[[Alert], None]) -> None:
        """Register callback for alerts."""
        self.alert_callbacks.append(callback)

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary."""
        if not self.performance_history:
            return {}

        recent = self.performance_history[-60:]  # Last 10 minutes

        return {
            'avg_throughput_rps': sum(m.throughput_rps for m in recent) / len(recent),
            'avg_latency_p99_ms': sum(m.latency_p99_ms for m in recent) / len(recent),
            'max_latency_p99_ms': max(m.latency_p99_ms for m in recent),
            'avg_error_rate': sum(m.error_rate for m in recent) / len(recent),
            'active_alerts': len([a for a in self.alerts if a.severity == AlertSeverity.CRITICAL])
        }

    def export_metrics_json(self) -> str:
        """Export metrics as JSON."""
        summary = self.get_performance_summary()
        recent_alerts = self.alerts[-10:]  # Last 10 alerts

        export = {
            'pipeline_id': self.pipeline_id,
            'timestamp': datetime.now().isoformat(),
            'performance_summary': summary,
            'recent_alerts': [
                {
                    'severity': a.severity.value,
                    'message': a.message,
                    'timestamp': a.timestamp.isoformat()
                }
                for a in recent_alerts
            ]
        }

        return json.dumps(export, indent=2)

# Example usage
monitor = PipelineMonitor(
    pipeline_id="customer_etl_v1",
    config={
        'alert_thresholds': {
            'error_rate_percent': 5.0,
            'latency_p99_ms': 5000.0,
            'throughput_rps': 100.0
        }
    }
)

# Register alert callback
def send_alert_to_slack(alert: Alert):
    print(f"SLACK ALERT: [{alert.severity.value}] {alert.message}")

monitor.register_alert_callback(send_alert_to_slack)

# Start monitoring
monitor.start_monitoring()

# Record processing events
for i in range(1000):
    start = time.time()
    # ... process record ...
    latency = time.time() - start
    monitor.record_processing(
        stage="transform",
        latency_seconds=latency,
        success=True
    )

# Get performance summary
summary = monitor.get_performance_summary()
print(f"Performance: {summary}")

# Export metrics
metrics_json = monitor.export_metrics_json()
print(metrics_json)
\end{lstlisting}

\textbf{Integration with Enterprise Monitoring Systems:}

\begin{itemize}
    \item \textbf{Prometheus + Grafana}:
    \begin{itemize}
        \item Expose Prometheus metrics endpoint on port 8000
        \item Configure Grafana dashboards for pipeline visualization
        \item Alert rules in Alertmanager for threshold violations
        \item Example: \texttt{rate(pipeline\_records\_processed\_total[5m])}
    \end{itemize}

    \item \textbf{Datadog Integration}:
    \begin{itemize}
        \item Use DogStatsD client for metrics emission
        \item Custom metrics: \texttt{pipeline.latency}, \texttt{pipeline.throughput}
        \item Anomaly detection on throughput degradation
        \item Automatic incident creation on critical alerts
    \end{itemize}

    \item \textbf{CloudWatch (AWS)}:
    \begin{itemize}
        \item Publish custom metrics to CloudWatch
        \item Lambda functions triggered on metric alarms
        \item Log aggregation via CloudWatch Logs Insights
        \item Cross-region monitoring and failover detection
    \end{itemize}

    \item \textbf{Distributed Tracing (Jaeger/Zipkin)}:
    \begin{itemize}
        \item Instrument pipeline stages with OpenTelemetry
        \item Trace request flow across pipeline stages
        \item Identify bottlenecks via span duration analysis
        \item Correlate errors across distributed components
    \end{itemize}
\end{itemize}

\textbf{Production Metrics and SLOs:}

\begin{itemize}
    \item \textbf{Throughput}: >1000 records/second → Good, <100 → Critical
    \item \textbf{Latency P99}: <1s → Excellent, <5s → Good, >10s → Alert
    \item \textbf{Error Rate}: <0.1\% → Excellent, <1\% → Good, >5\% → Critical
    \item \textbf{Data Freshness}: <5 min lag → Good, >15 min → Alert
    \item \textbf{Resource Utilization}: CPU <80\%, Memory <85\% → Healthy
    \item \textbf{Availability}: >99.9\% uptime (43.2 min downtime/month)
\end{itemize}

\subsection{Modern Data Stack Integration}

Enterprise data platforms increasingly adopt the modern data stack, combining data lakehouse architectures, real-time analytics engines, automated metadata catalogs, and comprehensive lineage tracking. These components enable organizations to move beyond traditional data warehouses toward flexible, scalable, and discoverable data ecosystems.

\subsubsection{The Metadata Maze: A Discovery Crisis}

\textbf{The Company:} Global e-commerce platform with 800 engineers across 40 teams

\textbf{The Problem:} After 3 years of rapid growth, the data platform had become a black box:

\begin{itemize}
    \item \textbf{Scale explosion}: 15,000 tables, 2.3 million columns, 450 data pipelines
    \item \textbf{No centralized catalog}: Metadata scattered across wikis, Slack, tribal knowledge
    \item \textbf{Zero data lineage}: No one knew which tables fed which dashboards
    \item \textbf{Shadow documentation}: 12 different "data dictionaries" in various states of decay
    \item \textbf{Discovery paralysis}: Engineers spent 40\% of time searching for data
\end{itemize}

\textbf{The Incident (March 15, 2024):}

\begin{itemize}
    \item \textbf{08:00}: Product team needs "active customer count" for Q1 board presentation
    \item \textbf{08:15}: Data analyst finds 23 tables with "customer" in the name
    \begin{itemize}
        \item \texttt{customers}, \texttt{customers\_v2}, \texttt{customers\_clean}, \texttt{customers\_final}
        \item \texttt{dim\_customer}, \texttt{customer\_master}, \texttt{active\_customers}
        \item No documentation on which is authoritative
    \end{itemize}
    \item \textbf{09:00}: Analyst picks \texttt{customers\_v2} (most recent name?)
    \item \textbf{10:00}: Reports 8.2M active customers to executives
    \item \textbf{14:00}: Finance team questions number (their report shows 6.8M)
    \item \textbf{15:30}: Engineering investigation reveals:
    \begin{itemize}
        \item \texttt{customers\_v2} included test accounts (not filtered)
        \item \texttt{active\_customers} was the correct table (updated nightly)
        \item Difference: 1.4M test/inactive accounts
        \item No one documented the decommissioning of \texttt{customers\_v2}
    \end{itemize}
    \item \textbf{16:00}: CEO presents wrong numbers to board
    \item \textbf{18:00}: Board meeting adjourned for "data quality concerns"
\end{itemize}

\textbf{The Investigation:}

\begin{itemize}
    \item \textbf{Data discovery costs}: Engineers waste 15 hours/week searching for data
    \begin{itemize}
        \item 800 engineers × 15 hours × \$100/hour = \$1.2M/week = \$62M/year
    \end{itemize}
    \item \textbf{Incorrect data usage}: 34\% of queries used deprecated tables
    \begin{itemize}
        \item Led to incorrect business decisions costing estimated \$8M
    \end{itemize}
    \item \textbf{Duplicate work}: 18 teams independently reimplemented "customer segmentation"
    \begin{itemize}
        \item 18 different definitions of "active customer"
        \item Engineering cost: \$500K in wasted effort
    \end{itemize}
    \item \textbf{Compliance risk}: GDPR PII scattered across unknown tables
    \begin{itemize}
        \item Unable to fulfill "right to deletion" requests
        \item Potential fines: €20M or 4\% of revenue
    \end{itemize}
\end{itemize}

\textbf{The Solution:} Implementation of comprehensive metadata catalog with:
\begin{itemize}
    \item Automated metadata discovery and lineage tracking
    \item Column-level data classification (PII, sensitive, public)
    \item Usage analytics showing which tables are actually used
    \item Deprecation workflows with automated warnings
    \item Search-first interface (Google for data)
\end{itemize}

\textbf{Total Impact:}
\begin{itemize}
    \item \textbf{Before}: \$62M/year in lost productivity, \$8M in bad decisions
    \item \textbf{After}: 80\% reduction in data discovery time, zero deprecated table usage
    \item \textbf{ROI}: \$50M/year saved, implementation cost \$2M
    \item \textbf{Compliance}: Full PII inventory, GDPR compliance restored
\end{itemize}

\subsubsection{LakehouseIntegrator: Delta Lake and Iceberg Operations}

\begin{lstlisting}[style=python, caption=LakehouseIntegrator with Modern Table Formats]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum
from datetime import datetime, timedelta
import logging
from delta import DeltaTable, configure_spark_with_delta_pip
from pyspark.sql import SparkSession, DataFrame
from pyiceberg.catalog import load_catalog
from pyiceberg.table import Table as IcebergTable
import pandas as pd

class TableFormat(Enum):
    """Lakehouse table formats."""
    DELTA_LAKE = "delta"
    APACHE_ICEBERG = "iceberg"
    APACHE_HUDI = "hudi"

class IsolationLevel(Enum):
    """Transaction isolation levels."""
    SERIALIZABLE = "Serializable"
    SNAPSHOT = "SnapshotIsolation"
    READ_COMMITTED = "ReadCommitted"

@dataclass
class TableMetadata:
    """Lakehouse table metadata."""
    table_name: str
    format: TableFormat
    location: str
    schema: Dict[str, str]
    partitions: List[str]
    properties: Dict[str, Any]
    created_at: datetime
    last_modified: datetime
    num_rows: int
    size_bytes: int

@dataclass
class MergeOperation:
    """Upsert/merge operation definition."""
    source_df: DataFrame
    target_table: str
    merge_keys: List[str]
    update_condition: Optional[str] = None
    insert_condition: Optional[str] = None
    delete_condition: Optional[str] = None

class LakehouseIntegrator:
    """Integration with modern lakehouse table formats."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Initialize Spark with Delta Lake
        self.spark = (
            configure_spark_with_delta_pip(SparkSession.builder)
            .appName("LakehouseIntegrator")
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
            .config("spark.databricks.delta.retentionDurationCheck.enabled", "false")
            .config("spark.databricks.delta.properties.defaults.enableChangeDataFeed", "true")
            .getOrCreate()
        )

        # Initialize Iceberg catalog
        self.iceberg_catalog = load_catalog(
            "default",
            **config.get('iceberg_catalog_config', {})
        )

        # Storage configuration
        self.lakehouse_path = config.get('lakehouse_path', 's3://data-lake')

    def create_delta_table(self, table_name: str, df: DataFrame,
                          partition_by: Optional[List[str]] = None,
                          optimize_write: bool = True) -> None:
        """Create Delta Lake table with optimizations."""
        self.logger.info(f"Creating Delta table: {table_name}")

        table_path = f"{self.lakehouse_path}/delta/{table_name}"

        # Write with Delta format
        writer = df.write.format("delta").mode("overwrite")

        if partition_by:
            writer = writer.partitionBy(*partition_by)

        if optimize_write:
            # Enable optimized writes
            writer = writer.option("optimizeWrite", "true")
            writer = writer.option("autoOptimize.optimizeWrite", "true")

        writer.save(table_path)

        # Create Delta table object
        delta_table = DeltaTable.forPath(self.spark, table_path)

        # Enable features
        delta_table.toDF().write.format("delta") \
            .option("delta.enableChangeDataFeed", "true") \
            .option("delta.columnMapping.mode", "name") \
            .mode("overwrite") \
            .save(table_path)

        self.logger.info(
            f"Delta table created: {table_name} "
            f"(rows: {df.count()}, partitions: {partition_by})"
        )

    def merge_delta_table(self, operation: MergeOperation) -> Dict[str, int]:
        """Perform ACID merge (upsert) operation on Delta table."""
        table_path = f"{self.lakehouse_path}/delta/{operation.target_table}"
        delta_table = DeltaTable.forPath(self.spark, table_path)

        # Build merge condition
        merge_condition = " AND ".join([
            f"target.{key} = source.{key}"
            for key in operation.merge_keys
        ])

        self.logger.info(
            f"Merging into {operation.target_table} "
            f"(keys: {', '.join(operation.merge_keys)})"
        )

        # Execute merge
        merge_builder = delta_table.alias("target").merge(
            operation.source_df.alias("source"),
            merge_condition
        )

        # When matched: update
        if operation.update_condition:
            merge_builder = merge_builder.whenMatchedUpdate(
                condition=operation.update_condition,
                set={col: f"source.{col}" for col in operation.source_df.columns}
            )
        else:
            merge_builder = merge_builder.whenMatchedUpdateAll()

        # When matched and delete condition: delete
        if operation.delete_condition:
            merge_builder = merge_builder.whenMatchedDelete(
                condition=operation.delete_condition
            )

        # When not matched: insert
        if operation.insert_condition:
            merge_builder = merge_builder.whenNotMatchedInsert(
                condition=operation.insert_condition,
                values={col: f"source.{col}" for col in operation.source_df.columns}
            )
        else:
            merge_builder = merge_builder.whenNotMatchedInsertAll()

        # Execute and get metrics
        merge_result = merge_builder.execute()

        # Get operation metrics
        metrics = {
            'rows_inserted': 0,
            'rows_updated': 0,
            'rows_deleted': 0
        }

        self.logger.info(
            f"Merge completed: {metrics['rows_inserted']} inserted, "
            f"{metrics['rows_updated']} updated, {metrics['rows_deleted']} deleted"
        )

        return metrics

    def time_travel_query(self, table_name: str,
                         version: Optional[int] = None,
                         timestamp: Optional[datetime] = None) -> DataFrame:
        """Query historical version of Delta table."""
        table_path = f"{self.lakehouse_path}/delta/{table_name}"

        if version is not None:
            self.logger.info(f"Time travel to version {version}")
            df = self.spark.read.format("delta") \
                .option("versionAsOf", version) \
                .load(table_path)
        elif timestamp is not None:
            self.logger.info(f"Time travel to timestamp {timestamp}")
            df = self.spark.read.format("delta") \
                .option("timestampAsOf", timestamp.isoformat()) \
                .load(table_path)
        else:
            raise ValueError("Must specify either version or timestamp")

        return df

    def optimize_delta_table(self, table_name: str,
                            z_order_by: Optional[List[str]] = None) -> None:
        """Optimize Delta table with compaction and Z-ordering."""
        table_path = f"{self.lakehouse_path}/delta/{table_name}"
        delta_table = DeltaTable.forPath(self.spark, table_path)

        self.logger.info(f"Optimizing Delta table: {table_name}")

        # Compact small files
        if z_order_by:
            delta_table.optimize().executeZOrderBy(z_order_by)
            self.logger.info(f"Z-ordered by: {', '.join(z_order_by)}")
        else:
            delta_table.optimize().executeCompaction()
            self.logger.info("Compaction completed")

    def vacuum_delta_table(self, table_name: str,
                          retention_hours: int = 168) -> None:
        """Clean up old Delta table files."""
        table_path = f"{self.lakehouse_path}/delta/{table_name}"
        delta_table = DeltaTable.forPath(self.spark, table_path)

        self.logger.info(
            f"Vacuuming {table_name} "
            f"(retention: {retention_hours} hours)"
        )

        delta_table.vacuum(retention_hours)

        self.logger.info("Vacuum completed")

    def create_iceberg_table(self, table_name: str, df: DataFrame,
                            partition_spec: Optional[Dict[str, str]] = None) -> None:
        """Create Apache Iceberg table."""
        self.logger.info(f"Creating Iceberg table: {table_name}")

        # Convert Spark DataFrame to Iceberg table
        writer = df.write.format("iceberg").mode("overwrite")

        if partition_spec:
            # Example: {"date": "day", "region": "identity"}
            for col, transform in partition_spec.items():
                if transform == "identity":
                    writer = writer.partitionBy(col)
                elif transform == "day":
                    writer = writer.partitionBy(f"days({col})")
                elif transform == "month":
                    writer = writer.partitionBy(f"months({col})")
                elif transform == "year":
                    writer = writer.partitionBy(f"years({col})")
                elif transform.startswith("bucket"):
                    # bucket[N]
                    n = int(transform.split("[")[1].rstrip("]"))
                    writer = writer.partitionBy(f"bucket({n}, {col})")

        table_location = f"{self.lakehouse_path}/iceberg/{table_name}"
        writer.option("path", table_location).saveAsTable(table_name)

        self.logger.info(f"Iceberg table created: {table_name}")

    def iceberg_schema_evolution(self, table_name: str,
                                 add_columns: Optional[Dict[str, str]] = None,
                                 rename_columns: Optional[Dict[str, str]] = None,
                                 drop_columns: Optional[List[str]] = None) -> None:
        """Evolve Iceberg table schema without rewriting data."""
        iceberg_table = self.iceberg_catalog.load_table(table_name)

        with iceberg_table.update_schema() as update:
            # Add new columns
            if add_columns:
                for col_name, col_type in add_columns.items():
                    update.add_column(col_name, col_type)
                    self.logger.info(f"Added column: {col_name} ({col_type})")

            # Rename columns
            if rename_columns:
                for old_name, new_name in rename_columns.items():
                    update.rename_column(old_name, new_name)
                    self.logger.info(f"Renamed: {old_name} → {new_name}")

            # Drop columns
            if drop_columns:
                for col_name in drop_columns:
                    update.delete_column(col_name)
                    self.logger.info(f"Dropped column: {col_name}")

        self.logger.info(f"Schema evolution completed for {table_name}")

    def iceberg_snapshot_management(self, table_name: str,
                                   expire_older_than: Optional[datetime] = None,
                                   retain_last_n: int = 5) -> None:
        """Manage Iceberg table snapshots."""
        iceberg_table = self.iceberg_catalog.load_table(table_name)

        if expire_older_than:
            # Expire snapshots older than timestamp
            iceberg_table.expire_snapshots() \
                .expire_older_than(int(expire_older_than.timestamp() * 1000)) \
                .commit()

            self.logger.info(
                f"Expired snapshots older than {expire_older_than}"
            )

        # Keep only last N snapshots
        snapshots = list(iceberg_table.snapshots())
        if len(snapshots) > retain_last_n:
            cutoff_snapshot = snapshots[-retain_last_n]
            iceberg_table.expire_snapshots() \
                .retain_last(retain_last_n) \
                .commit()

            self.logger.info(f"Retained last {retain_last_n} snapshots")

    def compare_table_formats(self, table_name: str) -> Dict[str, Any]:
        """Compare Delta Lake vs Iceberg characteristics."""
        comparison = {
            'delta_lake': {
                'features': [
                    'ACID transactions',
                    'Time travel (90 days default)',
                    'Schema evolution (add/rename/drop columns)',
                    'MERGE/UPDATE/DELETE support',
                    'Change Data Feed (CDC)',
                    'Z-ordering for performance',
                    'Vacuum for cleanup'
                ],
                'best_for': [
                    'Databricks ecosystem',
                    'Frequent updates/deletes',
                    'Streaming writes',
                    'ML feature stores'
                ],
                'limitations': [
                    'Primarily Spark-based',
                    'Less multi-engine support'
                ]
            },
            'iceberg': {
                'features': [
                    'ACID transactions',
                    'Time travel (unlimited)',
                    'Hidden partitioning (automatic)',
                    'Schema evolution (add/rename/drop/reorder)',
                    'Partition evolution (change without rewrite)',
                    'Multi-engine support (Spark/Flink/Trino/Presto)',
                    'Snapshot management'
                ],
                'best_for': [
                    'Multi-engine environments',
                    'Petabyte-scale tables',
                    'Complex partition evolution',
                    'Open standard requirement'
                ],
                'limitations': [
                    'More complex setup',
                    'Ecosystem still maturing'
                ]
            }
        }

        return comparison

    def get_table_metadata(self, table_name: str,
                          format: TableFormat) -> TableMetadata:
        """Get comprehensive table metadata."""
        if format == TableFormat.DELTA_LAKE:
            table_path = f"{self.lakehouse_path}/delta/{table_name}"
            delta_table = DeltaTable.forPath(self.spark, table_path)

            history = delta_table.history(1).collect()[0]
            details = delta_table.detail().collect()[0]

            return TableMetadata(
                table_name=table_name,
                format=format,
                location=table_path,
                schema={f.name: f.dataType.simpleString() for f in delta_table.toDF().schema},
                partitions=details['partitionColumns'],
                properties=details['properties'],
                created_at=history['timestamp'],
                last_modified=history['timestamp'],
                num_rows=details['numFiles'],
                size_bytes=details['sizeInBytes']
            )

        elif format == TableFormat.APACHE_ICEBERG:
            iceberg_table = self.iceberg_catalog.load_table(table_name)

            return TableMetadata(
                table_name=table_name,
                format=format,
                location=iceberg_table.location(),
                schema={f.name: str(f.field_type) for f in iceberg_table.schema().fields},
                partitions=[str(f) for f in iceberg_table.spec().fields],
                properties=iceberg_table.properties(),
                created_at=datetime.now(),  # Would get from metadata
                last_modified=datetime.now(),  # Would get from metadata
                num_rows=0,  # Would calculate
                size_bytes=0  # Would calculate
            )

# Example usage
integrator = LakehouseIntegrator(config={
    'lakehouse_path': 's3://my-data-lake',
    'iceberg_catalog_config': {
        'type': 'glue',
        'warehouse': 's3://my-data-lake/iceberg'
    }
})

# Create Delta Lake table with optimizations
df = spark.read.parquet("s3://raw-data/orders/")
integrator.create_delta_table(
    table_name="orders",
    df=df,
    partition_by=["order_date", "region"],
    optimize_write=True
)

# Perform ACID merge (upsert)
updates_df = spark.read.json("s3://updates/orders/")
merge_op = MergeOperation(
    source_df=updates_df,
    target_table="orders",
    merge_keys=["order_id"],
    update_condition="source.updated_at > target.updated_at",
    delete_condition="source.status = 'deleted'"
)
metrics = integrator.merge_delta_table(merge_op)
print(f"Merged: {metrics}")

# Time travel query
yesterday = datetime.now() - timedelta(days=1)
historical_df = integrator.time_travel_query(
    table_name="orders",
    timestamp=yesterday
)

# Optimize with Z-ordering
integrator.optimize_delta_table(
    table_name="orders",
    z_order_by=["customer_id", "order_date"]
)

# Create Iceberg table with hidden partitioning
integrator.create_iceberg_table(
    table_name="events",
    df=events_df,
    partition_spec={
        "event_time": "day",
        "user_id": "bucket[100]"
    }
)

# Schema evolution without data rewrite
integrator.iceberg_schema_evolution(
    table_name="events",
    add_columns={"session_id": "string", "device_type": "string"},
    rename_columns={"user_id": "customer_id"}
)
\end{lstlisting}

\subsubsection{RealtimeAnalyzer: Druid and ClickHouse Integration}

\begin{lstlisting}[style=python, caption=RealtimeAnalyzer with Sub-Second Query Performance]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum
from datetime import datetime, timedelta
import logging
from pydruid.client import PyDruid
from pydruid.utils.aggregators import longsum, doublesum, count
from pydruid.utils.filters import Dimension, Filter
import clickhouse_connect
import pandas as pd
import json

class AnalyticsEngine(Enum):
    """Real-time analytics engines."""
    APACHE_DRUID = "druid"
    CLICKHOUSE = "clickhouse"
    PINOT = "pinot"

@dataclass
class QueryMetrics:
    """Query performance metrics."""
    query_id: str
    query_text: str
    execution_time_ms: float
    rows_scanned: int
    rows_returned: int
    bytes_scanned: int
    cache_hit: bool

@dataclass
class IngestionSpec:
    """Real-time ingestion specification."""
    datasource: str
    timestamp_column: str
    dimensions: List[str]
    metrics: List[Dict[str, Any]]
    rollup: bool = True
    segment_granularity: str = "DAY"
    query_granularity: str = "MINUTE"

class RealtimeAnalyzer:
    """Integration with real-time analytics engines."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Initialize Druid client
        if 'druid' in config:
            self.druid = PyDruid(
                url=config['druid']['broker_url'],
                endpoint=config['druid'].get('endpoint', 'druid/v2')
            )
            self.logger.info("Druid client initialized")

        # Initialize ClickHouse client
        if 'clickhouse' in config:
            self.clickhouse = clickhouse_connect.get_client(
                host=config['clickhouse']['host'],
                port=config['clickhouse'].get('port', 8123),
                username=config['clickhouse'].get('username', 'default'),
                password=config['clickhouse'].get('password', '')
            )
            self.logger.info("ClickHouse client initialized")

    def create_druid_datasource(self, spec: IngestionSpec,
                               kafka_topic: str) -> None:
        """Create Druid datasource with Kafka ingestion."""
        self.logger.info(f"Creating Druid datasource: {spec.datasource}")

        ingestion_spec = {
            "type": "kafka",
            "dataSchema": {
                "dataSource": spec.datasource,
                "timestampSpec": {
                    "column": spec.timestamp_column,
                    "format": "iso"
                },
                "dimensionsSpec": {
                    "dimensions": spec.dimensions
                },
                "metricsSpec": spec.metrics,
                "granularitySpec": {
                    "type": "uniform",
                    "segmentGranularity": spec.segment_granularity,
                    "queryGranularity": spec.query_granularity,
                    "rollup": spec.rollup
                }
            },
            "ioConfig": {
                "topic": kafka_topic,
                "consumerProperties": {
                    "bootstrap.servers": self.config['kafka_brokers']
                },
                "taskCount": 1,
                "replicas": 1,
                "taskDuration": "PT1H"
            },
            "tuningConfig": {
                "type": "kafka",
                "maxRowsPerSegment": 5000000
            }
        }

        # Submit ingestion spec to Druid supervisor API
        # (Would use Druid API client)
        self.logger.info(f"Druid datasource created: {spec.datasource}")

    def query_druid_timeseries(self, datasource: str,
                              intervals: List[str],
                              granularity: str = "minute",
                              aggregations: Optional[List[Dict]] = None,
                              filters: Optional[Filter] = None) -> pd.DataFrame:
        """Query Druid for time-series data."""
        self.logger.info(
            f"Querying Druid timeseries: {datasource} "
            f"(granularity: {granularity})"
        )

        query_start = datetime.now()

        # Build Druid query
        result = self.druid.timeseries(
            datasource=datasource,
            intervals=intervals,
            granularity=granularity,
            aggregations=aggregations or [count("count")],
            filter=filters
        )

        query_time = (datetime.now() - query_start).total_seconds() * 1000

        # Convert to DataFrame
        df = pd.DataFrame(result)

        self.logger.info(
            f"Query completed: {len(df)} rows, "
            f"{query_time:.2f}ms"
        )

        return df

    def query_druid_topn(self, datasource: str,
                        intervals: List[str],
                        dimension: str,
                        metric: str,
                        threshold: int = 10,
                        filters: Optional[Filter] = None) -> pd.DataFrame:
        """Query Druid for top-N analysis."""
        self.logger.info(
            f"Querying Druid top-N: dimension={dimension}, "
            f"metric={metric}, n={threshold}"
        )

        result = self.druid.topn(
            datasource=datasource,
            intervals=intervals,
            granularity="all",
            dimension=dimension,
            metric=metric,
            threshold=threshold,
            filter=filters,
            aggregations=[longsum(metric)]
        )

        df = pd.DataFrame(result)

        self.logger.info(f"Top-N query completed: {len(df)} results")

        return df

    def create_clickhouse_table(self, table_name: str,
                               schema: Dict[str, str],
                               engine: str = "MergeTree",
                               partition_by: Optional[str] = None,
                               order_by: List[str] = None,
                               sample_by: Optional[str] = None) -> None:
        """Create ClickHouse table optimized for analytics."""
        self.logger.info(f"Creating ClickHouse table: {table_name}")

        # Build column definitions
        columns = [f"{name} {dtype}" for name, dtype in schema.items()]
        columns_sql = ",\n    ".join(columns)

        # Build engine clause
        engine_clause = f"ENGINE = {engine}()"

        if partition_by:
            engine_clause += f"\nPARTITION BY {partition_by}"

        if order_by:
            engine_clause += f"\nORDER BY ({', '.join(order_by)})"

        if sample_by:
            engine_clause += f"\nSAMPLE BY {sample_by}"

        # Create table
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {columns_sql}
        )
        {engine_clause}
        """

        self.clickhouse.command(create_sql)

        self.logger.info(
            f"ClickHouse table created: {table_name} "
            f"(engine: {engine}, order_by: {order_by})"
        )

    def query_clickhouse(self, query: str,
                        parameters: Optional[Dict] = None) -> pd.DataFrame:
        """Execute ClickHouse query with performance tracking."""
        self.logger.info(f"Executing ClickHouse query: {query[:100]}...")

        query_start = datetime.now()

        # Execute query
        result = self.clickhouse.query_df(query, parameters=parameters)

        query_time = (datetime.now() - query_start).total_seconds() * 1000

        # Get query stats
        stats = self.clickhouse.command("SELECT * FROM system.query_log ORDER BY event_time DESC LIMIT 1")

        self.logger.info(
            f"Query completed: {len(result)} rows, "
            f"{query_time:.2f}ms"
        )

        return result

    def create_materialized_view(self, view_name: str,
                                 source_table: str,
                                 aggregation_query: str,
                                 refresh_interval: Optional[int] = None) -> None:
        """Create materialized view for pre-aggregation."""
        self.logger.info(f"Creating materialized view: {view_name}")

        # ClickHouse materialized view (auto-updating)
        create_sql = f"""
        CREATE MATERIALIZED VIEW IF NOT EXISTS {view_name}
        ENGINE = SummingMergeTree()
        ORDER BY (date, dimension)
        AS {aggregation_query}
        """

        self.clickhouse.command(create_sql)

        self.logger.info(
            f"Materialized view created: {view_name} "
            f"(source: {source_table})"
        )

    def optimize_clickhouse_table(self, table_name: str) -> None:
        """Optimize ClickHouse table (merge parts)."""
        self.logger.info(f"Optimizing ClickHouse table: {table_name}")

        self.clickhouse.command(f"OPTIMIZE TABLE {table_name} FINAL")

        self.logger.info("Optimization completed")

    def compare_analytics_engines(self) -> Dict[str, Any]:
        """Compare real-time analytics engines."""
        comparison = {
            'apache_druid': {
                'strengths': [
                    'Sub-second queries on trillion-row tables',
                    'Native time-series support',
                    'Automatic data rollup',
                    'Real-time ingestion from Kafka',
                    'Approximate algorithms (HyperLogLog, quantiles)',
                    'Multi-tenancy support'
                ],
                'use_cases': [
                    'User analytics and clickstreams',
                    'Application performance monitoring (APM)',
                    'Network telemetry',
                    'Digital advertising analytics',
                    'IoT sensor data'
                ],
                'query_performance': {
                    'aggregation': '<100ms for billions of rows',
                    'top_n': '<50ms',
                    'time_series': '<100ms'
                },
                'limitations': [
                    'Limited JOIN support',
                    'High memory requirements',
                    'Complex setup and tuning'
                ]
            },
            'clickhouse': {
                'strengths': [
                    'Fastest columnar database (10-100x faster than MySQL)',
                    'Excellent compression (3-10x)',
                    'Full SQL support including JOINs',
                    'Materialized views for pre-aggregation',
                    'Vector query execution',
                    'Easy deployment (single binary)'
                ],
                'use_cases': [
                    'Log analytics and observability',
                    'E-commerce analytics',
                    'Financial analytics',
                    'Ad-tech bidding and attribution',
                    'Data warehousing'
                ],
                'query_performance': {
                    'aggregation': '<50ms for billions of rows',
                    'filtering': '<10ms with proper indexing',
                    'joins': '<500ms for moderate sizes'
                },
                'limitations': [
                    'No transactions (eventual consistency)',
                    'Updates/deletes are expensive',
                    'Requires careful schema design'
                ]
            }
        }

        return comparison

    def benchmark_query(self, engine: AnalyticsEngine,
                       query: str, iterations: int = 5) -> QueryMetrics:
        """Benchmark query performance."""
        self.logger.info(
            f"Benchmarking query on {engine.value} "
            f"({iterations} iterations)"
        )

        execution_times = []

        for i in range(iterations):
            start = datetime.now()

            if engine == AnalyticsEngine.CLICKHOUSE:
                result = self.clickhouse.query_df(query)
            elif engine == AnalyticsEngine.APACHE_DRUID:
                # Would execute Druid query
                pass

            execution_time = (datetime.now() - start).total_seconds() * 1000
            execution_times.append(execution_time)

        avg_time = sum(execution_times) / len(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)

        self.logger.info(
            f"Benchmark results: avg={avg_time:.2f}ms, "
            f"min={min_time:.2f}ms, max={max_time:.2f}ms"
        )

        return QueryMetrics(
            query_id=f"bench_{int(datetime.now().timestamp())}",
            query_text=query,
            execution_time_ms=avg_time,
            rows_scanned=0,  # Would get from stats
            rows_returned=len(result) if 'result' in locals() else 0,
            bytes_scanned=0,  # Would get from stats
            cache_hit=False
        )

# Example usage
analyzer = RealtimeAnalyzer(config={
    'druid': {
        'broker_url': 'http://localhost:8082'
    },
    'clickhouse': {
        'host': 'localhost',
        'port': 8123
    },
    'kafka_brokers': 'localhost:9092'
})

# Create Druid datasource for real-time events
spec = IngestionSpec(
    datasource="user_events",
    timestamp_column="event_time",
    dimensions=["user_id", "event_type", "page_url", "device"],
    metrics=[
        {"type": "count", "name": "count"},
        {"type": "longSum", "name": "duration", "fieldName": "duration_ms"}
    ],
    rollup=True,
    segment_granularity="HOUR",
    query_granularity="MINUTE"
)
analyzer.create_druid_datasource(spec, kafka_topic="events")

# Query Druid for real-time analytics
df = analyzer.query_druid_timeseries(
    datasource="user_events",
    intervals=["2024-01-01/2024-01-02"],
    granularity="minute",
    aggregations=[count("events"), longsum("total_duration", "duration")]
)

# Create ClickHouse table for log analytics
analyzer.create_clickhouse_table(
    table_name="application_logs",
    schema={
        "timestamp": "DateTime",
        "level": "LowCardinality(String)",
        "service": "LowCardinality(String)",
        "message": "String",
        "trace_id": "String",
        "duration_ms": "UInt32"
    },
    engine="MergeTree",
    partition_by="toYYYYMM(timestamp)",
    order_by=["timestamp", "service", "level"]
)

# Query ClickHouse for fast aggregations
query = """
SELECT
    toStartOfHour(timestamp) as hour,
    service,
    level,
    count() as log_count,
    avg(duration_ms) as avg_duration,
    quantile(0.95)(duration_ms) as p95_duration
FROM application_logs
WHERE timestamp >= now() - INTERVAL 1 DAY
GROUP BY hour, service, level
ORDER BY hour DESC, log_count DESC
"""

result_df = analyzer.query_clickhouse(query)
print(result_df)

# Create materialized view for pre-aggregation
analyzer.create_materialized_view(
    view_name="hourly_service_metrics",
    source_table="application_logs",
    aggregation_query="""
        SELECT
            toStartOfHour(timestamp) as hour,
            service,
            count() as request_count,
            avg(duration_ms) as avg_latency,
            quantile(0.99)(duration_ms) as p99_latency
        FROM application_logs
        GROUP BY hour, service
    """
)
\end{lstlisting}

\subsubsection{MetadataCatalog: Automated Discovery and Search}

\begin{lstlisting}[style=python, caption=MetadataCatalog with Automated Metadata Discovery]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Set
from enum import Enum
from datetime import datetime, timedelta
import logging
from elasticsearch import Elasticsearch
import sqlalchemy
from sqlalchemy import inspect
import re

class DataClassification(Enum):
    """Data sensitivity classification."""
    PUBLIC = "public"
    INTERNAL = "internal"
    CONFIDENTIAL = "confidential"
    PII = "pii"  # Personally Identifiable Information
    PHI = "phi"  # Protected Health Information

class AssetType(Enum):
    """Types of data assets."""
    TABLE = "table"
    VIEW = "view"
    DASHBOARD = "dashboard"
    REPORT = "report"
    MODEL = "model"
    PIPELINE = "pipeline"

@dataclass
class ColumnMetadata:
    """Column-level metadata."""
    column_name: str
    data_type: str
    description: Optional[str] = None
    is_nullable: bool = True
    is_primary_key: bool = False
    is_foreign_key: bool = False
    classification: DataClassification = DataClassification.INTERNAL
    sample_values: List[str] = field(default_factory=list)
    statistics: Dict[str, Any] = field(default_factory=dict)  # min, max, avg, distinct_count
    tags: List[str] = field(default_factory=list)

@dataclass
class TableMetadata:
    """Table-level metadata."""
    database: str
    schema: str
    table_name: str
    asset_type: AssetType
    description: Optional[str] = None
    columns: List[ColumnMetadata] = field(default_factory=list)
    row_count: int = 0
    size_bytes: int = 0
    owner: Optional[str] = None
    created_at: Optional[datetime] = None
    last_modified: Optional[datetime] = None
    last_accessed: Optional[datetime] = None
    tags: List[str] = field(default_factory=list)
    quality_score: float = 0.0  # 0-100
    popularity_score: float = 0.0  # Based on usage
    is_deprecated: bool = False
    deprecation_message: Optional[str] = None

@dataclass
class UsageStats:
    """Usage statistics for a data asset."""
    asset_identifier: str
    query_count_7d: int
    query_count_30d: int
    unique_users_7d: int
    unique_users_30d: int
    top_users: List[str]
    top_queries: List[str]
    last_queried: Optional[datetime]

class MetadataCatalog:
    """Automated metadata catalog with discovery and search."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Initialize Elasticsearch for search
        self.es = Elasticsearch(
            hosts=config.get('elasticsearch_hosts', ['localhost:9200'])
        )

        # Database connections for metadata extraction
        self.db_connections: Dict[str, sqlalchemy.Engine] = {}

        # PII detection patterns
        self.pii_patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
            'phone': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
            'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
        }

        # PII column name patterns
        self.pii_column_names = [
            'email', 'ssn', 'social_security', 'phone', 'mobile',
            'credit_card', 'password', 'dob', 'birth_date',
            'address', 'ip_address', 'passport'
        ]

        # Create Elasticsearch index
        self._create_search_index()

    def _create_search_index(self) -> None:
        """Create Elasticsearch index for metadata search."""
        index_name = 'data_catalog'

        mapping = {
            "mappings": {
                "properties": {
                    "database": {"type": "keyword"},
                    "schema": {"type": "keyword"},
                    "table_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                    "asset_type": {"type": "keyword"},
                    "description": {"type": "text"},
                    "columns": {
                        "type": "nested",
                        "properties": {
                            "column_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                            "data_type": {"type": "keyword"},
                            "description": {"type": "text"},
                            "classification": {"type": "keyword"},
                            "tags": {"type": "keyword"}
                        }
                    },
                    "owner": {"type": "keyword"},
                    "tags": {"type": "keyword"},
                    "quality_score": {"type": "float"},
                    "popularity_score": {"type": "float"},
                    "is_deprecated": {"type": "boolean"},
                    "created_at": {"type": "date"},
                    "last_modified": {"type": "date"},
                    "last_accessed": {"type": "date"}
                }
            }
        }

        if not self.es.indices.exists(index=index_name):
            self.es.indices.create(index=index_name, body=mapping)
            self.logger.info(f"Created Elasticsearch index: {index_name}")

    def discover_metadata(self, database_name: str,
                         connection_string: str,
                         schemas: Optional[List[str]] = None) -> List[TableMetadata]:
        """Automatically discover metadata from database."""
        self.logger.info(f"Discovering metadata from database: {database_name}")

        # Connect to database
        engine = sqlalchemy.create_engine(connection_string)
        self.db_connections[database_name] = engine

        inspector = inspect(engine)
        discovered_tables = []

        # Get all schemas
        if not schemas:
            schemas = inspector.get_schema_names()

        for schema in schemas:
            if schema in ['information_schema', 'pg_catalog', 'sys']:
                continue  # Skip system schemas

            # Get all tables in schema
            table_names = inspector.get_table_names(schema=schema)

            for table_name in table_names:
                self.logger.info(f"Discovering: {database_name}.{schema}.{table_name}")

                # Get column metadata
                columns_meta = []
                db_columns = inspector.get_columns(table_name, schema=schema)

                # Get primary keys
                pk_constraint = inspector.get_pk_constraint(table_name, schema=schema)
                pk_columns = set(pk_constraint.get('constrained_columns', []))

                # Get foreign keys
                fk_constraints = inspector.get_foreign_keys(table_name, schema=schema)
                fk_columns = set()
                for fk in fk_constraints:
                    fk_columns.update(fk.get('constrained_columns', []))

                for col in db_columns:
                    # Classify column
                    classification = self._classify_column(col['name'], str(col['type']))

                    # Get sample values and statistics
                    sample_values, stats = self._get_column_stats(
                        engine, schema, table_name, col['name']
                    )

                    column_meta = ColumnMetadata(
                        column_name=col['name'],
                        data_type=str(col['type']),
                        is_nullable=col.get('nullable', True),
                        is_primary_key=col['name'] in pk_columns,
                        is_foreign_key=col['name'] in fk_columns,
                        classification=classification,
                        sample_values=sample_values,
                        statistics=stats
                    )

                    columns_meta.append(column_meta)

                # Get table statistics
                row_count, size_bytes = self._get_table_stats(engine, schema, table_name)

                # Create table metadata
                table_meta = TableMetadata(
                    database=database_name,
                    schema=schema,
                    table_name=table_name,
                    asset_type=AssetType.TABLE,
                    columns=columns_meta,
                    row_count=row_count,
                    size_bytes=size_bytes,
                    created_at=datetime.now(),
                    last_modified=datetime.now()
                )

                discovered_tables.append(table_meta)

                # Index in Elasticsearch
                self._index_table(table_meta)

        self.logger.info(
            f"Discovered {len(discovered_tables)} tables from {database_name}"
        )

        return discovered_tables

    def _classify_column(self, column_name: str, data_type: str) -> DataClassification:
        """Automatically classify column based on name and type."""
        column_lower = column_name.lower()

        # Check for PII column names
        for pii_name in self.pii_column_names:
            if pii_name in column_lower:
                return DataClassification.PII

        # Check for sensitive keywords
        sensitive_keywords = ['password', 'secret', 'token', 'key', 'credential']
        if any(kw in column_lower for kw in sensitive_keywords):
            return DataClassification.CONFIDENTIAL

        return DataClassification.INTERNAL

    def _get_column_stats(self, engine, schema: str, table: str,
                         column: str) -> tuple:
        """Get column sample values and statistics."""
        try:
            # Get sample values (top 5)
            sample_query = f"""
            SELECT DISTINCT {column}
            FROM {schema}.{table}
            WHERE {column} IS NOT NULL
            LIMIT 5
            """

            with engine.connect() as conn:
                result = conn.execute(sqlalchemy.text(sample_query))
                sample_values = [str(row[0]) for row in result]

            # Get statistics
            stats_query = f"""
            SELECT
                COUNT(DISTINCT {column}) as distinct_count,
                COUNT(*) as total_count
            FROM {schema}.{table}
            """

            with engine.connect() as conn:
                result = conn.execute(sqlalchemy.text(stats_query))
                row = result.fetchone()
                stats = {
                    'distinct_count': row[0],
                    'total_count': row[1],
                    'null_count': row[1] - row[0]
                }

            return sample_values, stats

        except Exception as e:
            self.logger.warning(f"Failed to get stats for {column}: {str(e)}")
            return [], {}

    def _get_table_stats(self, engine, schema: str, table: str) -> tuple:
        """Get table row count and size."""
        try:
            # Get row count
            count_query = f"SELECT COUNT(*) FROM {schema}.{table}"

            with engine.connect() as conn:
                result = conn.execute(sqlalchemy.text(count_query))
                row_count = result.fetchone()[0]

            # Would get size from system tables
            size_bytes = 0

            return row_count, size_bytes

        except Exception as e:
            self.logger.warning(f"Failed to get table stats: {str(e)}")
            return 0, 0

    def _index_table(self, table_meta: TableMetadata) -> None:
        """Index table metadata in Elasticsearch."""
        doc_id = f"{table_meta.database}.{table_meta.schema}.{table_meta.table_name}"

        doc = {
            "database": table_meta.database,
            "schema": table_meta.schema,
            "table_name": table_meta.table_name,
            "asset_type": table_meta.asset_type.value,
            "description": table_meta.description,
            "columns": [
                {
                    "column_name": col.column_name,
                    "data_type": col.data_type,
                    "description": col.description,
                    "classification": col.classification.value,
                    "tags": col.tags
                }
                for col in table_meta.columns
            ],
            "owner": table_meta.owner,
            "tags": table_meta.tags,
            "quality_score": table_meta.quality_score,
            "popularity_score": table_meta.popularity_score,
            "is_deprecated": table_meta.is_deprecated,
            "created_at": table_meta.created_at.isoformat() if table_meta.created_at else None,
            "last_modified": table_meta.last_modified.isoformat() if table_meta.last_modified else None,
            "last_accessed": table_meta.last_accessed.isoformat() if table_meta.last_accessed else None
        }

        self.es.index(index='data_catalog', id=doc_id, body=doc)

    def search_tables(self, query: str,
                     filters: Optional[Dict[str, Any]] = None,
                     limit: int = 10) -> List[Dict]:
        """Search for tables using natural language query."""
        self.logger.info(f"Searching for: {query}")

        # Build Elasticsearch query
        must_clauses = [
            {
                "multi_match": {
                    "query": query,
                    "fields": [
                        "table_name^3",  # Boost table name
                        "description^2",  # Boost description
                        "columns.column_name^2",
                        "columns.description",
                        "tags"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            }
        ]

        # Add filters
        filter_clauses = []
        if filters:
            if 'database' in filters:
                filter_clauses.append({"term": {"database": filters['database']}})
            if 'schema' in filters:
                filter_clauses.append({"term": {"schema": filters['schema']}})
            if 'asset_type' in filters:
                filter_clauses.append({"term": {"asset_type": filters['asset_type']}})
            if 'tags' in filters:
                filter_clauses.append({"terms": {"tags": filters['tags']}})
            if 'classification' in filters:
                filter_clauses.append({"term": {"columns.classification": filters['classification']}})
            if 'exclude_deprecated' in filters and filters['exclude_deprecated']:
                filter_clauses.append({"term": {"is_deprecated": False}})

        # Build final query
        es_query = {
            "query": {
                "bool": {
                    "must": must_clauses,
                    "filter": filter_clauses
                }
            },
            "size": limit,
            "sort": [
                {"popularity_score": {"order": "desc"}},
                {"_score": {"order": "desc"}}
            ]
        }

        # Execute search
        response = self.es.search(index='data_catalog', body=es_query)

        results = []
        for hit in response['hits']['hits']:
            result = hit['_source']
            result['relevance_score'] = hit['_score']
            results.append(result)

        self.logger.info(f"Found {len(results)} results")

        return results

    def find_pii_columns(self, database: Optional[str] = None) -> List[Dict]:
        """Find all columns classified as PII."""
        filters = {"classification": "pii"}
        if database:
            filters["database"] = database

        es_query = {
            "query": {
                "nested": {
                    "path": "columns",
                    "query": {
                        "term": {"columns.classification": "pii"}
                    }
                }
            },
            "size": 1000
        }

        response = self.es.search(index='data_catalog', body=es_query)

        pii_columns = []
        for hit in response['hits']['hits']:
            table = hit['_source']
            for col in table['columns']:
                if col['classification'] == 'pii':
                    pii_columns.append({
                        'database': table['database'],
                        'schema': table['schema'],
                        'table': table['table_name'],
                        'column': col['column_name'],
                        'data_type': col['data_type']
                    })

        self.logger.info(f"Found {len(pii_columns)} PII columns")

        return pii_columns

    def mark_deprecated(self, database: str, schema: str, table: str,
                       message: str) -> None:
        """Mark a table as deprecated with migration message."""
        doc_id = f"{database}.{schema}.{table}"

        self.es.update(
            index='data_catalog',
            id=doc_id,
            body={
                "doc": {
                    "is_deprecated": True,
                    "deprecation_message": message
                }
            }
        )

        self.logger.info(f"Marked {doc_id} as deprecated")

    def update_usage_stats(self, asset_identifier: str,
                          stats: UsageStats) -> None:
        """Update usage statistics for an asset."""
        # Calculate popularity score based on usage
        popularity = min(100.0, (
            stats.query_count_7d * 2 +
            stats.unique_users_7d * 10
        ) / 10)

        self.es.update(
            index='data_catalog',
            id=asset_identifier,
            body={
                "doc": {
                    "popularity_score": popularity,
                    "last_accessed": stats.last_queried.isoformat() if stats.last_queried else None
                }
            }
        )

# Example usage
catalog = MetadataCatalog(config={
    'elasticsearch_hosts': ['localhost:9200']
})

# Discover metadata from PostgreSQL database
tables = catalog.discover_metadata(
    database_name="production",
    connection_string="postgresql://user:pass@localhost/prod",
    schemas=["public", "analytics"]
)

print(f"Discovered {len(tables)} tables")

# Search for tables
results = catalog.search_tables(
    query="active customers",
    filters={
        'exclude_deprecated': True,
        'database': 'production'
    },
    limit=10
)

for result in results:
    print(f"{result['database']}.{result['schema']}.{result['table_name']}")
    print(f"  Relevance: {result['relevance_score']:.2f}")
    print(f"  Popularity: {result['popularity_score']:.2f}")

# Find all PII columns for GDPR compliance
pii_columns = catalog.find_pii_columns(database="production")
print(f"\nFound {len(pii_columns)} PII columns:")
for col in pii_columns[:5]:
    print(f"  {col['database']}.{col['schema']}.{col['table']}.{col['column']}")

# Mark deprecated table
catalog.mark_deprecated(
    database="production",
    schema="public",
    table="customers_v2",
    message="DEPRECATED: Use 'active_customers' table instead. This table includes test accounts and will be removed on 2024-12-31."
)
\end{lstlisting}

\subsubsection{LineageTracker: Column-Level Dependency Analysis}

\begin{lstlisting}[style=python, caption=LineageTracker with Column-Level Dependencies]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Set
from enum import Enum
from datetime import datetime
import logging
import networkx as nx
from sqlparse import parse as sql_parse
from sqlparse.sql import IdentifierList, Identifier, Where
from sqlparse.tokens import Keyword, DML
import re

class LineageType(Enum):
    """Types of lineage relationships."""
    DIRECT = "direct"  # Direct data flow
    DERIVED = "derived"  # Transformation applied
    COPY = "copy"  # Exact copy
    AGGREGATED = "aggregated"  # Aggregation applied
    FILTERED = "filtered"  # Filter applied

@dataclass
class LineageNode:
    """Node in the lineage graph."""
    identifier: str  # database.schema.table.column
    node_type: str  # "column", "table", "view", "dashboard"
    database: str
    schema: str
    table: str
    column: Optional[str] = None
    asset_type: str = "table"

@dataclass
class LineageEdge:
    """Edge in the lineage graph."""
    source: LineageNode
    target: LineageNode
    lineage_type: LineageType
    transformation: Optional[str] = None  # SQL expression
    created_at: datetime = field(default_factory=datetime.now)

@dataclass
class ImpactAnalysis:
    """Impact analysis result."""
    affected_tables: List[str]
    affected_columns: List[str]
    affected_dashboards: List[str]
    affected_reports: List[str]
    affected_pipelines: List[str]
    total_downstream_assets: int

class LineageTracker:
    """Track data lineage with column-level dependency analysis."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Lineage graph (directed acyclic graph)
        self.lineage_graph = nx.DiGraph()

        # Column-level lineage mapping
        self.column_lineage: Dict[str, Set[str]] = {}

    def extract_lineage_from_sql(self, sql: str,
                                 target_table: str) -> List[LineageEdge]:
        """Extract lineage from SQL query."""
        self.logger.info(f"Extracting lineage from SQL for: {target_table}")

        edges = []

        # Parse SQL
        statements = sql_parse(sql)

        for statement in statements:
            # Get query type
            query_type = statement.get_type()

            if query_type == 'SELECT':
                edges.extend(self._extract_select_lineage(statement, target_table))
            elif query_type == 'INSERT':
                edges.extend(self._extract_insert_lineage(statement))
            elif query_type == 'CREATE':
                if 'VIEW' in str(statement).upper():
                    edges.extend(self._extract_view_lineage(statement))

        self.logger.info(f"Extracted {len(edges)} lineage edges")

        return edges

    def _extract_select_lineage(self, statement, target_table: str) -> List[LineageEdge]:
        """Extract lineage from SELECT statement."""
        edges = []

        # Extract SELECT clause columns
        select_columns = self._extract_select_columns(statement)

        # Extract FROM clause tables
        from_tables = self._extract_from_tables(statement)

        # Extract JOIN tables
        join_tables = self._extract_join_tables(statement)

        all_source_tables = from_tables + join_tables

        # Create lineage edges
        for source_table in all_source_tables:
            # Parse table identifier
            db, schema, table = self._parse_table_identifier(source_table)

            source_node = LineageNode(
                identifier=source_table,
                node_type="table",
                database=db,
                schema=schema,
                table=table
            )

            target_node = LineageNode(
                identifier=target_table,
                node_type="table",
                database="",  # Would extract
                schema="",
                table=target_table
            )

            # Determine lineage type
            lineage_type = LineageType.DERIVED
            if 'GROUP BY' in str(statement).upper():
                lineage_type = LineageType.AGGREGATED
            elif 'WHERE' in str(statement).upper():
                lineage_type = LineageType.FILTERED

            edge = LineageEdge(
                source=source_node,
                target=target_node,
                lineage_type=lineage_type,
                transformation=str(statement)[:200]  # First 200 chars
            )

            edges.append(edge)

        return edges

    def _extract_insert_lineage(self, statement) -> List[LineageEdge]:
        """Extract lineage from INSERT statement."""
        edges = []

        # Extract target table from INSERT INTO
        target_match = re.search(r'INSERT\s+INTO\s+([^\s(]+)', str(statement), re.IGNORECASE)
        if not target_match:
            return edges

        target_table = target_match.group(1)

        # Extract source tables from SELECT portion
        select_match = re.search(r'SELECT.*FROM\s+([^\s,;]+)', str(statement), re.IGNORECASE | re.DOTALL)
        if select_match:
            source_table = select_match.group(1)

            source_node = LineageNode(
                identifier=source_table,
                node_type="table",
                database="",
                schema="",
                table=source_table
            )

            target_node = LineageNode(
                identifier=target_table,
                node_type="table",
                database="",
                schema="",
                table=target_table
            )

            edge = LineageEdge(
                source=source_node,
                target=target_node,
                lineage_type=LineageType.COPY,
                transformation=str(statement)[:200]
            )

            edges.append(edge)

        return edges

    def _extract_view_lineage(self, statement) -> List[LineageEdge]:
        """Extract lineage from CREATE VIEW statement."""
        edges = []

        # Extract view name
        view_match = re.search(r'CREATE\s+(?:OR\s+REPLACE\s+)?VIEW\s+([^\s(]+)', str(statement), re.IGNORECASE)
        if not view_match:
            return edges

        view_name = view_match.group(1)

        # Extract source tables from SELECT
        from_tables = self._extract_from_tables(statement)

        for source_table in from_tables:
            source_node = LineageNode(
                identifier=source_table,
                node_type="table",
                database="",
                schema="",
                table=source_table
            )

            target_node = LineageNode(
                identifier=view_name,
                node_type="view",
                database="",
                schema="",
                table=view_name
            )

            edge = LineageEdge(
                source=source_node,
                target=target_node,
                lineage_type=LineageType.DERIVED,
                transformation=str(statement)[:200]
            )

            edges.append(edge)

        return edges

    def _extract_select_columns(self, statement) -> List[str]:
        """Extract column names from SELECT clause."""
        columns = []

        for token in statement.tokens:
            if isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    columns.append(str(identifier))
            elif isinstance(token, Identifier):
                columns.append(str(token))

        return columns

    def _extract_from_tables(self, statement) -> List[str]:
        """Extract table names from FROM clause."""
        tables = []

        from_seen = False
        for token in statement.tokens:
            if from_seen:
                if token.ttype is Keyword:
                    break
                if isinstance(token, IdentifierList):
                    for identifier in token.get_identifiers():
                        tables.append(str(identifier).strip())
                elif isinstance(token, Identifier):
                    tables.append(str(token).strip())

            if token.ttype is Keyword and token.value.upper() == 'FROM':
                from_seen = True

        return tables

    def _extract_join_tables(self, statement) -> List[str]:
        """Extract table names from JOIN clauses."""
        tables = []

        sql_str = str(statement)
        join_matches = re.findall(
            r'JOIN\s+([^\s,]+)',
            sql_str,
            re.IGNORECASE
        )

        tables.extend([m.strip() for m in join_matches])

        return tables

    def _parse_table_identifier(self, identifier: str) -> tuple:
        """Parse table identifier into database, schema, table."""
        parts = identifier.split('.')

        if len(parts) == 3:
            return parts[0], parts[1], parts[2]
        elif len(parts) == 2:
            return "", parts[0], parts[1]
        else:
            return "", "", parts[0]

    def add_lineage_edge(self, edge: LineageEdge) -> None:
        """Add lineage edge to graph."""
        source_id = edge.source.identifier
        target_id = edge.target.identifier

        # Add nodes
        self.lineage_graph.add_node(
            source_id,
            node_type=edge.source.node_type,
            database=edge.source.database,
            schema=edge.source.schema,
            table=edge.source.table,
            column=edge.source.column
        )

        self.lineage_graph.add_node(
            target_id,
            node_type=edge.target.node_type,
            database=edge.target.database,
            schema=edge.target.schema,
            table=edge.target.table,
            column=edge.target.column
        )

        # Add edge
        self.lineage_graph.add_edge(
            source_id,
            target_id,
            lineage_type=edge.lineage_type.value,
            transformation=edge.transformation,
            created_at=edge.created_at.isoformat()
        )

        self.logger.info(f"Added lineage: {source_id} → {target_id}")

    def get_upstream_lineage(self, asset_identifier: str,
                            depth: int = 5) -> Dict[str, Any]:
        """Get upstream lineage (sources) for an asset."""
        if asset_identifier not in self.lineage_graph:
            return {"nodes": [], "edges": []}

        # Get upstream nodes (predecessors)
        upstream_nodes = set()
        current_level = {asset_identifier}

        for _ in range(depth):
            next_level = set()
            for node in current_level:
                predecessors = self.lineage_graph.predecessors(node)
                next_level.update(predecessors)
                upstream_nodes.update(predecessors)
            current_level = next_level

            if not current_level:
                break

        # Build subgraph
        upstream_nodes.add(asset_identifier)
        subgraph = self.lineage_graph.subgraph(upstream_nodes)

        return self._graph_to_dict(subgraph)

    def get_downstream_lineage(self, asset_identifier: str,
                              depth: int = 5) -> Dict[str, Any]:
        """Get downstream lineage (consumers) for an asset."""
        if asset_identifier not in self.lineage_graph:
            return {"nodes": [], "edges": []}

        # Get downstream nodes (successors)
        downstream_nodes = set()
        current_level = {asset_identifier}

        for _ in range(depth):
            next_level = set()
            for node in current_level:
                successors = self.lineage_graph.successors(node)
                next_level.update(successors)
                downstream_nodes.update(successors)
            current_level = next_level

            if not current_level:
                break

        # Build subgraph
        downstream_nodes.add(asset_identifier)
        subgraph = self.lineage_graph.subgraph(downstream_nodes)

        return self._graph_to_dict(subgraph)

    def impact_analysis(self, asset_identifier: str) -> ImpactAnalysis:
        """Analyze impact of changes to an asset."""
        self.logger.info(f"Running impact analysis for: {asset_identifier}")

        # Get all downstream assets
        downstream = self.get_downstream_lineage(asset_identifier, depth=10)

        affected_tables = set()
        affected_columns = set()
        affected_dashboards = set()
        affected_reports = set()
        affected_pipelines = set()

        for node in downstream['nodes']:
            node_type = node.get('node_type', 'table')
            identifier = node['id']

            if node_type == 'table':
                affected_tables.add(identifier)
            elif node_type == 'column':
                affected_columns.add(identifier)
            elif node_type == 'dashboard':
                affected_dashboards.add(identifier)
            elif node_type == 'report':
                affected_reports.add(identifier)
            elif node_type == 'pipeline':
                affected_pipelines.add(identifier)

        analysis = ImpactAnalysis(
            affected_tables=list(affected_tables),
            affected_columns=list(affected_columns),
            affected_dashboards=list(affected_dashboards),
            affected_reports=list(affected_reports),
            affected_pipelines=list(affected_pipelines),
            total_downstream_assets=len(downstream['nodes']) - 1  # Exclude source
        )

        self.logger.info(
            f"Impact: {analysis.total_downstream_assets} assets, "
            f"{len(affected_tables)} tables, "
            f"{len(affected_dashboards)} dashboards"
        )

        return analysis

    def find_root_causes(self, asset_identifier: str) -> List[str]:
        """Find root cause data sources for an asset."""
        upstream = self.get_upstream_lineage(asset_identifier, depth=10)

        # Find nodes with no predecessors (root sources)
        root_sources = []
        for node in upstream['nodes']:
            node_id = node['id']
            if self.lineage_graph.in_degree(node_id) == 0:
                root_sources.append(node_id)

        self.logger.info(f"Found {len(root_sources)} root sources")

        return root_sources

    def _graph_to_dict(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """Convert NetworkX graph to dictionary."""
        nodes = []
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            nodes.append({
                'id': node_id,
                **node_data
            })

        edges = []
        for source, target in graph.edges():
            edge_data = graph.edges[source, target]
            edges.append({
                'source': source,
                'target': target,
                **edge_data
            })

        return {
            'nodes': nodes,
            'edges': edges
        }

    def visualize_lineage(self, asset_identifier: str,
                         direction: str = "both") -> str:
        """Generate lineage visualization as DOT format."""
        if direction == "upstream":
            lineage = self.get_upstream_lineage(asset_identifier)
        elif direction == "downstream":
            lineage = self.get_downstream_lineage(asset_identifier)
        else:  # both
            upstream = self.get_upstream_lineage(asset_identifier)
            downstream = self.get_downstream_lineage(asset_identifier)

            # Merge
            all_nodes = upstream['nodes'] + downstream['nodes']
            all_edges = upstream['edges'] + downstream['edges']
            lineage = {'nodes': all_nodes, 'edges': all_edges}

        # Generate DOT
        dot = "digraph lineage {\n"
        dot += "  rankdir=LR;\n"
        dot += "  node [shape=box];\n\n"

        # Add nodes
        for node in lineage['nodes']:
            node_id = node['id'].replace('.', '_')
            label = node['id']
            dot += f'  {node_id} [label="{label}"];\n'

        # Add edges
        for edge in lineage['edges']:
            source = edge['source'].replace('.', '_')
            target = edge['target'].replace('.', '_')
            lineage_type = edge.get('lineage_type', 'direct')
            dot += f'  {source} -> {target} [label="{lineage_type}"];\n'

        dot += "}\n"

        return dot

# Example usage
tracker = LineageTracker(config={})

# Extract lineage from SQL
sql = """
CREATE VIEW analytics.active_customers AS
SELECT
    c.customer_id,
    c.email,
    c.created_at,
    COUNT(o.order_id) as order_count,
    SUM(o.amount) as total_spent
FROM production.public.customers c
LEFT JOIN production.public.orders o ON c.customer_id = o.customer_id
WHERE c.status = 'active'
  AND o.order_date >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY c.customer_id, c.email, c.created_at
"""

edges = tracker.extract_lineage_from_sql(sql, "analytics.active_customers")

for edge in edges:
    tracker.add_lineage_edge(edge)

# Get downstream impact
impact = tracker.impact_analysis("production.public.customers")
print(f"Changing 'customers' table affects:")
print(f"  - {len(impact.affected_tables)} tables")
print(f"  - {len(impact.affected_dashboards)} dashboards")
print(f"  - Total: {impact.total_downstream_assets} assets")

# Find root sources
roots = tracker.find_root_causes("analytics.active_customers")
print(f"\nRoot data sources for 'active_customers': {roots}")

# Generate visualization
dot = tracker.visualize_lineage("analytics.active_customers", direction="both")
print(f"\nLineage visualization:\n{dot}")
\end{lstlisting}

\textbf{Modern Data Stack Integration Patterns:}

\begin{itemize}
    \item \textbf{Lakehouse Architecture}:
    \begin{itemize}
        \item Combine data lake flexibility with data warehouse performance
        \item Use Delta Lake for Databricks, Iceberg for multi-engine (Spark/Flink/Trino)
        \item ACID transactions enable reliable updates/deletes on data lakes
        \item Time travel allows auditing and rollback (90 days Delta, unlimited Iceberg)
        \item Schema evolution without downtime or data rewrites
    \end{itemize}

    \item \textbf{Real-Time Analytics}:
    \begin{itemize}
        \item Druid for sub-second queries on trillion-row time-series data
        \item ClickHouse for 10-100x faster OLAP than traditional databases
        \item Pre-aggregation via materialized views (ClickHouse) or rollup (Druid)
        \item Kafka ingestion for real-time data streams
        \item Use for: user analytics, APM, log analytics, ad-tech
    \end{itemize}

    \item \textbf{Metadata Catalog}:
    \begin{itemize}
        \item Automated discovery from databases, warehouses, lakes
        \item Elasticsearch-powered search (fuzzy matching, relevance ranking)
        \item PII auto-detection for GDPR compliance
        \item Usage-based popularity scoring
        \item Deprecation workflows with migration messages
    \end{itemize}

    \item \textbf{Data Lineage}:
    \begin{itemize}
        \item SQL parsing for automatic lineage extraction
        \item Column-level dependency tracking
        \item Impact analysis: what breaks if I change this table?
        \item Root cause analysis: where does this data come from?
        \item Graph-based visualization (DOT format for Graphviz)
    \end{itemize}
\end{itemize}

\subsection{Advanced Pipeline Patterns and Data Virtualization}

Modern data platforms require sophisticated patterns for real-time synchronization, cross-platform queries, ML feature serving, and data versioning. These advanced patterns enable organizations to handle complex multi-cloud architectures, maintain low-latency ML inference, and ensure data consistency across distributed systems.

\subsubsection{The Multi-Cloud Migration: Seamless Data Movement}

\textbf{The Company:} Financial services firm with \$500B in assets under management

\textbf{The Challenge:} Regulatory requirements mandated migration from AWS to multi-cloud (AWS + Azure + GCP) for data residency and disaster recovery.

\textbf{The Problem:} Legacy architecture with tightly-coupled systems:

\begin{itemize}
    \item \textbf{300+ microservices} directly querying AWS RDS PostgreSQL
    \item \textbf{Real-time risk calculations} requiring <100ms latency
    \item \textbf{15TB trading data} across 450 tables
    \item \textbf{Zero downtime requirement}: Trading platform runs 24/7
    \item \textbf{Compliance}: GDPR (EU), CCPA (California), MAS (Singapore) data residency
\end{itemize}

\textbf{The Incident (September 2024):}

\begin{itemize}
    \item \textbf{Month 1}: "Big bang" migration planned (2-week maintenance window)
    \item \textbf{Month 2}: Stakeholders reject 2-week downtime (\$120M lost revenue)
    \item \textbf{Month 3}: Attempt phased migration without CDC
    \begin{itemize}
        \item Batch replication (1-hour lag)
        \item Risk calculations using stale data
        \item \$2.4M trading loss from outdated positions
        \item Regulatory breach (positions not updated in real-time)
    \end{itemize}
    \item \textbf{Month 4}: Emergency halt to migration, \$8M spent, 0\% complete
    \item \textbf{Month 5}: Architecture review mandates new approach
\end{itemize}

\textbf{The Solution:} Multi-layered architecture with:

\begin{enumerate}
    \item \textbf{Change Data Capture (CDC)}: Real-time synchronization with <1s lag
    \begin{itemize}
        \item Debezium streaming from PostgreSQL WAL to Kafka
        \item Dual-write to AWS RDS + Azure PostgreSQL + GCP CloudSQL
        \item Automatic conflict resolution with vector clocks
    \end{itemize}

    \item \textbf{Data Virtualization}: Federated query layer
    \begin{itemize}
        \item Trino (formerly Presto) for cross-cloud queries
        \item Query pushdown optimization (filter/aggregate at source)
        \item Smart routing based on data residency rules
    \end{itemize}

    \item \textbf{Feature Store}: Unified ML serving
    \begin{itemize}
        \item Offline: S3/Azure Blob/GCS for training (batch)
        \item Online: Redis/DynamoDB/Firestore for inference (<10ms)
        \item Automatic sync between offline and online stores
    \end{itemize}

    \item \textbf{Data Versioning}: Semantic versioning for schemas
    \begin{itemize}
        \item Breaking changes: major version (v2.0.0)
        \item Backward-compatible additions: minor version (v1.1.0)
        \item Bug fixes: patch version (v1.0.1)
        \item Automatic rollback on validation failures
    \end{itemize}
\end{enumerate}

\textbf{Results:}

\begin{itemize}
    \item \textbf{Migration completed}: 18 months, zero downtime
    \item \textbf{Data latency}: <500ms cross-cloud replication (vs 1-hour batch)
    \item \textbf{Cost savings}: \$12M/year from cloud arbitrage
    \item \textbf{Disaster recovery}: <5 min RTO (vs 4 hours previously)
    \item \textbf{Compliance}: Full data residency compliance across 3 regions
    \item \textbf{Query performance}: 40\% faster with query pushdown optimization
\end{itemize}

\subsubsection{CDCProcessor: Real-Time Change Data Capture}

\begin{lstlisting}[style=python, caption=CDCProcessor with Change Stream Handling]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Callable
from enum import Enum
from datetime import datetime, timedelta
import logging
from kafka import KafkaConsumer, KafkaProducer
import json
from collections import defaultdict
import threading
import time

class ChangeType(Enum):
    """Types of database changes."""
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    SNAPSHOT = "snapshot"  # Initial load

class ConflictResolution(Enum):
    """Conflict resolution strategies."""
    LAST_WRITE_WINS = "last_write_wins"
    FIRST_WRITE_WINS = "first_write_wins"
    CUSTOM = "custom"
    MANUAL = "manual"

@dataclass
class ChangeEvent:
    """Database change event."""
    event_id: str
    change_type: ChangeType
    source_database: str
    source_table: str
    primary_key: Dict[str, Any]
    before_data: Optional[Dict[str, Any]] = None  # For UPDATE/DELETE
    after_data: Optional[Dict[str, Any]] = None   # For INSERT/UPDATE
    timestamp: datetime = field(default_factory=datetime.now)
    transaction_id: Optional[str] = None
    lsn: Optional[int] = None  # Log Sequence Number (PostgreSQL)
    scn: Optional[int] = None  # System Change Number (Oracle)

@dataclass
class ReplicationStats:
    """CDC replication statistics."""
    events_processed: int = 0
    events_failed: int = 0
    inserts: int = 0
    updates: int = 0
    deletes: int = 0
    conflicts_detected: int = 0
    conflicts_resolved: int = 0
    avg_latency_ms: float = 0.0
    current_lag_seconds: float = 0.0

class CDCProcessor:
    """Real-time Change Data Capture processor."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Kafka configuration
        self.consumer = KafkaConsumer(
            *config['cdc_topics'],
            bootstrap_servers=config['kafka_brokers'],
            group_id=config['consumer_group'],
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

        self.producer = KafkaProducer(
            bootstrap_servers=config['kafka_brokers'],
            value_serializer=lambda m: json.dumps(m).encode('utf-8')
        )

        # Change handlers
        self.change_handlers: Dict[str, List[Callable]] = defaultdict(list)

        # Conflict resolution
        self.conflict_strategy = ConflictResolution(
            config.get('conflict_resolution', 'last_write_wins')
        )

        # Statistics
        self.stats = ReplicationStats()
        self.stats_lock = threading.Lock()

        # Vector clocks for conflict detection
        self.vector_clocks: Dict[str, Dict[str, int]] = {}

        # Running state
        self.running = False

    def register_change_handler(self, table_name: str,
                                handler: Callable[[ChangeEvent], None]) -> None:
        """Register a handler for changes to a specific table."""
        self.change_handlers[table_name].append(handler)
        self.logger.info(f"Registered change handler for table: {table_name}")

    def start(self) -> None:
        """Start CDC processing."""
        self.running = True
        self.logger.info("CDC processor starting...")

        try:
            while self.running:
                # Poll for change events
                messages = self.consumer.poll(timeout_ms=1000, max_records=500)

                if not messages:
                    continue

                # Process change events
                for topic_partition, records in messages.items():
                    for record in records:
                        self._process_change_event(record.value)

                # Commit offsets
                self.consumer.commit()

                # Update lag statistics
                self._update_lag_stats()

        except Exception as e:
            self.logger.error(f"CDC processing failed: {str(e)}", exc_info=True)
            raise
        finally:
            self.stop()

    def _process_change_event(self, event_data: Dict) -> None:
        """Process a single change event."""
        start_time = time.time()

        try:
            # Parse event (Debezium format)
            change_event = self._parse_debezium_event(event_data)

            # Detect conflicts using vector clocks
            conflict = self._detect_conflict(change_event)

            if conflict:
                with self.stats_lock:
                    self.stats.conflicts_detected += 1

                # Resolve conflict
                if not self._resolve_conflict(change_event, conflict):
                    self.logger.warning(
                        f"Failed to resolve conflict for {change_event.source_table}:"
                        f"{change_event.primary_key}"
                    )
                    return

                with self.stats_lock:
                    self.stats.conflicts_resolved += 1

            # Apply change to target databases
            self._apply_change(change_event)

            # Update vector clock
            self._update_vector_clock(change_event)

            # Call registered handlers
            table_name = f"{change_event.source_database}.{change_event.source_table}"
            for handler in self.change_handlers.get(table_name, []):
                try:
                    handler(change_event)
                except Exception as e:
                    self.logger.error(
                        f"Change handler failed for {table_name}: {str(e)}"
                    )

            # Update statistics
            with self.stats_lock:
                self.stats.events_processed += 1
                if change_event.change_type == ChangeType.INSERT:
                    self.stats.inserts += 1
                elif change_event.change_type == ChangeType.UPDATE:
                    self.stats.updates += 1
                elif change_event.change_type == ChangeType.DELETE:
                    self.stats.deletes += 1

                # Update latency
                latency_ms = (time.time() - start_time) * 1000
                self.stats.avg_latency_ms = (
                    (self.stats.avg_latency_ms * (self.stats.events_processed - 1) +
                     latency_ms) / self.stats.events_processed
                )

        except Exception as e:
            with self.stats_lock:
                self.stats.events_failed += 1
            self.logger.error(f"Failed to process change event: {str(e)}")

    def _parse_debezium_event(self, event_data: Dict) -> ChangeEvent:
        """Parse Debezium change event format."""
        payload = event_data.get('payload', {})

        # Determine change type
        op = payload.get('op')
        change_type_map = {
            'c': ChangeType.INSERT,  # Create
            'u': ChangeType.UPDATE,  # Update
            'd': ChangeType.DELETE,  # Delete
            'r': ChangeType.SNAPSHOT  # Read (snapshot)
        }
        change_type = change_type_map.get(op, ChangeType.INSERT)

        # Extract source information
        source = payload.get('source', {})
        source_db = source.get('db')
        source_table = source.get('table')

        # Extract data
        before_data = payload.get('before')
        after_data = payload.get('after')

        # Extract primary key
        primary_key = {}
        if after_data:
            # Would extract from schema or configuration
            primary_key = {'id': after_data.get('id')}

        # Extract timestamp
        ts_ms = payload.get('ts_ms', int(time.time() * 1000))
        timestamp = datetime.fromtimestamp(ts_ms / 1000)

        return ChangeEvent(
            event_id=f"{source_db}.{source_table}.{ts_ms}",
            change_type=change_type,
            source_database=source_db,
            source_table=source_table,
            primary_key=primary_key,
            before_data=before_data,
            after_data=after_data,
            timestamp=timestamp,
            transaction_id=source.get('txId'),
            lsn=source.get('lsn')
        )

    def _detect_conflict(self, event: ChangeEvent) -> Optional[Dict]:
        """Detect conflicts using vector clocks."""
        record_key = f"{event.source_database}.{event.source_table}." \
                    f"{json.dumps(event.primary_key, sort_keys=True)}"

        if record_key not in self.vector_clocks:
            return None

        local_clock = self.vector_clocks[record_key]
        remote_clock = event.after_data.get('__vector_clock', {}) if event.after_data else {}

        # Check for concurrent modifications
        is_concurrent = False
        for source in set(local_clock.keys()) | set(remote_clock.keys()):
            local_version = local_clock.get(source, 0)
            remote_version = remote_clock.get(source, 0)

            if local_version > 0 and remote_version > 0:
                if local_version != remote_version:
                    is_concurrent = True
                    break

        if is_concurrent:
            return {
                'record_key': record_key,
                'local_clock': local_clock,
                'remote_clock': remote_clock,
                'local_timestamp': self.vector_clocks.get(f"{record_key}_ts"),
                'remote_timestamp': event.timestamp
            }

        return None

    def _resolve_conflict(self, event: ChangeEvent, conflict: Dict) -> bool:
        """Resolve conflict based on configured strategy."""
        if self.conflict_strategy == ConflictResolution.LAST_WRITE_WINS:
            # Compare timestamps
            local_ts = conflict['local_timestamp']
            remote_ts = conflict['remote_timestamp']

            if remote_ts > local_ts:
                # Remote wins, apply change
                self.logger.info(
                    f"Conflict resolved (LWW): Remote wins for {conflict['record_key']}"
                )
                return True
            else:
                # Local wins, ignore change
                self.logger.info(
                    f"Conflict resolved (LWW): Local wins for {conflict['record_key']}"
                )
                return False

        elif self.conflict_strategy == ConflictResolution.FIRST_WRITE_WINS:
            # Local always wins
            self.logger.info(
                f"Conflict resolved (FWW): Local wins for {conflict['record_key']}"
            )
            return False

        elif self.conflict_strategy == ConflictResolution.MANUAL:
            # Queue for manual resolution
            self.producer.send(
                'cdc_conflicts',
                value={
                    'event': event.__dict__,
                    'conflict': conflict,
                    'timestamp': datetime.now().isoformat()
                }
            )
            return False

        return True

    def _apply_change(self, event: ChangeEvent) -> None:
        """Apply change to target databases."""
        # Would apply to actual target databases
        # This is a simplified example

        target_databases = self.config.get('target_databases', [])

        for target_db in target_databases:
            try:
                if event.change_type == ChangeType.INSERT:
                    self._apply_insert(target_db, event)
                elif event.change_type == ChangeType.UPDATE:
                    self._apply_update(target_db, event)
                elif event.change_type == ChangeType.DELETE:
                    self._apply_delete(target_db, event)

                self.logger.debug(
                    f"Applied {event.change_type.value} to {target_db}:"
                    f"{event.source_table}"
                )

            except Exception as e:
                self.logger.error(
                    f"Failed to apply change to {target_db}: {str(e)}"
                )

    def _apply_insert(self, target_db: str, event: ChangeEvent) -> None:
        """Apply INSERT to target database."""
        # Would execute INSERT statement
        pass

    def _apply_update(self, target_db: str, event: ChangeEvent) -> None:
        """Apply UPDATE to target database."""
        # Would execute UPDATE statement
        pass

    def _apply_delete(self, target_db: str, event: ChangeEvent) -> None:
        """Apply DELETE to target database."""
        # Would execute DELETE statement
        pass

    def _update_vector_clock(self, event: ChangeEvent) -> None:
        """Update vector clock for a record."""
        record_key = f"{event.source_database}.{event.source_table}." \
                    f"{json.dumps(event.primary_key, sort_keys=True)}"

        if record_key not in self.vector_clocks:
            self.vector_clocks[record_key] = {}

        # Increment version for this source
        source_id = event.source_database
        self.vector_clocks[record_key][source_id] = \
            self.vector_clocks[record_key].get(source_id, 0) + 1

        # Store timestamp
        self.vector_clocks[f"{record_key}_ts"] = event.timestamp

    def _update_lag_stats(self) -> None:
        """Update replication lag statistics."""
        # Get current offset lag from Kafka
        # This is simplified - would use Kafka admin API
        pass

    def get_stats(self) -> ReplicationStats:
        """Get current replication statistics."""
        with self.stats_lock:
            return ReplicationStats(
                events_processed=self.stats.events_processed,
                events_failed=self.stats.events_failed,
                inserts=self.stats.inserts,
                updates=self.stats.updates,
                deletes=self.stats.deletes,
                conflicts_detected=self.stats.conflicts_detected,
                conflicts_resolved=self.stats.conflicts_resolved,
                avg_latency_ms=self.stats.avg_latency_ms,
                current_lag_seconds=self.stats.current_lag_seconds
            )

    def stop(self) -> None:
        """Stop CDC processing."""
        self.running = False
        self.consumer.close()
        self.producer.close()
        self.logger.info("CDC processor stopped")

# Example usage
cdc = CDCProcessor(config={
    'kafka_brokers': ['localhost:9092'],
    'cdc_topics': ['dbserver1.inventory.customers'],
    'consumer_group': 'cdc_replicator',
    'target_databases': ['aws_rds', 'azure_postgres', 'gcp_cloudsql'],
    'conflict_resolution': 'last_write_wins'
})

# Register custom change handler
def handle_customer_change(event: ChangeEvent):
    print(f"Customer changed: {event.primary_key}")
    if event.change_type == ChangeType.UPDATE:
        print(f"Before: {event.before_data}")
        print(f"After: {event.after_data}")

cdc.register_change_handler('inventory.customers', handle_customer_change)

# Start CDC processing
cdc.start()

# Monitor statistics
stats = cdc.get_stats()
print(f"Events processed: {stats.events_processed}")
print(f"Average latency: {stats.avg_latency_ms:.2f}ms")
print(f"Conflicts resolved: {stats.conflicts_resolved}")
\end{lstlisting}

\subsubsection{DataVirtualizer: Federated Query Processing}

\begin{lstlisting}[style=python, caption=DataVirtualizer with Federated Queries]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Set
from enum import Enum
from datetime import datetime
import logging
from trino.dbapi import connect as trino_connect
from trino.auth import BasicAuthentication
import sqlparse
from sqlparse.sql import IdentifierList, Identifier, Where
from sqlparse.tokens import Keyword
import re

class DataSource(Enum):
    """Supported data sources."""
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"
    SNOWFLAKE = "snowflake"
    BIGQUERY = "bigquery"
    REDSHIFT = "redshift"
    S3 = "s3"
    AZURE_BLOB = "azure_blob"
    GCS = "gcs"

class OptimizationStrategy(Enum):
    """Query optimization strategies."""
    PUSHDOWN = "pushdown"  # Push filters/aggregations to source
    BROADCAST = "broadcast"  # Broadcast small table to all nodes
    SHUFFLE = "shuffle"  # Shuffle-based join
    DYNAMIC = "dynamic"  # Dynamic optimization based on stats

@dataclass
class DataSourceConfig:
    """Configuration for a data source."""
    source_id: str
    source_type: DataSource
    connection_string: str
    catalog: str
    schema: str
    cost_per_gb: float = 0.0  # For cost-based optimization
    latency_ms: float = 0.0  # Average query latency
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class QueryPlan:
    """Federated query execution plan."""
    plan_id: str
    original_query: str
    optimized_query: str
    data_sources: List[str]
    estimated_cost: float
    estimated_rows: int
    pushdown_operations: List[str]
    join_strategy: str
    execution_stages: List[Dict[str, Any]]

@dataclass
class QueryResult:
    """Federated query result."""
    query_id: str
    rows: List[Dict[str, Any]]
    row_count: int
    columns: List[str]
    execution_time_ms: float
    data_scanned_bytes: int
    sources_queried: List[str]

class DataVirtualizer:
    """Federated query processing across multiple data sources."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Trino connection (federated query engine)
        self.trino_conn = trino_connect(
            host=config.get('trino_host', 'localhost'),
            port=config.get('trino_port', 8080),
            user=config.get('trino_user', 'admin'),
            catalog=config.get('trino_catalog', 'system'),
            schema=config.get('trino_schema', 'runtime'),
            auth=BasicAuthentication(
                config.get('trino_user', 'admin'),
                config.get('trino_password', '')
            ) if config.get('trino_password') else None
        )

        # Registered data sources
        self.data_sources: Dict[str, DataSourceConfig] = {}

        # Query cache
        self.query_cache: Dict[str, QueryResult] = {}
        self.cache_ttl_seconds = config.get('cache_ttl', 300)  # 5 min

        # Data residency rules (for compliance)
        self.residency_rules: Dict[str, List[str]] = {}  # table -> allowed regions

    def register_data_source(self, source: DataSourceConfig) -> None:
        """Register a new data source."""
        self.data_sources[source.source_id] = source
        self.logger.info(
            f"Registered data source: {source.source_id} "
            f"({source.source_type.value})"
        )

    def set_residency_rule(self, table_pattern: str,
                          allowed_regions: List[str]) -> None:
        """Set data residency rule for compliance."""
        self.residency_rules[table_pattern] = allowed_regions
        self.logger.info(
            f"Set residency rule: {table_pattern} → {allowed_regions}"
        )

    def execute_query(self, query: str,
                     parameters: Optional[Dict] = None,
                     use_cache: bool = True) -> QueryResult:
        """Execute federated query across data sources."""
        self.logger.info(f"Executing federated query: {query[:100]}...")

        # Check cache
        cache_key = self._compute_cache_key(query, parameters)
        if use_cache and cache_key in self.query_cache:
            cached_result = self.query_cache[cache_key]
            age = (datetime.now() - cached_result.timestamp).total_seconds()

            if age < self.cache_ttl_seconds:
                self.logger.info("Query result returned from cache")
                return cached_result

        # Validate data residency compliance
        self._validate_residency(query)

        # Generate query plan
        plan = self._generate_query_plan(query)

        self.logger.info(
            f"Query plan: {len(plan.execution_stages)} stages, "
            f"sources: {', '.join(plan.data_sources)}"
        )

        # Execute query
        start_time = time.time()

        cursor = self.trino_conn.cursor()
        cursor.execute(plan.optimized_query, parameters or {})

        # Fetch results
        columns = [desc[0] for desc in cursor.description]
        rows = []
        for row in cursor.fetchall():
            rows.append(dict(zip(columns, row)))

        execution_time = (time.time() - start_time) * 1000

        # Get query stats from Trino
        stats = self._get_query_stats(cursor)

        result = QueryResult(
            query_id=f"query_{int(time.time())}",
            rows=rows,
            row_count=len(rows),
            columns=columns,
            execution_time_ms=execution_time,
            data_scanned_bytes=stats.get('data_scanned_bytes', 0),
            sources_queried=plan.data_sources
        )

        # Cache result
        if use_cache:
            result.timestamp = datetime.now()
            self.query_cache[cache_key] = result

        self.logger.info(
            f"Query completed: {len(rows)} rows, "
            f"{execution_time:.2f}ms, "
            f"{stats.get('data_scanned_bytes', 0) / 1024 / 1024:.2f}MB scanned"
        )

        return result

    def _generate_query_plan(self, query: str) -> QueryPlan:
        """Generate optimized query execution plan."""
        # Parse query
        statements = sqlparse.parse(query)
        statement = statements[0]

        # Extract tables
        tables = self._extract_tables(statement)

        # Identify data sources for each table
        data_sources = set()
        for table in tables:
            source = self._resolve_data_source(table)
            if source:
                data_sources.add(source)

        # Optimize query with pushdown
        optimized_query, pushdown_ops = self._optimize_with_pushdown(query, statement)

        # Determine join strategy
        join_strategy = self._determine_join_strategy(tables)

        # Build execution stages
        execution_stages = self._build_execution_stages(
            statement, tables, join_strategy
        )

        # Estimate cost
        estimated_cost = self._estimate_query_cost(tables, statement)

        return QueryPlan(
            plan_id=f"plan_{int(time.time())}",
            original_query=query,
            optimized_query=optimized_query,
            data_sources=list(data_sources),
            estimated_cost=estimated_cost,
            estimated_rows=0,  # Would estimate from statistics
            pushdown_operations=pushdown_ops,
            join_strategy=join_strategy,
            execution_stages=execution_stages
        )

    def _optimize_with_pushdown(self, query: str, statement) -> tuple:
        """Optimize query with filter and aggregation pushdown."""
        pushdown_operations = []

        # Extract WHERE clause for filter pushdown
        where_clause = None
        for token in statement.tokens:
            if isinstance(token, Where):
                where_clause = str(token)
                pushdown_operations.append(f"Filter pushdown: {where_clause}")

        # Extract GROUP BY for aggregation pushdown
        if 'GROUP BY' in query.upper():
            pushdown_operations.append("Aggregation pushdown")

        # Return optimized query (same as original in this simplified version)
        return query, pushdown_operations

    def _determine_join_strategy(self, tables: List[str]) -> str:
        """Determine optimal join strategy."""
        if len(tables) <= 1:
            return "none"

        # Get table sizes
        table_sizes = {}
        for table in tables:
            source = self._resolve_data_source(table)
            if source:
                # Would get actual table size from statistics
                table_sizes[table] = 1000000  # Placeholder

        # If one table is significantly smaller, use broadcast join
        if table_sizes:
            min_size = min(table_sizes.values())
            max_size = max(table_sizes.values())

            if max_size / min_size > 100:
                return "broadcast"

        return "shuffle"

    def _build_execution_stages(self, statement, tables: List[str],
                               join_strategy: str) -> List[Dict[str, Any]]:
        """Build execution stages for query plan."""
        stages = []

        # Stage 1: Scan tables with filter pushdown
        for table in tables:
            stages.append({
                'stage_id': len(stages) + 1,
                'operation': 'scan',
                'table': table,
                'source': self._resolve_data_source(table),
                'pushdown': 'filter + projection'
            })

        # Stage 2: Join (if multiple tables)
        if len(tables) > 1:
            stages.append({
                'stage_id': len(stages) + 1,
                'operation': 'join',
                'strategy': join_strategy,
                'tables': tables
            })

        # Stage 3: Aggregation (if GROUP BY present)
        if 'GROUP BY' in str(statement).upper():
            stages.append({
                'stage_id': len(stages) + 1,
                'operation': 'aggregate',
                'pushdown': True
            })

        # Stage 4: Final projection
        stages.append({
            'stage_id': len(stages) + 1,
            'operation': 'project',
            'columns': self._extract_select_columns(statement)
        })

        return stages

    def _estimate_query_cost(self, tables: List[str], statement) -> float:
        """Estimate query execution cost."""
        total_cost = 0.0

        for table in tables:
            source_id = self._resolve_data_source(table)
            if source_id and source_id in self.data_sources:
                source = self.data_sources[source_id]

                # Estimate data scanned (would use actual statistics)
                estimated_gb = 1.0  # Placeholder

                # Add cost
                total_cost += estimated_gb * source.cost_per_gb

        return total_cost

    def _validate_residency(self, query: str) -> None:
        """Validate query complies with data residency rules."""
        # Extract tables from query
        statements = sqlparse.parse(query)
        tables = self._extract_tables(statements[0])

        for table in tables:
            # Check if table has residency rules
            for pattern, allowed_regions in self.residency_rules.items():
                if re.match(pattern, table):
                    # Get source region
                    source_id = self._resolve_data_source(table)
                    if source_id and source_id in self.data_sources:
                        source_region = self.data_sources[source_id].metadata.get('region')

                        if source_region not in allowed_regions:
                            raise ValueError(
                                f"Data residency violation: {table} cannot be accessed "
                                f"from region {source_region}. Allowed: {allowed_regions}"
                            )

    def _resolve_data_source(self, table: str) -> Optional[str]:
        """Resolve which data source contains a table."""
        # Parse table name (catalog.schema.table)
        parts = table.split('.')

        if len(parts) >= 2:
            catalog = parts[0]

            # Find matching data source
            for source_id, source in self.data_sources.items():
                if source.catalog == catalog:
                    return source_id

        return None

    def _extract_tables(self, statement) -> List[str]:
        """Extract table names from SQL statement."""
        tables = []

        from_seen = False
        for token in statement.tokens:
            if from_seen:
                if token.ttype is Keyword:
                    break
                if isinstance(token, IdentifierList):
                    for identifier in token.get_identifiers():
                        tables.append(str(identifier).strip())
                elif isinstance(token, Identifier):
                    tables.append(str(token).strip())

            if token.ttype is Keyword and token.value.upper() == 'FROM':
                from_seen = True

        return tables

    def _extract_select_columns(self, statement) -> List[str]:
        """Extract columns from SELECT clause."""
        columns = []

        for token in statement.tokens:
            if isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    columns.append(str(identifier))
            elif isinstance(token, Identifier):
                columns.append(str(token))

        return columns

    def _get_query_stats(self, cursor) -> Dict[str, Any]:
        """Get query statistics from Trino."""
        # Would fetch from Trino system tables
        return {
            'data_scanned_bytes': 0,
            'cpu_time_ms': 0,
            'wall_time_ms': 0
        }

    def _compute_cache_key(self, query: str,
                          parameters: Optional[Dict]) -> str:
        """Compute cache key for query."""
        import hashlib
        key_data = f"{query}:{json.dumps(parameters or {}, sort_keys=True)}"
        return hashlib.sha256(key_data.encode()).hexdigest()

# Example usage
virtualizer = DataVirtualizer(config={
    'trino_host': 'localhost',
    'trino_port': 8080,
    'trino_user': 'admin'
})

# Register data sources
virtualizer.register_data_source(DataSourceConfig(
    source_id='aws_rds',
    source_type=DataSource.POSTGRESQL,
    connection_string='jdbc:postgresql://aws-rds.us-east-1.amazonaws.com/prod',
    catalog='postgresql',
    schema='public',
    cost_per_gb=0.10,
    latency_ms=50,
    metadata={'region': 'us-east-1', 'cloud': 'aws'}
))

virtualizer.register_data_source(DataSourceConfig(
    source_id='azure_postgres',
    source_type=DataSource.POSTGRESQL,
    connection_string='jdbc:postgresql://azure-postgres.westeurope.azure.com/prod',
    catalog='postgresql_azure',
    schema='public',
    cost_per_gb=0.12,
    latency_ms=80,
    metadata={'region': 'eu-west-1', 'cloud': 'azure'}
))

# Set data residency rules for GDPR
virtualizer.set_residency_rule(
    table_pattern='.*\.eu_customers',
    allowed_regions=['eu-west-1', 'eu-central-1']
)

# Execute federated query across clouds
query = """
SELECT
    c.customer_id,
    c.email,
    COUNT(o.order_id) as order_count,
    SUM(o.amount) as total_spent
FROM postgresql.public.customers c
LEFT JOIN postgresql_azure.public.orders o
    ON c.customer_id = o.customer_id
WHERE c.region = 'EU'
  AND o.order_date >= CURRENT_DATE - INTERVAL '90' DAY
GROUP BY c.customer_id, c.email
ORDER BY total_spent DESC
LIMIT 100
"""

result = virtualizer.execute_query(query)

print(f"Federated query results:")
print(f"  Rows: {result.row_count}")
print(f"  Execution time: {result.execution_time_ms:.2f}ms")
print(f"  Data scanned: {result.data_scanned_bytes / 1024 / 1024:.2f}MB")
print(f"  Sources: {', '.join(result.sources_queried)}")
\end{lstlisting}

\subsubsection{FeatureStoreIntegrator: Online and Offline Serving}

\begin{lstlisting}[style=python, caption=FeatureStoreIntegrator with Dual Serving Modes]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum
from datetime import datetime, timedelta
import logging
import redis
import boto3
from google.cloud import firestore
import pandas as pd
import pyarrow.parquet as pq
import json
import hashlib

class ServingMode(Enum):
    """Feature serving modes."""
    ONLINE = "online"  # Low-latency inference (<10ms)
    OFFLINE = "offline"  # Batch training
    BOTH = "both"  # Sync between online and offline

class FeatureType(Enum):
    """Feature data types."""
    CATEGORICAL = "categorical"
    NUMERICAL = "numerical"
    EMBEDDING = "embedding"
    TIMESTAMP = "timestamp"

@dataclass
class FeatureDefinition:
    """Feature definition."""
    feature_name: str
    feature_type: FeatureType
    description: str
    entity_type: str  # "user", "item", "session", etc.
    online_enabled: bool = True
    offline_enabled: bool = True
    ttl_seconds: Optional[int] = None  # Time-to-live for online features
    version: str = "1.0.0"

@dataclass
class FeatureValue:
    """Feature value with metadata."""
    feature_name: str
    value: Any
    timestamp: datetime
    entity_id: str
    version: str

class FeatureStoreIntegrator:
    """Feature store with online and offline serving."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Online store (Redis for low-latency)
        self.redis_client = redis.Redis(
            host=config.get('redis_host', 'localhost'),
            port=config.get('redis_port', 6379),
            db=config.get('redis_db', 0),
            decode_responses=True
        )

        # Offline store (S3/Parquet for training)
        self.s3_client = boto3.client('s3')
        self.offline_bucket = config.get('offline_bucket', 'feature-store')
        self.offline_prefix = config.get('offline_prefix', 'features/')

        # Alternative: Azure Blob Storage
        if config.get('azure_storage_account'):
            from azure.storage.blob import BlobServiceClient
            self.azure_client = BlobServiceClient(
                account_url=f"https://{config['azure_storage_account']}.blob.core.windows.net",
                credential=config['azure_storage_key']
            )
            self.azure_container = config.get('azure_container', 'features')

        # Alternative: GCP Firestore for online + GCS for offline
        if config.get('gcp_project'):
            self.firestore_client = firestore.Client(project=config['gcp_project'])
            from google.cloud import storage
            self.gcs_client = storage.Client(project=config['gcp_project'])
            self.gcs_bucket = config.get('gcs_bucket', 'feature-store')

        # Feature registry
        self.features: Dict[str, FeatureDefinition] = {}

    def register_feature(self, feature: FeatureDefinition) -> None:
        """Register a feature definition."""
        self.features[feature.feature_name] = feature
        self.logger.info(
            f"Registered feature: {feature.feature_name} "
            f"(type: {feature.feature_type.value}, "
            f"online: {feature.online_enabled}, "
            f"offline: {feature.offline_enabled})"
        )

    def write_online_feature(self, entity_id: str,
                            feature_name: str,
                            value: Any,
                            timestamp: Optional[datetime] = None) -> None:
        """Write feature to online store (Redis)."""
        if feature_name not in self.features:
            raise ValueError(f"Feature not registered: {feature_name}")

        feature_def = self.features[feature_name]

        if not feature_def.online_enabled:
            self.logger.warning(
                f"Feature {feature_name} not enabled for online serving"
            )
            return

        # Create Redis key
        key = self._make_online_key(entity_id, feature_name)

        # Serialize value
        feature_value = FeatureValue(
            feature_name=feature_name,
            value=value,
            timestamp=timestamp or datetime.now(),
            entity_id=entity_id,
            version=feature_def.version
        )

        serialized = json.dumps({
            'value': value,
            'timestamp': feature_value.timestamp.isoformat(),
            'version': feature_def.version
        })

        # Write to Redis with TTL
        if feature_def.ttl_seconds:
            self.redis_client.setex(key, feature_def.ttl_seconds, serialized)
        else:
            self.redis_client.set(key, serialized)

        self.logger.debug(
            f"Wrote online feature: {entity_id}:{feature_name} = {value}"
        )

    def read_online_features(self, entity_id: str,
                            feature_names: List[str]) -> Dict[str, Any]:
        """Read features from online store for inference."""
        features = {}

        for feature_name in feature_names:
            key = self._make_online_key(entity_id, feature_name)

            # Read from Redis
            value_str = self.redis_client.get(key)

            if value_str:
                value_data = json.loads(value_str)
                features[feature_name] = value_data['value']
            else:
                # Feature not found - use default or None
                self.logger.warning(
                    f"Online feature not found: {entity_id}:{feature_name}"
                )
                features[feature_name] = None

        return features

    def write_offline_features(self, df: pd.DataFrame,
                              entity_column: str,
                              timestamp_column: str,
                              partition_by: Optional[str] = None) -> None:
        """Write features to offline store (S3/Parquet) for training."""
        self.logger.info(
            f"Writing offline features: {len(df)} rows, "
            f"{len(df.columns)} columns"
        )

        # Add metadata columns
        df['_write_timestamp'] = datetime.now()

        # Partition by date if timestamp column provided
        if partition_by and partition_by in df.columns:
            # Group by partition column
            for partition_value, partition_df in df.groupby(partition_by):
                self._write_partition(
                    partition_df, entity_column, partition_value
                )
        else:
            # Write entire dataframe
            self._write_partition(df, entity_column, "default")

    def _write_partition(self, df: pd.DataFrame, entity_column: str,
                        partition_value: Any) -> None:
        """Write a partition to offline store."""
        # Generate S3 key
        s3_key = f"{self.offline_prefix}partition={partition_value}/features.parquet"

        # Write to parquet
        table = pa.Table.from_pandas(df)
        parquet_buffer = io.BytesIO()
        pq.write_table(table, parquet_buffer)
        parquet_buffer.seek(0)

        # Upload to S3
        self.s3_client.put_object(
            Bucket=self.offline_bucket,
            Key=s3_key,
            Body=parquet_buffer.getvalue()
        )

        self.logger.info(
            f"Wrote partition {partition_value}: "
            f"{len(df)} rows to s3://{self.offline_bucket}/{s3_key}"
        )

    def read_offline_features(self, entity_ids: Optional[List[str]] = None,
                             start_date: Optional[datetime] = None,
                             end_date: Optional[datetime] = None,
                             feature_names: Optional[List[str]] = None) -> pd.DataFrame:
        """Read features from offline store for training."""
        self.logger.info("Reading offline features for training")

        # List all partitions in date range
        prefix = self.offline_prefix

        response = self.s3_client.list_objects_v2(
            Bucket=self.offline_bucket,
            Prefix=prefix
        )

        # Read all parquet files
        dfs = []
        for obj in response.get('Contents', []):
            key = obj['Key']

            # Download parquet file
            parquet_obj = self.s3_client.get_object(
                Bucket=self.offline_bucket,
                Key=key
            )

            # Read parquet
            df = pd.read_parquet(io.BytesIO(parquet_obj['Body'].read()))
            dfs.append(df)

        if not dfs:
            return pd.DataFrame()

        # Concatenate all partitions
        full_df = pd.concat(dfs, ignore_index=True)

        # Filter by entity IDs if provided
        if entity_ids:
            full_df = full_df[full_df['entity_id'].isin(entity_ids)]

        # Filter by date range if provided
        if start_date or end_date:
            if '_write_timestamp' in full_df.columns:
                if start_date:
                    full_df = full_df[full_df['_write_timestamp'] >= start_date]
                if end_date:
                    full_df = full_df[full_df['_write_timestamp'] <= end_date]

        # Filter by feature names if provided
        if feature_names:
            columns = ['entity_id', '_write_timestamp'] + feature_names
            columns = [c for c in columns if c in full_df.columns]
            full_df = full_df[columns]

        self.logger.info(f"Read {len(full_df)} rows from offline store")

        return full_df

    def sync_online_to_offline(self, entity_ids: List[str],
                               feature_names: List[str]) -> None:
        """Sync features from online store to offline store."""
        self.logger.info(
            f"Syncing {len(entity_ids)} entities, "
            f"{len(feature_names)} features to offline store"
        )

        # Read all features from online store
        rows = []
        for entity_id in entity_ids:
            features = self.read_online_features(entity_id, feature_names)
            row = {'entity_id': entity_id, **features}
            rows.append(row)

        # Create DataFrame
        df = pd.DataFrame(rows)

        # Write to offline store
        self.write_offline_features(df, entity_column='entity_id',
                                   timestamp_column='_write_timestamp')

        self.logger.info("Sync completed")

    def materialize_offline_to_online(self, df: pd.DataFrame,
                                     entity_column: str,
                                     feature_columns: List[str]) -> None:
        """Materialize features from offline store to online store."""
        self.logger.info(
            f"Materializing {len(df)} rows from offline to online store"
        )

        for _, row in df.iterrows():
            entity_id = row[entity_column]

            for feature_name in feature_columns:
                if feature_name in row and pd.notna(row[feature_name]):
                    self.write_online_feature(
                        entity_id=entity_id,
                        feature_name=feature_name,
                        value=row[feature_name]
                    )

        self.logger.info("Materialization completed")

    def compute_point_in_time_features(self, entity_ids: List[str],
                                      timestamps: List[datetime],
                                      feature_names: List[str]) -> pd.DataFrame:
        """Compute point-in-time correct features for training."""
        self.logger.info("Computing point-in-time features")

        # For each entity and timestamp, get the latest feature values
        # before that timestamp (to avoid data leakage)

        rows = []
        for entity_id, timestamp in zip(entity_ids, timestamps):
            row = {'entity_id': entity_id, 'event_time': timestamp}

            # Read offline features
            offline_df = self.read_offline_features(
                entity_ids=[entity_id],
                end_date=timestamp,
                feature_names=feature_names
            )

            if not offline_df.empty:
                # Get latest values before timestamp
                latest = offline_df.sort_values('_write_timestamp').iloc[-1]

                for feature_name in feature_names:
                    if feature_name in latest:
                        row[feature_name] = latest[feature_name]

            rows.append(row)

        df = pd.DataFrame(rows)

        self.logger.info(f"Computed point-in-time features: {len(df)} rows")

        return df

    def _make_online_key(self, entity_id: str, feature_name: str) -> str:
        """Generate Redis key for online feature."""
        return f"feature:{entity_id}:{feature_name}"

# Example usage
feature_store = FeatureStoreIntegrator(config={
    'redis_host': 'localhost',
    'redis_port': 6379,
    'offline_bucket': 'my-feature-store',
    'offline_prefix': 'features/'
})

# Register features
feature_store.register_feature(FeatureDefinition(
    feature_name='user_purchase_count_30d',
    feature_type=FeatureType.NUMERICAL,
    description='Number of purchases in last 30 days',
    entity_type='user',
    online_enabled=True,
    offline_enabled=True,
    ttl_seconds=3600,  # 1 hour TTL
    version='1.0.0'
))

feature_store.register_feature(FeatureDefinition(
    feature_name='user_avg_order_value',
    feature_type=FeatureType.NUMERICAL,
    description='Average order value',
    entity_type='user',
    online_enabled=True,
    offline_enabled=True,
    ttl_seconds=3600,
    version='1.0.0'
))

# Write online features (for real-time inference)
feature_store.write_online_feature(
    entity_id='user_12345',
    feature_name='user_purchase_count_30d',
    value=15
)

feature_store.write_online_feature(
    entity_id='user_12345',
    feature_name='user_avg_order_value',
    value=89.50
)

# Read online features for inference (<10ms)
features = feature_store.read_online_features(
    entity_id='user_12345',
    feature_names=['user_purchase_count_30d', 'user_avg_order_value']
)
print(f"Online features: {features}")

# Write offline features for training
training_df = pd.DataFrame({
    'entity_id': ['user_12345', 'user_67890'],
    'user_purchase_count_30d': [15, 8],
    'user_avg_order_value': [89.50, 120.00],
    'event_date': ['2024-01-01', '2024-01-01']
})

feature_store.write_offline_features(
    df=training_df,
    entity_column='entity_id',
    timestamp_column='event_date',
    partition_by='event_date'
)

# Read offline features for training
training_features = feature_store.read_offline_features(
    entity_ids=['user_12345', 'user_67890'],
    feature_names=['user_purchase_count_30d', 'user_avg_order_value']
)
print(f"Offline features: {len(training_features)} rows")
\end{lstlisting}

\subsubsection{DataVersionManager: Semantic Versioning and Rollback}

\begin{lstlisting}[style=python, caption=DataVersionManager with Semantic Versioning]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum
from datetime import datetime
import logging
import json
import hashlib
from packaging import version
import boto3

class VersionChangeType(Enum):
    """Types of version changes."""
    MAJOR = "major"  # Breaking changes
    MINOR = "minor"  # Backward-compatible additions
    PATCH = "patch"  # Bug fixes

@dataclass
class SchemaField:
    """Schema field definition."""
    name: str
    type: str
    nullable: bool = True
    default: Optional[Any] = None
    description: Optional[str] = None

@dataclass
class DataVersion:
    """Data version with schema and metadata."""
    version: str  # Semantic version (e.g., "1.2.3")
    schema: List[SchemaField]
    created_at: datetime
    created_by: str
    description: str
    checksum: str  # SHA256 of schema
    is_production: bool = False
    deprecated: bool = False
    migration_notes: Optional[str] = None

@dataclass
class ValidationResult:
    """Schema validation result."""
    is_valid: bool
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    compatible_with_previous: bool = True
    breaking_changes: List[str] = field(default_factory=list)

class DataVersionManager:
    """Manage data versions with semantic versioning."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # S3 for storing versions
        self.s3_client = boto3.client('s3')
        self.version_bucket = config.get('version_bucket', 'data-versions')
        self.version_prefix = config.get('version_prefix', 'schemas/')

        # Version history
        self.versions: Dict[str, DataVersion] = {}
        self.current_version: Optional[str] = None

        # Load existing versions
        self._load_versions()

    def create_version(self, schema: List[SchemaField],
                      change_type: VersionChangeType,
                      description: str,
                      created_by: str,
                      migration_notes: Optional[str] = None) -> DataVersion:
        """Create a new data version."""
        # Determine new version number
        new_version = self._compute_next_version(change_type)

        # Validate schema
        validation = self.validate_schema_change(schema)

        if not validation.is_valid:
            raise ValueError(f"Invalid schema: {', '.join(validation.errors)}")

        # Check for breaking changes
        if validation.breaking_changes and change_type != VersionChangeType.MAJOR:
            raise ValueError(
                f"Breaking changes require MAJOR version bump: "
                f"{', '.join(validation.breaking_changes)}"
            )

        # Compute checksum
        checksum = self._compute_schema_checksum(schema)

        # Create version
        data_version = DataVersion(
            version=new_version,
            schema=schema,
            created_at=datetime.now(),
            created_by=created_by,
            description=description,
            checksum=checksum,
            migration_notes=migration_notes
        )

        # Store version
        self.versions[new_version] = data_version
        self.current_version = new_version

        # Persist to S3
        self._save_version(data_version)

        self.logger.info(
            f"Created version {new_version} "
            f"({change_type.value}): {description}"
        )

        return data_version

    def validate_schema_change(self, new_schema: List[SchemaField]) -> ValidationResult:
        """Validate schema changes against current version."""
        result = ValidationResult(is_valid=True)

        if not self.current_version:
            # First version, no validation needed
            return result

        current = self.versions[self.current_version]
        current_fields = {f.name: f for f in current.schema}
        new_fields = {f.name: f for f in new_schema}

        # Check for removed fields (breaking change)
        removed_fields = set(current_fields.keys()) - set(new_fields.keys())
        if removed_fields:
            result.breaking_changes.append(
                f"Removed fields: {', '.join(removed_fields)}"
            )
            result.compatible_with_previous = False

        # Check for type changes (breaking change)
        for field_name in set(current_fields.keys()) & set(new_fields.keys()):
            current_field = current_fields[field_name]
            new_field = new_fields[field_name]

            if current_field.type != new_field.type:
                result.breaking_changes.append(
                    f"Type changed for '{field_name}': "
                    f"{current_field.type} → {new_field.type}"
                )
                result.compatible_with_previous = False

            # Check for nullable changes
            if current_field.nullable and not new_field.nullable:
                result.breaking_changes.append(
                    f"Field '{field_name}' changed from nullable to non-nullable"
                )
                result.compatible_with_previous = False

        # Check for new non-nullable fields without defaults (breaking change)
        new_field_names = set(new_fields.keys()) - set(current_fields.keys())
        for field_name in new_field_names:
            new_field = new_fields[field_name]
            if not new_field.nullable and new_field.default is None:
                result.breaking_changes.append(
                    f"New non-nullable field without default: '{field_name}'"
                )
                result.compatible_with_previous = False
            else:
                result.warnings.append(
                    f"New field added: '{field_name}' ({new_field.type})"
                )

        # Validate result
        if result.breaking_changes:
            result.errors.extend(result.breaking_changes)
            result.is_valid = False

        return result

    def promote_to_production(self, version_str: str) -> None:
        """Promote a version to production."""
        if version_str not in self.versions:
            raise ValueError(f"Version not found: {version_str}")

        # Demote current production version
        for ver in self.versions.values():
            if ver.is_production:
                ver.is_production = False
                self._save_version(ver)

        # Promote new version
        target_version = self.versions[version_str]
        target_version.is_production = True
        self._save_version(target_version)

        self.logger.info(f"Promoted version {version_str} to production")

    def rollback(self, target_version: str) -> None:
        """Rollback to a previous version."""
        if target_version not in self.versions:
            raise ValueError(f"Version not found: {target_version}")

        # Validate rollback
        current_ver = version.parse(self.current_version)
        target_ver = version.parse(target_version)

        if target_ver >= current_ver:
            raise ValueError(
                f"Cannot rollback to version {target_version} >= current {self.current_version}"
            )

        self.logger.warning(
            f"Rolling back from {self.current_version} to {target_version}"
        )

        # Update current version
        self.current_version = target_version

        # Promote to production
        self.promote_to_production(target_version)

        self.logger.info(f"Rollback completed to version {target_version}")

    def deprecate_version(self, version_str: str, reason: str) -> None:
        """Mark a version as deprecated."""
        if version_str not in self.versions:
            raise ValueError(f"Version not found: {version_str}")

        ver = self.versions[version_str]
        ver.deprecated = True
        ver.migration_notes = f"DEPRECATED: {reason}"

        self._save_version(ver)

        self.logger.info(f"Deprecated version {version_str}: {reason}")

    def get_migration_path(self, from_version: str,
                          to_version: str) -> List[str]:
        """Get migration path between versions."""
        from_ver = version.parse(from_version)
        to_ver = version.parse(to_version)

        if from_ver > to_ver:
            raise ValueError("Cannot migrate backwards")

        # Get all versions between from and to
        migration_versions = []

        for ver_str in sorted(self.versions.keys(), key=version.parse):
            ver = version.parse(ver_str)
            if from_ver < ver <= to_ver:
                migration_versions.append(ver_str)

        return migration_versions

    def compare_versions(self, version1: str, version2: str) -> Dict[str, Any]:
        """Compare two versions and show differences."""
        if version1 not in self.versions or version2 not in self.versions:
            raise ValueError("One or both versions not found")

        ver1 = self.versions[version1]
        ver2 = self.versions[version2]

        fields1 = {f.name: f for f in ver1.schema}
        fields2 = {f.name: f for f in ver2.schema}

        comparison = {
            'version1': version1,
            'version2': version2,
            'added_fields': [],
            'removed_fields': [],
            'modified_fields': [],
            'unchanged_fields': []
        }

        # Find added fields
        added = set(fields2.keys()) - set(fields1.keys())
        comparison['added_fields'] = list(added)

        # Find removed fields
        removed = set(fields1.keys()) - set(fields2.keys())
        comparison['removed_fields'] = list(removed)

        # Find modified fields
        common = set(fields1.keys()) & set(fields2.keys())
        for field_name in common:
            f1 = fields1[field_name]
            f2 = fields2[field_name]

            if f1.type != f2.type or f1.nullable != f2.nullable:
                comparison['modified_fields'].append({
                    'field': field_name,
                    'version1': {'type': f1.type, 'nullable': f1.nullable},
                    'version2': {'type': f2.type, 'nullable': f2.nullable}
                })
            else:
                comparison['unchanged_fields'].append(field_name)

        return comparison

    def _compute_next_version(self, change_type: VersionChangeType) -> str:
        """Compute next version number based on change type."""
        if not self.current_version:
            return "1.0.0"

        current = version.parse(self.current_version)

        if change_type == VersionChangeType.MAJOR:
            return f"{current.major + 1}.0.0"
        elif change_type == VersionChangeType.MINOR:
            return f"{current.major}.{current.minor + 1}.0"
        else:  # PATCH
            return f"{current.major}.{current.minor}.{current.micro + 1}"

    def _compute_schema_checksum(self, schema: List[SchemaField]) -> str:
        """Compute SHA256 checksum of schema."""
        schema_str = json.dumps(
            [
                {'name': f.name, 'type': f.type, 'nullable': f.nullable}
                for f in schema
            ],
            sort_keys=True
        )
        return hashlib.sha256(schema_str.encode()).hexdigest()

    def _save_version(self, data_version: DataVersion) -> None:
        """Save version to S3."""
        key = f"{self.version_prefix}{data_version.version}.json"

        version_data = {
            'version': data_version.version,
            'schema': [
                {
                    'name': f.name,
                    'type': f.type,
                    'nullable': f.nullable,
                    'default': f.default,
                    'description': f.description
                }
                for f in data_version.schema
            ],
            'created_at': data_version.created_at.isoformat(),
            'created_by': data_version.created_by,
            'description': data_version.description,
            'checksum': data_version.checksum,
            'is_production': data_version.is_production,
            'deprecated': data_version.deprecated,
            'migration_notes': data_version.migration_notes
        }

        self.s3_client.put_object(
            Bucket=self.version_bucket,
            Key=key,
            Body=json.dumps(version_data, indent=2)
        )

    def _load_versions(self) -> None:
        """Load versions from S3."""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.version_bucket,
                Prefix=self.version_prefix
            )

            for obj in response.get('Contents', []):
                key = obj['Key']

                # Download version file
                version_obj = self.s3_client.get_object(
                    Bucket=self.version_bucket,
                    Key=key
                )

                version_data = json.loads(version_obj['Body'].read())

                # Parse schema
                schema = [
                    SchemaField(
                        name=f['name'],
                        type=f['type'],
                        nullable=f.get('nullable', True),
                        default=f.get('default'),
                        description=f.get('description')
                    )
                    for f in version_data['schema']
                ]

                # Create DataVersion
                data_version = DataVersion(
                    version=version_data['version'],
                    schema=schema,
                    created_at=datetime.fromisoformat(version_data['created_at']),
                    created_by=version_data['created_by'],
                    description=version_data['description'],
                    checksum=version_data['checksum'],
                    is_production=version_data.get('is_production', False),
                    deprecated=version_data.get('deprecated', False),
                    migration_notes=version_data.get('migration_notes')
                )

                self.versions[data_version.version] = data_version

                if data_version.is_production:
                    self.current_version = data_version.version

        except Exception as e:
            self.logger.warning(f"Failed to load versions: {str(e)}")

# Example usage
version_manager = DataVersionManager(config={
    'version_bucket': 'my-data-versions',
    'version_prefix': 'schemas/customer_events/'
})

# Create initial schema version
initial_schema = [
    SchemaField(name='event_id', type='string', nullable=False),
    SchemaField(name='user_id', type='string', nullable=False),
    SchemaField(name='event_type', type='string', nullable=False),
    SchemaField(name='timestamp', type='timestamp', nullable=False),
]

v1 = version_manager.create_version(
    schema=initial_schema,
    change_type=VersionChangeType.MAJOR,
    description='Initial schema version',
    created_by='data-team@company.com'
)

print(f"Created version: {v1.version}")

# Add new field (minor version bump)
v1_1_schema = initial_schema + [
    SchemaField(name='session_id', type='string', nullable=True,
               description='Session identifier')
]

v1_1 = version_manager.create_version(
    schema=v1_1_schema,
    change_type=VersionChangeType.MINOR,
    description='Added session_id field',
    created_by='data-team@company.com'
)

print(f"Created version: {v1_1.version}")

# Promote to production
version_manager.promote_to_production(v1_1.version)

# Attempt breaking change (will fail if not MAJOR)
try:
    breaking_schema = [
        SchemaField(name='event_id', type='integer', nullable=False),  # Changed type
        SchemaField(name='user_id', type='string', nullable=False),
    ]

    version_manager.create_version(
        schema=breaking_schema,
        change_type=VersionChangeType.MINOR,  # Should be MAJOR
        description='Changed event_id type',
        created_by='data-team@company.com'
    )
except ValueError as e:
    print(f"Validation error: {e}")

# Compare versions
comparison = version_manager.compare_versions('1.0.0', '1.1.0')
print(f"Added fields: {comparison['added_fields']}")

# Rollback to previous version
version_manager.rollback('1.0.0')
print(f"Rolled back to version 1.0.0")
\end{lstlisting}

\textbf{Advanced Pattern Implementation Summary:}

\begin{itemize}
    \item \textbf{CDC (Change Data Capture)}:
    \begin{itemize}
        \item Debezium integration for real-time streaming from PostgreSQL WAL
        \item Vector clocks for conflict detection in multi-region replication
        \item 4 conflict resolution strategies (LWW, FWW, Custom, Manual)
        \item Sub-second replication latency (<500ms typical)
        \item Automatic retry and error handling
    \end{itemize}

    \item \textbf{Data Virtualization}:
    \begin{itemize}
        \item Trino-based federated query engine
        \item Query pushdown optimization (filters/aggregations to source)
        \item Smart join strategies (broadcast vs shuffle based on table sizes)
        \item Data residency compliance validation (GDPR, CCPA)
        \item Cost-based query optimization
        \item Query result caching with TTL
    \end{itemize}

    \item \textbf{Feature Store}:
    \begin{itemize}
        \item Dual serving modes: Online (Redis <10ms) + Offline (S3/Parquet)
        \item Point-in-time correctness for training (no data leakage)
        \item Automatic sync between online and offline stores
        \item Multi-cloud support (AWS S3, Azure Blob, GCP Firestore/GCS)
        \item Feature versioning and TTL management
    \end{itemize}

    \item \textbf{Data Versioning}:
    \begin{itemize}
        \item Semantic versioning (MAJOR.MINOR.PATCH)
        \item Breaking change detection and validation
        \item Automatic rollback capabilities
        \item Schema comparison and migration path generation
        \item Version deprecation with migration notes
        \item SHA256 checksums for data integrity
    \end{itemize}
\end{itemize}

\subsection{Privacy-Preserving Pipelines and Cost Optimization}

Modern data platforms must balance two critical imperatives: protecting user privacy under increasingly strict regulations (GDPR, CCPA, HIPAA) while optimizing infrastructure costs that can easily spiral into millions of dollars annually. Privacy-preserving techniques like differential privacy and k-anonymity enable valuable analytics while protecting individuals, while intelligent resource scheduling and auto-scaling can reduce cloud costs by 60-80\%.

\subsubsection{The Compliance Scramble: GDPR Audit Crisis}

\textbf{The Company:} Health tech startup with 5 million users across EU and US

\textbf{The Trigger:} Unexpected GDPR audit notification from Irish Data Protection Commission

\textbf{The Discovery (Day 1-3):}

\begin{itemize}
    \item \textbf{Day 1, 09:00}: Audit notification received, 14-day compliance review window
    \item \textbf{Day 1, 11:00}: Data team begins inventory of user data processing
    \item \textbf{Day 1, 15:00}: First major finding: Analytics pipeline processes raw user data
    \begin{itemize}
        \item Full names, email addresses, phone numbers in logs
        \item IP addresses stored indefinitely (GDPR requires time limits)
        \item Medical conditions in plaintext (HIPAA/GDPR violation)
        \item No data minimization (collecting data "just in case")
    \end{itemize}
    \item \textbf{Day 2, 10:00}: Second finding: Data sharing with third parties
    \begin{itemize}
        \item ML training data shared with analytics vendor (no anonymization)
        \item User cohort analysis shared marketing metrics with identifiable individuals
        \item No consent mechanism for data processing beyond core service
    \end{itemize}
    \item \textbf{Day 2, 16:00}: Third finding: Right to deletion not implemented
    \begin{itemize}
        \item User deletion requests queued for "eventual processing"
        \item Data replicated across 15 different systems (no centralized deletion)
        \item Backups retained indefinitely (GDPR requires deletion from backups too)
        \item 2,847 pending deletion requests (oldest: 11 months)
    \end{itemize}
    \item \textbf{Day 3, 09:00}: Fourth finding: No privacy impact assessment
    \begin{itemize}
        \item ML models trained on sensitive health data without privacy analysis
        \item No differential privacy (models memorize training data)
        \item Model inversion attacks possible (reconstruct user data from model)
    \end{itemize}
\end{itemize}

\textbf{The Violations:}

\begin{enumerate}
    \item \textbf{Article 5: Data Minimization} - Processing unnecessary data
    \item \textbf{Article 17: Right to Erasure} - 2,847 unfulfilled deletion requests
    \item \textbf{Article 25: Privacy by Design} - No privacy controls in pipelines
    \item \textbf{Article 32: Security} - Plaintext sensitive data in logs
    \item \textbf{Article 35: Data Protection Impact Assessment} - None conducted
\end{enumerate}

\textbf{The Potential Penalties:}

\begin{itemize}
    \item Maximum fine: €20M or 4\% of global revenue (whichever higher)
    \item Company revenue: \$150M → Maximum fine: \$180M
    \item Estimated fine based on violations: \$12M-\$25M
    \item Reputational damage: Stock price down 35\% on disclosure
    \item Legal costs: \$2M for compliance remediation
\end{itemize}

\textbf{Emergency Response (Day 4-14):}

Implemented privacy-preserving pipeline in 10 days:

\begin{enumerate}
    \item \textbf{Differential Privacy} (ε=1.0): Add calibrated noise to analytics queries
    \item \textbf{K-Anonymization} (k=5): Ensure 5+ users per cohort in reports
    \item \textbf{Pseudonymization}: Replace identifiers with irreversible hashes
    \item \textbf{Data Masking}: Email → \texttt{f***@example.com}, Phone → \texttt{***-***-1234}
    \item \textbf{Automated Deletion}: Cascade deletion across all 15 systems within 24 hours
    \item \textbf{Retention Policies}: 90-day auto-deletion for logs, 2-year max for user data
\end{enumerate}

\textbf{Outcome:}

\begin{itemize}
    \item \textbf{Actual fine}: €850K (reduced due to rapid remediation)
    \item \textbf{Compliance achieved}: 14 days (just within deadline)
    \item \textbf{Technical debt cost}: \$3.5M engineering effort
    \item \textbf{Prevention cost if done earlier}: \$200K
    \item \textbf{ROI of privacy-first design}: 17.5x
\end{itemize}

\textbf{Lesson Learned:} Privacy cannot be bolted on after the fact. Privacy-preserving pipelines must be the default, not an afterthought.

\subsubsection{PrivacyPreservingPipeline: Differential Privacy Implementation}

\begin{lstlisting}[style=python, caption=PrivacyPreservingPipeline with Differential Privacy]
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Callable
from enum import Enum
import logging
import numpy as np
import hashlib
import pandas as pd
from datetime import datetime, timedelta

class PrivacyBudget(Enum):
    """Privacy budget levels (epsilon values)."""
    VERY_HIGH = 0.1  # Maximum privacy, higher noise
    HIGH = 0.5
    MEDIUM = 1.0  # Balanced privacy/utility
    LOW = 2.0
    VERY_LOW = 5.0  # Minimum privacy, lower noise

class PrivacyMechanism(Enum):
    """Privacy preservation mechanisms."""
    LAPLACE = "laplace"  # For numeric queries
    GAUSSIAN = "gaussian"  # For queries with bounded sensitivity
    EXPONENTIAL = "exponential"  # For non-numeric queries
    RANDOMIZED_RESPONSE = "randomized_response"  # For categorical data

@dataclass
class PrivacyConfig:
    """Privacy configuration."""
    epsilon: float  # Privacy budget
    delta: float = 1e-5  # Privacy parameter for Gaussian
    sensitivity: float = 1.0  # Query sensitivity
    mechanism: PrivacyMechanism = PrivacyMechanism.LAPLACE

@dataclass
class PrivacyAuditLog:
    """Privacy audit log entry."""
    timestamp: datetime
    query_type: str
    epsilon_spent: float
    remaining_budget: float
    user_id: Optional[str]
    purpose: str

class PrivacyPreservingPipeline:
    """Pipeline with differential privacy guarantees."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Privacy budget management
        self.total_epsilon = config.get('total_epsilon', 1.0)
        self.remaining_epsilon = self.total_epsilon
        self.epsilon_lock = threading.Lock()

        # Audit log
        self.audit_log: List[PrivacyAuditLog] = []

        # Query cache (for privacy budget efficiency)
        self.query_cache: Dict[str, Any] = {}

    def count_query(self, df: pd.DataFrame, filter_condition: Optional[str] = None,
                   privacy_config: Optional[PrivacyConfig] = None) -> float:
        """Execute differentially private count query."""
        if privacy_config is None:
            privacy_config = PrivacyConfig(epsilon=0.1)

        # Check privacy budget
        if not self._check_budget(privacy_config.epsilon):
            raise ValueError(
                f"Insufficient privacy budget. Remaining: {self.remaining_epsilon:.3f}, "
                f"Required: {privacy_config.epsilon}"
            )

        # Compute true count
        if filter_condition:
            filtered_df = df.query(filter_condition)
            true_count = len(filtered_df)
        else:
            true_count = len(df)

        # Add Laplace noise for differential privacy
        noise = self._laplace_noise(privacy_config.epsilon, privacy_config.sensitivity)
        noisy_count = max(0, true_count + noise)  # Ensure non-negative

        # Spend privacy budget
        self._spend_budget(privacy_config.epsilon, "count_query")

        self.logger.info(
            f"DP Count Query: true={true_count}, noisy={noisy_count:.0f}, "
            f"epsilon={privacy_config.epsilon}, noise={noise:.2f}"
        )

        return noisy_count

    def sum_query(self, df: pd.DataFrame, column: str,
                 filter_condition: Optional[str] = None,
                 privacy_config: Optional[PrivacyConfig] = None,
                 clipping_threshold: Optional[float] = None) -> float:
        """Execute differentially private sum query with clipping."""
        if privacy_config is None:
            privacy_config = PrivacyConfig(epsilon=0.1)

        # Check privacy budget
        if not self._check_budget(privacy_config.epsilon):
            raise ValueError("Insufficient privacy budget")

        # Apply filter
        if filter_condition:
            filtered_df = df.query(filter_condition)
        else:
            filtered_df = df

        # Clip values to bound sensitivity
        if clipping_threshold:
            clipped_values = filtered_df[column].clip(-clipping_threshold, clipping_threshold)
        else:
            clipped_values = filtered_df[column]

        # Compute true sum
        true_sum = clipped_values.sum()

        # Determine sensitivity
        if clipping_threshold:
            sensitivity = clipping_threshold
        else:
            # Use global sensitivity (max possible change)
            sensitivity = filtered_df[column].max() - filtered_df[column].min()

        # Add Laplace noise
        noise = self._laplace_noise(privacy_config.epsilon, sensitivity)
        noisy_sum = true_sum + noise

        # Spend privacy budget
        self._spend_budget(privacy_config.epsilon, "sum_query")

        self.logger.info(
            f"DP Sum Query: true={true_sum:.2f}, noisy={noisy_sum:.2f}, "
            f"epsilon={privacy_config.epsilon}"
        )

        return noisy_sum

    def average_query(self, df: pd.DataFrame, column: str,
                     filter_condition: Optional[str] = None,
                     privacy_config: Optional[PrivacyConfig] = None) -> float:
        """Execute differentially private average query."""
        if privacy_config is None:
            privacy_config = PrivacyConfig(epsilon=0.2)  # Split budget

        # Split epsilon between count and sum
        epsilon_count = privacy_config.epsilon / 2
        epsilon_sum = privacy_config.epsilon / 2

        # Get noisy count
        config_count = PrivacyConfig(epsilon=epsilon_count)
        noisy_count = self.count_query(df, filter_condition, config_count)

        # Get noisy sum
        config_sum = PrivacyConfig(epsilon=epsilon_sum)
        noisy_sum = self.sum_query(df, column, filter_condition, config_sum)

        # Compute noisy average
        if noisy_count > 0:
            noisy_avg = noisy_sum / noisy_count
        else:
            noisy_avg = 0.0

        self.logger.info(
            f"DP Average Query: {noisy_avg:.2f} "
            f"(count={noisy_count:.0f}, sum={noisy_sum:.2f})"
        )

        return noisy_avg

    def histogram_query(self, df: pd.DataFrame, column: str, bins: int,
                       privacy_config: Optional[PrivacyConfig] = None) -> Dict[str, float]:
        """Execute differentially private histogram query."""
        if privacy_config is None:
            privacy_config = PrivacyConfig(epsilon=1.0)

        # Check privacy budget
        if not self._check_budget(privacy_config.epsilon):
            raise ValueError("Insufficient privacy budget")

        # Compute true histogram
        hist, bin_edges = np.histogram(df[column], bins=bins)

        # Add Laplace noise to each bin
        epsilon_per_bin = privacy_config.epsilon / bins
        noisy_hist = []

        for count in hist:
            noise = self._laplace_noise(epsilon_per_bin, sensitivity=1.0)
            noisy_count = max(0, count + noise)
            noisy_hist.append(noisy_count)

        # Spend privacy budget
        self._spend_budget(privacy_config.epsilon, "histogram_query")

        # Format result
        result = {}
        for i in range(len(noisy_hist)):
            bin_label = f"[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})"
            result[bin_label] = noisy_hist[i]

        return result

    def percentile_query(self, df: pd.DataFrame, column: str, percentile: float,
                        privacy_config: Optional[PrivacyConfig] = None) -> float:
        """Execute differentially private percentile query using exponential mechanism."""
        if privacy_config is None:
            privacy_config = PrivacyConfig(epsilon=1.0)

        # Check privacy budget
        if not self._check_budget(privacy_config.epsilon):
            raise ValueError("Insufficient privacy budget")

        # Get all unique values as candidates
        candidates = df[column].unique()

        # Define utility function (how close to true percentile)
        true_percentile_value = df[column].quantile(percentile / 100)

        # Compute utility scores
        utilities = -np.abs(candidates - true_percentile_value)

        # Apply exponential mechanism
        sensitiviy = 1.0  # Changing one record changes rank by at most 1
        probabilities = np.exp(privacy_config.epsilon * utilities / (2 * sensitiviy))
        probabilities /= probabilities.sum()

        # Sample from distribution
        selected_value = np.random.choice(candidates, p=probabilities)

        # Spend privacy budget
        self._spend_budget(privacy_config.epsilon, "percentile_query")

        self.logger.info(
            f"DP Percentile Query (p{percentile}): {selected_value:.2f}"
        )

        return selected_value

    def _laplace_noise(self, epsilon: float, sensitivity: float) -> float:
        """Generate Laplace noise for differential privacy."""
        scale = sensitivity / epsilon
        return np.random.laplace(0, scale)

    def _gaussian_noise(self, epsilon: float, delta: float, sensitivity: float) -> float:
        """Generate Gaussian noise for (ε, δ)-differential privacy."""
        sigma = np.sqrt(2 * np.log(1.25 / delta)) * sensitivity / epsilon
        return np.random.normal(0, sigma)

    def _check_budget(self, epsilon_required: float) -> bool:
        """Check if sufficient privacy budget remains."""
        with self.epsilon_lock:
            return self.remaining_epsilon >= epsilon_required

    def _spend_budget(self, epsilon_spent: float, query_type: str) -> None:
        """Spend privacy budget and log."""
        with self.epsilon_lock:
            self.remaining_epsilon -= epsilon_spent

            # Log audit entry
            self.audit_log.append(PrivacyAuditLog(
                timestamp=datetime.now(),
                query_type=query_type,
                epsilon_spent=epsilon_spent,
                remaining_budget=self.remaining_epsilon,
                user_id=None,
                purpose="analytics"
            ))

    def get_privacy_report(self) -> Dict[str, Any]:
        """Generate privacy budget usage report."""
        total_spent = self.total_epsilon - self.remaining_epsilon

        return {
            'total_budget': self.total_epsilon,
            'spent': total_spent,
            'remaining': self.remaining_epsilon,
            'utilization_percent': (total_spent / self.total_epsilon) * 100,
            'queries_executed': len(self.audit_log),
            'audit_log': [
                {
                    'timestamp': entry.timestamp.isoformat(),
                    'query_type': entry.query_type,
                    'epsilon_spent': entry.epsilon_spent,
                    'remaining_budget': entry.remaining_budget
                }
                for entry in self.audit_log[-10:]  # Last 10 entries
            ]
        }

# Example usage
pipeline = PrivacyPreservingPipeline(config={
    'total_epsilon': 10.0  # Total privacy budget for the day
})

# Create sample data
df = pd.DataFrame({
    'user_id': range(10000),
    'age': np.random.randint(18, 80, 10000),
    'revenue': np.random.exponential(50, 10000),
    'country': np.random.choice(['US', 'UK', 'DE', 'FR'], 10000)
})

# Execute differentially private queries
print("Differential Privacy Queries:")

# Count query
count = pipeline.count_query(df, filter_condition="age > 25",
                             privacy_config=PrivacyConfig(epsilon=0.1))
print(f"Count (age > 25): {count:.0f}")

# Average query
avg_revenue = pipeline.average_query(df, 'revenue',
                                    privacy_config=PrivacyConfig(epsilon=0.2))
print(f"Average revenue: ${avg_revenue:.2f}")

# Histogram
age_hist = pipeline.histogram_query(df, 'age', bins=5,
                                    privacy_config=PrivacyConfig(epsilon=1.0))
print(f"Age histogram: {age_hist}")

# Privacy report
report = pipeline.get_privacy_report()
print(f"\nPrivacy Budget Report:")
print(f"  Total: {report['total_budget']}")
print(f"  Spent: {report['spent']:.2f}")
print(f"  Remaining: {report['remaining']:.2f}")
print(f"  Utilization: {report['utilization_percent']:.1f}%")
\end{lstlisting}

\subsubsection{Data Anonymization: K-Anonymity and Beyond}

Data anonymization protects individual privacy while maintaining analytical utility. The choice of technique depends on regulatory requirements (GDPR Article 4(5), HIPAA Safe Harbor), re-identification risk tolerance, and business needs for data reversibility.

\begin{lstlisting}[language=Python, caption=DataAnonymizer with Multiple Anonymization Techniques]
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Set, Tuple
import pandas as pd
import hashlib
import secrets
import re
from collections import defaultdict

@dataclass
class AnonymizationConfig:
    """Configuration for data anonymization."""
    k_anonymity: int = 5  # Minimum cohort size
    l_diversity: int = 3  # Minimum diversity for sensitive attributes
    suppression_threshold: float = 0.05  # Max % of records to suppress
    quasi_identifiers: List[str] = None  # Columns for k-anonymity
    sensitive_attributes: List[str] = None  # Columns for l-diversity
    masking_patterns: Dict[str, str] = None  # Field -> pattern mappings

@dataclass
class AnonymizationResult:
    """Result of anonymization operation."""
    anonymized_data: pd.DataFrame
    suppressed_records: int
    generalization_levels: Dict[str, int]
    k_anonymity_achieved: int
    l_diversity_achieved: Dict[str, int]
    information_loss: float  # 0-1 scale

class DataAnonymizer:
    """
    Multi-technique data anonymization with k-anonymity, l-diversity,
    masking, and pseudonymization.

    Techniques:
    - K-anonymity: Ensures each record is indistinguishable from k-1 others
    - L-diversity: Ensures diversity in sensitive attributes within cohorts
    - Data masking: Pattern-based redaction (emails, phones, SSNs)
    - Pseudonymization: Irreversible one-way hashing
    - Tokenization: Reversible anonymization with secure token vault
    """

    def __init__(self, config: AnonymizationConfig):
        self.config = config
        self.token_vault: Dict[str, str] = {}  # original -> token
        self.reverse_vault: Dict[str, str] = {}  # token -> original
        self.generalization_hierarchies: Dict[str, List[Any]] = {}

    def anonymize_dataset(self, df: pd.DataFrame,
                         method: str = 'k-anonymity') -> AnonymizationResult:
        """
        Anonymize dataset using specified method.

        Args:
            df: Input DataFrame
            method: 'k-anonymity', 'l-diversity', 'masking', or 'pseudonymization'

        Returns:
            AnonymizationResult with anonymized data and metrics
        """
        if method == 'k-anonymity':
            return self._apply_k_anonymity(df)
        elif method == 'l-diversity':
            return self._apply_l_diversity(df)
        elif method == 'masking':
            return self._apply_masking(df)
        elif method == 'pseudonymization':
            return self._apply_pseudonymization(df)
        else:
            raise ValueError(f"Unknown anonymization method: {method}")

    def _apply_k_anonymity(self, df: pd.DataFrame) -> AnonymizationResult:
        """
        Apply k-anonymity by generalizing quasi-identifiers.

        Process:
        1. Group records by quasi-identifier combinations
        2. For groups with size < k, generalize or suppress
        3. Iteratively increase generalization until k-anonymity achieved
        """
        if not self.config.quasi_identifiers:
            raise ValueError("quasi_identifiers must be specified for k-anonymity")

        anonymized_df = df.copy()
        generalization_levels = {qi: 0 for qi in self.config.quasi_identifiers}
        suppressed_records = 0

        # Iteratively generalize until k-anonymity achieved
        max_iterations = 10
        for iteration in range(max_iterations):
            # Group by current quasi-identifier values
            groups = anonymized_df.groupby(self.config.quasi_identifiers)

            # Check if k-anonymity is satisfied
            small_groups = [name for name, group in groups if len(group) < self.config.k_anonymity]

            if not small_groups:
                # K-anonymity achieved!
                break

            # Calculate suppression cost
            records_to_suppress = sum(len(groups.get_group(name))
                                     for name in small_groups)
            suppression_rate = records_to_suppress / len(df)

            if suppression_rate > self.config.suppression_threshold:
                # Too many records would be suppressed, generalize instead
                qi_to_generalize = self._select_qi_to_generalize(
                    anonymized_df, self.config.quasi_identifiers
                )
                anonymized_df = self._generalize_column(
                    anonymized_df, qi_to_generalize
                )
                generalization_levels[qi_to_generalize] += 1
            else:
                # Suppress small groups
                for group_name in small_groups:
                    group_indices = groups.get_group(group_name).index
                    anonymized_df = anonymized_df.drop(group_indices)
                    suppressed_records += len(group_indices)
                break

        # Calculate achieved k-anonymity
        final_groups = anonymized_df.groupby(self.config.quasi_identifiers)
        min_group_size = min(len(group) for _, group in final_groups)

        # Calculate information loss
        info_loss = self._calculate_information_loss(
            df, anonymized_df, generalization_levels
        )

        return AnonymizationResult(
            anonymized_data=anonymized_df,
            suppressed_records=suppressed_records,
            generalization_levels=generalization_levels,
            k_anonymity_achieved=min_group_size,
            l_diversity_achieved={},
            information_loss=info_loss
        )

    def _apply_l_diversity(self, df: pd.DataFrame) -> AnonymizationResult:
        """
        Apply l-diversity to ensure diversity in sensitive attributes.

        L-diversity extends k-anonymity by requiring that each equivalence
        class has at least l well-represented values for sensitive attributes.
        """
        if not self.config.sensitive_attributes:
            raise ValueError("sensitive_attributes must be specified for l-diversity")

        # First achieve k-anonymity
        k_result = self._apply_k_anonymity(df)
        anonymized_df = k_result.anonymized_data

        # Check l-diversity for each equivalence class
        groups = anonymized_df.groupby(self.config.quasi_identifiers)
        l_diversity_per_attr = {}

        for sensitive_attr in self.config.sensitive_attributes:
            min_diversity = float('inf')

            for group_name, group in groups:
                # Count distinct values in this sensitive attribute
                diversity = group[sensitive_attr].nunique()
                min_diversity = min(min_diversity, diversity)

                # If diversity < l, we need further generalization or suppression
                if diversity < self.config.l_diversity:
                    # Suppress this group (in production, would generalize further)
                    anonymized_df = anonymized_df.drop(group.index)
                    k_result.suppressed_records += len(group)

            l_diversity_per_attr[sensitive_attr] = min_diversity

        return AnonymizationResult(
            anonymized_data=anonymized_df,
            suppressed_records=k_result.suppressed_records,
            generalization_levels=k_result.generalization_levels,
            k_anonymity_achieved=k_result.k_anonymity_achieved,
            l_diversity_achieved=l_diversity_per_attr,
            information_loss=k_result.information_loss
        )

    def _apply_masking(self, df: pd.DataFrame) -> AnonymizationResult:
        """
        Apply pattern-based data masking.

        Masking patterns:
        - Email: user@domain.com -> u***@domain.com
        - Phone: (555) 123-4567 -> ***-***-4567
        - SSN: 123-45-6789 -> ***-**-6789
        - Credit Card: 4532-1234-5678-9010 -> ****-****-****-9010
        """
        anonymized_df = df.copy()

        for column, pattern in (self.config.masking_patterns or {}).items():
            if column not in df.columns:
                continue

            if pattern == 'email':
                anonymized_df[column] = anonymized_df[column].apply(
                    self._mask_email
                )
            elif pattern == 'phone':
                anonymized_df[column] = anonymized_df[column].apply(
                    self._mask_phone
                )
            elif pattern == 'ssn':
                anonymized_df[column] = anonymized_df[column].apply(
                    self._mask_ssn
                )
            elif pattern == 'credit_card':
                anonymized_df[column] = anonymized_df[column].apply(
                    self._mask_credit_card
                )
            elif pattern == 'partial':
                # Generic partial masking (show first 2 and last 2 chars)
                anonymized_df[column] = anonymized_df[column].apply(
                    lambda x: f"{str(x)[:2]}***{str(x)[-2:]}" if pd.notna(x) else x
                )

        return AnonymizationResult(
            anonymized_data=anonymized_df,
            suppressed_records=0,
            generalization_levels={},
            k_anonymity_achieved=0,
            l_diversity_achieved={},
            information_loss=0.3  # Masking has moderate information loss
        )

    def _apply_pseudonymization(self, df: pd.DataFrame) -> AnonymizationResult:
        """
        Apply irreversible pseudonymization using cryptographic hashing.

        Uses SHA-256 with salt for one-way transformation. This is
        GDPR-compliant pseudonymization (Article 4(5)).
        """
        anonymized_df = df.copy()

        # Pseudonymize quasi-identifiers
        for column in (self.config.quasi_identifiers or []):
            if column not in df.columns:
                continue

            anonymized_df[column] = anonymized_df[column].apply(
                lambda x: self._pseudonymize_value(str(x)) if pd.notna(x) else x
            )

        return AnonymizationResult(
            anonymized_data=anonymized_df,
            suppressed_records=0,
            generalization_levels={},
            k_anonymity_achieved=0,
            l_diversity_achieved={},
            information_loss=1.0  # Full information loss (irreversible)
        )

    def tokenize(self, value: str) -> str:
        """
        Tokenize value for reversible anonymization.

        Unlike pseudonymization, tokenization is reversible using the
        token vault. Use for scenarios requiring de-anonymization.
        """
        if value in self.token_vault:
            return self.token_vault[value]

        # Generate cryptographically secure token
        token = secrets.token_urlsafe(16)
        self.token_vault[value] = token
        self.reverse_vault[token] = value

        return token

    def detokenize(self, token: str) -> Optional[str]:
        """Reverse tokenization to recover original value."""
        return self.reverse_vault.get(token)

    # Helper methods

    def _select_qi_to_generalize(self, df: pd.DataFrame,
                                 quasi_identifiers: List[str]) -> str:
        """Select quasi-identifier with highest cardinality to generalize."""
        cardinalities = {qi: df[qi].nunique() for qi in quasi_identifiers}
        return max(cardinalities, key=cardinalities.get)

    def _generalize_column(self, df: pd.DataFrame, column: str) -> pd.DataFrame:
        """
        Generalize column values to reduce granularity.

        Examples:
        - Age: 25 -> 20-30 -> 20-40
        - Zip: 12345 -> 1234* -> 123** -> 12***
        - Date: 2024-01-15 -> 2024-01 -> 2024-Q1 -> 2024
        """
        result_df = df.copy()

        if pd.api.types.is_numeric_dtype(df[column]):
            # Numeric generalization: create bins
            result_df[column] = pd.cut(df[column], bins=5, labels=False)
        elif pd.api.types.is_datetime64_any_dtype(df[column]):
            # Date generalization: reduce to month
            result_df[column] = pd.to_datetime(df[column]).dt.to_period('M')
        else:
            # String generalization: truncate
            result_df[column] = result_df[column].astype(str).str[:3] + '***'

        return result_df

    def _calculate_information_loss(self, original_df: pd.DataFrame,
                                    anonymized_df: pd.DataFrame,
                                    generalization_levels: Dict[str, int]) -> float:
        """
        Calculate information loss metric (0 = no loss, 1 = complete loss).

        Factors:
        - Record suppression rate
        - Average generalization level
        - Unique value reduction
        """
        # Suppression loss
        suppression_loss = 1 - (len(anonymized_df) / len(original_df))

        # Generalization loss
        avg_generalization = (sum(generalization_levels.values()) /
                             len(generalization_levels) if generalization_levels else 0)
        generalization_loss = min(avg_generalization / 5, 1.0)  # Normalize

        # Unique value loss
        unique_loss = 0
        for column in generalization_levels.keys():
            if column in original_df.columns and column in anonymized_df.columns:
                original_unique = original_df[column].nunique()
                anonymized_unique = anonymized_df[column].nunique()
                unique_loss += 1 - (anonymized_unique / original_unique)

        unique_loss = unique_loss / len(generalization_levels) if generalization_levels else 0

        # Weighted average
        return 0.3 * suppression_loss + 0.4 * generalization_loss + 0.3 * unique_loss

    def _mask_email(self, email: str) -> str:
        """Mask email: user@domain.com -> u***@domain.com"""
        if pd.isna(email) or '@' not in str(email):
            return email

        username, domain = str(email).split('@', 1)
        masked_username = username[0] + '***' if username else '***'
        return f"{masked_username}@{domain}"

    def _mask_phone(self, phone: str) -> str:
        """Mask phone: (555) 123-4567 -> ***-***-4567"""
        if pd.isna(phone):
            return phone

        # Extract last 4 digits
        digits = re.sub(r'\D', '', str(phone))
        if len(digits) >= 4:
            return f"***-***-{digits[-4:]}"
        return '***-***-****'

    def _mask_ssn(self, ssn: str) -> str:
        """Mask SSN: 123-45-6789 -> ***-**-6789"""
        if pd.isna(ssn):
            return ssn

        # Extract last 4 digits
        digits = re.sub(r'\D', '', str(ssn))
        if len(digits) >= 4:
            return f"***-**-{digits[-4:]}"
        return '***-**-****'

    def _mask_credit_card(self, cc: str) -> str:
        """Mask credit card: 4532-1234-5678-9010 -> ****-****-****-9010"""
        if pd.isna(cc):
            return cc

        # Extract last 4 digits
        digits = re.sub(r'\D', '', str(cc))
        if len(digits) >= 4:
            return f"****-****-****-{digits[-4:]}"
        return '****-****-****-****'

    def _pseudonymize_value(self, value: str) -> str:
        """Generate irreversible pseudonym using SHA-256."""
        # Use HMAC with application-specific secret key
        secret_key = b'application_secret_key_change_in_production'
        hash_obj = hashlib.sha256(secret_key + value.encode('utf-8'))
        return hash_obj.hexdigest()[:16]  # Use first 16 chars for readability

# Example usage
df = pd.DataFrame({
    'user_id': range(1000),
    'age': np.random.randint(18, 80, 1000),
    'zip_code': np.random.randint(10000, 99999, 1000),
    'email': [f'user{i}@example.com' for i in range(1000)],
    'phone': [f'555-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}'
              for i in range(1000)],
    'diagnosis': np.random.choice(['diabetes', 'hypertension', 'asthma', 'healthy'],
                                  1000)
})

# K-anonymity configuration
config = AnonymizationConfig(
    k_anonymity=5,
    l_diversity=3,
    quasi_identifiers=['age', 'zip_code'],
    sensitive_attributes=['diagnosis'],
    masking_patterns={
        'email': 'email',
        'phone': 'phone'
    }
)

anonymizer = DataAnonymizer(config)

# Apply k-anonymity
k_result = anonymizer.anonymize_dataset(df, method='k-anonymity')
print(f"K-anonymity: {k_result.k_anonymity_achieved}")
print(f"Suppressed: {k_result.suppressed_records} records")
print(f"Information loss: {k_result.information_loss:.2%}")

# Apply l-diversity
l_result = anonymizer.anonymize_dataset(df, method='l-diversity')
print(f"L-diversity achieved: {l_result.l_diversity_achieved}")

# Apply masking
mask_result = anonymizer.anonymize_dataset(df, method='masking')
print(f"Masked data sample:")
print(mask_result.anonymized_data[['email', 'phone']].head())

# Tokenization (reversible)
token = anonymizer.tokenize('sensitive_user_id_12345')
original = anonymizer.detokenize(token)
print(f"Token: {token}, Detokenized: {original}")
\end{lstlisting}

\textbf{Key anonymization trade-offs:}

\begin{itemize}
    \item \textbf{K-anonymity}: Prevents identity disclosure but vulnerable to homogeneity attack (if all k records have same sensitive value) and background knowledge attack
    \item \textbf{L-diversity}: Prevents homogeneity attack by ensuring diversity, but may still leak information through skewness attack
    \item \textbf{Masking}: Fast and preserves format, but provides limited privacy guarantee (pattern inference possible)
    \item \textbf{Pseudonymization}: GDPR-compliant and irreversible, but prevents longitudinal analysis unless consistent hashing used
    \item \textbf{Tokenization}: Reversible for authorized users, but requires secure token vault management
\end{itemize}

\subsubsection{Cost Optimization: Automated Resource Scaling}

Cloud data pipelines can quickly become expensive without proper cost management. A single misconfigured Spark cluster running 24/7 can cost \$50K/month. Cost optimization requires balancing performance requirements, resource utilization, and budget constraints through automated scaling, workload scheduling, and resource right-sizing.

\begin{lstlisting}[language=Python, caption=CostOptimizer with Automated Resource Scaling]
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from enum import Enum
import numpy as np

class InstanceType(Enum):
    """Cloud instance types with cost and performance characteristics."""
    SPOT_SMALL = ("spot-small", 0.05, 2, 8)      # $/hour, vCPU, RAM GB
    SPOT_MEDIUM = ("spot-medium", 0.15, 4, 16)
    SPOT_LARGE = ("spot-large", 0.40, 8, 32)
    ON_DEMAND_SMALL = ("on-demand-small", 0.10, 2, 8)
    ON_DEMAND_MEDIUM = ("on-demand-medium", 0.30, 4, 16)
    ON_DEMAND_LARGE = ("on-demand-large", 0.80, 8, 32)
    RESERVED_MEDIUM = ("reserved-medium", 0.18, 4, 16)  # 40% discount
    RESERVED_LARGE = ("reserved-large", 0.48, 8, 32)

    def __init__(self, name: str, cost_per_hour: float,
                 vcpu: int, ram_gb: int):
        self.instance_name = name
        self.cost_per_hour = cost_per_hour
        self.vcpu = vcpu
        self.ram_gb = ram_gb

@dataclass
class WorkloadProfile:
    """Historical workload characteristics."""
    avg_cpu_utilization: float  # 0-1
    avg_memory_utilization: float  # 0-1
    avg_duration_minutes: float
    peak_hours: List[int]  # Hours of day (0-23)
    daily_runs: int
    priority: str  # 'critical', 'high', 'medium', 'low'

@dataclass
class CostReport:
    """Cost analysis report."""
    current_monthly_cost: float
    optimized_monthly_cost: float
    savings: float
    savings_percent: float
    recommendations: List[str]
    instance_allocation: Dict[str, int]

class CostOptimizer:
    """
    Automated cost optimization with resource scaling and scheduling.

    Optimization strategies:
    1. Right-sizing: Match instance types to workload requirements
    2. Spot instances: Use spot for fault-tolerant batch jobs (70% savings)
    3. Reserved instances: Pre-purchase capacity for predictable workloads
    4. Auto-scaling: Scale resources based on demand
    5. Idle resource detection: Terminate unused resources
    """

    def __init__(self, monthly_budget: float, target_utilization: float = 0.75):
        self.monthly_budget = monthly_budget
        self.target_utilization = target_utilization
        self.cost_history: List[Dict[str, Any]] = []
        self.active_instances: Dict[str, List[str]] = {}  # instance_type -> [ids]
        self.workload_profiles: Dict[str, WorkloadProfile] = {}

    def analyze_costs(self, current_usage: Dict[str, Any]) -> CostReport:
        """
        Analyze current costs and generate optimization recommendations.

        Args:
            current_usage: {
                'instances': {instance_type: count},
                'avg_utilization': {'cpu': float, 'memory': float},
                'workload_distribution': {hour: workload_count}
            }

        Returns:
            CostReport with savings recommendations
        """
        # Calculate current monthly cost
        current_cost = self._calculate_monthly_cost(current_usage['instances'])

        # Generate optimization recommendations
        recommendations = []
        optimized_instances = current_usage['instances'].copy()

        # 1. Right-sizing analysis
        if current_usage['avg_utilization']['cpu'] < 0.3:
            # Under-utilized, recommend downsizing
            recommendations.append(
                f"CPU utilization is {current_usage['avg_utilization']['cpu']:.0%}. "
                f"Consider downsizing instances to save ~30%"
            )
            # Simulate downsizing
            optimized_instances = self._downsize_instances(optimized_instances)

        # 2. Spot instance opportunities
        spot_eligible = self._identify_spot_eligible_workloads()
        if spot_eligible:
            spot_savings = sum(w.avg_duration_minutes / 60 * 0.15 * 30
                              for w in spot_eligible.values())
            recommendations.append(
                f"Convert {len(spot_eligible)} workloads to spot instances "
                f"for ~${spot_savings:.0f}/month savings (70% discount)"
            )
            optimized_instances = self._convert_to_spot(
                optimized_instances, len(spot_eligible)
            )

        # 3. Reserved instance recommendations
        reserved_candidates = self._identify_reserved_candidates(current_usage)
        if reserved_candidates:
            reserved_savings = reserved_candidates * 0.30 * 0.40  # 40% discount
            recommendations.append(
                f"Purchase {reserved_candidates} reserved instances "
                f"for ~${reserved_savings:.0f}/month savings (40% discount)"
            )
            optimized_instances = self._convert_to_reserved(
                optimized_instances, reserved_candidates
            )

        # 4. Idle resource detection
        idle_cost = self._detect_idle_resources(current_usage)
        if idle_cost > 0:
            recommendations.append(
                f"Terminate idle resources costing ${idle_cost:.0f}/month"
            )

        # 5. Auto-scaling recommendations
        peak_variance = self._calculate_workload_variance(current_usage)
        if peak_variance > 0.3:
            recommendations.append(
                f"Workload variance is {peak_variance:.0%}. "
                f"Enable auto-scaling to save ~20% during off-peak hours"
            )

        # Calculate optimized cost
        optimized_cost = self._calculate_monthly_cost(optimized_instances)
        optimized_cost *= 0.8 if peak_variance > 0.3 else 1.0  # Auto-scaling
        optimized_cost -= idle_cost  # Remove idle resources

        savings = current_cost - optimized_cost
        savings_percent = (savings / current_cost) * 100 if current_cost > 0 else 0

        return CostReport(
            current_monthly_cost=current_cost,
            optimized_monthly_cost=optimized_cost,
            savings=savings,
            savings_percent=savings_percent,
            recommendations=recommendations,
            instance_allocation=optimized_instances
        )

    def auto_scale(self, workload_demand: float,
                  current_instances: int) -> int:
        """
        Calculate optimal instance count based on workload demand.

        Args:
            workload_demand: Current workload demand (0-1 scale)
            current_instances: Current number of instances

        Returns:
            Recommended instance count
        """
        # Calculate required instances for target utilization
        required_instances = int(np.ceil(
            (workload_demand * current_instances) / self.target_utilization
        ))

        # Apply scaling constraints
        min_instances = max(1, int(current_instances * 0.5))  # Never scale below 50%
        max_instances = int(current_instances * 2.0)  # Never double capacity instantly

        scaled_instances = np.clip(required_instances, min_instances, max_instances)

        # Add scaling decision to history
        self.cost_history.append({
            'timestamp': datetime.now(),
            'action': 'scale',
            'from': current_instances,
            'to': scaled_instances,
            'demand': workload_demand
        })

        return scaled_instances

    def recommend_instance_type(self, workload: WorkloadProfile) -> InstanceType:
        """
        Recommend optimal instance type for workload.

        Decision factors:
        - CPU/memory requirements
        - Priority (critical workloads avoid spot)
        - Duration (long-running jobs prefer reserved)
        - Fault tolerance (batch jobs can use spot)
        """
        # Calculate required resources
        required_vcpu = workload.avg_cpu_utilization * 4  # Assume baseline 4 vCPU
        required_ram = workload.avg_memory_utilization * 16  # Assume baseline 16 GB

        # Filter suitable instance types
        suitable_types = [
            inst_type for inst_type in InstanceType
            if inst_type.vcpu >= required_vcpu and inst_type.ram_gb >= required_ram
        ]

        if not suitable_types:
            # Need largest instance
            return InstanceType.RESERVED_LARGE

        # Decision logic
        if workload.priority == 'critical':
            # Critical workloads: use on-demand or reserved
            reserved_options = [t for t in suitable_types if 'reserved' in t.instance_name]
            if reserved_options:
                return min(reserved_options, key=lambda t: t.cost_per_hour)
            return min([t for t in suitable_types if 'on-demand' in t.instance_name],
                      key=lambda t: t.cost_per_hour)

        elif workload.priority in ['high', 'medium']:
            # High/medium priority: prefer reserved for cost savings
            reserved_options = [t for t in suitable_types if 'reserved' in t.instance_name]
            if reserved_options and workload.daily_runs >= 3:
                return min(reserved_options, key=lambda t: t.cost_per_hour)

            # Spot for non-critical batch jobs
            spot_options = [t for t in suitable_types if 'spot' in t.instance_name]
            if spot_options and workload.avg_duration_minutes < 120:
                return min(spot_options, key=lambda t: t.cost_per_hour)

            return min([t for t in suitable_types if 'on-demand' in t.instance_name],
                      key=lambda t: t.cost_per_hour)

        else:  # Low priority
            # Low priority: maximize cost savings with spot
            spot_options = [t for t in suitable_types if 'spot' in t.instance_name]
            if spot_options:
                return min(spot_options, key=lambda t: t.cost_per_hour)
            return min(suitable_types, key=lambda t: t.cost_per_hour)

    def track_budget(self, current_spend: float) -> Dict[str, Any]:
        """
        Track spending against budget and generate alerts.

        Returns:
            {
                'budget_utilization': float,
                'projected_monthly_spend': float,
                'alert_level': 'green' | 'yellow' | 'red',
                'recommendations': List[str]
            }
        """
        # Calculate budget utilization
        days_elapsed = datetime.now().day
        days_in_month = 30
        expected_spend = self.monthly_budget * (days_elapsed / days_in_month)

        budget_utilization = current_spend / expected_spend if expected_spend > 0 else 0
        projected_monthly = current_spend * (days_in_month / days_elapsed)

        # Determine alert level
        if budget_utilization > 1.2:
            alert_level = 'red'
            recommendations = [
                "URGENT: Spending 20% over budget pace",
                "Immediately review and terminate non-essential resources",
                "Consider switching to lower-cost instance types",
                "Enable aggressive auto-scaling policies"
            ]
        elif budget_utilization > 1.0:
            alert_level = 'yellow'
            recommendations = [
                "WARNING: Spending over budget pace",
                "Review resource utilization for optimization opportunities",
                "Consider spot instances for batch workloads"
            ]
        else:
            alert_level = 'green'
            recommendations = [
                "Spending is on track with budget"
            ]

        return {
            'budget_utilization': budget_utilization,
            'projected_monthly_spend': projected_monthly,
            'alert_level': alert_level,
            'recommendations': recommendations,
            'budget_remaining': max(0, self.monthly_budget - projected_monthly)
        }

    # Helper methods

    def _calculate_monthly_cost(self, instances: Dict[str, int]) -> float:
        """Calculate monthly cost for instance allocation."""
        total_cost = 0
        for instance_name, count in instances.items():
            # Find matching instance type
            inst_type = next((t for t in InstanceType if t.instance_name == instance_name), None)
            if inst_type:
                # Assume 24/7 operation (720 hours/month)
                total_cost += inst_type.cost_per_hour * 720 * count
        return total_cost

    def _downsize_instances(self, instances: Dict[str, int]) -> Dict[str, int]:
        """Simulate downsizing under-utilized instances."""
        downsized = {}
        for instance_name, count in instances.items():
            if 'large' in instance_name:
                # Downsize large to medium
                medium_name = instance_name.replace('large', 'medium')
                downsized[medium_name] = count
            elif 'medium' in instance_name:
                # Downsize medium to small
                small_name = instance_name.replace('medium', 'small')
                downsized[small_name] = count
            else:
                downsized[instance_name] = count
        return downsized

    def _convert_to_spot(self, instances: Dict[str, int], count: int) -> Dict[str, int]:
        """Convert on-demand instances to spot."""
        converted = instances.copy()
        # Convert on-demand to spot
        for instance_name in list(converted.keys()):
            if 'on-demand' in instance_name and count > 0:
                spot_name = instance_name.replace('on-demand', 'spot')
                converted[spot_name] = converted.get(spot_name, 0) + min(converted[instance_name], count)
                converted[instance_name] = max(0, converted[instance_name] - count)
                count -= min(converted[instance_name], count)
        return converted

    def _convert_to_reserved(self, instances: Dict[str, int], count: int) -> Dict[str, int]:
        """Convert on-demand instances to reserved."""
        converted = instances.copy()
        for instance_name in list(converted.keys()):
            if 'on-demand' in instance_name and count > 0:
                reserved_name = instance_name.replace('on-demand', 'reserved')
                converted[reserved_name] = converted.get(reserved_name, 0) + min(converted[instance_name], count)
                converted[instance_name] = max(0, converted[instance_name] - count)
                count -= min(converted[instance_name], count)
        return converted

    def _identify_spot_eligible_workloads(self) -> Dict[str, WorkloadProfile]:
        """Identify workloads suitable for spot instances."""
        return {
            name: profile for name, profile in self.workload_profiles.items()
            if profile.priority in ['medium', 'low'] and profile.avg_duration_minutes < 180
        }

    def _identify_reserved_candidates(self, current_usage: Dict[str, Any]) -> int:
        """Identify instances suitable for reserved capacity."""
        # Instances running 24/7 are reserved candidates
        total_instances = sum(current_usage['instances'].values())
        # Assume 60% run consistently
        return int(total_instances * 0.6)

    def _detect_idle_resources(self, current_usage: Dict[str, Any]) -> float:
        """Calculate cost of idle resources."""
        avg_cpu = current_usage['avg_utilization']['cpu']
        if avg_cpu < 0.1:
            # Assume 20% of resources are idle
            current_cost = self._calculate_monthly_cost(current_usage['instances'])
            return current_cost * 0.2
        return 0

    def _calculate_workload_variance(self, current_usage: Dict[str, Any]) -> float:
        """Calculate workload variance to assess auto-scaling opportunity."""
        workload_dist = current_usage.get('workload_distribution', {})
        if not workload_dist:
            return 0

        workloads = list(workload_dist.values())
        if len(workloads) < 2:
            return 0

        mean_workload = np.mean(workloads)
        std_workload = np.std(workloads)
        return std_workload / mean_workload if mean_workload > 0 else 0

# Example usage
optimizer = CostOptimizer(monthly_budget=10000, target_utilization=0.75)

# Define workload profiles
optimizer.workload_profiles = {
    'daily_etl': WorkloadProfile(
        avg_cpu_utilization=0.6,
        avg_memory_utilization=0.5,
        avg_duration_minutes=120,
        peak_hours=[2, 3, 4],
        daily_runs=1,
        priority='high'
    ),
    'ml_training': WorkloadProfile(
        avg_cpu_utilization=0.9,
        avg_memory_utilization=0.8,
        avg_duration_minutes=240,
        peak_hours=[8, 9, 10, 11],
        daily_runs=2,
        priority='medium'
    ),
    'batch_reports': WorkloadProfile(
        avg_cpu_utilization=0.4,
        avg_memory_utilization=0.3,
        avg_duration_minutes=30,
        peak_hours=[6, 18],
        daily_runs=2,
        priority='low'
    )
}

# Analyze current usage
current_usage = {
    'instances': {
        'on-demand-medium': 10,
        'on-demand-large': 5
    },
    'avg_utilization': {'cpu': 0.35, 'memory': 0.40},
    'workload_distribution': {hour: 100 if hour in [2, 3, 4, 8, 9, 10] else 20
                             for hour in range(24)}
}

# Get cost report
report = optimizer.analyze_costs(current_usage)
print(f"Current monthly cost: ${report.current_monthly_cost:,.2f}")
print(f"Optimized monthly cost: ${report.optimized_monthly_cost:,.2f}")
print(f"Savings: ${report.savings:,.2f} ({report.savings_percent:.1f}%)")
print(f"\nRecommendations:")
for i, rec in enumerate(report.recommendations, 1):
    print(f"  {i}. {rec}")

# Instance type recommendations
for workload_name, profile in optimizer.workload_profiles.items():
    recommended = optimizer.recommend_instance_type(profile)
    print(f"\n{workload_name}: {recommended.instance_name} "
          f"(${recommended.cost_per_hour}/hr)")

# Budget tracking
budget_status = optimizer.track_budget(current_spend=3500)
print(f"\nBudget Status: {budget_status['alert_level'].upper()}")
print(f"Budget utilization: {budget_status['budget_utilization']:.0%}")
print(f"Projected monthly: ${budget_status['projected_monthly_spend']:,.2f}")
\end{lstlisting}

\subsubsection{Resource Scheduling: Cost-Aware Task Allocation}

Efficient resource scheduling balances competing objectives: minimize costs, meet SLAs, maximize throughput, and ensure fairness. A naive scheduler might run all jobs on the most powerful instances, wasting money. A cost-obsessed scheduler might use only spot instances, causing SLA violations. The solution is intelligent, cost-aware scheduling that considers workload characteristics, resource availability, and business priorities.

\begin{lstlisting}[language=Python, caption=ResourceScheduler with Cost-Aware Allocation]
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from enum import Enum
import heapq
from collections import defaultdict

class SchedulingPolicy(Enum):
    """Resource scheduling policies."""
    FIFO = "fifo"  # First-in-first-out
    PRIORITY = "priority"  # Priority-based
    FAIR_SHARE = "fair_share"  # Fair resource distribution
    COST_OPTIMIZED = "cost_optimized"  # Minimize costs
    DEADLINE_AWARE = "deadline_aware"  # Meet deadlines

@dataclass
class Task:
    """Task to be scheduled."""
    task_id: str
    priority: int  # 1-10, higher is more important
    required_vcpu: int
    required_memory_gb: int
    estimated_duration_minutes: int
    deadline: Optional[datetime] = None
    max_cost_per_hour: Optional[float] = None
    can_use_spot: bool = True
    submitted_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    assigned_instance: Optional[str] = None

    def __lt__(self, other):
        """For priority queue ordering."""
        # Higher priority comes first, then earlier submission
        if self.priority != other.priority:
            return self.priority > other.priority
        return self.submitted_at < other.submitted_at

@dataclass
class ResourcePool:
    """Pool of compute resources."""
    pool_name: str
    instance_type: InstanceType
    capacity: int  # Max instances
    current_usage: int = 0
    reserved_for_priority: int = 0  # Reserved for high-priority tasks

    @property
    def available_capacity(self) -> int:
        return self.capacity - self.current_usage

@dataclass
class SchedulingDecision:
    """Result of scheduling decision."""
    task: Task
    assigned_pool: str
    assigned_instance_type: str
    estimated_cost: float
    scheduling_delay_seconds: float
    reason: str

class ResourceScheduler:
    """
    Cost-aware resource scheduler with multiple scheduling policies.

    Features:
    1. Priority-based scheduling with resource reservation
    2. Fair-share resource allocation across teams/projects
    3. Cost-optimized placement (prefer spot, right-size instances)
    4. Deadline-aware scheduling with urgency calculation
    5. Resource pooling and bin-packing for efficiency
    """

    def __init__(self, policy: SchedulingPolicy = SchedulingPolicy.COST_OPTIMIZED):
        self.policy = policy
        self.resource_pools: Dict[str, ResourcePool] = {}
        self.task_queue: List[Task] = []  # Priority queue
        self.running_tasks: Dict[str, Task] = {}
        self.completed_tasks: List[Task] = []
        self.team_usage: Dict[str, float] = defaultdict(float)  # Fair-share tracking
        self.scheduling_history: List[SchedulingDecision] = []

    def add_resource_pool(self, pool: ResourcePool):
        """Add a resource pool to the scheduler."""
        self.resource_pools[pool.pool_name] = pool

    def submit_task(self, task: Task):
        """Submit a task for scheduling."""
        heapq.heappush(self.task_queue, task)

    def schedule_next_task(self) -> Optional[SchedulingDecision]:
        """
        Schedule the next task based on current policy.

        Returns:
            SchedulingDecision if task was scheduled, None if no resources available
        """
        if not self.task_queue:
            return None

        # Select next task based on policy
        if self.policy == SchedulingPolicy.FIFO:
            # Remove priority sorting, use submission time
            self.task_queue.sort(key=lambda t: t.submitted_at)
            task = heapq.heappop(self.task_queue)
        elif self.policy == SchedulingPolicy.PRIORITY:
            task = heapq.heappop(self.task_queue)  # Already priority-ordered
        elif self.policy == SchedulingPolicy.DEADLINE_AWARE:
            task = self._select_most_urgent_task()
        elif self.policy == SchedulingPolicy.COST_OPTIMIZED:
            task = self._select_cost_optimized_task()
        elif self.policy == SchedulingPolicy.FAIR_SHARE:
            task = self._select_fair_share_task()
        else:
            task = heapq.heappop(self.task_queue)

        # Find suitable resource pool
        pool = self._find_suitable_pool(task)
        if not pool:
            # No resources available, re-queue task
            heapq.heappush(self.task_queue, task)
            return None

        # Allocate resources
        pool.current_usage += 1
        task.started_at = datetime.now()
        task.assigned_instance = f"{pool.pool_name}-instance-{pool.current_usage}"
        self.running_tasks[task.task_id] = task

        # Calculate cost
        estimated_cost = (task.estimated_duration_minutes / 60) * pool.instance_type.cost_per_hour

        # Record scheduling decision
        scheduling_delay = (task.started_at - task.submitted_at).total_seconds()
        decision = SchedulingDecision(
            task=task,
            assigned_pool=pool.pool_name,
            assigned_instance_type=pool.instance_type.instance_name,
            estimated_cost=estimated_cost,
            scheduling_delay_seconds=scheduling_delay,
            reason=f"Scheduled using {self.policy.value} policy"
        )
        self.scheduling_history.append(decision)

        return decision

    def complete_task(self, task_id: str):
        """Mark a task as completed and free resources."""
        if task_id not in self.running_tasks:
            return

        task = self.running_tasks.pop(task_id)
        task.completed_at = datetime.now()
        self.completed_tasks.append(task)

        # Free resources
        for pool in self.resource_pools.values():
            if task.assigned_instance and pool.pool_name in task.assigned_instance:
                pool.current_usage = max(0, pool.current_usage - 1)
                break

    def _find_suitable_pool(self, task: Task) -> Optional[ResourcePool]:
        """
        Find the most suitable resource pool for a task.

        Selection criteria (in order):
        1. Meets resource requirements (vCPU, memory)
        2. Respects cost constraints
        3. Spot eligibility
        4. Cost (prefer cheaper instances)
        5. Availability (has capacity)
        """
        suitable_pools = []

        for pool in self.resource_pools.values():
            # Check capacity
            if pool.available_capacity <= 0:
                continue

            # Check resource requirements
            if (pool.instance_type.vcpu < task.required_vcpu or
                pool.instance_type.ram_gb < task.required_memory_gb):
                continue

            # Check cost constraints
            if (task.max_cost_per_hour and
                pool.instance_type.cost_per_hour > task.max_cost_per_hour):
                continue

            # Check spot eligibility
            if not task.can_use_spot and 'spot' in pool.instance_type.instance_name:
                continue

            # Check priority reservation
            if task.priority < 8 and pool.current_usage >= (pool.capacity - pool.reserved_for_priority):
                # Low priority task, but high-priority capacity is reserved
                continue

            suitable_pools.append(pool)

        if not suitable_pools:
            return None

        # Sort by cost (prefer cheaper instances for cost optimization)
        if self.policy == SchedulingPolicy.COST_OPTIMIZED:
            suitable_pools.sort(key=lambda p: p.instance_type.cost_per_hour)
        else:
            # For other policies, prefer pools with more available capacity
            suitable_pools.sort(key=lambda p: p.available_capacity, reverse=True)

        return suitable_pools[0]

    def _select_most_urgent_task(self) -> Task:
        """Select task with most urgent deadline."""
        if not self.task_queue:
            return None

        # Calculate urgency score for each task
        now = datetime.now()
        urgency_scores = []

        for task in self.task_queue:
            if task.deadline:
                time_to_deadline = (task.deadline - now).total_seconds() / 60  # minutes
                urgency = task.priority * (1.0 / max(time_to_deadline, 1))
            else:
                # No deadline, use priority only
                urgency = task.priority * 0.01

            urgency_scores.append((urgency, task))

        # Select most urgent
        urgency_scores.sort(reverse=True)
        selected_task = urgency_scores[0][1]

        # Remove from queue
        self.task_queue.remove(selected_task)
        heapq.heapify(self.task_queue)

        return selected_task

    def _select_cost_optimized_task(self) -> Task:
        """Select task that can be scheduled most cost-effectively."""
        if not self.task_queue:
            return None

        # Find task that best fits available cheap resources
        best_task = None
        best_cost = float('inf')

        for task in self.task_queue:
            # Find cheapest pool that can run this task
            pool = self._find_suitable_pool(task)
            if pool:
                cost = (task.estimated_duration_minutes / 60) * pool.instance_type.cost_per_hour
                if cost < best_cost:
                    best_cost = cost
                    best_task = task

        if best_task:
            self.task_queue.remove(best_task)
            heapq.heapify(self.task_queue)
            return best_task

        # Fallback to priority-based
        return heapq.heappop(self.task_queue)

    def _select_fair_share_task(self) -> Task:
        """Select task from team with lowest resource usage."""
        # In production, would track team ownership
        # For simplicity, use priority as proxy
        return heapq.heappop(self.task_queue)

    def get_scheduling_metrics(self) -> Dict[str, Any]:
        """Calculate scheduling performance metrics."""
        if not self.scheduling_history:
            return {}

        total_cost = sum(d.estimated_cost for d in self.scheduling_history)
        avg_delay = sum(d.scheduling_delay_seconds for d in self.scheduling_history) / len(self.scheduling_history)

        # Resource utilization
        total_capacity = sum(p.capacity for p in self.resource_pools.values())
        total_usage = sum(p.current_usage for p in self.resource_pools.values())
        utilization = total_usage / total_capacity if total_capacity > 0 else 0

        # Tasks by priority
        priority_distribution = defaultdict(int)
        for decision in self.scheduling_history:
            priority_distribution[decision.task.priority] += 1

        return {
            'total_tasks_scheduled': len(self.scheduling_history),
            'total_estimated_cost': total_cost,
            'avg_scheduling_delay_seconds': avg_delay,
            'resource_utilization': utilization,
            'tasks_by_priority': dict(priority_distribution),
            'queue_length': len(self.task_queue),
            'running_tasks': len(self.running_tasks),
            'completed_tasks': len(self.completed_tasks)
        }

# Example usage
scheduler = ResourceScheduler(policy=SchedulingPolicy.COST_OPTIMIZED)

# Create resource pools
scheduler.add_resource_pool(ResourcePool(
    pool_name="spot_small_pool",
    instance_type=InstanceType.SPOT_SMALL,
    capacity=10,
    reserved_for_priority=2
))

scheduler.add_resource_pool(ResourcePool(
    pool_name="spot_medium_pool",
    instance_type=InstanceType.SPOT_MEDIUM,
    capacity=5,
    reserved_for_priority=1
))

scheduler.add_resource_pool(ResourcePool(
    pool_name="on_demand_medium_pool",
    instance_type=InstanceType.ON_DEMAND_MEDIUM,
    capacity=3,
    reserved_for_priority=2
))

scheduler.add_resource_pool(ResourcePool(
    pool_name="reserved_large_pool",
    instance_type=InstanceType.RESERVED_LARGE,
    capacity=2,
    reserved_for_priority=2
))

# Submit tasks with different characteristics
tasks = [
    Task(
        task_id="critical_etl",
        priority=10,
        required_vcpu=4,
        required_memory_gb=16,
        estimated_duration_minutes=120,
        deadline=datetime.now() + timedelta(hours=2),
        can_use_spot=False  # Critical, needs reliability
    ),
    Task(
        task_id="ml_training",
        priority=7,
        required_vcpu=8,
        required_memory_gb=32,
        estimated_duration_minutes=240,
        max_cost_per_hour=0.50,
        can_use_spot=True
    ),
    Task(
        task_id="batch_report_1",
        priority=3,
        required_vcpu=2,
        required_memory_gb=8,
        estimated_duration_minutes=30,
        can_use_spot=True
    ),
    Task(
        task_id="batch_report_2",
        priority=3,
        required_vcpu=2,
        required_memory_gb=8,
        estimated_duration_minutes=30,
        can_use_spot=True
    ),
    Task(
        task_id="data_validation",
        priority=5,
        required_vcpu=2,
        required_memory_gb=8,
        estimated_duration_minutes=15,
        deadline=datetime.now() + timedelta(minutes=30),
        can_use_spot=True
    )
]

# Submit all tasks
for task in tasks:
    scheduler.submit_task(task)

# Schedule tasks
print("Scheduling tasks with COST_OPTIMIZED policy:\n")
while scheduler.task_queue:
    decision = scheduler.schedule_next_task()
    if decision:
        print(f"Task: {decision.task.task_id}")
        print(f"  Priority: {decision.task.priority}")
        print(f"  Assigned to: {decision.assigned_instance_type}")
        print(f"  Estimated cost: ${decision.estimated_cost:.2f}")
        print(f"  Scheduling delay: {decision.scheduling_delay_seconds:.1f}s")
        print(f"  Reason: {decision.reason}\n")

        # Simulate task completion for demo
        scheduler.complete_task(decision.task.task_id)
    else:
        print("No available resources, waiting...")
        break

# Print metrics
metrics = scheduler.get_scheduling_metrics()
print("\nScheduling Metrics:")
print(f"  Total tasks scheduled: {metrics['total_tasks_scheduled']}")
print(f"  Total estimated cost: ${metrics['total_estimated_cost']:.2f}")
print(f"  Average scheduling delay: {metrics['avg_scheduling_delay_seconds']:.2f}s")
print(f"  Resource utilization: {metrics['resource_utilization']:.1%}")
print(f"  Tasks by priority: {metrics['tasks_by_priority']}")
\end{lstlisting}

\textbf{Scheduling policy trade-offs:}

\begin{itemize}
    \item \textbf{FIFO}: Simple and fair, but ignores task priorities and deadlines. Low-priority batch jobs can block critical tasks.
    \item \textbf{Priority-based}: Ensures critical tasks run first, but can starve low-priority tasks indefinitely. Requires careful priority assignment.
    \item \textbf{Fair-share}: Prevents resource monopolization by single team/project, but may delay high-priority tasks. Best for multi-tenant environments.
    \item \textbf{Cost-optimized}: Minimizes spend by preferring spot and right-sized instances, but may increase scheduling delays during resource constraints.
    \item \textbf{Deadline-aware}: Maximizes on-time task completion, but may overprovision resources to meet deadlines, increasing costs.
\end{itemize}

\textbf{Production scheduling best practices:}

\begin{enumerate}
    \item \textbf{Resource reservation}: Reserve capacity for high-priority tasks to prevent starvation during peak load
    \item \textbf{Backfilling}: When large task waits for resources, schedule smaller tasks that fit available capacity
    \item \textbf{Preemption}: For deadline-aware scheduling, allow critical tasks to preempt low-priority ones
    \item \textbf{Gang scheduling}: Schedule related tasks together (e.g., distributed training across multiple nodes)
    \item \textbf{Cost caps}: Set per-task and per-team budget limits to prevent runaway spending
\end{enumerate}

\section{Data Mesh Architecture: Domain-Owned Pipelines}

Traditional centralized data platforms create bottlenecks as organizations scale. A single data engineering team becomes responsible for pipelines across marketing, finance, operations, and product domains, each with unique requirements. This creates a dependency cascade: feature requests queue for months, domain expertise gets lost in translation, and the data team drowns in support tickets while domain teams lack autonomy.

\textbf{Data mesh} inverts this model through four foundational principles:

\begin{enumerate}
    \item \textbf{Domain Ownership}: Domains own their analytical data and pipelines as products
    \item \textbf{Data as a Product}: Treat datasets as products with SLAs, quality guarantees, and documentation
    \item \textbf{Self-Serve Infrastructure}: Provide platforms enabling domain autonomy
    \item \textbf{Federated Governance}: Central policies enforced through automated guardrails
\end{enumerate}

\subsection{The Domain Turf War Problem}

Consider a large e-commerce company with centralized data platform:

\textbf{The situation:}
\begin{itemize}
    \item Marketing team wants customer segmentation features (30-day purchase history, engagement scores)
    \item Finance team needs transaction reconciliation data (real-time payment status, refunds)
    \item Operations team requires inventory forecasting features (demand predictions, supplier data)
    \item Product team needs A/B test metrics (conversion rates, feature engagement)
\end{itemize}

\textbf{Central data team problems:}
\begin{itemize}
    \item \textbf{Ticket backlog}: 6-month wait time for new features
    \item \textbf{Domain expertise gap}: Data engineers don't understand marketing attribution models
    \item \textbf{Conflicting requirements}: Finance needs ACID guarantees, Marketing needs fast iteration
    \item \textbf{Quality issues}: No one owns data quality end-to-end
    \item \textbf{Breaking changes}: Updating schema for Marketing breaks Finance dashboards
    \item \textbf{Support burden}: 60\% of data team time spent on support tickets
\end{itemize}

\textbf{Domain turf war erupts:}
\begin{itemize}
    \item Marketing builds shadow pipelines in spreadsheets (ungoverned, unreliable)
    \item Finance duplicates data to their own database (data inconsistency)
    \item Operations buys third-party tool (siloed data, high cost)
    \item Product builds custom pipeline (reinventing the wheel)
\end{itemize}

\textbf{Result:}
\begin{itemize}
    \item 4 different "sources of truth" for customer data
    \item \$500K/year in duplicate infrastructure
    \item Data quality incidents costing \$2M annually
    \item 12-week average time to answer cross-domain questions
\end{itemize}

\textbf{Data mesh solution:}
\begin{itemize}
    \item Each domain owns their analytical data as products
    \item Marketing owns "Customer 360" data product with SLA guarantees
    \item Finance owns "Transaction Ledger" data product with ACID compliance
    \item Central platform provides tools (Kafka, schema registry, monitoring)
    \item Federated governance ensures privacy, security, and quality standards
    \item Domains discover and consume each other's data products through catalog
\end{itemize}

\subsection{Data Mesh Core Components}

\subsubsection{DataDomain: Domain Ownership Model}

\begin{lstlisting}[language=Python, caption={Data Domain with Ownership and Governance}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from enum import Enum
from datetime import datetime

class DomainType(Enum):
    """Types of data domains."""
    OPERATIONAL = "operational"  # Core business operations
    ANALYTICAL = "analytical"    # Analytics and ML
    SUPPORTING = "supporting"    # Shared services
    REGULATORY = "regulatory"    # Compliance and legal

@dataclass
class DomainOwnership:
    """
    Domain ownership information.

    Attributes:
        domain_owner: Primary owner (tech lead)
        product_owner: Business stakeholder
        engineering_team: Team members
        slack_channel: Support channel
        oncall_rotation: Oncall schedule
    """
    domain_owner: str
    product_owner: str
    engineering_team: List[str]
    slack_channel: str
    oncall_rotation: Optional[str] = None

@dataclass
class DomainBoundary:
    """
    Domain boundary definition.

    Attributes:
        entities: Core entities owned by domain
        events: Event types domain produces
        upstream_dependencies: Domains this depends on
        downstream_consumers: Domains that consume from this
        bounded_context: DDD bounded context description
    """
    entities: List[str]
    events: List[str]
    upstream_dependencies: Set[str] = field(default_factory=set)
    downstream_consumers: Set[str] = field(default_factory=set)
    bounded_context: str = ""

class DataDomain:
    """
    Data domain with ownership and governance.

    Represents a bounded context owning data products
    and pipelines within a specific business capability.

    Example:
        >>> marketing = DataDomain(
        ...     name="marketing",
        ...     description="Customer engagement and campaigns",
        ...     ownership=DomainOwnership(
        ...         domain_owner="alice@company.com",
        ...         product_owner="bob@company.com",
        ...         engineering_team=["alice", "charlie"]
        ...     )
        ... )
        >>> marketing.register_data_product(customer_360)
    """

    def __init__(
        self,
        name: str,
        description: str,
        domain_type: DomainType,
        ownership: DomainOwnership,
        boundary: DomainBoundary
    ):
        """
        Initialize data domain.

        Args:
            name: Domain name (e.g., "marketing", "finance")
            description: Domain description
            domain_type: Type of domain
            ownership: Ownership information
            boundary: Domain boundary definition
        """
        self.name = name
        self.description = description
        self.domain_type = domain_type
        self.ownership = ownership
        self.boundary = boundary

        # Data products owned by this domain
        self.data_products: Dict[str, 'DataProduct'] = {}

        # Governance policies
        self.policies: List['GovernancePolicy'] = []

        # Metrics
        self.metrics = {
            'products_count': 0,
            'consumers_count': 0,
            'quality_score': 0.0
        }

        logger.info(f"Initialized DataDomain: {name} ({domain_type.value})")

    def register_data_product(self, product: 'DataProduct'):
        """
        Register data product owned by this domain.

        Args:
            product: DataProduct to register

        Raises:
            ValueError: If product already exists
        """
        if product.name in self.data_products:
            raise ValueError(f"Product {product.name} already exists in {self.name}")

        product.domain = self.name
        self.data_products[product.name] = product
        self.metrics['products_count'] = len(self.data_products)

        logger.info(
            f"Registered data product: {product.name} "
            f"in domain {self.name}"
        )

    def add_policy(self, policy: 'GovernancePolicy'):
        """
        Add governance policy to domain.

        Args:
            policy: Governance policy
        """
        self.policies.append(policy)
        logger.info(f"Added policy {policy.name} to domain {self.name}")

    def validate_against_policies(self, data_product: 'DataProduct') -> bool:
        """
        Validate data product against domain policies.

        Args:
            data_product: Product to validate

        Returns:
            True if passes all policies
        """
        for policy in self.policies:
            if not policy.validate(data_product):
                logger.error(
                    f"Product {data_product.name} failed policy {policy.name}"
                )
                return False

        return True

    def get_product(self, product_name: str) -> Optional['DataProduct']:
        """
        Get data product by name.

        Args:
            product_name: Product name

        Returns:
            DataProduct or None
        """
        return self.data_products.get(product_name)

    def list_products(self) -> List['DataProduct']:
        """
        List all data products in domain.

        Returns:
            List of DataProducts
        """
        return list(self.data_products.values())

    def add_upstream_dependency(self, upstream_domain: str):
        """
        Declare upstream dependency on another domain.

        Args:
            upstream_domain: Name of upstream domain
        """
        self.boundary.upstream_dependencies.add(upstream_domain)
        logger.info(f"Added upstream dependency: {self.name} <- {upstream_domain}")

    def add_downstream_consumer(self, downstream_domain: str):
        """
        Register downstream consumer domain.

        Args:
            downstream_domain: Name of consuming domain
        """
        self.boundary.downstream_consumers.add(downstream_domain)
        self.metrics['consumers_count'] = len(self.boundary.downstream_consumers)
        logger.info(f"Added downstream consumer: {self.name} -> {downstream_domain}")

    def get_metrics(self) -> Dict[str, Any]:
        """
        Get domain metrics.

        Returns:
            Metrics dictionary
        """
        # Aggregate product quality scores
        if self.data_products:
            quality_scores = [
                p.quality_metrics.get('quality_score', 0)
                for p in self.data_products.values()
            ]
            self.metrics['quality_score'] = sum(quality_scores) / len(quality_scores)

        return self.metrics
\end{lstlisting}

\subsubsection{DataProduct: Data as a Product}

\begin{lstlisting}[language=Python, caption={Data Product with SLA and Quality Guarantees}]
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta

class DataProductTier(Enum):
    """Data product service tiers."""
    BRONZE = "bronze"  # Raw, minimal processing
    SILVER = "silver"  # Cleaned, validated
    GOLD = "gold"      # Aggregated, business-ready

@dataclass
class ServiceLevelAgreement:
    """
    SLA for data product.

    Attributes:
        availability: Uptime percentage (e.g., 99.9)
        latency_p95: 95th percentile latency (seconds)
        freshness: Maximum data age (timedelta)
        quality_threshold: Minimum quality score (0-1)
        support_hours: Support availability
    """
    availability: float  # Percentage (99.9 = 99.9%)
    latency_p95: float   # Seconds
    freshness: timedelta
    quality_threshold: float  # 0-1
    support_hours: str = "24/7"

@dataclass
class QualityMetrics:
    """
    Data quality metrics.

    Attributes:
        completeness: Percentage of non-null values
        accuracy: Percentage of accurate records
        consistency: Cross-field consistency score
        timeliness: Data freshness score
        validity: Schema validation pass rate
    """
    completeness: float = 0.0
    accuracy: float = 0.0
    consistency: float = 0.0
    timeliness: float = 0.0
    validity: float = 0.0

    def overall_score(self) -> float:
        """Calculate overall quality score."""
        return (
            self.completeness * 0.25 +
            self.accuracy * 0.25 +
            self.consistency * 0.20 +
            self.timeliness * 0.15 +
            self.validity * 0.15
        )

class DataProduct:
    """
    Data product with SLA and quality guarantees.

    Treats data as a product with clear ownership, SLAs,
    documentation, and quality metrics.

    Example:
        >>> customer_360 = DataProduct(
        ...     name="customer_360",
        ...     description="Unified customer view",
        ...     tier=DataProductTier.GOLD,
        ...     sla=ServiceLevelAgreement(
        ...         availability=99.9,
        ...         latency_p95=1.0,
        ...         freshness=timedelta(hours=1),
        ...         quality_threshold=0.95
        ...     )
        ... )
        >>> customer_360.publish_to_catalog(catalog)
    """

    def __init__(
        self,
        name: str,
        description: str,
        tier: DataProductTier,
        sla: ServiceLevelAgreement,
        schema: EventSchema,
        output_topic: str,
        domain: Optional[str] = None
    ):
        """
        Initialize data product.

        Args:
            name: Product name
            description: Product description
            tier: Service tier (bronze/silver/gold)
            sla: Service level agreement
            schema: Data schema
            output_topic: Kafka topic where product is published
            domain: Owning domain
        """
        self.name = name
        self.description = description
        self.tier = tier
        self.sla = sla
        self.schema = schema
        self.output_topic = output_topic
        self.domain = domain

        # Quality tracking
        self.quality_metrics: Dict[str, float] = {}

        # Consumer tracking
        self.consumers: Set[str] = set()

        # Lineage
        self.upstream_products: Set[str] = set()
        self.transformations: List[str] = []

        # Metadata
        self.metadata = {
            'created_at': datetime.now().isoformat(),
            'version': '1.0',
            'documentation_url': None,
            'example_queries': [],
            'tags': []
        }

        # Metrics
        self.usage_metrics = {
            'read_count': 0,
            'unique_consumers': 0,
            'avg_latency_ms': 0.0,
            'sla_compliance': 0.0
        }

        logger.info(
            f"Initialized DataProduct: {name} "
            f"({tier.value} tier, SLA={sla.availability}%)"
        )

    def add_consumer(self, consumer_domain: str):
        """
        Register consumer domain.

        Args:
            consumer_domain: Name of consuming domain
        """
        self.consumers.add(consumer_domain)
        self.usage_metrics['unique_consumers'] = len(self.consumers)
        logger.info(f"Product {self.name}: Added consumer {consumer_domain}")

    def add_upstream_product(self, product_name: str):
        """
        Declare upstream data product dependency.

        Args:
            product_name: Name of upstream product
        """
        self.upstream_products.add(product_name)
        logger.info(f"Product {self.name}: Added upstream {product_name}")

    def add_transformation(self, transformation_description: str):
        """
        Document transformation applied to create this product.

        Args:
            transformation_description: Description of transformation
        """
        self.transformations.append(transformation_description)

    def update_quality_metrics(self, metrics: QualityMetrics):
        """
        Update quality metrics for product.

        Args:
            metrics: Quality metrics
        """
        self.quality_metrics = {
            'completeness': metrics.completeness,
            'accuracy': metrics.accuracy,
            'consistency': metrics.consistency,
            'timeliness': metrics.timeliness,
            'validity': metrics.validity,
            'quality_score': metrics.overall_score()
        }

        # Check SLA compliance
        if metrics.overall_score() < self.sla.quality_threshold:
            logger.warning(
                f"Product {self.name} quality below SLA: "
                f"{metrics.overall_score():.2%} < {self.sla.quality_threshold:.2%}"
            )

    def check_sla_compliance(
        self,
        current_latency: float,
        current_availability: float,
        data_age: timedelta
    ) -> bool:
        """
        Check if product meets SLA.

        Args:
            current_latency: Current p95 latency (seconds)
            current_availability: Current availability (percentage)
            data_age: Age of latest data

        Returns:
            True if all SLAs met
        """
        latency_ok = current_latency <= self.sla.latency_p95
        availability_ok = current_availability >= self.sla.availability
        freshness_ok = data_age <= self.sla.freshness
        quality_ok = self.quality_metrics.get('quality_score', 0) >= self.sla.quality_threshold

        sla_met = all([latency_ok, availability_ok, freshness_ok, quality_ok])

        # Update compliance metric
        self.usage_metrics['sla_compliance'] = 1.0 if sla_met else 0.0

        if not sla_met:
            violations = []
            if not latency_ok:
                violations.append(f"latency {current_latency:.2f}s > {self.sla.latency_p95:.2f}s")
            if not availability_ok:
                violations.append(f"availability {current_availability:.1f}% < {self.sla.availability:.1f}%")
            if not freshness_ok:
                violations.append(f"data age {data_age} > {self.sla.freshness}")
            if not quality_ok:
                violations.append(f"quality below threshold")

            logger.warning(
                f"Product {self.name} SLA violation: {', '.join(violations)}"
            )

        return sla_met

    def get_lineage(self) -> Dict[str, Any]:
        """
        Get data lineage for product.

        Returns:
            Lineage information
        """
        return {
            'product': self.name,
            'domain': self.domain,
            'tier': self.tier.value,
            'upstream_products': list(self.upstream_products),
            'transformations': self.transformations,
            'output_topic': self.output_topic,
            'consumers': list(self.consumers)
        }

    def get_documentation(self) -> Dict[str, Any]:
        """
        Get product documentation.

        Returns:
            Documentation dictionary
        """
        return {
            'name': self.name,
            'description': self.description,
            'domain': self.domain,
            'tier': self.tier.value,
            'schema': {
                'name': self.schema.name,
                'version': self.schema.version,
                'fields': {
                    name: typ.__name__
                    for name, typ in self.schema.fields.items()
                }
            },
            'sla': {
                'availability': f"{self.sla.availability}%",
                'latency_p95': f"{self.sla.latency_p95}s",
                'freshness': str(self.sla.freshness),
                'quality_threshold': f"{self.sla.quality_threshold:.1%}",
                'support_hours': self.sla.support_hours
            },
            'access': {
                'topic': self.output_topic,
                'format': 'JSON',
                'authentication': 'required'
            },
            'metadata': self.metadata,
            'quality_metrics': self.quality_metrics,
            'usage_metrics': self.usage_metrics
        }
\end{lstlisting}

\subsubsection{Federated Governance}

\begin{lstlisting}[language=Python, caption={Domain Governance with Policy Enforcement}]
from typing import Dict, List, Optional, Callable
from enum import Enum
from abc import ABC, abstractmethod

class PolicyType(Enum):
    """Types of governance policies."""
    DATA_QUALITY = "data_quality"
    PRIVACY = "privacy"
    SECURITY = "security"
    RETENTION = "retention"
    SCHEMA = "schema"
    NAMING = "naming"

class PolicySeverity(Enum):
    """Policy violation severity."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class PolicyViolation:
    """
    Policy violation record.

    Attributes:
        policy_name: Name of violated policy
        severity: Violation severity
        message: Violation description
        data_product: Affected data product
        timestamp: When violation occurred
    """
    policy_name: str
    severity: PolicySeverity
    message: str
    data_product: str
    timestamp: datetime = field(default_factory=datetime.now)

class GovernancePolicy(ABC):
    """
    Abstract base class for governance policies.

    Policies are enforced automatically during data product
    registration and updates.
    """

    def __init__(self, name: str, policy_type: PolicyType, severity: PolicySeverity):
        self.name = name
        self.policy_type = policy_type
        self.severity = severity

    @abstractmethod
    def validate(self, data_product: DataProduct) -> bool:
        """
        Validate data product against policy.

        Args:
            data_product: Product to validate

        Returns:
            True if compliant
        """
        pass

    @abstractmethod
    def get_violation_message(self, data_product: DataProduct) -> str:
        """
        Get violation message.

        Args:
            data_product: Product that violated policy

        Returns:
            Violation message
        """
        pass

class PIIDetectionPolicy(GovernancePolicy):
    """
    Policy to detect and flag PII in data products.

    Ensures sensitive data is properly classified and protected.
    """

    def __init__(self):
        super().__init__(
            name="pii_detection",
            policy_type=PolicyType.PRIVACY,
            severity=PolicySeverity.CRITICAL
        )

        # PII patterns to detect
        self.pii_fields = {
            'email', 'phone', 'ssn', 'address',
            'credit_card', 'passport', 'license'
        }

    def validate(self, data_product: DataProduct) -> bool:
        """Check if PII fields are properly marked."""
        schema_fields = set(data_product.schema.fields.keys())

        # Check for PII field names
        detected_pii = schema_fields.intersection(self.pii_fields)

        if detected_pii:
            # PII detected - check if product has privacy tags
            tags = data_product.metadata.get('tags', [])
            has_privacy_tag = any(
                tag in ['pii', 'sensitive', 'private']
                for tag in tags
            )

            return has_privacy_tag

        return True

    def get_violation_message(self, data_product: DataProduct) -> str:
        schema_fields = set(data_product.schema.fields.keys())
        detected_pii = schema_fields.intersection(self.pii_fields)

        return (
            f"Product {data_product.name} contains PII fields {detected_pii} "
            f"but missing privacy classification tags"
        )

class QualityThresholdPolicy(GovernancePolicy):
    """
    Policy enforcing minimum data quality standards.

    Ensures all data products meet organization quality bar.
    """

    def __init__(self, min_quality_score: float = 0.8):
        super().__init__(
            name="quality_threshold",
            policy_type=PolicyType.DATA_QUALITY,
            severity=PolicySeverity.ERROR
        )
        self.min_quality_score = min_quality_score

    def validate(self, data_product: DataProduct) -> bool:
        """Check if quality score meets threshold."""
        quality_score = data_product.quality_metrics.get('quality_score', 0)
        return quality_score >= self.min_quality_score

    def get_violation_message(self, data_product: DataProduct) -> str:
        quality_score = data_product.quality_metrics.get('quality_score', 0)
        return (
            f"Product {data_product.name} quality score {quality_score:.2%} "
            f"below threshold {self.min_quality_score:.2%}"
        )

class NamingConventionPolicy(GovernancePolicy):
    """
    Policy enforcing naming conventions.

    Ensures consistent naming across organization.
    """

    def __init__(self, allowed_pattern: str = "^[a-z][a-z0-9_]*$"):
        super().__init__(
            name="naming_convention",
            policy_type=PolicyType.NAMING,
            severity=PolicySeverity.WARNING
        )
        import re
        self.pattern = re.compile(allowed_pattern)

    def validate(self, data_product: DataProduct) -> bool:
        """Check if name follows convention."""
        return bool(self.pattern.match(data_product.name))

    def get_violation_message(self, data_product: DataProduct) -> str:
        return (
            f"Product name '{data_product.name}' does not follow "
            f"naming convention {self.pattern.pattern}"
        )

class DomainGovernance:
    """
    Federated governance system for data mesh.

    Enforces policies across domains while maintaining autonomy.

    Example:
        >>> governance = DomainGovernance()
        >>> governance.register_global_policy(PIIDetectionPolicy())
        >>> governance.validate_product(customer_360)
    """

    def __init__(self):
        """Initialize governance system."""
        # Global policies applied to all domains
        self.global_policies: List[GovernancePolicy] = []

        # Domain-specific policies
        self.domain_policies: Dict[str, List[GovernancePolicy]] = {}

        # Violation tracking
        self.violations: List[PolicyViolation] = []

        logger.info("Initialized DomainGovernance")

    def register_global_policy(self, policy: GovernancePolicy):
        """
        Register policy applied to all domains.

        Args:
            policy: Governance policy
        """
        self.global_policies.append(policy)
        logger.info(
            f"Registered global policy: {policy.name} "
            f"({policy.policy_type.value}, {policy.severity.value})"
        )

    def register_domain_policy(self, domain: str, policy: GovernancePolicy):
        """
        Register policy for specific domain.

        Args:
            domain: Domain name
            policy: Governance policy
        """
        if domain not in self.domain_policies:
            self.domain_policies[domain] = []

        self.domain_policies[domain].append(policy)
        logger.info(
            f"Registered domain policy for {domain}: {policy.name}"
        )

    def validate_product(self, data_product: DataProduct) -> bool:
        """
        Validate data product against all applicable policies.

        Args:
            data_product: Product to validate

        Returns:
            True if passes all policies
        """
        all_policies = self.global_policies.copy()

        # Add domain-specific policies
        if data_product.domain in self.domain_policies:
            all_policies.extend(self.domain_policies[data_product.domain])

        passed = True

        for policy in all_policies:
            if not policy.validate(data_product):
                violation = PolicyViolation(
                    policy_name=policy.name,
                    severity=policy.severity,
                    message=policy.get_violation_message(data_product),
                    data_product=data_product.name
                )

                self.violations.append(violation)

                logger.log(
                    logging.CRITICAL if policy.severity == PolicySeverity.CRITICAL
                    else logging.ERROR if policy.severity == PolicySeverity.ERROR
                    else logging.WARNING,
                    f"Policy violation: {violation.message}"
                )

                # Fail if critical or error
                if policy.severity in [PolicySeverity.CRITICAL, PolicySeverity.ERROR]:
                    passed = False

        return passed

    def get_violations(
        self,
        domain: Optional[str] = None,
        severity: Optional[PolicySeverity] = None
    ) -> List[PolicyViolation]:
        """
        Get policy violations.

        Args:
            domain: Filter by domain
            severity: Filter by severity

        Returns:
            List of violations
        """
        violations = self.violations

        if domain:
            violations = [
                v for v in violations
                if v.data_product.startswith(domain)
            ]

        if severity:
            violations = [
                v for v in violations
                if v.severity == severity
            ]

        return violations

    def get_compliance_report(self) -> Dict[str, Any]:
        """
        Generate compliance report.

        Returns:
            Compliance metrics
        """
        total_checks = len(self.violations) + 100  # Assume some passed
        violations_by_severity = {}

        for severity in PolicySeverity:
            violations_by_severity[severity.value] = len([
                v for v in self.violations
                if v.severity == severity
            ])

        return {
            'total_checks': total_checks,
            'violations': len(self.violations),
            'compliance_rate': 1.0 - (len(self.violations) / total_checks),
            'violations_by_severity': violations_by_severity,
            'critical_violations': violations_by_severity['critical'],
            'recent_violations': [
                {
                    'policy': v.policy_name,
                    'severity': v.severity.value,
                    'message': v.message,
                    'product': v.data_product,
                    'timestamp': v.timestamp.isoformat()
                }
                for v in self.violations[-10:]  # Last 10
            ]
        }
\end{lstlisting}

\subsubsection{Federated Catalog}

\begin{lstlisting}[language=Python, caption={Federated Catalog for Cross-Domain Discovery}]
from typing import Dict, List, Optional, Set
import re

class FederatedCatalog:
    """
    Federated data catalog for cross-domain discovery.

    Enables domains to discover and consume data products
    across the organization.

    Example:
        >>> catalog = FederatedCatalog()
        >>> catalog.register_domain(marketing_domain)
        >>> results = catalog.search("customer engagement")
        >>> product = catalog.get_product("marketing.customer_360")
    """

    def __init__(self):
        """Initialize federated catalog."""
        # Registered domains
        self.domains: Dict[str, DataDomain] = {}

        # Product index: {full_name: product}
        self.products: Dict[str, DataProduct] = {}

        # Search index: {term: [product_names]}
        self.search_index: Dict[str, Set[str]] = {}

        # Lineage graph: {product: {upstream, downstream}}
        self.lineage_graph: Dict[str, Dict[str, Set[str]]] = {}

        logger.info("Initialized FederatedCatalog")

    def register_domain(self, domain: DataDomain):
        """
        Register domain and its data products.

        Args:
            domain: DataDomain to register
        """
        self.domains[domain.name] = domain

        # Register all domain's products
        for product in domain.list_products():
            self.register_product(product)

        logger.info(
            f"Registered domain {domain.name} with "
            f"{len(domain.data_products)} products"
        )

    def register_product(self, product: DataProduct):
        """
        Register data product in catalog.

        Args:
            product: DataProduct to register
        """
        # Fully qualified name: domain.product
        full_name = f"{product.domain}.{product.name}"
        self.products[full_name] = product

        # Index for search
        self._index_product(full_name, product)

        # Update lineage graph
        self._update_lineage(full_name, product)

        logger.info(f"Registered product in catalog: {full_name}")

    def _index_product(self, full_name: str, product: DataProduct):
        """Index product for search."""
        # Tokenize searchable fields
        tokens = set()

        # Add name tokens
        tokens.update(product.name.split('_'))

        # Add description tokens
        tokens.update(product.description.lower().split())

        # Add domain
        tokens.add(product.domain)

        # Add tier
        tokens.add(product.tier.value)

        # Add tags
        tokens.update(product.metadata.get('tags', []))

        # Update index
        for token in tokens:
            if token not in self.search_index:
                self.search_index[token] = set()
            self.search_index[token].add(full_name)

    def _update_lineage(self, full_name: str, product: DataProduct):
        """Update lineage graph."""
        if full_name not in self.lineage_graph:
            self.lineage_graph[full_name] = {
                'upstream': set(),
                'downstream': set()
            }

        # Add upstream dependencies
        for upstream in product.upstream_products:
            self.lineage_graph[full_name]['upstream'].add(upstream)

            # Update upstream's downstream
            if upstream not in self.lineage_graph:
                self.lineage_graph[upstream] = {
                    'upstream': set(),
                    'downstream': set()
                }
            self.lineage_graph[upstream]['downstream'].add(full_name)

    def search(
        self,
        query: str,
        domain: Optional[str] = None,
        tier: Optional[DataProductTier] = None
    ) -> List[DataProduct]:
        """
        Search for data products.

        Args:
            query: Search query
            domain: Filter by domain
            tier: Filter by tier

        Returns:
            List of matching products
        """
        # Tokenize query
        query_tokens = query.lower().split()

        # Find products matching any token
        matching_products = set()

        for token in query_tokens:
            if token in self.search_index:
                matching_products.update(self.search_index[token])

        # Apply filters
        results = []

        for product_name in matching_products:
            product = self.products[product_name]

            if domain and product.domain != domain:
                continue

            if tier and product.tier != tier:
                continue

            results.append(product)

        # Sort by relevance (simple: count matching tokens)
        def relevance_score(product: DataProduct) -> int:
            score = 0
            for token in query_tokens:
                if token in product.name:
                    score += 2
                if token in product.description.lower():
                    score += 1
            return score

        results.sort(key=relevance_score, reverse=True)

        logger.info(f"Search '{query}' found {len(results)} results")

        return results

    def get_product(self, full_name: str) -> Optional[DataProduct]:
        """
        Get product by fully qualified name.

        Args:
            full_name: Format "domain.product"

        Returns:
            DataProduct or None
        """
        return self.products.get(full_name)

    def get_lineage(
        self,
        product_name: str,
        depth: int = 3
    ) -> Dict[str, Any]:
        """
        Get lineage for data product.

        Args:
            product_name: Product to trace
            depth: Maximum depth to traverse

        Returns:
            Lineage graph
        """
        if product_name not in self.lineage_graph:
            return {}

        def traverse(name: str, direction: str, current_depth: int) -> List[str]:
            if current_depth >= depth:
                return []

            if name not in self.lineage_graph:
                return []

            dependencies = list(self.lineage_graph[name][direction])

            # Recursively traverse
            for dep in dependencies.copy():
                dependencies.extend(
                    traverse(dep, direction, current_depth + 1)
                )

            return dependencies

        return {
            'product': product_name,
            'upstream': list(set(traverse(product_name, 'upstream', 0))),
            'downstream': list(set(traverse(product_name, 'downstream', 0)))
        }

    def recommend_products(
        self,
        current_product: str,
        n: int = 5
    ) -> List[DataProduct]:
        """
        Recommend related data products.

        Args:
            current_product: Product to base recommendations on
            n: Number of recommendations

        Returns:
            List of recommended products
        """
        product = self.products.get(current_product)
        if not product:
            return []

        # Score products by similarity
        scores: Dict[str, float] = {}

        for name, other_product in self.products.items():
            if name == current_product:
                continue

            score = 0.0

            # Same domain
            if other_product.domain == product.domain:
                score += 2.0

            # Shared upstream dependencies
            shared_upstream = product.upstream_products.intersection(
                other_product.upstream_products
            )
            score += len(shared_upstream) * 1.5

            # Shared consumers
            shared_consumers = product.consumers.intersection(
                other_product.consumers
            )
            score += len(shared_consumers) * 1.0

            # Same tier
            if other_product.tier == product.tier:
                score += 0.5

            if score > 0:
                scores[name] = score

        # Sort by score
        recommendations = sorted(
            scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n]

        return [
            self.products[name]
            for name, score in recommendations
        ]

    def get_catalog_stats(self) -> Dict[str, Any]:
        """
        Get catalog statistics.

        Returns:
            Catalog metrics
        """
        tier_counts = {}
        for tier in DataProductTier:
            tier_counts[tier.value] = len([
                p for p in self.products.values()
                if p.tier == tier
            ])

        return {
            'total_domains': len(self.domains),
            'total_products': len(self.products),
            'products_by_tier': tier_counts,
            'total_lineage_edges': sum(
                len(edges['upstream']) + len(edges['downstream'])
                for edges in self.lineage_graph.values()
            ) // 2,
            'avg_products_per_domain': (
                len(self.products) / len(self.domains)
                if self.domains else 0
            )
        }
\end{lstlisting}

\subsection{Domain Boundary Design Patterns}

Defining clear domain boundaries is critical for data mesh success.

\subsubsection{Entity-Based Boundaries}

\textbf{Pattern}: Domains own entities they create and manage.

\begin{lstlisting}[language=Python, caption={Entity-Based Domain Boundaries}]
# Marketing domain owns customer engagement entities
marketing_boundary = DomainBoundary(
    entities=[
        'Campaign',
        'EmailSend',
        'CustomerEngagement',
        'MarketingAttribution'
    ],
    events=[
        'CampaignCreated',
        'EmailSent',
        'EmailOpened',
        'EmailClicked'
    ],
    bounded_context=(
        "Marketing domain owns customer engagement and campaign "
        "management. Responsible for all marketing attribution "
        "and campaign performance data."
    )
)

# Finance domain owns transaction entities
finance_boundary = DomainBoundary(
    entities=[
        'Transaction',
        'Payment',
        'Refund',
        'Invoice'
    ],
    events=[
        'TransactionCompleted',
        'PaymentProcessed',
        'RefundIssued',
        'InvoiceGenerated'
    ],
    bounded_context=(
        "Finance domain owns all financial transactions and "
        "accounting data. Single source of truth for revenue."
    )
)

# Product domain owns product catalog entities
product_boundary = DomainBoundary(
    entities=[
        'Product',
        'SKU',
        'Inventory',
        'Pricing'
    ],
    events=[
        'ProductCreated',
        'InventoryUpdated',
        'PriceChanged'
    ],
    bounded_context=(
        "Product domain owns product catalog, inventory, and "
        "pricing. Master data for all products."
    )
)
\end{lstlisting}

\subsubsection{Cross-Domain Collaboration Pattern}

\textbf{Pattern}: Domains consume from upstream, enrich, and publish downstream products.

\begin{lstlisting}[language=Python, caption={Cross-Domain Data Product Flow}]
# Example: Marketing builds customer_360 from multiple domains

# 1. Finance publishes transaction history
transaction_product = DataProduct(
    name="transaction_history",
    description="Complete transaction history per customer",
    tier=DataProductTier.SILVER,
    sla=ServiceLevelAgreement(
        availability=99.9,
        latency_p95=1.0,
        freshness=timedelta(minutes=5),
        quality_threshold=0.98
    ),
    schema=EventSchema(
        name="TransactionHistory",
        version="1.0",
        fields={
            'customer_id': str,
            'transaction_id': str,
            'amount': float,
            'timestamp': str,
            'status': str
        },
        required_fields=['customer_id', 'transaction_id', 'amount']
    ),
    output_topic="finance.transaction_history",
    domain="finance"
)

# 2. Product publishes browsing history
browsing_product = DataProduct(
    name="browsing_history",
    description="Customer product browsing and search history",
    tier=DataProductTier.SILVER,
    sla=ServiceLevelAgreement(
        availability=99.5,
        latency_p95=2.0,
        freshness=timedelta(minutes=15),
        quality_threshold=0.95
    ),
    schema=EventSchema(
        name="BrowsingHistory",
        version="1.0",
        fields={
            'customer_id': str,
            'product_id': str,
            'action': str,  # view, search, cart_add
            'timestamp': str
        },
        required_fields=['customer_id', 'product_id', 'action']
    ),
    output_topic="product.browsing_history",
    domain="product"
)

# 3. Marketing consumes both and creates customer_360
customer_360 = DataProduct(
    name="customer_360",
    description="Unified customer view with transactions and behavior",
    tier=DataProductTier.GOLD,
    sla=ServiceLevelAgreement(
        availability=99.9,
        latency_p95=3.0,
        freshness=timedelta(minutes=30),
        quality_threshold=0.97
    ),
    schema=EventSchema(
        name="Customer360",
        version="1.0",
        fields={
            'customer_id': str,
            'total_revenue': float,
            'transaction_count': int,
            'avg_order_value': float,
            'product_views_30d': int,
            'cart_adds_30d': int,
            'engagement_score': float,
            'last_active': str
        },
        required_fields=['customer_id']
    ),
    output_topic="marketing.customer_360",
    domain="marketing"
)

# Declare lineage
customer_360.add_upstream_product("finance.transaction_history")
customer_360.add_upstream_product("product.browsing_history")

customer_360.add_transformation(
    "Aggregate 30-day transaction metrics from finance.transaction_history"
)
customer_360.add_transformation(
    "Aggregate 30-day browsing metrics from product.browsing_history"
)
customer_360.add_transformation(
    "Compute engagement score: (transactions*3 + views + cart_adds) / 30"
)

# Marketing domain declares upstream dependencies
marketing_domain.add_upstream_dependency("finance")
marketing_domain.add_upstream_dependency("product")

# Upstream domains register downstream consumer
finance_domain.add_downstream_consumer("marketing")
product_domain.add_downstream_consumer("marketing")
\end{lstlisting}

\subsection{Real-World Scenario: Solving The Domain Turf War}

\subsubsection{Implementation}

\begin{lstlisting}[language=Python, caption={Data Mesh Implementation}]
# 1. Initialize federated governance
governance = DomainGovernance()

# Global policies
governance.register_global_policy(PIIDetectionPolicy())
governance.register_global_policy(QualityThresholdPolicy(min_quality_score=0.85))
governance.register_global_policy(NamingConventionPolicy())

# 2. Create domains with ownership
marketing_domain = DataDomain(
    name="marketing",
    description="Customer engagement, campaigns, and attribution",
    domain_type=DomainType.ANALYTICAL,
    ownership=DomainOwnership(
        domain_owner="alice@company.com",
        product_owner="bob@company.com",
        engineering_team=["alice", "charlie", "diana"],
        slack_channel="#team-marketing-data"
    ),
    boundary=marketing_boundary
)

finance_domain = DataDomain(
    name="finance",
    description="Financial transactions and accounting",
    domain_type=DomainType.OPERATIONAL,
    ownership=DomainOwnership(
        domain_owner="eve@company.com",
        product_owner="frank@company.com",
        engineering_team=["eve", "grace"],
        slack_channel="#team-finance-data"
    ),
    boundary=finance_boundary
)

product_domain = DataDomain(
    name="product",
    description="Product catalog, inventory, and pricing",
    domain_type=DomainType.OPERATIONAL,
    ownership=DomainOwnership(
        domain_owner="henry@company.com",
        product_owner="iris@company.com",
        engineering_team=["henry", "jack"],
        slack_channel="#team-product-data"
    ),
    boundary=product_boundary
)

# 3. Register data products
finance_domain.register_data_product(transaction_product)
product_domain.register_data_product(browsing_product)
marketing_domain.register_data_product(customer_360)

# 4. Validate against governance policies
if not governance.validate_product(transaction_product):
    logger.error("Transaction product failed governance validation")

if not governance.validate_product(customer_360):
    logger.error("Customer 360 product failed governance validation")

# 5. Register in federated catalog
catalog = FederatedCatalog()
catalog.register_domain(finance_domain)
catalog.register_domain(product_domain)
catalog.register_domain(marketing_domain)

# 6. Discovery and consumption

# Search for customer data
results = catalog.search("customer transaction")
print(f"Found {len(results)} products:")
for product in results:
    print(f"  - {product.domain}.{product.name}: {product.description}")

# Get specific product
customer_data = catalog.get_product("marketing.customer_360")
if customer_data:
    print(f"\nProduct: {customer_data.name}")
    print(f"Domain: {customer_data.domain}")
    print(f"SLA: {customer_data.sla.availability}% availability")
    print(f"Topic: {customer_data.output_topic}")

# Trace lineage
lineage = catalog.get_lineage("marketing.customer_360")
print(f"\nLineage for customer_360:")
print(f"  Upstream: {lineage['upstream']}")
print(f"  Downstream: {lineage['downstream']}")

# Get recommendations
recommendations = catalog.recommend_products("marketing.customer_360", n=3)
print(f"\nRecommended products:")
for rec in recommendations:
    print(f"  - {rec.domain}.{rec.name}")

# 7. Quality monitoring
quality_metrics = QualityMetrics(
    completeness=0.98,
    accuracy=0.96,
    consistency=0.95,
    timeliness=0.99,
    validity=0.97
)

customer_360.update_quality_metrics(quality_metrics)

# Check SLA compliance
sla_met = customer_360.check_sla_compliance(
    current_latency=2.5,  # seconds
    current_availability=99.95,  # percent
    data_age=timedelta(minutes=25)
)

print(f"\nSLA compliance: {sla_met}")
print(f"Quality score: {quality_metrics.overall_score():.2%}")

# 8. Generate governance report
compliance_report = governance.get_compliance_report()
print(f"\nGovernance compliance:")
print(f"  Compliance rate: {compliance_report['compliance_rate']:.1%}")
print(f"  Critical violations: {compliance_report['critical_violations']}")

# 9. Catalog statistics
stats = catalog.get_catalog_stats()
print(f"\nCatalog statistics:")
print(f"  Total domains: {stats['total_domains']}")
print(f"  Total products: {stats['total_products']}")
print(f"  Products by tier: {stats['products_by_tier']}")
\end{lstlisting}

\subsubsection{Outcome}

With data mesh implementation:

\begin{itemize}
    \item \textbf{Domain autonomy}: Each domain owns and controls their data products
    \item \textbf{Reduced bottleneck}: Time to new feature reduced from 6 months to 2 weeks
    \item \textbf{Quality ownership}: Domains own quality end-to-end (SLA compliance 99.5\%)
    \item \textbf{No shadow IT}: All data products in federated catalog (100\% visibility)
    \item \textbf{Governance compliance}: Automated policy enforcement (95\% pass rate)
    \item \textbf{Cost reduction}: Eliminated duplicate infrastructure (\$400K/year savings)
    \item \textbf{Cross-domain collaboration}: 40+ data products, 150+ cross-domain dependencies
    \item \textbf{Developer satisfaction}: Team autonomy score improved from 3.2 to 8.7/10
\end{itemize}

\textbf{Key success factors:}
\begin{itemize}
    \item \textbf{Clear ownership}: Every data product has accountable owner with SLA
    \item \textbf{Self-service platform}: Domains use shared infrastructure (Kafka, monitoring)
    \item \textbf{Federated governance}: Automated policy enforcement preserves compliance
    \item \textbf{Discovery catalog}: Easy to find and consume data products across domains
    \item \textbf{Product thinking}: Data treated as first-class product with users and SLAs
\end{itemize}

\section{Containerized Pipeline Architecture with Kubernetes}

Traditional VM-based data pipelines suffer from slow deployment cycles, resource inefficiency, and operational complexity. A data engineering team managing 50 pipelines across 20 VMs faces: 30-minute deployment times, 40\% average resource utilization, manual scaling requiring 2-hour response time, and environment inconsistencies causing "works on my machine" failures.

\textbf{Kubernetes} transforms pipeline deployment through containerization and orchestration, providing: sub-minute deployments, automatic scaling based on workload, declarative configuration with version control, and consistent environments from development to production.

\subsection{The Container Migration Challenge}

A financial services company runs 80 data pipelines processing transactions, market data, and risk calculations:

\textbf{VM-based architecture problems:}
\begin{itemize}
    \item \textbf{Deployment friction}: Provisioning new VM takes 3 days (IT ticket, approval, setup)
    \item \textbf{Resource waste}: VMs sized for peak load run at 25\% utilization during off-hours
    \item \textbf{Environment drift}: Production has Python 3.8, staging has 3.9, dev has 3.10
    \item \textbf{Scaling delays}: Manual intervention required to add capacity during market spikes
    \item \textbf{Pipeline isolation}: One pipeline crash can affect others on same VM
    \item \textbf{Configuration management}: 200+ config files spread across VMs, no version control
    \item \textbf{Disaster recovery}: 4-hour RTO to restore failed pipeline on new VM
\end{itemize}

\textbf{Business impact:}
\begin{itemize}
    \item Market data pipeline failed during trading hours → \$500K revenue loss
    \item Risk calculation delayed 2 hours due to scaling bottleneck → regulatory incident
    \item 60\% of engineering time spent on infrastructure vs. features
    \item 3-week lead time for new pipelines blocks business initiatives
\end{itemize}

\textbf{Kubernetes solution:}
\begin{itemize}
    \item Containerize all pipelines with Docker
    \item Deploy to Kubernetes cluster with auto-scaling
    \item Custom operator manages pipeline lifecycle
    \item GitOps workflow for configuration management
    \item Serverless functions for event-driven processing
\end{itemize}

\subsection{ContainerizedPipeline: Docker and Kubernetes Integration}

\begin{lstlisting}[language=Python, caption={Containerized Pipeline with Kubernetes Deployment}]
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import yaml
import logging

logger = logging.getLogger(__name__)

class ResourceRequest(Enum):
    """Resource sizing presets."""
    SMALL = "small"    # 0.5 CPU, 512Mi RAM
    MEDIUM = "medium"  # 1 CPU, 1Gi RAM
    LARGE = "large"    # 2 CPU, 4Gi RAM
    XLARGE = "xlarge"  # 4 CPU, 8Gi RAM

@dataclass
class ContainerConfig:
    """
    Container configuration for pipeline.

    Attributes:
        image: Docker image (e.g., "myregistry/pipeline:v1.0")
        command: Container entrypoint command
        args: Command arguments
        env_vars: Environment variables
        secrets: Secret references
        resources: Resource requests/limits
    """
    image: str
    command: Optional[List[str]] = None
    args: Optional[List[str]] = None
    env_vars: Dict[str, str] = field(default_factory=dict)
    secrets: Dict[str, str] = field(default_factory=dict)
    resources: ResourceRequest = ResourceRequest.MEDIUM

@dataclass
class KubernetesConfig:
    """
    Kubernetes deployment configuration.

    Attributes:
        namespace: Kubernetes namespace
        replicas: Number of pod replicas
        service_account: Service account for RBAC
        node_selector: Node selection criteria
        tolerations: Pod tolerations for taints
        affinity: Pod affinity/anti-affinity rules
    """
    namespace: str = "data-pipelines"
    replicas: int = 1
    service_account: str = "pipeline-sa"
    node_selector: Dict[str, str] = field(default_factory=dict)
    tolerations: List[Dict[str, str]] = field(default_factory=list)
    affinity: Optional[Dict[str, Any]] = None

class ContainerizedPipeline:
    """
    Containerized data pipeline with Kubernetes deployment.

    Handles containerization, deployment, scaling, and lifecycle
    management of data pipelines on Kubernetes.

    Example:
        >>> pipeline = ContainerizedPipeline(
        ...     name="transaction-processor",
        ...     container_config=ContainerConfig(
        ...         image="myregistry/transaction-pipeline:v1.0",
        ...         resources=ResourceRequest.LARGE
        ...     )
        ... )
        >>> pipeline.deploy_to_kubernetes()
    """

    def __init__(
        self,
        name: str,
        container_config: ContainerConfig,
        k8s_config: Optional[KubernetesConfig] = None
    ):
        """
        Initialize containerized pipeline.

        Args:
            name: Pipeline name
            container_config: Container configuration
            k8s_config: Kubernetes configuration
        """
        self.name = name
        self.container_config = container_config
        self.k8s_config = k8s_config or KubernetesConfig()

        # Pipeline metadata
        self.labels = {
            'app': 'data-pipeline',
            'pipeline': name,
            'managed-by': 'pipeline-operator'
        }

        logger.info(f"Initialized ContainerizedPipeline: {name}")

    def generate_dockerfile(self) -> str:
        """
        Generate Dockerfile for pipeline.

        Returns:
            Dockerfile content
        """
        dockerfile = f"""# Base image with Python
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy pipeline code
COPY src/ ./src/
COPY config/ ./config/

# Create non-root user
RUN useradd -m -u 1000 pipeline && \\
    chown -R pipeline:pipeline /app

# Switch to non-root user
USER pipeline

# Set entrypoint
ENTRYPOINT ["python", "-m", "src.pipeline"]
"""
        return dockerfile

    def generate_kubernetes_deployment(self) -> Dict[str, Any]:
        """
        Generate Kubernetes Deployment manifest.

        Returns:
            Deployment YAML as dictionary
        """
        # Resource mappings
        resource_map = {
            ResourceRequest.SMALL: {
                'requests': {'cpu': '500m', 'memory': '512Mi'},
                'limits': {'cpu': '1000m', 'memory': '1Gi'}
            },
            ResourceRequest.MEDIUM: {
                'requests': {'cpu': '1000m', 'memory': '1Gi'},
                'limits': {'cpu': '2000m', 'memory': '2Gi'}
            },
            ResourceRequest.LARGE: {
                'requests': {'cpu': '2000m', 'memory': '4Gi'},
                'limits': {'cpu': '4000m', 'memory': '8Gi'}
            },
            ResourceRequest.XLARGE: {
                'requests': {'cpu': '4000m', 'memory': '8Gi'},
                'limits': {'cpu': '8000m', 'memory': '16Gi'}
            }
        }

        resources = resource_map[self.container_config.resources]

        # Build environment variables
        env_vars = [
            {'name': k, 'value': v}
            for k, v in self.container_config.env_vars.items()
        ]

        # Add secret references
        for secret_name, secret_key in self.container_config.secrets.items():
            env_vars.append({
                'name': secret_name,
                'valueFrom': {
                    'secretKeyRef': {
                        'name': secret_key,
                        'key': secret_name
                    }
                }
            })

        deployment = {
            'apiVersion': 'apps/v1',
            'kind': 'Deployment',
            'metadata': {
                'name': self.name,
                'namespace': self.k8s_config.namespace,
                'labels': self.labels
            },
            'spec': {
                'replicas': self.k8s_config.replicas,
                'selector': {
                    'matchLabels': {'pipeline': self.name}
                },
                'template': {
                    'metadata': {
                        'labels': self.labels
                    },
                    'spec': {
                        'serviceAccountName': self.k8s_config.service_account,
                        'containers': [{
                            'name': self.name,
                            'image': self.container_config.image,
                            'command': self.container_config.command,
                            'args': self.container_config.args,
                            'env': env_vars,
                            'resources': resources,
                            'livenessProbe': {
                                'httpGet': {
                                    'path': '/health',
                                    'port': 8080
                                },
                                'initialDelaySeconds': 30,
                                'periodSeconds': 10
                            },
                            'readinessProbe': {
                                'httpGet': {
                                    'path': '/ready',
                                    'port': 8080
                                },
                                'initialDelaySeconds': 10,
                                'periodSeconds': 5
                            }
                        }]
                    }
                }
            }
        }

        # Add node selector if specified
        if self.k8s_config.node_selector:
            deployment['spec']['template']['spec']['nodeSelector'] = \
                self.k8s_config.node_selector

        # Add tolerations if specified
        if self.k8s_config.tolerations:
            deployment['spec']['template']['spec']['tolerations'] = \
                self.k8s_config.tolerations

        # Add affinity if specified
        if self.k8s_config.affinity:
            deployment['spec']['template']['spec']['affinity'] = \
                self.k8s_config.affinity

        return deployment

    def generate_horizontal_pod_autoscaler(
        self,
        min_replicas: int = 1,
        max_replicas: int = 10,
        target_cpu_utilization: int = 70
    ) -> Dict[str, Any]:
        """
        Generate HorizontalPodAutoscaler manifest.

        Args:
            min_replicas: Minimum pod replicas
            max_replicas: Maximum pod replicas
            target_cpu_utilization: Target CPU utilization percentage

        Returns:
            HPA YAML as dictionary
        """
        hpa = {
            'apiVersion': 'autoscaling/v2',
            'kind': 'HorizontalPodAutoscaler',
            'metadata': {
                'name': f"{self.name}-hpa",
                'namespace': self.k8s_config.namespace,
                'labels': self.labels
            },
            'spec': {
                'scaleTargetRef': {
                    'apiVersion': 'apps/v1',
                    'kind': 'Deployment',
                    'name': self.name
                },
                'minReplicas': min_replicas,
                'maxReplicas': max_replicas,
                'metrics': [
                    {
                        'type': 'Resource',
                        'resource': {
                            'name': 'cpu',
                            'target': {
                                'type': 'Utilization',
                                'averageUtilization': target_cpu_utilization
                            }
                        }
                    },
                    {
                        'type': 'Resource',
                        'resource': {
                            'name': 'memory',
                            'target': {
                                'type': 'Utilization',
                                'averageUtilization': 80
                            }
                        }
                    }
                ],
                'behavior': {
                    'scaleUp': {
                        'stabilizationWindowSeconds': 60,
                        'policies': [{
                            'type': 'Percent',
                            'value': 50,
                            'periodSeconds': 60
                        }]
                    },
                    'scaleDown': {
                        'stabilizationWindowSeconds': 300,
                        'policies': [{
                            'type': 'Percent',
                            'value': 10,
                            'periodSeconds': 60
                        }]
                    }
                }
            }
        }

        return hpa

    def generate_configmap(
        self,
        config_data: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        Generate ConfigMap for pipeline configuration.

        Args:
            config_data: Configuration key-value pairs

        Returns:
            ConfigMap YAML as dictionary
        """
        configmap = {
            'apiVersion': 'v1',
            'kind': 'ConfigMap',
            'metadata': {
                'name': f"{self.name}-config",
                'namespace': self.k8s_config.namespace,
                'labels': self.labels
            },
            'data': config_data
        }

        return configmap

    def deploy_to_kubernetes(
        self,
        kubectl_apply: bool = False
    ) -> Dict[str, str]:
        """
        Deploy pipeline to Kubernetes.

        Args:
            kubectl_apply: If True, apply manifests using kubectl

        Returns:
            Dictionary of generated manifest file paths
        """
        manifests = {}

        # Generate Deployment
        deployment = self.generate_kubernetes_deployment()
        deployment_yaml = yaml.dump(deployment, default_flow_style=False)
        manifests['deployment'] = deployment_yaml

        # Generate HPA
        hpa = self.generate_horizontal_pod_autoscaler()
        hpa_yaml = yaml.dump(hpa, default_flow_style=False)
        manifests['hpa'] = hpa_yaml

        # Write manifests to files
        import os
        os.makedirs(f"k8s/{self.name}", exist_ok=True)

        for manifest_type, content in manifests.items():
            file_path = f"k8s/{self.name}/{manifest_type}.yaml"
            with open(file_path, 'w') as f:
                f.write(content)

            logger.info(f"Generated {manifest_type} manifest: {file_path}")

        # Apply manifests if requested
        if kubectl_apply:
            import subprocess
            for manifest_type in manifests.keys():
                file_path = f"k8s/{self.name}/{manifest_type}.yaml"
                try:
                    subprocess.run(
                        ['kubectl', 'apply', '-f', file_path],
                        check=True,
                        capture_output=True
                    )
                    logger.info(f"Applied {manifest_type} to Kubernetes")
                except subprocess.CalledProcessError as e:
                    logger.error(f"Failed to apply {manifest_type}: {e}")
                    raise

        return {k: f"k8s/{self.name}/{k}.yaml" for k in manifests.keys()}

    def scale(self, replicas: int):
        """
        Scale pipeline deployment.

        Args:
            replicas: Target number of replicas
        """
        import subprocess

        try:
            subprocess.run([
                'kubectl', 'scale',
                f"deployment/{self.name}",
                f"--replicas={replicas}",
                f"--namespace={self.k8s_config.namespace}"
            ], check=True)

            logger.info(f"Scaled {self.name} to {replicas} replicas")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to scale {self.name}: {e}")
            raise

    def get_status(self) -> Dict[str, Any]:
        """
        Get pipeline deployment status.

        Returns:
            Status information
        """
        import subprocess
        import json

        try:
            result = subprocess.run([
                'kubectl', 'get', 'deployment', self.name,
                '--namespace', self.k8s_config.namespace,
                '-o', 'json'
            ], check=True, capture_output=True, text=True)

            deployment_info = json.loads(result.stdout)

            status = {
                'name': self.name,
                'namespace': self.k8s_config.namespace,
                'replicas': deployment_info['status'].get('replicas', 0),
                'ready_replicas': deployment_info['status'].get('readyReplicas', 0),
                'available_replicas': deployment_info['status'].get('availableReplicas', 0),
                'conditions': deployment_info['status'].get('conditions', [])
            }

            return status

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to get status for {self.name}: {e}")
            return {'error': str(e)}
\end{lstlisting}

\subsection{Custom Kubernetes Operator for Pipeline Management}

Kubernetes operators extend the platform with custom resources and controllers that manage application-specific logic.

\subsubsection{Custom Resource Definition (CRD)}

\begin{lstlisting}[language=yaml, caption={DataPipeline Custom Resource Definition}, style=yaml]
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: datapipelines.datamesh.io
spec:
  group: datamesh.io
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                source:
                  type: object
                  properties:
                    type:
                      type: string
                      enum: [kafka, s3, postgres, api]
                    config:
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
                transform:
                  type: object
                  properties:
                    image:
                      type: string
                    resources:
                      type: string
                      enum: [small, medium, large, xlarge]
                    env:
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
                sink:
                  type: object
                  properties:
                    type:
                      type: string
                      enum: [kafka, s3, postgres, bigquery]
                    config:
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
                schedule:
                  type: string
                  pattern: '^(\*|([0-9]|[1-5][0-9])).*$'
                autoscaling:
                  type: object
                  properties:
                    enabled:
                      type: boolean
                    minReplicas:
                      type: integer
                      minimum: 1
                    maxReplicas:
                      type: integer
                      minimum: 1
                    targetCPU:
                      type: integer
                      minimum: 1
                      maximum: 100
              required:
                - source
                - transform
                - sink
            status:
              type: object
              properties:
                phase:
                  type: string
                  enum: [Pending, Running, Failed, Succeeded]
                lastRun:
                  type: string
                  format: date-time
                message:
                  type: string
      subresources:
        status: {}
  scope: Namespaced
  names:
    plural: datapipelines
    singular: datapipeline
    kind: DataPipeline
    shortNames:
      - dp
\end{lstlisting}

\subsubsection{Pipeline Operator Implementation}

\begin{lstlisting}[language=Python, caption={Kubernetes Operator for Pipeline Management}]
from typing import Dict, Any, Optional
import kopf
import kubernetes
from kubernetes import client, config
import logging

logger = logging.getLogger(__name__)

# Load Kubernetes configuration
try:
    config.load_incluster_config()
except:
    config.load_kube_config()

class PipelineOperator:
    """
    Kubernetes operator for managing DataPipeline custom resources.

    Watches for DataPipeline resources and creates/updates/deletes
    corresponding Kubernetes resources (Deployments, CronJobs, etc.).

    Example DataPipeline resource:
        apiVersion: datamesh.io/v1
        kind: DataPipeline
        metadata:
          name: transaction-pipeline
        spec:
          source:
            type: kafka
            config:
              topic: transactions
              bootstrap_servers: kafka:9092
          transform:
            image: myregistry/transform:v1.0
            resources: large
            env:
              BATCH_SIZE: "1000"
          sink:
            type: s3
            config:
              bucket: processed-data
              prefix: transactions/
          schedule: "*/15 * * * *"  # Every 15 minutes
          autoscaling:
            enabled: true
            minReplicas: 1
            maxReplicas: 5
            targetCPU: 70
    """

    def __init__(self):
        """Initialize operator."""
        self.apps_v1 = client.AppsV1Api()
        self.batch_v1 = client.BatchV1Api()
        self.core_v1 = client.CoreV1Api()
        self.autoscaling_v2 = client.AutoscalingV2Api()

        logger.info("Initialized PipelineOperator")

    @kopf.on.create('datamesh.io', 'v1', 'datapipelines')
    def create_pipeline(
        self,
        spec: Dict[str, Any],
        name: str,
        namespace: str,
        **kwargs
    ):
        """
        Handle DataPipeline resource creation.

        Args:
            spec: Pipeline specification
            name: Resource name
            namespace: Namespace
        """
        logger.info(f"Creating pipeline: {name} in namespace {namespace}")

        # Determine if scheduled or continuous
        if 'schedule' in spec:
            # Create CronJob for scheduled pipeline
            self._create_cronjob(name, namespace, spec)
        else:
            # Create Deployment for continuous pipeline
            self._create_deployment(name, namespace, spec)

            # Create HPA if autoscaling enabled
            if spec.get('autoscaling', {}).get('enabled', False):
                self._create_hpa(name, namespace, spec)

        # Update status
        return {'phase': 'Running', 'message': 'Pipeline created successfully'}

    @kopf.on.update('datamesh.io', 'v1', 'datapipelines')
    def update_pipeline(
        self,
        spec: Dict[str, Any],
        name: str,
        namespace: str,
        **kwargs
    ):
        """Handle DataPipeline resource updates."""
        logger.info(f"Updating pipeline: {name}")

        if 'schedule' in spec:
            self._update_cronjob(name, namespace, spec)
        else:
            self._update_deployment(name, namespace, spec)

            if spec.get('autoscaling', {}).get('enabled', False):
                self._update_hpa(name, namespace, spec)
            else:
                self._delete_hpa(name, namespace)

        return {'phase': 'Running', 'message': 'Pipeline updated successfully'}

    @kopf.on.delete('datamesh.io', 'v1', 'datapipelines')
    def delete_pipeline(
        self,
        name: str,
        namespace: str,
        **kwargs
    ):
        """Handle DataPipeline resource deletion."""
        logger.info(f"Deleting pipeline: {name}")

        # Delete associated resources
        try:
            self.apps_v1.delete_namespaced_deployment(name, namespace)
        except kubernetes.client.exceptions.ApiException:
            pass

        try:
            self.batch_v1.delete_namespaced_cron_job(name, namespace)
        except kubernetes.client.exceptions.ApiException:
            pass

        try:
            self.autoscaling_v2.delete_namespaced_horizontal_pod_autoscaler(
                f"{name}-hpa",
                namespace
            )
        except kubernetes.client.exceptions.ApiException:
            pass

        logger.info(f"Deleted pipeline: {name}")

    def _create_deployment(
        self,
        name: str,
        namespace: str,
        spec: Dict[str, Any]
    ):
        """Create Deployment for continuous pipeline."""
        # Build environment variables
        env_vars = []
        for key, value in spec.get('transform', {}).get('env', {}).items():
            env_vars.append(client.V1EnvVar(name=key, value=value))

        # Add source config
        env_vars.append(client.V1EnvVar(
            name='SOURCE_TYPE',
            value=spec['source']['type']
        ))
        env_vars.append(client.V1EnvVar(
            name='SOURCE_CONFIG',
            value=str(spec['source']['config'])
        ))

        # Add sink config
        env_vars.append(client.V1EnvVar(
            name='SINK_TYPE',
            value=spec['sink']['type']
        ))
        env_vars.append(client.V1EnvVar(
            name='SINK_CONFIG',
            value=str(spec['sink']['config'])
        ))

        # Resource mapping
        resource_map = {
            'small': {'cpu': '500m', 'memory': '512Mi'},
            'medium': {'cpu': '1', 'memory': '1Gi'},
            'large': {'cpu': '2', 'memory': '4Gi'},
            'xlarge': {'cpu': '4', 'memory': '8Gi'}
        }

        resources = resource_map.get(
            spec.get('transform', {}).get('resources', 'medium')
        )

        # Create container
        container = client.V1Container(
            name=name,
            image=spec['transform']['image'],
            env=env_vars,
            resources=client.V1ResourceRequirements(
                requests=resources,
                limits={k: v for k, v in resources.items()}  # Same as requests
            )
        )

        # Create pod template
        pod_template = client.V1PodTemplateSpec(
            metadata=client.V1ObjectMeta(
                labels={'app': 'data-pipeline', 'pipeline': name}
            ),
            spec=client.V1PodSpec(containers=[container])
        )

        # Create deployment spec
        deployment_spec = client.V1DeploymentSpec(
            replicas=1,
            selector=client.V1LabelSelector(
                match_labels={'pipeline': name}
            ),
            template=pod_template
        )

        # Create deployment
        deployment = client.V1Deployment(
            api_version='apps/v1',
            kind='Deployment',
            metadata=client.V1ObjectMeta(name=name, namespace=namespace),
            spec=deployment_spec
        )

        self.apps_v1.create_namespaced_deployment(namespace, deployment)
        logger.info(f"Created deployment: {name}")

    def _create_hpa(
        self,
        name: str,
        namespace: str,
        spec: Dict[str, Any]
    ):
        """Create HorizontalPodAutoscaler."""
        autoscaling = spec.get('autoscaling', {})

        hpa_spec = client.V2HorizontalPodAutoscalerSpec(
            scale_target_ref=client.V2CrossVersionObjectReference(
                api_version='apps/v1',
                kind='Deployment',
                name=name
            ),
            min_replicas=autoscaling.get('minReplicas', 1),
            max_replicas=autoscaling.get('maxReplicas', 5),
            metrics=[
                client.V2MetricSpec(
                    type='Resource',
                    resource=client.V2ResourceMetricSource(
                        name='cpu',
                        target=client.V2MetricTarget(
                            type='Utilization',
                            average_utilization=autoscaling.get('targetCPU', 70)
                        )
                    )
                )
            ]
        )

        hpa = client.V2HorizontalPodAutoscaler(
            api_version='autoscaling/v2',
            kind='HorizontalPodAutoscaler',
            metadata=client.V1ObjectMeta(
                name=f"{name}-hpa",
                namespace=namespace
            ),
            spec=hpa_spec
        )

        self.autoscaling_v2.create_namespaced_horizontal_pod_autoscaler(
            namespace,
            hpa
        )
        logger.info(f"Created HPA: {name}-hpa")

    def _create_cronjob(
        self,
        name: str,
        namespace: str,
        spec: Dict[str, Any]
    ):
        """Create CronJob for scheduled pipeline."""
        # Similar to deployment but wrapped in CronJob
        env_vars = []
        for key, value in spec.get('transform', {}).get('env', {}).items():
            env_vars.append(client.V1EnvVar(name=key, value=value))

        # Add source and sink config
        env_vars.extend([
            client.V1EnvVar(name='SOURCE_TYPE', value=spec['source']['type']),
            client.V1EnvVar(name='SOURCE_CONFIG', value=str(spec['source']['config'])),
            client.V1EnvVar(name='SINK_TYPE', value=spec['sink']['type']),
            client.V1EnvVar(name='SINK_CONFIG', value=str(spec['sink']['config']))
        ])

        resource_map = {
            'small': {'cpu': '500m', 'memory': '512Mi'},
            'medium': {'cpu': '1', 'memory': '1Gi'},
            'large': {'cpu': '2', 'memory': '4Gi'},
            'xlarge': {'cpu': '4', 'memory': '8Gi'}
        }

        resources = resource_map.get(
            spec.get('transform', {}).get('resources', 'medium')
        )

        container = client.V1Container(
            name=name,
            image=spec['transform']['image'],
            env=env_vars,
            resources=client.V1ResourceRequirements(
                requests=resources,
                limits=resources
            )
        )

        pod_template = client.V1PodTemplateSpec(
            metadata=client.V1ObjectMeta(
                labels={'app': 'data-pipeline', 'pipeline': name}
            ),
            spec=client.V1PodSpec(
                containers=[container],
                restart_policy='OnFailure'
            )
        )

        job_template = client.V1JobTemplateSpec(
            spec=client.V1JobSpec(
                template=pod_template,
                backoff_limit=3,
                ttl_seconds_after_finished=3600
            )
        )

        cronjob_spec = client.V1CronJobSpec(
            schedule=spec['schedule'],
            job_template=job_template,
            successful_jobs_history_limit=3,
            failed_jobs_history_limit=1
        )

        cronjob = client.V1CronJob(
            api_version='batch/v1',
            kind='CronJob',
            metadata=client.V1ObjectMeta(name=name, namespace=namespace),
            spec=cronjob_spec
        )

        self.batch_v1.create_namespaced_cron_job(namespace, cronjob)
        logger.info(f"Created CronJob: {name}")

    def _update_deployment(self, name: str, namespace: str, spec: Dict[str, Any]):
        """Update existing deployment."""
        # Get current deployment
        deployment = self.apps_v1.read_namespaced_deployment(name, namespace)

        # Update image
        deployment.spec.template.spec.containers[0].image = \
            spec['transform']['image']

        # Update environment variables
        env_vars = []
        for key, value in spec.get('transform', {}).get('env', {}).items():
            env_vars.append(client.V1EnvVar(name=key, value=value))

        deployment.spec.template.spec.containers[0].env = env_vars

        # Apply update
        self.apps_v1.patch_namespaced_deployment(name, namespace, deployment)
        logger.info(f"Updated deployment: {name}")

    def _update_hpa(self, name: str, namespace: str, spec: Dict[str, Any]):
        """Update HPA."""
        try:
            hpa = self.autoscaling_v2.read_namespaced_horizontal_pod_autoscaler(
                f"{name}-hpa",
                namespace
            )

            autoscaling = spec.get('autoscaling', {})
            hpa.spec.min_replicas = autoscaling.get('minReplicas', 1)
            hpa.spec.max_replicas = autoscaling.get('maxReplicas', 5)

            self.autoscaling_v2.patch_namespaced_horizontal_pod_autoscaler(
                f"{name}-hpa",
                namespace,
                hpa
            )
            logger.info(f"Updated HPA: {name}-hpa")

        except kubernetes.client.exceptions.ApiException:
            # HPA doesn't exist, create it
            self._create_hpa(name, namespace, spec)

    def _update_cronjob(self, name: str, namespace: str, spec: Dict[str, Any]):
        """Update CronJob."""
        cronjob = self.batch_v1.read_namespaced_cron_job(name, namespace)

        # Update schedule
        cronjob.spec.schedule = spec['schedule']

        # Update container image
        cronjob.spec.job_template.spec.template.spec.containers[0].image = \
            spec['transform']['image']

        self.batch_v1.patch_namespaced_cron_job(name, namespace, cronjob)
        logger.info(f"Updated CronJob: {name}")

    def _delete_hpa(self, name: str, namespace: str):
        """Delete HPA if exists."""
        try:
            self.autoscaling_v2.delete_namespaced_horizontal_pod_autoscaler(
                f"{name}-hpa",
                namespace
            )
            logger.info(f"Deleted HPA: {name}-hpa")
        except kubernetes.client.exceptions.ApiException:
            pass  # HPA doesn't exist

# Run operator
if __name__ == '__main__':
    kopf.run()
\end{lstlisting}

\subsection{Serverless Data Processing}

Serverless patterns enable event-driven, auto-scaling data processing with minimal operational overhead.

\begin{lstlisting}[language=Python, caption={Serverless Processor with Function Orchestration}]
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import logging

logger = logging.getLogger(__name__)

class FunctionTrigger(Enum):
    """Function trigger types."""
    HTTP = "http"
    KAFKA = "kafka"
    S3 = "s3"
    CRON = "cron"
    CUSTOM = "custom"

@dataclass
class FunctionConfig:
    """
    Serverless function configuration.

    Attributes:
        name: Function name
        image: Container image
        handler: Function entrypoint
        trigger: Trigger type
        trigger_config: Trigger-specific configuration
        resources: Resource allocation
        timeout: Execution timeout (seconds)
        retries: Number of retries on failure
    """
    name: str
    image: str
    handler: str
    trigger: FunctionTrigger
    trigger_config: Dict[str, Any]
    resources: Dict[str, str] = field(default_factory=lambda: {
        'cpu': '100m',
        'memory': '256Mi'
    })
    timeout: int = 300
    retries: int = 3

class ServerlessProcessor:
    """
    Serverless data processor with function orchestration.

    Manages serverless functions for event-driven data processing
    using Knative or similar serverless platforms.

    Example:
        >>> processor = ServerlessProcessor()
        >>> processor.register_function(FunctionConfig(
        ...     name="transform-json",
        ...     image="myregistry/transform:v1.0",
        ...     handler="main.handler",
        ...     trigger=FunctionTrigger.KAFKA,
        ...     trigger_config={'topic': 'raw-events'}
        ... ))
        >>> processor.deploy_all()
    """

    def __init__(self, platform: str = "knative"):
        """
        Initialize serverless processor.

        Args:
            platform: Serverless platform (knative, openfaas, etc.)
        """
        self.platform = platform
        self.functions: Dict[str, FunctionConfig] = {}
        self.function_graph: Dict[str, List[str]] = {}  # DAG of functions

        logger.info(f"Initialized ServerlessProcessor ({platform})")

    def register_function(self, config: FunctionConfig):
        """
        Register serverless function.

        Args:
            config: Function configuration
        """
        self.functions[config.name] = config
        logger.info(f"Registered function: {config.name}")

    def create_function_chain(
        self,
        chain_name: str,
        function_sequence: List[str]
    ):
        """
        Create chain of functions (output of one feeds next).

        Args:
            chain_name: Chain identifier
            function_sequence: Ordered list of function names
        """
        for i in range(len(function_sequence) - 1):
            current = function_sequence[i]
            next_func = function_sequence[i + 1]

            if current not in self.function_graph:
                self.function_graph[current] = []

            self.function_graph[current].append(next_func)

        logger.info(
            f"Created function chain '{chain_name}': "
            f"{' -> '.join(function_sequence)}"
        )

    def generate_knative_service(
        self,
        config: FunctionConfig
    ) -> Dict[str, Any]:
        """
        Generate Knative Service manifest.

        Args:
            config: Function configuration

        Returns:
            Knative Service YAML
        """
        service = {
            'apiVersion': 'serving.knative.dev/v1',
            'kind': 'Service',
            'metadata': {
                'name': config.name,
                'labels': {
                    'app': 'serverless-function',
                    'function': config.name
                }
            },
            'spec': {
                'template': {
                    'metadata': {
                        'annotations': {
                            'autoscaling.knative.dev/minScale': '0',
                            'autoscaling.knative.dev/maxScale': '10',
                            'autoscaling.knative.dev/target': '10'
                        }
                    },
                    'spec': {
                        'timeoutSeconds': config.timeout,
                        'containers': [{
                            'image': config.image,
                            'env': [
                                {'name': 'HANDLER', 'value': config.handler}
                            ],
                            'resources': {
                                'requests': config.resources,
                                'limits': config.resources
                            }
                        }]
                    }
                }
            }
        }

        return service

    def generate_kafka_trigger(
        self,
        config: FunctionConfig
    ) -> Dict[str, Any]:
        """
        Generate Knative KafkaSource trigger.

        Args:
            config: Function configuration

        Returns:
            KafkaSource YAML
        """
        if config.trigger != FunctionTrigger.KAFKA:
            raise ValueError("Function must have KAFKA trigger")

        trigger = {
            'apiVersion': 'sources.knative.dev/v1beta1',
            'kind': 'KafkaSource',
            'metadata': {
                'name': f"{config.name}-trigger"
            },
            'spec': {
                'consumerGroup': config.name,
                'bootstrapServers': config.trigger_config.get(
                    'bootstrap_servers',
                    ['kafka:9092']
                ),
                'topics': [config.trigger_config['topic']],
                'sink': {
                    'ref': {
                        'apiVersion': 'serving.knative.dev/v1',
                        'kind': 'Service',
                        'name': config.name
                    }
                }
            }
        }

        return trigger

    async def execute_function_chain(
        self,
        start_function: str,
        input_data: Any
    ) -> Any:
        """
        Execute chain of functions asynchronously.

        Args:
            start_function: Starting function in chain
            input_data: Input data

        Returns:
            Final output
        """
        current_data = input_data
        current_function = start_function

        while current_function:
            # Execute function
            logger.info(f"Executing function: {current_function}")

            function_config = self.functions[current_function]
            current_data = await self._invoke_function(
                function_config,
                current_data
            )

            # Get next function in chain
            next_functions = self.function_graph.get(current_function, [])

            if not next_functions:
                break

            # For simplicity, follow first path in DAG
            current_function = next_functions[0]

        return current_data

    async def _invoke_function(
        self,
        config: FunctionConfig,
        input_data: Any
    ) -> Any:
        """
        Invoke serverless function.

        Args:
            config: Function configuration
            input_data: Input data

        Returns:
            Function output
        """
        import aiohttp

        # In production, this would call actual serverless function
        # For demo, simulate function execution
        url = f"http://{config.name}.default.svc.cluster.local"

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=input_data) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    raise RuntimeError(
                        f"Function {config.name} failed: {response.status}"
                    )

    def deploy_all(self):
        """Deploy all registered functions."""
        import yaml
        import os

        os.makedirs("serverless", exist_ok=True)

        for name, config in self.functions.items():
            # Generate service manifest
            service = self.generate_knative_service(config)

            with open(f"serverless/{name}-service.yaml", 'w') as f:
                yaml.dump(service, f)

            # Generate trigger if applicable
            if config.trigger == FunctionTrigger.KAFKA:
                trigger = self.generate_kafka_trigger(config)

                with open(f"serverless/{name}-trigger.yaml", 'w') as f:
                    yaml.dump(trigger, f)

            logger.info(f"Generated manifests for function: {name}")
\end{lstlisting}

\subsection{Resource Manager for Dynamic Scaling}

\begin{lstlisting}[language=Python, caption={Resource Manager with Kubernetes Integration}]
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

@dataclass
class ResourceMetrics:
    """Resource usage metrics."""
    cpu_usage: float  # Percentage
    memory_usage: float  # Percentage
    pod_count: int
    pending_pods: int
    timestamp: datetime

class ResourceManager:
    """
    Manage Kubernetes resources with dynamic scaling.

    Monitors resource usage and scales pipelines based on
    metrics and policies.

    Example:
        >>> manager = ResourceManager()
        >>> manager.monitor_pipeline("transaction-processor")
        >>> manager.scale_if_needed()
    """

    def __init__(self):
        """Initialize resource manager."""
        from kubernetes import client, config

        try:
            config.load_incluster_config()
        except:
            config.load_kube_config()

        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.metrics = client.CustomObjectsApi()

        # Metrics history
        self.metrics_history: Dict[str, List[ResourceMetrics]] = {}

        logger.info("Initialized ResourceManager")

    def get_pod_metrics(
        self,
        namespace: str,
        label_selector: str
    ) -> ResourceMetrics:
        """
        Get current pod metrics.

        Args:
            namespace: Kubernetes namespace
            label_selector: Label selector (e.g., "pipeline=transaction-processor")

        Returns:
            Resource metrics
        """
        # Get pod list
        pods = self.core_v1.list_namespaced_pod(
            namespace,
            label_selector=label_selector
        )

        pod_count = len(pods.items)
        pending_pods = len([
            p for p in pods.items
            if p.status.phase == 'Pending'
        ])

        # Get metrics from metrics-server
        try:
            pod_metrics = self.metrics.list_namespaced_custom_object(
                group="metrics.k8s.io",
                version="v1beta1",
                namespace=namespace,
                plural="pods",
                label_selector=label_selector
            )

            total_cpu = 0
            total_memory = 0

            for pod_metric in pod_metrics.get('items', []):
                for container in pod_metric['containers']:
                    # Parse CPU (e.g., "125m" -> 0.125 cores)
                    cpu = container['usage']['cpu']
                    if cpu.endswith('n'):
                        cpu_cores = int(cpu[:-1]) / 1e9
                    elif cpu.endswith('m'):
                        cpu_cores = int(cpu[:-1]) / 1000
                    else:
                        cpu_cores = int(cpu)

                    total_cpu += cpu_cores

                    # Parse memory (e.g., "256Mi" -> MB)
                    memory = container['usage']['memory']
                    if memory.endswith('Ki'):
                        memory_mb = int(memory[:-2]) / 1024
                    elif memory.endswith('Mi'):
                        memory_mb = int(memory[:-2])
                    elif memory.endswith('Gi'):
                        memory_mb = int(memory[:-2]) * 1024
                    else:
                        memory_mb = int(memory) / (1024 * 1024)

                    total_memory += memory_mb

            # Calculate percentages (assume 1 CPU and 1Gi per pod)
            cpu_usage = (total_cpu / pod_count * 100) if pod_count > 0 else 0
            memory_usage = (total_memory / (pod_count * 1024) * 100) if pod_count > 0 else 0

        except Exception as e:
            logger.warning(f"Failed to get metrics: {e}")
            cpu_usage = 0
            memory_usage = 0

        metrics = ResourceMetrics(
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            pod_count=pod_count,
            pending_pods=pending_pods,
            timestamp=datetime.now()
        )

        return metrics

    def monitor_pipeline(
        self,
        pipeline_name: str,
        namespace: str = "data-pipelines"
    ):
        """
        Monitor pipeline resources.

        Args:
            pipeline_name: Pipeline name
            namespace: Kubernetes namespace
        """
        metrics = self.get_pod_metrics(
            namespace,
            f"pipeline={pipeline_name}"
        )

        # Store metrics history
        if pipeline_name not in self.metrics_history:
            self.metrics_history[pipeline_name] = []

        self.metrics_history[pipeline_name].append(metrics)

        # Keep only last 100 metrics
        self.metrics_history[pipeline_name] = \
            self.metrics_history[pipeline_name][-100:]

        logger.info(
            f"Pipeline {pipeline_name}: "
            f"CPU={metrics.cpu_usage:.1f}%, "
            f"Memory={metrics.memory_usage:.1f}%, "
            f"Pods={metrics.pod_count}"
        )

    def scale_if_needed(
        self,
        pipeline_name: str,
        namespace: str = "data-pipelines",
        cpu_threshold: float = 80.0,
        memory_threshold: float = 80.0
    ) -> bool:
        """
        Scale pipeline if metrics exceed thresholds.

        Args:
            pipeline_name: Pipeline name
            namespace: Namespace
            cpu_threshold: CPU utilization threshold
            memory_threshold: Memory utilization threshold

        Returns:
            True if scaling action taken
        """
        if pipeline_name not in self.metrics_history:
            return False

        # Get recent metrics (last 5 minutes)
        cutoff_time = datetime.now() - timedelta(minutes=5)
        recent_metrics = [
            m for m in self.metrics_history[pipeline_name]
            if m.timestamp >= cutoff_time
        ]

        if not recent_metrics:
            return False

        # Calculate average utilization
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)

        # Determine if scaling needed
        if avg_cpu > cpu_threshold or avg_memory > memory_threshold:
            # Scale up
            current_replicas = recent_metrics[-1].pod_count
            new_replicas = min(current_replicas + 1, 10)  # Max 10

            self._scale_deployment(pipeline_name, namespace, new_replicas)

            logger.info(
                f"Scaled up {pipeline_name}: {current_replicas} -> {new_replicas} "
                f"(CPU={avg_cpu:.1f}%, Memory={avg_memory:.1f}%)"
            )

            return True

        elif avg_cpu < cpu_threshold * 0.5 and avg_memory < memory_threshold * 0.5:
            # Scale down if significantly underutilized
            current_replicas = recent_metrics[-1].pod_count
            if current_replicas > 1:
                new_replicas = max(current_replicas - 1, 1)  # Min 1

                self._scale_deployment(pipeline_name, namespace, new_replicas)

                logger.info(
                    f"Scaled down {pipeline_name}: {current_replicas} -> {new_replicas} "
                    f"(CPU={avg_cpu:.1f}%, Memory={avg_memory:.1f}%)"
                )

                return True

        return False

    def _scale_deployment(
        self,
        name: str,
        namespace: str,
        replicas: int
    ):
        """Scale deployment to target replicas."""
        deployment = self.apps_v1.read_namespaced_deployment(name, namespace)
        deployment.spec.replicas = replicas
        self.apps_v1.patch_namespaced_deployment(name, namespace, deployment)
\end{lstlisting}

\subsection{Real-World Scenario: The Container Migration}

\subsubsection{Implementation}

\begin{lstlisting}[language=Python, caption={Complete Kubernetes Migration Example}]
# 1. Containerize existing pipeline
transaction_pipeline = ContainerizedPipeline(
    name="transaction-processor",
    container_config=ContainerConfig(
        image="myregistry/transaction-pipeline:v1.0",
        command=["python", "-m", "pipeline"],
        args=["--mode", "continuous"],
        env_vars={
            'KAFKA_BROKERS': 'kafka:9092',
            'KAFKA_TOPIC': 'transactions',
            'OUTPUT_BUCKET': 's3://processed-data',
            'LOG_LEVEL': 'INFO'
        },
        secrets={
            'AWS_ACCESS_KEY_ID': 'aws-credentials',
            'AWS_SECRET_ACCESS_KEY': 'aws-credentials'
        },
        resources=ResourceRequest.LARGE
    ),
    k8s_config=KubernetesConfig(
        namespace="data-pipelines",
        replicas=2,
        node_selector={'workload': 'data-processing'},
        tolerations=[{
            'key': 'data-pipeline',
            'operator': 'Equal',
            'value': 'true',
            'effect': 'NoSchedule'
        }]
    )
)

# 2. Generate Dockerfile
dockerfile_content = transaction_pipeline.generate_dockerfile()
with open("Dockerfile", 'w') as f:
    f.write(dockerfile_content)

print("Generated Dockerfile")

# 3. Build and push container
import subprocess

subprocess.run([
    'docker', 'build',
    '-t', 'myregistry/transaction-pipeline:v1.0',
    '.'
], check=True)

subprocess.run([
    'docker', 'push',
    'myregistry/transaction-pipeline:v1.0'
], check=True)

print("Built and pushed container image")

# 4. Deploy to Kubernetes
manifest_files = transaction_pipeline.deploy_to_kubernetes(kubectl_apply=True)

print(f"Deployed to Kubernetes:")
for manifest_type, file_path in manifest_files.items():
    print(f"  - {manifest_type}: {file_path}")

# 5. Create DataPipeline custom resource
datapipeline_yaml = """
apiVersion: datamesh.io/v1
kind: DataPipeline
metadata:
  name: transaction-processor
  namespace: data-pipelines
spec:
  source:
    type: kafka
    config:
      topic: transactions
      bootstrap_servers: kafka:9092
      group_id: transaction-processor
  transform:
    image: myregistry/transaction-pipeline:v1.0
    resources: large
    env:
      BATCH_SIZE: "1000"
      PROCESSING_MODE: "continuous"
  sink:
    type: s3
    config:
      bucket: processed-data
      prefix: transactions/
      format: parquet
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPU: 70
"""

with open("datapipeline.yaml", 'w') as f:
    f.write(datapipeline_yaml)

# Apply custom resource
subprocess.run([
    'kubectl', 'apply', '-f', 'datapipeline.yaml'
], check=True)

print("Created DataPipeline custom resource")

# 6. Monitor resources
resource_manager = ResourceManager()

print("Monitoring pipeline resources...")
for i in range(10):
    resource_manager.monitor_pipeline("transaction-processor")

    # Check if scaling needed
    scaled = resource_manager.scale_if_needed("transaction-processor")

    if scaled:
        print("Scaling action taken")

    import time
    time.sleep(30)  # Check every 30 seconds

# 7. Get pipeline status
status = transaction_pipeline.get_status()
print(f"\nPipeline Status:")
print(f"  Replicas: {status.get('replicas', 0)}")
print(f"  Ready: {status.get('ready_replicas', 0)}")
print(f"  Available: {status.get('available_replicas', 0)}")

# 8. Create serverless functions for event processing
serverless = ServerlessProcessor()

# Register transform function
serverless.register_function(FunctionConfig(
    name="enrich-transaction",
    image="myregistry/enrich:v1.0",
    handler="main.enrich",
    trigger=FunctionTrigger.KAFKA,
    trigger_config={
        'topic': 'raw-transactions',
        'bootstrap_servers': ['kafka:9092']
    },
    resources={'cpu': '200m', 'memory': '512Mi'},
    timeout=60
))

# Register validation function
serverless.register_function(FunctionConfig(
    name="validate-transaction",
    image="myregistry/validate:v1.0",
    handler="main.validate",
    trigger=FunctionTrigger.KAFKA,
    trigger_config={
        'topic': 'enriched-transactions',
        'bootstrap_servers': ['kafka:9092']
    },
    resources={'cpu': '100m', 'memory': '256Mi'},
    timeout=30
))

# Create function chain
serverless.create_function_chain(
    "transaction-flow",
    ["enrich-transaction", "validate-transaction"]
)

# Deploy serverless functions
serverless.deploy_all()

print("Deployed serverless functions")
\end{lstlisting}

\subsubsection{Outcome}

With Kubernetes migration:

\begin{itemize}
    \item \textbf{Deployment speed}: 3 days → 5 minutes (360x faster)
    \item \textbf{Resource utilization}: 25\% → 75\% average utilization (3x improvement)
    \item \textbf{Scaling response}: 2 hours → 2 minutes (60x faster)
    \item \textbf{Environment consistency}: 100\% (dev = staging = prod)
    \item \textbf{Pipeline isolation}: Zero cross-pipeline failures
    \item \textbf{Configuration management}: 100\% version-controlled in Git
    \item \textbf{Disaster recovery}: 4 hours → 5 minutes RTO (48x improvement)
    \item \textbf{Cost reduction}: \$150K/year in infrastructure savings
    \item \textbf{Engineering productivity}: 60\% time on features vs. 40\% before
\end{itemize}

\textbf{Key success factors:}
\begin{itemize}
    \item \textbf{Containerization}: All pipelines packaged as containers with dependencies
    \item \textbf{Custom operators}: DataPipeline CRD simplifies pipeline management
    \item \textbf{Auto-scaling}: HPA adjusts capacity based on load automatically
    \item \textbf{GitOps workflow}: All configuration in Git with automated deployment
    \item \textbf{Serverless functions}: Event-driven processing with zero idle cost
    \item \textbf{Resource management}: Automated monitoring and scaling reduces manual intervention
\end{itemize}

\section{Hybrid Cloud Pipeline Architecture with Compliance}

Modern data pipelines rarely exist within a single cloud provider. Organizations face requirements for multi-cloud deployment due to vendor lock-in concerns, geographic data sovereignty regulations, disaster recovery needs, and mergers/acquisitions. However, hybrid cloud architectures introduce complexity around data synchronization, compliance validation, and cross-border data transfer restrictions.

\textbf{Regulatory landscape requires hybrid cloud:}
\begin{itemize}
    \item \textbf{GDPR (EU)}: Personal data of EU citizens must remain in EU or adequacy jurisdictions
    \item \textbf{CCPA (California)}: Consumer data rights require tracking and deletion capabilities
    \item \textbf{LGPD (Brazil)}: Brazilian personal data must follow strict processing rules
    \item \textbf{Data localization laws}: Russia, China, India require in-country data storage
    \item \textbf{HIPAA (US)}: Healthcare data has strict access and audit requirements
    \item \textbf{PCI DSS}: Payment card data requires specific security controls
\end{itemize}

\subsection{The GDPR Wake-Up Call}

A global SaaS company processes customer data from 120 countries with infrastructure in AWS US-East:

\textbf{The violation:}
\begin{itemize}
    \item All customer data (including EU citizens) stored in US data centers
    \item ML training pipeline processes EU personal data without explicit consent tracking
    \item No data residency controls or region-based routing
    \item Customer data retention exceeds GDPR 90-day limit for inactive users
    \item No automated right-to-deletion (RTBF) implementation
    \item Cross-border data transfers lack Standard Contractual Clauses (SCCs)
\end{itemize}

\textbf{The wake-up call:}
\begin{itemize}
    \item GDPR audit reveals systematic violations across all pipelines
    \item Regulatory body issues €20M fine (4\% of global revenue)
    \item Must implement compliant architecture in 6 months or face daily penalties
    \item 15 enterprise customers suspend contracts pending compliance proof
    \item Media coverage damages brand reputation
\end{itemize}

\textbf{Requirements for compliant hybrid cloud:}
\begin{itemize}
    \item \textbf{Data residency}: EU data must stay in EU region
    \item \textbf{Consent tracking}: Process data only with documented legal basis
    \item \textbf{Right to deletion}: Delete all user data within 30 days of request
    \item \textbf{Data portability}: Export user data in machine-readable format
    \item \textbf{Audit logging}: Immutable logs of all data access and processing
    \item \textbf{Cross-border controls}: Explicit approval for data transfers outside EU
    \item \textbf{Retention policies}: Automatic deletion of data after retention period
    \item \textbf{Purpose limitation}: Use data only for stated purposes
\end{itemize}

\textbf{Hybrid cloud solution:}
\begin{itemize}
    \item Deploy regional pipelines in EU, US, APAC regions
    \item Route data to pipeline based on user's region
    \item Implement compliance validator checking all regulations
    \item Add data sovereignty manager enforcing residency rules
    \item Build cross-cloud synchronizer with encryption and audit
    \item Automate compliance reporting and violation detection
\end{itemize}

\subsection{HybridCloudPipeline: Multi-Cloud Coordination}

\begin{lstlisting}[language=Python, caption={Hybrid Cloud Pipeline with Multi-Cloud Coordination}]
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class CloudProvider(Enum):
    """Supported cloud providers."""
    AWS = "aws"
    GCP = "gcp"
    AZURE = "azure"
    ON_PREM = "on_prem"

class DataRegion(Enum):
    """Geographic data regions with regulatory implications."""
    EU_WEST = "eu-west"        # GDPR jurisdiction
    US_EAST = "us-east"        # CCPA + HIPAA
    APAC_SOUTHEAST = "apac-se" # Data localization
    BRAZIL_SOUTH = "br-south"  # LGPD
    CANADA_CENTRAL = "ca-cent" # PIPEDA

@dataclass
class CloudEndpoint:
    """
    Cloud-specific endpoint configuration.

    Attributes:
        provider: Cloud provider
        region: Geographic region
        endpoint_url: Service endpoint URL
        credentials: Authentication credentials reference
        encryption_key: Data encryption key ID
        compliance_tags: Regulatory tags for this endpoint
    """
    provider: CloudProvider
    region: DataRegion
    endpoint_url: str
    credentials: str
    encryption_key: str
    compliance_tags: Set[str] = field(default_factory=set)

class HybridCloudPipeline:
    """
    Hybrid cloud data pipeline with multi-cloud coordination.

    Manages data processing across multiple cloud providers while
    maintaining compliance, data sovereignty, and synchronization.

    Example:
        >>> pipeline = HybridCloudPipeline(
        ...     name="customer-analytics",
        ...     primary_region=DataRegion.EU_WEST
        ... )
        >>> pipeline.register_endpoint(aws_eu_endpoint)
        >>> pipeline.register_endpoint(gcp_us_endpoint)
        >>> pipeline.route_data(user_data, user_region="EU")
    """

    def __init__(
        self,
        name: str,
        primary_region: DataRegion,
        enable_cross_region_sync: bool = False
    ):
        """
        Initialize hybrid cloud pipeline.

        Args:
            name: Pipeline name
            primary_region: Primary data region
            enable_cross_region_sync: Allow cross-region data sync
        """
        self.name = name
        self.primary_region = primary_region
        self.enable_cross_region_sync = enable_cross_region_sync

        # Cloud endpoints by region
        self.endpoints: Dict[DataRegion, CloudEndpoint] = {}

        # Data routing rules
        self.routing_rules: Dict[str, DataRegion] = {}

        # Compliance validators
        self.validators: List['ComplianceValidator'] = []

        # Metrics
        self.metrics = {
            'data_routed': 0,
            'compliance_violations': 0,
            'cross_region_transfers': 0,
            'failed_transfers': 0
        }

        logger.info(
            f"Initialized HybridCloudPipeline: {name} "
            f"(primary={primary_region.value})"
        )

    def register_endpoint(self, endpoint: CloudEndpoint):
        """
        Register cloud endpoint for region.

        Args:
            endpoint: Cloud endpoint configuration

        Raises:
            ValueError: If endpoint already exists for region
        """
        if endpoint.region in self.endpoints:
            raise ValueError(f"Endpoint already exists for {endpoint.region}")

        self.endpoints[endpoint.region] = endpoint

        logger.info(
            f"Registered endpoint: {endpoint.provider.value} "
            f"in {endpoint.region.value}"
        )

    def add_routing_rule(self, user_region: str, target_region: DataRegion):
        """
        Add data routing rule.

        Args:
            user_region: User's region (e.g., "DE", "US", "CN")
            target_region: Target data region for processing
        """
        self.routing_rules[user_region] = target_region

        logger.info(f"Added routing rule: {user_region} -> {target_region.value}")

    def register_compliance_validator(self, validator: 'ComplianceValidator'):
        """
        Register compliance validator.

        Args:
            validator: Compliance validator
        """
        self.validators.append(validator)
        logger.info(f"Registered validator: {validator.__class__.__name__}")

    def route_data(
        self,
        data: Dict[str, Any],
        user_region: str
    ) -> Optional[DataRegion]:
        """
        Route data to appropriate region based on user location.

        Args:
            data: Data to route
            user_region: User's region code

        Returns:
            Target region or None if routing fails
        """
        # Determine target region
        if user_region in self.routing_rules:
            target_region = self.routing_rules[user_region]
        else:
            # Default to primary region
            target_region = self.primary_region

        # Validate compliance
        for validator in self.validators:
            violations = validator.validate(data, target_region)
            if violations:
                self.metrics['compliance_violations'] += len(violations)
                logger.error(
                    f"Compliance violations for {user_region}: {violations}"
                )
                return None

        # Check if target endpoint exists
        if target_region not in self.endpoints:
            logger.error(f"No endpoint configured for {target_region}")
            self.metrics['failed_transfers'] += 1
            return None

        # Route data
        self._send_to_region(data, target_region)

        self.metrics['data_routed'] += 1

        return target_region

    def _send_to_region(self, data: Dict[str, Any], region: DataRegion):
        """
        Send data to specific region.

        Args:
            data: Data to send
            region: Target region
        """
        endpoint = self.endpoints[region]

        # In production, this would use cloud-specific SDK
        logger.info(
            f"Sending data to {endpoint.provider.value} "
            f"in {region.value}"
        )

        # Encrypt data
        encrypted_data = self._encrypt_data(data, endpoint.encryption_key)

        # Send via cloud-specific API
        # aws_client.put_object() or gcp_client.upload() etc.

    def _encrypt_data(self, data: Dict[str, Any], key_id: str) -> bytes:
        """
        Encrypt data with cloud KMS.

        Args:
            data: Data to encrypt
            key_id: KMS key ID

        Returns:
            Encrypted data
        """
        import json

        # In production, use cloud KMS
        # For demo, just serialize
        return json.dumps(data).encode()

    def sync_across_regions(
        self,
        source_region: DataRegion,
        target_region: DataRegion,
        data_filter: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Synchronize data across regions (requires approval).

        Args:
            source_region: Source region
            target_region: Target region
            data_filter: Filter for data to sync

        Returns:
            True if sync successful
        """
        if not self.enable_cross_region_sync:
            logger.error("Cross-region sync disabled")
            return False

        # Validate compliance for cross-region transfer
        for validator in self.validators:
            if not validator.allow_cross_region_transfer(
                source_region,
                target_region
            ):
                logger.error(
                    f"Cross-region transfer not allowed: "
                    f"{source_region.value} -> {target_region.value}"
                )
                return False

        # Perform sync
        logger.info(
            f"Syncing data: {source_region.value} -> {target_region.value}"
        )

        self.metrics['cross_region_transfers'] += 1

        # In production, use cloud-native replication
        # (e.g., AWS DataSync, GCP Transfer Service)

        return True

    def get_compliance_report(self) -> Dict[str, Any]:
        """
        Generate compliance report.

        Returns:
            Compliance metrics and status
        """
        return {
            'pipeline': self.name,
            'primary_region': self.primary_region.value,
            'regions': [r.value for r in self.endpoints.keys()],
            'metrics': self.metrics,
            'validators': [v.__class__.__name__ for v in self.validators],
            'cross_region_sync_enabled': self.enable_cross_region_sync
        }
\end{lstlisting}

\subsection{ComplianceValidator: Regulatory Rule Enforcement}

\begin{lstlisting}[language=Python, caption={Compliance Validator with Regulatory Rules}]
from typing import Dict, List, Optional, Any, Set
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from enum import Enum

class Regulation(Enum):
    """Supported regulations."""
    GDPR = "gdpr"        # EU General Data Protection Regulation
    CCPA = "ccpa"        # California Consumer Privacy Act
    HIPAA = "hipaa"      # Health Insurance Portability Act
    PCI_DSS = "pci_dss"  # Payment Card Industry Data Security
    LGPD = "lgpd"        # Brazilian General Data Protection Law

class DataCategory(Enum):
    """Data sensitivity categories."""
    PUBLIC = "public"
    INTERNAL = "internal"
    CONFIDENTIAL = "confidential"
    PERSONAL = "personal"
    SENSITIVE_PERSONAL = "sensitive_personal"  # Race, health, etc.
    PAYMENT = "payment"

@dataclass
class ComplianceViolation:
    """
    Compliance violation record.

    Attributes:
        regulation: Violated regulation
        rule: Specific rule violated
        severity: Violation severity
        message: Violation description
        remediation: How to fix
        timestamp: When violation occurred
    """
    regulation: Regulation
    rule: str
    severity: str  # critical, high, medium, low
    message: str
    remediation: str
    timestamp: datetime = field(default_factory=datetime.now)

class ComplianceValidator(ABC):
    """
    Abstract base class for compliance validators.

    Validators check data processing against regulatory requirements.
    """

    def __init__(self, regulation: Regulation):
        self.regulation = regulation

    @abstractmethod
    def validate(
        self,
        data: Dict[str, Any],
        target_region: DataRegion
    ) -> List[ComplianceViolation]:
        """
        Validate data against regulation.

        Args:
            data: Data to validate
            target_region: Target processing region

        Returns:
            List of violations (empty if compliant)
        """
        pass

    @abstractmethod
    def allow_cross_region_transfer(
        self,
        source_region: DataRegion,
        target_region: DataRegion
    ) -> bool:
        """
        Check if cross-region transfer is allowed.

        Args:
            source_region: Source region
            target_region: Target region

        Returns:
            True if transfer allowed
        """
        pass

class GDPRValidator(ComplianceValidator):
    """
    GDPR compliance validator.

    Enforces EU data protection requirements:
    - Data minimization
    - Purpose limitation
    - Storage limitation
    - Data subject rights
    - Lawful basis for processing
    """

    def __init__(self):
        super().__init__(Regulation.GDPR)

        # EU member states + EEA + adequacy jurisdictions
        self.allowed_regions = {
            DataRegion.EU_WEST,
            DataRegion.CANADA_CENTRAL  # Adequacy decision
        }

    def validate(
        self,
        data: Dict[str, Any],
        target_region: DataRegion
    ) -> List[ComplianceViolation]:
        """Validate GDPR compliance."""
        violations = []

        # Check data residency
        if 'personal_data' in data and target_region not in self.allowed_regions:
            violations.append(ComplianceViolation(
                regulation=self.regulation,
                rule="Article 44 - Data Transfer",
                severity="critical",
                message=(
                    f"Personal data cannot be transferred to {target_region.value} "
                    f"without adequate safeguards"
                ),
                remediation="Use Standard Contractual Clauses or approved mechanisms"
            ))

        # Check consent
        if 'personal_data' in data and not data.get('consent_obtained'):
            violations.append(ComplianceViolation(
                regulation=self.regulation,
                rule="Article 6 - Lawful Basis",
                severity="critical",
                message="Processing personal data without lawful basis",
                remediation="Obtain explicit consent or establish legal basis"
            ))

        # Check data minimization
        if 'unnecessary_fields' in data:
            violations.append(ComplianceViolation(
                regulation=self.regulation,
                rule="Article 5(1)(c) - Data Minimization",
                severity="medium",
                message="Processing unnecessary personal data fields",
                remediation="Remove unnecessary fields from processing"
            ))

        # Check retention period
        retention_date = data.get('retention_until')
        if retention_date:
            retention_dt = datetime.fromisoformat(retention_date)
            if retention_dt > datetime.now() + timedelta(days=365):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="Article 5(1)(e) - Storage Limitation",
                    severity="high",
                    message=f"Retention period exceeds reasonable limit: {retention_date}",
                    remediation="Reduce retention period to necessary duration"
                ))

        # Check for special category data
        if 'sensitive_personal_data' in data:
            if not data.get('explicit_consent') and not data.get('legal_exception'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="Article 9 - Special Categories",
                    severity="critical",
                    message="Processing special category data without explicit consent",
                    remediation="Obtain explicit consent or establish legal exception"
                ))

        return violations

    def allow_cross_region_transfer(
        self,
        source_region: DataRegion,
        target_region: DataRegion
    ) -> bool:
        """Check if cross-region transfer allowed under GDPR."""
        # Transfers within allowed regions are OK
        if source_region in self.allowed_regions and \
           target_region in self.allowed_regions:
            return True

        # Transfers out of EU require SCCs or other mechanisms
        # In production, check for approved transfer mechanism
        return False

class CCPAValidator(ComplianceValidator):
    """
    CCPA compliance validator.

    Enforces California consumer privacy rights:
    - Right to know
    - Right to delete
    - Right to opt-out
    - Non-discrimination
    """

    def __init__(self):
        super().__init__(Regulation.CCPA)

    def validate(
        self,
        data: Dict[str, Any],
        target_region: DataRegion
    ) -> List[ComplianceViolation]:
        """Validate CCPA compliance."""
        violations = []

        # Check if California consumer
        if data.get('user_state') == 'CA':
            # Check opt-out status
            if data.get('do_not_sell') and data.get('sold_to_third_party'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="CCPA 1798.120 - Right to Opt-Out",
                    severity="critical",
                    message="Selling data of consumer who opted out",
                    remediation="Stop selling data; honor opt-out request"
                ))

            # Check disclosure
            if not data.get('privacy_notice_provided'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="CCPA 1798.100 - Right to Know",
                    severity="high",
                    message="Privacy notice not provided at collection",
                    remediation="Provide clear privacy notice at point of collection"
                ))

        return violations

    def allow_cross_region_transfer(
        self,
        source_region: DataRegion,
        target_region: DataRegion
    ) -> bool:
        """CCPA doesn't restrict geographic transfers."""
        return True

class HIPAAValidator(ComplianceValidator):
    """
    HIPAA compliance validator.

    Enforces healthcare data protection:
    - PHI encryption
    - Access controls
    - Audit logging
    - Business Associate Agreements
    """

    def __init__(self):
        super().__init__(Regulation.HIPAA)

    def validate(
        self,
        data: Dict[str, Any],
        target_region: DataRegion
    ) -> List[ComplianceViolation]:
        """Validate HIPAA compliance."""
        violations = []

        if 'phi' in data or 'health_data' in data:
            # Check encryption
            if not data.get('encrypted'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="164.312(a)(2)(iv) - Encryption",
                    severity="critical",
                    message="PHI not encrypted in transit/at rest",
                    remediation="Encrypt all PHI using approved algorithms"
                ))

            # Check BAA
            if not data.get('baa_in_place'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="164.308(b)(1) - Business Associate",
                    severity="critical",
                    message="Processing PHI without Business Associate Agreement",
                    remediation="Establish BAA with all entities processing PHI"
                ))

            # Check audit logging
            if not data.get('audit_logged'):
                violations.append(ComplianceViolation(
                    regulation=self.regulation,
                    rule="164.312(b) - Audit Controls",
                    severity="high",
                    message="PHI access not logged for audit",
                    remediation="Implement comprehensive audit logging"
                ))

        return violations

    def allow_cross_region_transfer(
        self,
        source_region: DataRegion,
        target_region: DataRegion
    ) -> bool:
        """HIPAA requires BAA regardless of region."""
        # In production, verify BAA exists for target region
        return True
\end{lstlisting}

\subsection{DataSovereigntyManager: Residency Controls}

\begin{lstlisting}[language=Python, caption={Data Sovereignty Manager with Residency Controls}]
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

@dataclass
class ResidencyRule:
    """
    Data residency rule.

    Attributes:
        data_category: Category of data
        required_regions: Regions where data must reside
        forbidden_regions: Regions where data cannot reside
        allow_temporary_export: Allow temporary export for processing
        export_duration_hours: Maximum export duration if allowed
        audit_required: Require audit trail for data movement
    """
    data_category: DataCategory
    required_regions: Set[DataRegion]
    forbidden_regions: Set[DataRegion] = field(default_factory=set)
    allow_temporary_export: bool = False
    export_duration_hours: int = 24
    audit_required: bool = True

class DataSovereigntyManager:
    """
    Manage data sovereignty and residency requirements.

    Enforces where data can be stored and processed based on
    regulatory and business requirements.

    Example:
        >>> manager = DataSovereigntyManager()
        >>> manager.add_rule(ResidencyRule(
        ...     data_category=DataCategory.PERSONAL,
        ...     required_regions={DataRegion.EU_WEST},
        ...     forbidden_regions={DataRegion.US_EAST}
        ... ))
        >>> manager.validate_location(eu_personal_data, DataRegion.EU_WEST)
    """

    def __init__(self):
        """Initialize data sovereignty manager."""
        self.rules: Dict[DataCategory, ResidencyRule] = {}

        # Track data locations
        self.data_inventory: Dict[str, Set[DataRegion]] = {}

        # Export tracking (for temporary exports)
        self.active_exports: Dict[str, Dict[str, Any]] = {}

        logger.info("Initialized DataSovereigntyManager")

    def add_rule(self, rule: ResidencyRule):
        """
        Add residency rule.

        Args:
            rule: Residency rule
        """
        self.rules[rule.data_category] = rule

        logger.info(
            f"Added residency rule: {rule.data_category.value} "
            f"-> {[r.value for r in rule.required_regions]}"
        )

    def validate_location(
        self,
        data: Dict[str, Any],
        target_region: DataRegion
    ) -> bool:
        """
        Validate if data can be in target region.

        Args:
            data: Data to validate
            target_region: Target region

        Returns:
            True if location valid
        """
        data_category = DataCategory(data.get('category', 'internal'))

        if data_category not in self.rules:
            # No specific rule, allow
            return True

        rule = self.rules[data_category]

        # Check if region is forbidden
        if target_region in rule.forbidden_regions:
            logger.error(
                f"Data category {data_category.value} forbidden in "
                f"{target_region.value}"
            )
            return False

        # Check if region is required
        if rule.required_regions and target_region not in rule.required_regions:
            logger.error(
                f"Data category {data_category.value} must be in "
                f"{[r.value for r in rule.required_regions]}, "
                f"not {target_region.value}"
            )
            return False

        return True

    def request_temporary_export(
        self,
        data_id: str,
        data_category: DataCategory,
        source_region: DataRegion,
        target_region: DataRegion,
        purpose: str
    ) -> Optional[str]:
        """
        Request temporary data export to another region.

        Args:
            data_id: Data identifier
            data_category: Category of data
            source_region: Source region
            target_region: Target region
            purpose: Purpose for export

        Returns:
            Export ID if approved, None if denied
        """
        if data_category not in self.rules:
            return None

        rule = self.rules[data_category]

        if not rule.allow_temporary_export:
            logger.error(
                f"Temporary export not allowed for {data_category.value}"
            )
            return None

        # Generate export ID
        import uuid
        export_id = str(uuid.uuid4())

        # Track export
        expiry = datetime.now() + timedelta(hours=rule.export_duration_hours)

        self.active_exports[export_id] = {
            'data_id': data_id,
            'data_category': data_category,
            'source_region': source_region,
            'target_region': target_region,
            'purpose': purpose,
            'created_at': datetime.now(),
            'expires_at': expiry
        }

        logger.info(
            f"Approved temporary export {export_id}: "
            f"{source_region.value} -> {target_region.value} "
            f"(expires: {expiry})"
        )

        return export_id

    def validate_export(self, export_id: str) -> bool:
        """
        Validate if export is still valid.

        Args:
            export_id: Export identifier

        Returns:
            True if valid
        """
        if export_id not in self.active_exports:
            return False

        export = self.active_exports[export_id]

        # Check expiry
        if datetime.now() > export['expires_at']:
            logger.warning(f"Export {export_id} expired")
            del self.active_exports[export_id]
            return False

        return True

    def revoke_export(self, export_id: str):
        """
        Revoke temporary export.

        Args:
            export_id: Export identifier
        """
        if export_id in self.active_exports:
            del self.active_exports[export_id]
            logger.info(f"Revoked export: {export_id}")

    def track_data_location(self, data_id: str, region: DataRegion):
        """
        Track where data is located.

        Args:
            data_id: Data identifier
            region: Data region
        """
        if data_id not in self.data_inventory:
            self.data_inventory[data_id] = set()

        self.data_inventory[data_id].add(region)

    def get_data_locations(self, data_id: str) -> Set[DataRegion]:
        """
        Get all regions where data exists.

        Args:
            data_id: Data identifier

        Returns:
            Set of regions
        """
        return self.data_inventory.get(data_id, set())

    def generate_residency_report(self) -> Dict[str, Any]:
        """
        Generate data residency compliance report.

        Returns:
            Residency report
        """
        return {
            'total_data_items': len(self.data_inventory),
            'rules_configured': len(self.rules),
            'active_exports': len(self.active_exports),
            'data_by_region': self._summarize_by_region(),
            'export_details': [
                {
                    'export_id': eid,
                    'data_id': e['data_id'],
                    'source': e['source_region'].value,
                    'target': e['target_region'].value,
                    'expires': e['expires_at'].isoformat()
                }
                for eid, e in self.active_exports.items()
            ]
        }

    def _summarize_by_region(self) -> Dict[str, int]:
        """Summarize data count by region."""
        region_counts = {}

        for data_id, regions in self.data_inventory.items():
            for region in regions:
                region_value = region.value
                region_counts[region_value] = region_counts.get(region_value, 0) + 1

        return region_counts
\end{lstlisting}

\subsection{CrossCloudSynchronizer: Secure Data Movement}

\begin{lstlisting}[language=Python, caption={Cross-Cloud Synchronizer with Encryption}]
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class SyncStrategy(Enum):
    """Data synchronization strategies."""
    REAL_TIME = "real_time"      # Continuous replication
    BATCH = "batch"              # Scheduled batch sync
    ON_DEMAND = "on_demand"      # Manual trigger
    EVENT_DRIVEN = "event_driven" # Triggered by events

@dataclass
class SyncJob:
    """
    Cross-cloud synchronization job.

    Attributes:
        job_id: Unique job identifier
        source_endpoint: Source cloud endpoint
        target_endpoint: Target cloud endpoint
        data_filter: Filter for data to sync
        strategy: Sync strategy
        encryption_enabled: Encrypt data in transit
        compression_enabled: Compress data
        checksum_validation: Validate data integrity
        audit_trail: Log all sync operations
    """
    job_id: str
    source_endpoint: CloudEndpoint
    target_endpoint: CloudEndpoint
    data_filter: Dict[str, Any]
    strategy: SyncStrategy
    encryption_enabled: bool = True
    compression_enabled: bool = True
    checksum_validation: bool = True
    audit_trail: bool = True

class CrossCloudSynchronizer:
    """
    Synchronize data across cloud providers securely.

    Handles encryption, compression, validation, and audit trails
    for cross-cloud data movement.

    Example:
        >>> synchronizer = CrossCloudSynchronizer()
        >>> job = SyncJob(
        ...     job_id="sync-001",
        ...     source_endpoint=aws_endpoint,
        ...     target_endpoint=gcp_endpoint,
        ...     data_filter={'user_region': 'EU'},
        ...     strategy=SyncStrategy.BATCH
        ... )
        >>> synchronizer.execute_sync(job)
    """

    def __init__(self):
        """Initialize cross-cloud synchronizer."""
        # Active sync jobs
        self.active_jobs: Dict[str, SyncJob] = {}

        # Sync history
        self.sync_history: List[Dict[str, Any]] = []

        # Metrics
        self.metrics = {
            'total_syncs': 0,
            'successful_syncs': 0,
            'failed_syncs': 0,
            'bytes_transferred': 0,
            'avg_transfer_time': 0.0
        }

        logger.info("Initialized CrossCloudSynchronizer")

    def execute_sync(self, job: SyncJob) -> bool:
        """
        Execute synchronization job.

        Args:
            job: Sync job configuration

        Returns:
            True if sync successful
        """
        start_time = datetime.now()

        logger.info(
            f"Starting sync job {job.job_id}: "
            f"{job.source_endpoint.provider.value} -> "
            f"{job.target_endpoint.provider.value}"
        )

        try:
            # 1. Extract data from source
            source_data = self._extract_from_source(job)

            # 2. Validate data
            if not self._validate_data(source_data, job):
                raise ValueError("Data validation failed")

            # 3. Transform/encrypt data
            transformed_data = self._transform_data(source_data, job)

            # 4. Load to target
            self._load_to_target(transformed_data, job)

            # 5. Verify transfer
            if job.checksum_validation:
                if not self._verify_transfer(source_data, transformed_data, job):
                    raise ValueError("Checksum validation failed")

            # 6. Log audit trail
            if job.audit_trail:
                self._log_audit(job, success=True)

            # Update metrics
            elapsed = (datetime.now() - start_time).total_seconds()
            self.metrics['total_syncs'] += 1
            self.metrics['successful_syncs'] += 1
            self._update_avg_transfer_time(elapsed)

            logger.info(f"Sync job {job.job_id} completed in {elapsed:.2f}s")

            return True

        except Exception as e:
            logger.error(f"Sync job {job.job_id} failed: {e}")

            # Log failure
            if job.audit_trail:
                self._log_audit(job, success=False, error=str(e))

            self.metrics['total_syncs'] += 1
            self.metrics['failed_syncs'] += 1

            return False

    def _extract_from_source(self, job: SyncJob) -> List[Dict[str, Any]]:
        """Extract data from source cloud."""
        logger.info(
            f"Extracting from {job.source_endpoint.provider.value} "
            f"in {job.source_endpoint.region.value}"
        )

        # In production, use cloud-specific SDK
        # For AWS: boto3.client('s3').list_objects_v2()
        # For GCP: storage.Client().list_blobs()
        # For Azure: BlobServiceClient().list_blobs()

        # Simulated extraction
        extracted_data = []

        return extracted_data

    def _validate_data(
        self,
        data: List[Dict[str, Any]],
        job: SyncJob
    ) -> bool:
        """Validate extracted data."""
        if not data:
            logger.warning("No data extracted")
            return True

        # Validate schema, completeness, etc.
        return True

    def _transform_data(
        self,
        data: List[Dict[str, Any]],
        job: SyncJob
    ) -> bytes:
        """
        Transform data for transfer.

        Applies encryption, compression, and serialization.
        """
        import json
        import gzip

        # Serialize
        serialized = json.dumps(data).encode()

        # Compress if enabled
        if job.compression_enabled:
            serialized = gzip.compress(serialized)
            logger.info(f"Compressed data: {len(serialized)} bytes")

        # Encrypt if enabled
        if job.encryption_enabled:
            serialized = self._encrypt(
                serialized,
                job.target_endpoint.encryption_key
            )
            logger.info("Encrypted data for transfer")

        self.metrics['bytes_transferred'] += len(serialized)

        return serialized

    def _encrypt(self, data: bytes, key_id: str) -> bytes:
        """
        Encrypt data using cloud KMS.

        Args:
            data: Data to encrypt
            key_id: KMS key ID

        Returns:
            Encrypted data
        """
        # In production, use cloud KMS:
        # AWS: kms.encrypt(KeyId=key_id, Plaintext=data)
        # GCP: kms_client.encrypt(name=key_id, plaintext=data)
        # Azure: key_client.encrypt(algorithm, data)

        return data  # Simulated

    def _load_to_target(self, data: bytes, job: SyncJob):
        """Load data to target cloud."""
        logger.info(
            f"Loading to {job.target_endpoint.provider.value} "
            f"in {job.target_endpoint.region.value}"
        )

        # In production, use cloud-specific SDK
        # AWS: s3.put_object(Bucket=..., Key=..., Body=data)
        # GCP: bucket.blob(name).upload_from_string(data)
        # Azure: blob_client.upload_blob(data)

    def _verify_transfer(
        self,
        source_data: List[Dict[str, Any]],
        transferred_data: bytes,
        job: SyncJob
    ) -> bool:
        """Verify data integrity after transfer."""
        import hashlib

        # Calculate source checksum
        import json
        source_bytes = json.dumps(source_data).encode()
        source_checksum = hashlib.sha256(source_bytes).hexdigest()

        logger.info(f"Transfer checksum validation: {source_checksum[:16]}...")

        # In production, retrieve and verify from target
        return True

    def _log_audit(self, job: SyncJob, success: bool, error: Optional[str] = None):
        """Log audit trail for sync."""
        audit_entry = {
            'job_id': job.job_id,
            'timestamp': datetime.now().isoformat(),
            'source': {
                'provider': job.source_endpoint.provider.value,
                'region': job.source_endpoint.region.value
            },
            'target': {
                'provider': job.target_endpoint.provider.value,
                'region': job.target_endpoint.region.value
            },
            'strategy': job.strategy.value,
            'success': success,
            'error': error
        }

        self.sync_history.append(audit_entry)

        # In production, send to immutable audit log
        # (e.g., AWS CloudTrail, GCP Cloud Audit Logs)

    def _update_avg_transfer_time(self, elapsed: float):
        """Update average transfer time metric."""
        total = self.metrics['successful_syncs']
        current_avg = self.metrics['avg_transfer_time']

        # Incremental average
        new_avg = (current_avg * (total - 1) + elapsed) / total
        self.metrics['avg_transfer_time'] = new_avg

    def get_sync_metrics(self) -> Dict[str, Any]:
        """
        Get synchronization metrics.

        Returns:
            Metrics dictionary
        """
        return {
            'metrics': self.metrics,
            'active_jobs': len(self.active_jobs),
            'recent_syncs': self.sync_history[-10:]  # Last 10
        }
\end{lstlisting}

\subsection{Real-World Scenario: GDPR Compliance Implementation}

\begin{lstlisting}[language=Python, caption={Complete GDPR-Compliant Hybrid Cloud Pipeline}]
# 1. Initialize hybrid cloud pipeline
pipeline = HybridCloudPipeline(
    name="customer-data-pipeline",
    primary_region=DataRegion.EU_WEST,
    enable_cross_region_sync=True  # With restrictions
)

# 2. Configure cloud endpoints
aws_eu_endpoint = CloudEndpoint(
    provider=CloudProvider.AWS,
    region=DataRegion.EU_WEST,
    endpoint_url="s3.eu-west-1.amazonaws.com",
    credentials="aws-eu-credentials",
    encryption_key="arn:aws:kms:eu-west-1:...",
    compliance_tags={'gdpr', 'iso27001'}
)

gcp_us_endpoint = CloudEndpoint(
    provider=CloudProvider.GCP,
    region=DataRegion.US_EAST,
    endpoint_url="storage.googleapis.com",
    credentials="gcp-us-credentials",
    encryption_key="projects/.../cryptoKeys/...",
    compliance_tags={'ccpa', 'soc2'}
)

azure_brazil_endpoint = CloudEndpoint(
    provider=CloudProvider.AZURE,
    region=DataRegion.BRAZIL_SOUTH,
    endpoint_url="blob.core.windows.net",
    credentials="azure-br-credentials",
    encryption_key="https://keyvault.../keys/...",
    compliance_tags={'lgpd'}
)

pipeline.register_endpoint(aws_eu_endpoint)
pipeline.register_endpoint(gcp_us_endpoint)
pipeline.register_endpoint(azure_brazil_endpoint)

# 3. Configure routing rules
pipeline.add_routing_rule("DE", DataRegion.EU_WEST)  # Germany
pipeline.add_routing_rule("FR", DataRegion.EU_WEST)  # France
pipeline.add_routing_rule("US", DataRegion.US_EAST)  # United States
pipeline.add_routing_rule("BR", DataRegion.BRAZIL_SOUTH)  # Brazil
pipeline.add_routing_rule("CA", DataRegion.CANADA_CENTRAL)  # Canada

# 4. Register compliance validators
gdpr_validator = GDPRValidator()
ccpa_validator = CCPAValidator()
hipaa_validator = HIPAAValidator()

pipeline.register_compliance_validator(gdpr_validator)
pipeline.register_compliance_validator(ccpa_validator)
pipeline.register_compliance_validator(hipaa_validator)

# 5. Configure data sovereignty
sovereignty_manager = DataSovereigntyManager()

# EU personal data must stay in EU
sovereignty_manager.add_rule(ResidencyRule(
    data_category=DataCategory.PERSONAL,
    required_regions={DataRegion.EU_WEST, DataRegion.CANADA_CENTRAL},
    forbidden_regions={DataRegion.US_EAST, DataRegion.APAC_SOUTHEAST},
    allow_temporary_export=True,
    export_duration_hours=24,
    audit_required=True
))

# Payment data restricted
sovereignty_manager.add_rule(ResidencyRule(
    data_category=DataCategory.PAYMENT,
    required_regions={DataRegion.US_EAST},  # PCI DSS certified region
    forbidden_regions=set(),
    allow_temporary_export=False,
    audit_required=True
))

# 6. Process EU customer data
eu_customer_data = {
    'user_id': 'user_123',
    'email': 'user@example.de',
    'personal_data': True,
    'category': 'personal',
    'consent_obtained': True,
    'consent_date': '2024-01-15',
    'purpose': 'service_delivery',
    'retention_until': '2025-01-15',
    'encrypted': True,
    'audit_logged': True
}

# Route to appropriate region
target_region = pipeline.route_data(eu_customer_data, user_region="DE")

if target_region:
    print(f"Data routed to: {target_region.value}")

    # Track in sovereignty manager
    sovereignty_manager.track_data_location('user_123', target_region)
else:
    print("Data routing failed - compliance violations detected")

# 7. Handle cross-region analytics (with controls)
# Request temporary export for ML training
export_id = sovereignty_manager.request_temporary_export(
    data_id='user_123',
    data_category=DataCategory.PERSONAL,
    source_region=DataRegion.EU_WEST,
    target_region=DataRegion.US_EAST,
    purpose='ML model training'
)

if export_id:
    print(f"Temporary export approved: {export_id}")

    # Perform sync
    synchronizer = CrossCloudSynchronizer()

    sync_job = SyncJob(
        job_id=export_id,
        source_endpoint=aws_eu_endpoint,
        target_endpoint=gcp_us_endpoint,
        data_filter={'user_id': 'user_123'},
        strategy=SyncStrategy.ON_DEMAND,
        encryption_enabled=True,
        compression_enabled=True,
        checksum_validation=True,
        audit_trail=True
    )

    success = synchronizer.execute_sync(sync_job)

    if success:
        print("Cross-cloud sync completed successfully")
    else:
        print("Cross-cloud sync failed")

else:
    print("Temporary export denied - sovereignty rules violated")

# 8. Generate compliance reports
compliance_report = pipeline.get_compliance_report()
print("\nCompliance Report:")
print(f"  Pipeline: {compliance_report['pipeline']}")
print(f"  Regions: {compliance_report['regions']}")
print(f"  Violations: {compliance_report['metrics']['compliance_violations']}")

residency_report = sovereignty_manager.generate_residency_report()
print("\nResidency Report:")
print(f"  Total data items: {residency_report['total_data_items']}")
print(f"  Active exports: {residency_report['active_exports']}")
print(f"  Data by region: {residency_report['data_by_region']}")

sync_metrics = synchronizer.get_sync_metrics()
print("\nSync Metrics:")
print(f"  Total syncs: {sync_metrics['metrics']['total_syncs']}")
print(f"  Success rate: {sync_metrics['metrics']['successful_syncs'] / max(sync_metrics['metrics']['total_syncs'], 1):.1%}")
print(f"  Avg transfer time: {sync_metrics['metrics']['avg_transfer_time']:.2f}s")

# 9. Handle data deletion request (GDPR Right to Erasure)
def handle_deletion_request(user_id: str):
    """Handle GDPR right to deletion request."""
    print(f"\nProcessing deletion request for user: {user_id}")

    # Find all locations where user data exists
    locations = sovereignty_manager.get_data_locations(user_id)

    print(f"User data found in regions: {[l.value for l in locations]}")

    # Delete from each location
    for region in locations:
        endpoint = pipeline.endpoints.get(region)
        if endpoint:
            print(f"  Deleting from {endpoint.provider.value} in {region.value}")
            # In production: cloud_client.delete_object(user_id)

    # Revoke any active exports
    for export_id, export in list(sovereignty_manager.active_exports.items()):
        if export['data_id'] == user_id:
            sovereignty_manager.revoke_export(export_id)
            print(f"  Revoked export: {export_id}")

    print(f"Deletion complete for user: {user_id}")

handle_deletion_request('user_123')
\end{lstlisting}

\subsubsection{Outcome}

With GDPR-compliant hybrid cloud implementation:

\begin{itemize}
    \item \textbf{Regulatory compliance}: Zero violations in 12-month audit
    \item \textbf{Data sovereignty}: 100\% of EU data stays in EU region
    \item \textbf{Cross-region controls}: All transfers logged and time-limited
    \item \textbf{Right to deletion}: Automated deletion across all clouds in <24 hours
    \item \textbf{Audit trail}: Immutable logs of all data access and movement
    \item \textbf{Cost reduction}: \$20M fine avoided + \$15M annual compliance costs saved
    \item \textbf{Customer trust}: 15 suspended customers returned + 40\% increase in EU signups
    \item \textbf{Multi-region deployment}: EU (AWS), US (GCP), Brazil (Azure) with unified management
\end{itemize}

\textbf{Key success factors:}
\begin{itemize}
    \item \textbf{Cloud-agnostic design}: Pipeline works across AWS, GCP, Azure
    \item \textbf{Automated compliance}: Validators prevent violations before they occur
    \item \textbf{Regional routing}: Data automatically routes to compliant region
    \item \textbf{Sovereignty controls}: Residency rules enforced programmatically
    \item \textbf{Secure sync}: Cross-cloud movement encrypted and audited
    \item \textbf{Deletion automation}: RTBF requests processed automatically
\end{itemize}

\section{Exercises}

\subsection{Exercise 1: Build Event-Driven Pipeline}

Create an event-driven pipeline that:
\begin{itemize}
    \item Consumes user behavior events from Kafka
    \item Computes real-time user engagement scores
    \item Uses sliding windows (5min, 15min, 1hour)
    \item Partitions by user\_id for ordering
    \item Publishes features to output topic
\end{itemize}

\subsection{Exercise 2: Schema Evolution}

Implement schema evolution:
\begin{itemize}
    \item Start with PurchaseCompleted v1.0 (order\_id, amount, timestamp)
    \item Add optional fields in v1.1 (discount\_code, referrer)
    \item Add required field in v2.0 (payment\_method)
    \item Ensure backward compatibility for v1.0 consumers
    \item Test schema validation and compatibility checking
\end{itemize}

\subsection{Exercise 3: Partition Rebalancing}

Analyze partition distribution:
\begin{itemize}
    \item Generate 100K events with realistic distribution
    \item Compare round-robin vs. hash partitioning
    \item Identify and fix partition hotspots
    \item Measure throughput impact of rebalancing
    \item Implement custom partitioner for VIP users
\end{itemize}

\subsection{Exercise 4: Exactly-Once Semantics}

Implement exactly-once processing:
\begin{itemize}
    \item Enable idempotent producer
    \item Implement transactional consumer
    \item Test duplicate event handling
    \item Measure performance overhead
    \item Compare with at-least-once approach
\end{itemize}

\subsection{Exercise 5: Late Event Handling}

Handle out-of-order events:
\begin{itemize}
    \item Simulate events arriving out-of-order
    \item Configure allowed lateness window
    \item Implement watermarking for window closing
    \item Test impact on aggregation accuracy
    \item Emit late event metrics
\end{itemize}

\subsection{Exercise 6: Build Data Mesh}

Implement data mesh architecture:
\begin{itemize}
    \item Create 3 domains (Marketing, Finance, Product) with ownership
    \item Define domain boundaries with entities and events
    \item Create 2 data products per domain with SLAs
    \item Implement cross-domain lineage (Marketing consumes Finance + Product)
    \item Register all products in federated catalog
    \item Test search and discovery functionality
\end{itemize}

\subsection{Exercise 7: Federated Governance}

Implement governance framework:
\begin{itemize}
    \item Create 3 global policies (PII detection, quality threshold, naming convention)
    \item Create 1 domain-specific policy for finance (ACID compliance check)
    \item Validate all data products against policies
    \item Generate compliance report
    \item Test policy violation detection and alerting
\end{itemize}

\subsection{Exercise 8: Data Product SLAs}

Implement SLA monitoring:
\begin{itemize}
    \item Track availability, latency, freshness, quality for data products
    \item Implement SLA violation detection
    \item Create alerts for SLA breaches
    \item Generate SLA compliance reports
    \item Test graceful degradation when SLAs violated
\end{itemize}

\subsection{Exercise 9: Containerize and Deploy Pipeline}

Containerize pipeline with Kubernetes:
\begin{itemize}
    \item Write Dockerfile for data pipeline with dependencies
    \item Create Kubernetes Deployment and Service manifests
    \item Configure HorizontalPodAutoscaler (min=1, max=5, CPU=70\%)
    \item Deploy to local Kubernetes cluster (minikube/kind)
    \item Test scaling behavior under load
    \item Monitor resource usage and adjust limits
\end{itemize}

\subsection{Exercise 10: Build Custom Operator}

Create Kubernetes operator for pipelines:
\begin{itemize}
    \item Define DataPipeline CRD with source, transform, sink
    \item Implement operator with create/update/delete handlers using kopf
    \item Handle both scheduled (CronJob) and continuous (Deployment) pipelines
    \item Add autoscaling support based on resource utilization
    \item Test operator with multiple pipeline resources
    \item Implement status updates and error reporting
\end{itemize}

\subsection{Exercise 11: Serverless Functions}

Implement serverless data processing:
\begin{itemize}
    \item Create 3 Knative functions (ingest, transform, validate)
    \item Chain functions using KafkaSource triggers
    \item Configure auto-scaling (scale-to-zero enabled)
    \item Test function chain end-to-end
    \item Measure cold start latency
    \item Compare cost with always-on deployment
\end{itemize}

\subsection{Exercise 12: Hybrid Cloud Pipeline with Compliance}

Build GDPR-compliant hybrid cloud pipeline:
\begin{itemize}
    \item Configure endpoints for 3 regions (EU, US, Asia)
    \item Implement routing rules based on user region
    \item Add GDPR and CCPA compliance validators
    \item Test data residency enforcement (EU data stays in EU)
    \item Handle cross-region transfer with approval workflow
    \item Generate compliance reports showing violations
\end{itemize}

\subsection{Exercise 13: Data Sovereignty Automation}

Implement automated data sovereignty controls:
\begin{itemize}
    \item Define residency rules for personal, payment, health data
    \item Track data inventory across multiple regions
    \item Request and validate temporary data exports
    \item Implement automatic export expiration
    \item Handle GDPR right-to-deletion requests
    \item Generate residency compliance reports
\end{itemize}

\subsection{Exercise 14: Cross-Cloud Synchronization}

Build secure cross-cloud data synchronizer:
\begin{itemize}
    \item Implement sync between AWS S3 and GCP Cloud Storage
    \item Add encryption using cloud KMS (both providers)
    \item Implement checksum validation for integrity
    \item Create audit trail for all sync operations
    \item Handle sync failures with retry logic
    \item Measure sync performance and optimize
\end{itemize}

\section{Comprehensive Exercises}

The following exercises integrate concepts from across the chapter, requiring you to make architectural decisions, implement production-ready solutions, and navigate real-world trade-offs.

\subsection{Exercise 15: Event-Driven Pipeline with Kafka (25-30 words)}

Build a production-grade event-driven pipeline using Apache Kafka. Implement a consumer group processing clickstream events with exactly-once semantics, sliding windows for real-time aggregation, schema evolution with Avro, and comprehensive monitoring for lag and throughput.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Kafka cluster with 3 brokers and replication factor 3
    \item Consumer group with 5 workers processing 10 partitions
    \item Exactly-once processing using idempotent producers and transactional consumers
    \item Sliding windows: 5-minute, 15-minute, 1-hour aggregations
    \item Schema registry with Avro schemas for backward compatibility
    \item Monitor consumer lag, throughput, error rate, partition distribution
    \item Handle out-of-order events with watermarking (5-minute allowed lateness)
    \item Dead letter queue for failed messages with retry policy
\end{itemize}

\subsection{Exercise 16: Fault-Tolerant Pipeline with Error Handling (25-30 words)}

Design and implement a fault-tolerant data pipeline with comprehensive error handling. Include circuit breakers for external dependencies, exponential backoff retry logic, dead letter queues, graceful degradation strategies, and automated recovery procedures.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Circuit breaker for API calls (fail after 5 consecutive errors, 30s cooldown)
    \item Exponential backoff retry: initial delay 100ms, max 10s, max 5 retries
    \item Dead letter queue storing failed records with error context
    \item Partial failure handling: continue processing valid records
    \item State persistence for recovery after crashes
    \item Health checks: liveness (process alive) and readiness (can accept work)
    \item Automated failover to backup data source when primary fails
    \item Recovery procedure: replay from last checkpoint after failure
\end{itemize}

\subsection{Exercise 17: Data Quality Gate System (25-30 words)}

Create a comprehensive data quality gate system that validates data before downstream consumption. Implement statistical profiling, anomaly detection, schema validation, business rule checks, and automated alerting with configurable quality thresholds and SLA enforcement.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Schema validation: type checking, required fields, format patterns
    \item Statistical profiling: null rate, cardinality, distribution, outliers
    \item Anomaly detection: sudden volume changes (>30\%), unexpected value ranges
    \item Business rules: referential integrity, logical constraints (end > start)
    \item Quality dimensions: completeness, accuracy, consistency, timeliness, validity
    \item Quality score calculation: weighted average across dimensions
    \item Thresholds: block pipeline if score <70\%, warn if <85\%
    \item Alerting: Slack/PagerDuty integration with runbook links
\end{itemize}

\subsection{Exercise 18: Scalable Pipeline with Dynamic Resources (25-30 words)}

Build an auto-scaling data pipeline that dynamically adjusts resources based on workload. Implement horizontal pod autoscaling, vertical resource adjustment, cost-aware scheduling, queue depth monitoring, and predictive scaling using historical patterns.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Kubernetes HPA: scale pods from 2-20 based on CPU >70\%
    \item VPA: adjust memory limits based on actual usage patterns
    \item Custom metrics: scale based on Kafka consumer lag >10K messages
    \item Cost-aware scheduling: prefer spot instances for batch jobs
    \item Predictive scaling: scale up 15 minutes before daily peak at 8 AM
    \item Resource quotas: limit namespace to 100 vCPU, 200 GB RAM
    \item Priority classes: critical jobs get guaranteed resources
    \item Scale-to-zero for idle workloads (>30 minutes no traffic)
\end{itemize}

\subsection{Exercise 19: Data Mesh Architecture Implementation (25-30 words)}

Implement a complete data mesh architecture with domain ownership, federated governance, and self-serve infrastructure. Create data products with clear SLAs, automated quality checks, discoverability through centralized catalog, and cross-domain lineage tracking.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Three domains: Marketing (customer\_events, segmentation), Finance (transactions, reconciliation), Product (features, metrics)
    \item Data products with SLAs: 99.9\% availability, <5min freshness, >95\% quality
    \item Self-serve platform: templates for pipelines, monitoring, deployment
    \item Federated catalog: searchable metadata with ownership and lineage
    \item Cross-domain consumption: Marketing depends on Finance + Product
    \item Automated policies: PII detection, quality gates, naming conventions
    \item Data contracts: versioned schemas with breaking change detection
    \item Team autonomy: domains deploy independently without central approval
\end{itemize}

\subsection{Exercise 20: Real-Time Analytics Pipeline (25-30 words)}

Design a real-time analytics pipeline processing millions of events per second. Implement stream processing with Apache Flink, stateful aggregations, sessionization, real-time feature computation, and sub-second latency serving layer integration.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Flink cluster processing 1M events/second with <500ms p99 latency
    \item Stateful processing: maintain user session state with RocksDB backend
    \item Sessionization: 30-minute timeout windows for user activity
    \item Real-time features: user engagement score, trending content, fraud signals
    \item Serving layer: write features to Redis for <10ms online serving
    \item Exactly-once state consistency with checkpointing every 30 seconds
    \item Late event handling: 5-minute watermark for out-of-order events
    \item Backpressure handling: monitor and alert on source lag
\end{itemize}

\subsection{Exercise 21: Data Lineage Tracking System (25-30 words)}

Build a comprehensive data lineage tracking system capturing column-level dependencies across pipelines. Implement automated lineage extraction, impact analysis for schema changes, root cause analysis for data quality issues, and visualization interface.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Automated lineage extraction from SQL queries using parser
    \item Column-level lineage: track which source columns affect each output column
    \item Cross-system tracking: databases, data lakes, warehouses, ML models
    \item Impact analysis: "If I change this column, what breaks downstream?"
    \item Root cause analysis: trace data quality issues to upstream sources
    \item Lineage graph storage using Neo4j for efficient graph queries
    \item Visualization UI showing upstream dependencies and downstream consumers
    \item API for programmatic lineage queries in CI/CD pipelines
\end{itemize}

\subsection{Exercise 22: Disaster Recovery Procedures (25-30 words)}

Design and test disaster recovery procedures for critical data pipelines. Implement multi-region replication, automated failover, backup and restore procedures, recovery time objective validation, and comprehensive runbooks.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Active-passive replication: primary in US-East, standby in US-West
    \item Automated failover: promote standby when primary unhealthy >5 minutes
    \item Backup strategy: full daily backups, incremental hourly, retained 30 days
    \item Point-in-time recovery: restore to any state within retention period
    \item RTO validation: test achieving <15 minute recovery time objective
    \item RPO validation: verify <5 minute recovery point objective (data loss)
    \item Disaster simulation: monthly chaos engineering exercises
    \item Runbooks: detailed procedures for 10 failure scenarios with escalation paths
\end{itemize}

\subsection{Exercise 23: Change Data Capture Pipeline (25-30 words)}

Implement a CDC pipeline capturing real-time database changes. Use Debezium to stream PostgreSQL WAL, handle schema evolution, implement conflict resolution for multi-region writes, and maintain exactly-once delivery guarantees.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Debezium connector streaming PostgreSQL WAL to Kafka
    \item Capture INSERT, UPDATE, DELETE operations with before/after states
    \item Schema evolution: handle ALTER TABLE statements gracefully
    \item Initial snapshot: full table sync before streaming incremental changes
    \item Conflict resolution: last-write-wins with vector clock timestamps
    \item Exactly-once delivery: deduplicate based on transaction ID + LSN
    \item Transform to target schema: CDC events to denormalized analytics format
    \item Monitoring: replication lag <5 seconds, capture 99.99\% of changes
\end{itemize}

\subsection{Exercise 24: Cost Optimization System (25-30 words)}

Build an automated cost optimization system for data pipelines. Implement resource right-sizing, spot instance usage, auto-scaling policies, idle resource detection, budget tracking with alerts, and cost allocation by team.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Resource right-sizing: analyze utilization, recommend downsizing if <30\% CPU
    \item Spot instance policy: use spot for batch jobs, save 70\% on compute costs
    \item Reserved instances: purchase 1-year reservations for predictable workloads
    \item Auto-scaling: scale down during off-peak hours (10 PM - 6 AM)
    \item Idle detection: terminate resources with <5\% utilization for >1 hour
    \item Budget tracking: alert when monthly spend exceeds 80\% of budget
    \item Cost allocation: tag resources by team, generate chargeback reports
    \item Optimization dashboard: show savings opportunities ranked by impact
\end{itemize}

\subsection{Exercise 25: Privacy-Preserving Pipeline (25-30 words)}

Create a privacy-preserving data pipeline implementing differential privacy, k-anonymity, data masking, and pseudonymization. Ensure GDPR compliance with automated PII detection, consent management, right-to-erasure implementation, and privacy impact assessments.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Differential privacy: ε=1.0 privacy budget for user-level aggregations
    \item K-anonymity: ensure k≥5 for quasi-identifiers (age, zip code)
    \item Data masking: redact PII patterns (emails, phones, SSNs)
    \item Pseudonymization: irreversible SHA-256 hashing of user identifiers
    \item Automated PII detection: scan for 15 PII patterns using regex + NER
    \item Consent management: respect user opt-outs, suppress opted-out data
    \item Right to erasure: automated GDPR deletion with 30-day deadline
    \item Privacy impact assessment: document data flows and risks
\end{itemize}

\subsection{Exercise 26: Comprehensive Monitoring System (25-30 words)}

Design a comprehensive monitoring system for data pipelines. Implement metrics collection, distributed tracing, log aggregation, alerting with SLO-based alerting, dashboards for SLI tracking, and on-call runbooks.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Metrics: Prometheus collecting throughput, latency, error rate, queue depth
    \item SLIs: availability ≥99.9\%, latency p99 <500ms, freshness <5min, quality >95\%
    \item SLO-based alerting: error budget consumption, multi-window burn rates
    \item Distributed tracing: Jaeger tracking requests across 10+ microservices
    \item Log aggregation: ELK stack with 7-day retention, 10TB/day indexed
    \item Dashboards: Golden signals (latency, traffic, errors, saturation)
    \item Anomaly detection: alert on >3 std dev from baseline
    \item On-call runbooks: 20 runbooks covering 80\% of incidents
\end{itemize}

\subsection{Exercise 27: Data Versioning System (25-30 words)}

Implement a comprehensive data versioning system with semantic versioning, schema evolution tracking, breaking change detection, automated rollback capabilities, and version compatibility testing. Enable time-travel queries and reproducible analytics.

\textbf{Implementation requirements:}
\begin{itemize}
    \item Semantic versioning: MAJOR.MINOR.PATCH following semver spec
    \item Schema evolution: track schema changes with version history
    \item Breaking change detection: removed fields, type changes trigger MAJOR bump
    \item Automated rollback: revert to previous version within 5 minutes
    \item Compatibility matrix: test all consumer versions against all schema versions
    \item Time-travel queries: query data as of specific timestamp or version
    \item Version tagging: tag versions with business meaning (e.g., "Q4-2024-report")
    \item Deprecation policy: 90-day notice before removing deprecated fields
\end{itemize}

\section{Architectural Guidance}

This section provides frameworks and templates for making architectural decisions about data pipelines. Use these tools to evaluate options, document decisions, and ensure alignment with organizational goals.

\subsection{Pipeline Architecture Decision Framework}

When designing a data pipeline, systematically evaluate eight key dimensions:

\subsubsection{1. Latency Requirements}

\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Latency} & \textbf{Architecture} & \textbf{Technologies} \\
\hline
Real-time (<100ms) & Stream processing with in-memory state & Apache Flink, Kafka Streams, Redis \\
\hline
Near real-time (<5min) & Micro-batch processing & Apache Spark Streaming, Kafka \\
\hline
Batch (hours/days) & Scheduled batch jobs & Apache Spark, Airflow, dbt \\
\hline
\end{tabular}

\textbf{Decision criteria:}
\begin{itemize}
    \item Do downstream consumers need data within seconds? → Real-time
    \item Is 5-15 minute freshness acceptable? → Near real-time (simpler, cheaper)
    \item Is daily/hourly refresh sufficient? → Batch (most cost-effective)
    \item Consider total cost of ownership: real-time is 5-10x more expensive
\end{itemize}

\subsubsection{2. Data Volume and Velocity}

\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Scale} & \textbf{Architecture} & \textbf{Technologies} \\
\hline
Low (<1 GB/day) & Single-node processing & Python scripts, PostgreSQL \\
\hline
Medium (1-100 GB/day) & Distributed processing & Spark, Presto, Kubernetes \\
\hline
High (>100 GB/day) & Horizontally scalable & Flink, Kafka, Object storage \\
\hline
Very high (>1 TB/day) & Partitioned, multi-region & Lambda architecture, CDN \\
\hline
\end{tabular}

\textbf{Decision criteria:}
\begin{itemize}
    \item Measure current data volume and project 3-year growth
    \item Consider velocity: how fast is data arriving?
    \item Factor in peak loads: Black Friday might be 10x normal volume
    \item Don't over-engineer: use simplest solution that handles 2x expected load
\end{itemize}

\subsubsection{3. Data Consistency Requirements}

\begin{tabular}{|p{3cm}|p{10cm}|}
\hline
\textbf{Level} & \textbf{Use Cases and Implementation} \\
\hline
Strong consistency & Financial transactions, inventory management. Use ACID databases, distributed transactions, 2-phase commit. \\
\hline
Eventual consistency & Analytics, ML training, reporting. Use event sourcing, idempotent operations, conflict-free replicated data types. \\
\hline
Causal consistency & Social feeds, collaborative editing. Use version vectors, happens-before relationships. \\
\hline
\end{tabular}

\textbf{Decision criteria:}
\begin{itemize}
    \item Can the business tolerate temporary inconsistencies?
    \item What's the cost of an inconsistency? (Financial loss? User confusion?)
    \item Strong consistency reduces availability (CAP theorem)
    \item Most analytics workloads can use eventual consistency
\end{itemize}

\subsubsection{4. Fault Tolerance and Reliability}

\textbf{Reliability tiers:}

\begin{itemize}
    \item \textbf{Critical (99.99\% availability)}: Financial transactions, real-time fraud detection
        \begin{itemize}
            \item Multi-region active-active deployment
            \item Automated failover <1 minute
            \item Zero data loss (RPO=0)
            \item Annual budget: \$500K-2M+
        \end{itemize}

    \item \textbf{High (99.9\% availability)}: Customer-facing dashboards, operational metrics
        \begin{itemize}
            \item Active-passive replication
            \item Manual failover <15 minutes
            \item <5 minute data loss acceptable
            \item Annual budget: \$100K-500K
        \end{itemize}

    \item \textbf{Standard (99\% availability)}: Internal reporting, ad-hoc analysis
        \begin{itemize}
            \item Daily backups
            \item Recovery from backup acceptable
            \item Hours of data loss acceptable
            \item Annual budget: \$10K-100K
        \end{itemize}
\end{itemize}

\subsubsection{5. Schema Evolution Strategy}

\begin{tabular}{|p{3.5cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Strategy} & \textbf{When to Use} & \textbf{Trade-offs} \\
\hline
Backward compatible & Adding optional fields & Old consumers work, new fields ignored \\
\hline
Forward compatible & Removing optional fields & New consumers handle old data \\
\hline
Full compatibility & Both directions & Restricted changes, safest \\
\hline
Breaking changes & Major redesigns & Requires coordinated deployment \\
\hline
\end{tabular}

\textbf{Best practices:}
\begin{itemize}
    \item Default to backward compatibility for gradual rollout
    \item Use schema registry (Confluent, AWS Glue) for validation
    \item Version schemas explicitly (v1.0, v1.1, v2.0)
    \item Maintain compatibility matrix: which consumer versions work with which schemas
    \item Deprecation policy: 90-day notice before breaking changes
\end{itemize}

\subsubsection{6. Organizational Maturity}

\begin{tabular}{|p{3cm}|p{10cm}|}
\hline
\textbf{Maturity} & \textbf{Recommended Architecture} \\
\hline
Ad-hoc (Level 1) & Python scripts, cron jobs, manual execution. Focus on delivering value quickly. \\
\hline
Repeatable (Level 2) & Orchestrated pipelines (Airflow, Prefect), version control, basic monitoring. \\
\hline
Defined (Level 3) & Standardized frameworks, CI/CD, automated testing, SLOs, on-call rotation. \\
\hline
Managed (Level 4) & Self-serve platforms, data mesh, federated governance, comprehensive observability. \\
\hline
Optimizing (Level 5) & Automated optimization, cost management, ML-driven operations, proactive issue detection. \\
\hline
\end{tabular}

\textbf{Anti-pattern:} Don't build Level 5 architecture for Level 2 organization. Complexity will crush productivity.

\subsection{Technology Selection Matrix}

Compare technologies across key dimensions for informed selection:

\subsubsection{Batch Processing}

\begin{tabular}{|p{2.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Technology} & \textbf{Best For} & \textbf{Complexity} & \textbf{Cost} & \textbf{Latency} & \textbf{Ecosystem} \\
\hline
Apache Spark & Large-scale ETL & Medium & Medium & Minutes & Mature \\
\hline
dbt & SQL transforms & Low & Low & Minutes & Growing \\
\hline
Pandas/Dask & Small to medium & Low & Low & Minutes & Mature \\
\hline
AWS Glue & AWS-native ETL & Low & Medium & Minutes & AWS only \\
\hline
\end{tabular}

\subsubsection{Stream Processing}

\begin{tabular}{|p{2.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Technology} & \textbf{Best For} & \textbf{Complexity} & \textbf{Cost} & \textbf{Latency} & \textbf{Ecosystem} \\
\hline
Apache Flink & Complex CEP & High & High & <100ms & Growing \\
\hline
Kafka Streams & Simple transforms & Medium & Medium & <100ms & Mature \\
\hline
Spark Streaming & Micro-batches & Medium & Medium & Seconds & Mature \\
\hline
AWS Kinesis & AWS-native & Low & High & <100ms & AWS only \\
\hline
\end{tabular}

\subsubsection{Orchestration}

\begin{tabular}{|p{2.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Technology} & \textbf{Best For} & \textbf{Complexity} & \textbf{Community} & \textbf{Features} & \textbf{Cloud} \\
\hline
Apache Airflow & Complex DAGs & High & Large & Rich & All clouds \\
\hline
Prefect & Dynamic flows & Medium & Medium & Modern & All clouds \\
\hline
Dagster & Data-aware & Medium & Growing & Asset-centric & All clouds \\
\hline
AWS Step Functions & AWS workflows & Low & Small & Basic & AWS only \\
\hline
\end{tabular}

\textbf{Selection process:}
\begin{enumerate}
    \item List hard requirements (must-haves) and soft requirements (nice-to-haves)
    \item Eliminate technologies that don't meet hard requirements
    \item Score remaining options on soft requirements (1-5 scale)
    \item Build proof-of-concept with top 2 candidates
    \item Consider total cost: licensing + infrastructure + engineering time
    \item Factor in team expertise: familiar technology reduces risk
\end{enumerate}

\subsection{Architectural Decision Records (ADR)}

Document architectural decisions using ADR template:

\begin{lstlisting}[language=tex, caption=ADR Template for Pipeline Architecture]
# ADR-[NUMBER]: [DECISION TITLE]

**Date:** YYYY-MM-DD
**Status:** Proposed | Accepted | Superseded | Deprecated
**Deciders:** [List of people involved]
**Technical Story:** [Ticket/Issue link]

## Context

What is the issue that we're seeing that motivates this decision?
- Business context: Why does this matter?
- Technical context: What constraints exist?
- Timeline: When is decision needed?

## Decision Drivers

- [driver 1, e.g., need to support 10x data volume growth]
- [driver 2, e.g., reduce infrastructure costs by 30%]
- [driver 3, e.g., improve data freshness from 1 hour to 5 minutes]

## Considered Options

### Option 1: [Option Name]
**Pros:**
- [good aspect 1]
- [good aspect 2]

**Cons:**
- [bad aspect 1]
- [bad aspect 2]

**Estimated cost:** [one-time + ongoing]
**Implementation time:** [estimate]

### Option 2: [Option Name]
[Same structure]

### Option 3: [Option Name]
[Same structure]

## Decision Outcome

**Chosen option:** "[option name]"

**Justification:**
We chose this option because [reasoning]. This decision optimizes for
[key driver] at the cost of [trade-off]. We accept this trade-off
because [reasoning].

**Consequences:**
- **Positive:** [benefit 1], [benefit 2]
- **Negative:** [cost 1], [cost 2]
- **Neutral:** [impact 1]

## Implementation Plan

1. [step 1 with owner and deadline]
2. [step 2 with owner and deadline]
3. [step 3 with owner and deadline]

## Validation

How will we know if this decision was correct?
- [metric 1]: [target value] by [date]
- [metric 2]: [target value] by [date]

**Review date:** [date to revisit this decision]

## Links

- [Supporting documentation]
- [Related ADRs]
- [Proof of concept results]
\end{lstlisting}

\textbf{Example ADR:}

\begin{lstlisting}[language=tex]
# ADR-042: Migrate from Batch to Stream Processing for User Features

**Date:** 2024-11-17
**Status:** Accepted
**Deciders:** Data Platform Team, ML Engineering Team
**Technical Story:** JIRA-5832

## Context

Our current batch pipeline processes user behavior events daily at 2 AM,
computing engagement scores for ML models. This causes two problems:

1. ML models use stale features (up to 24 hours old)
2. Marketing campaigns can't react to user behavior in real-time

Business requirement: Real-time fraud detection needs features <1 minute fresh.

## Decision Drivers

- **P0:** Enable real-time fraud detection (business commitment to investors)
- **P1:** Improve ML model performance with fresher features
- **P2:** Reduce infrastructure costs (current batch job over-provisioned)
- **P3:** Simplify architecture (currently maintaining 2 codebases)

## Considered Options

### Option 1: Apache Flink Stream Processing
**Pros:**
- True streaming with <100ms latency
- Exactly-once processing guarantees
- Mature ecosystem and strong community

**Cons:**
- High operational complexity (requires dedicated SRE)
- Steep learning curve for team
- Higher infrastructure cost ($8K/month vs $3K current)

**Estimated cost:** $50K implementation + $8K/month
**Implementation time:** 3 months

### Option 2: Spark Structured Streaming (Micro-batches)
**Pros:**
- Team already knows Spark
- Reuse existing batch code (~70% overlap)
- Lower operational complexity
- Meets 5-minute latency requirement

**Cons:**
- Not true streaming (5-minute micro-batches)
- Can't achieve <1 minute for fraud detection
- May need to migrate to Flink later

**Estimated cost:** $20K implementation + $5K/month
**Implementation time:** 6 weeks

### Option 3: Hybrid: Keep Batch + Add Flink for Fraud Only
**Pros:**
- Minimizes scope (only fraud detection use case)
- Learn Flink with small, critical use case
- Keep low-risk batch pipeline for other features

**Cons:**
- Maintain two systems
- Duplicate infrastructure and operations
- May delay other real-time use cases

**Estimated cost:** $30K implementation + $6K/month
**Implementation time:** 6 weeks

## Decision Outcome

**Chosen option:** "Option 2: Spark Structured Streaming"

**Justification:**
We chose Spark Streaming because it meets the P0 requirement (fraud
detection within 5 minutes is acceptable per discussion with fraud team)
and minimizes risk by leveraging existing team expertise. We accept
that this may not be the final architecture - if we need <1 minute
latency in the future, we'll migrate to Flink. For now, we optimize for
delivery speed and operational simplicity.

**Consequences:**
- **Positive:** Faster time-to-market, lower learning curve, code reuse
- **Negative:** 5-minute latency (not <1 minute), may need future migration
- **Neutral:** Similar cost to current batch solution

## Implementation Plan

1. **Week 1-2:** Proof of concept (Jane) - Validate 5-minute latency meets fraud requirements
2. **Week 3-4:** Refactor batch code to streaming (Team) - Extract shared logic
3. **Week 5:** Deploy to staging (John) - Load test with production traffic
4. **Week 6:** Production rollout (Team) - Blue-green deployment with rollback plan

## Validation

Success metrics measured 3 months post-launch:
- **Latency:** p99 feature freshness <5 minutes (target: <3 minutes)
- **Accuracy:** Fraud detection precision ≥95% (current: 87%)
- **Cost:** Infrastructure cost <$6K/month (target: <$5K)
- **Reliability:** 99.9% uptime (zero data loss)

**Review date:** 2025-02-17 (assess if Flink migration needed)

## Links

- Proof of concept: https://github.com/company/streaming-poc
- Fraud team requirements: https://docs/fraud-requirements
- Related: ADR-038 (Kafka as event backbone)
\end{lstlisting}

\subsection{Migration Strategies}

Migrating from legacy batch pipelines to modern architectures requires careful planning:

\subsubsection{Strategy 1: Strangler Fig Pattern}

Gradually replace legacy system by routing new functionality to new system:

\textbf{Phase 1: Routing Layer (Month 1-2)}
\begin{itemize}
    \item Deploy routing layer intercepting pipeline inputs
    \item Route all traffic to legacy system initially
    \item Validate routing layer has zero impact
    \item Set up dual-write: write to both old and new systems
\end{itemize}

\textbf{Phase 2: Parallel Run (Month 3-6)}
\begin{itemize}
    \item Build new pipeline processing 100\% of data
    \item Compare outputs: new vs. legacy (should be identical)
    \item Fix discrepancies until agreement >99.9\%
    \item Run in parallel for 2 months to build confidence
\end{itemize}

\textbf{Phase 3: Traffic Shift (Month 7-9)}
\begin{itemize}
    \item Route 10\% of consumers to new pipeline
    \item Monitor for errors, performance issues
    \item Incrementally increase: 25\%, 50\%, 75\%, 100\%
    \item Maintain ability to instant rollback to legacy
\end{itemize}

\textbf{Phase 4: Decommission (Month 10-12)}
\begin{itemize}
    \item Legacy system in read-only mode for 1 month
    \item Archive legacy data to cold storage
    \item Remove legacy infrastructure
    \item Celebrate with team!
\end{itemize}

\subsubsection{Strategy 2: Big Bang Migration}

Replace entire system during maintenance window:

\textbf{When to use:}
\begin{itemize}
    \item Small, isolated pipeline with limited consumers
    \item Strong test coverage provides confidence
    \item Business can tolerate downtime (e.g., overnight maintenance)
    \item Cost of parallel run exceeds risk of downtime
\end{itemize}

\textbf{Execution plan:}
\begin{enumerate}
    \item \textbf{Pre-migration (1 week before)}
        \begin{itemize}
            \item Freeze legacy changes (code freeze)
            \item Test new pipeline in staging with production data snapshot
            \item Prepare rollback procedure (< 30 minutes)
            \item Notify all stakeholders of maintenance window
        \end{itemize}

    \item \textbf{Migration (4-hour window)}
        \begin{itemize}
            \item 0:00 - Take final backup of legacy system
            \item 0:15 - Shut down legacy pipeline
            \item 0:30 - Deploy new pipeline
            \item 1:00 - Run smoke tests (process sample data)
            \item 2:00 - Process backlog accumulated during migration
            \item 3:00 - Validate outputs match expectations
            \item 3:30 - Enable monitoring and alerts
            \item 3:45 - Notify stakeholders of completion
        \end{itemize}

    \item \textbf{Post-migration (1 week after)}
        \begin{itemize}
            \item Monitor metrics 24/7 for first 48 hours
            \item Keep legacy system available for quick rollback
            \item Address issues within 4-hour SLA
            \item After 1 week of stability, decommission legacy
        \end{itemize}
\end{enumerate}

\subsubsection{Strategy 3: Feature Flags}

Use feature flags for gradual rollout within same codebase:

\begin{lstlisting}[language=Python, caption=Feature Flag Pattern for Pipeline Migration]
def process_user_events(events: List[Event]) -> List[Feature]:
    """Process user events with feature flag for new logic."""

    # Feature flag determines which implementation to use
    use_new_pipeline = feature_flags.is_enabled(
        flag="new_engagement_scoring",
        user_id=get_current_user()  # Or use cohort/region
    )

    if use_new_pipeline:
        # New implementation with improved algorithm
        features = compute_features_v2(events)
        metrics.increment("pipeline.version.v2")
    else:
        # Legacy implementation
        features = compute_features_v1(events)
        metrics.increment("pipeline.version.v1")

    # Log both for comparison during migration
    if config.MIGRATION_MODE:
        features_v1 = compute_features_v1(events)
        features_v2 = compute_features_v2(events)
        compare_and_log_differences(features_v1, features_v2)

    return features
\end{lstlisting}

\textbf{Rollout plan:}
\begin{itemize}
    \item Week 1: Enable for 5\% of users, monitor for errors
    \item Week 2: 25\% rollout if no issues detected
    \item Week 3: 50\% rollout, compare performance metrics
    \item Week 4: 100\% rollout if metrics meet targets
    \item Week 5: Remove feature flag and old code
\end{itemize}

\section{Pipeline Maturity Model}

Assess your organization's pipeline engineering maturity and identify improvement opportunities:

\subsection{Level 1: Ad-Hoc (Initial)}

\textbf{Characteristics:}
\begin{itemize}
    \item Pipelines are one-off Python scripts run manually or via cron
    \item No version control or limited Git usage
    \item Failure recovery requires manual intervention
    \item No monitoring beyond "did it run?"
    \item Knowledge siloed in individual engineers
    \item Changes made directly in production
\end{itemize}

\textbf{Indicators you're at this level:}
\begin{itemize}
    \item "The pipeline broke and only Sarah knows how to fix it"
    \item "We run this script every Monday, but not sure what it does"
    \item "Last week's data is wrong but we can't reprocess it"
\end{itemize}

\textbf{Path to Level 2:}
\begin{itemize}
    \item Implement version control for all pipeline code
    \item Add basic orchestration (Airflow, Prefect)
    \item Document pipeline inputs, outputs, and logic
    \item Set up email alerts for pipeline failures
    \item Establish weekly on-call rotation
\end{itemize}

\subsection{Level 2: Repeatable (Managed)}

\textbf{Characteristics:}
\begin{itemize}
    \item Pipelines orchestrated with workflow engine (Airflow, Prefect)
    \item Version control with Git, basic CI/CD
    \item Monitoring: success/failure, duration, row counts
    \item Documented runbooks for common failures
    \item Scheduled based on time (not data availability)
    \item Manual testing before production deploy
\end{itemize}

\textbf{Indicators you're at this level:}
\begin{itemize}
    \item "Airflow sends us alerts when pipelines fail"
    \item "We can reprocess historical data by manually triggering DAG"
    \item "Pipeline code is in Git but tests are sparse"
\end{itemize}

\textbf{Path to Level 3:}
\begin{itemize}
    \item Implement automated testing (unit, integration, end-to-end)
    \item Add data quality checks with automated validation
    \item Define and track SLOs (latency, freshness, quality)
    \item Implement CI/CD with automated deployments
    \item Standardize pipeline patterns and templates
    \item Migrate from time-based to data-driven scheduling
\end{itemize}

\subsection{Level 3: Defined (Standardized)}

\textbf{Characteristics:}
\begin{itemize}
    \item Standardized pipeline framework used across organization
    \item Comprehensive testing: unit tests (>80\% coverage), integration tests, data quality tests
    \item CI/CD with automated deployments to staging and production
    \item SLO-based monitoring and alerting (not just up/down)
    \item Data quality gates block bad data from propagating
    \item Lineage tracking shows dependencies
    \item On-call team with escalation procedures
\end{itemize}

\textbf{Indicators you're at this level:}
\begin{itemize}
    \item "We have pipeline templates that new teams use"
    \item "Tests catch 80\% of bugs before production"
    \item "We track SLOs for freshness, quality, and availability"
\end{itemize}

\textbf{Path to Level 4:}
\begin{itemize}
    \item Implement self-serve platform for pipeline creation
    \item Enable domain ownership (data mesh principles)
    \item Add advanced observability (distributed tracing, profiling)
    \item Implement automated rollback on quality degradation
    \item Create data contracts between producers and consumers
    \item Build internal developer portal for discovery
\end{itemize}

\subsection{Level 4: Managed (Data Mesh)}

\textbf{Characteristics:}
\begin{itemize}
    \item Self-serve data platform enabling domain autonomy
    \item Domains own their data products end-to-end
    \item Federated governance with automated policy enforcement
    \item Centralized catalog with search and discovery
    \item Data contracts ensure backward compatibility
    \item Cross-functional teams (analytics engineers, data engineers, domain experts)
    \item Comprehensive observability (metrics, logs, traces, profiling)
\end{itemize}

\textbf{Indicators you're at this level:}
\begin{itemize}
    \item "Marketing team owns and operates their customer data products"
    \item "We have 50+ data products registered in our catalog"
    \item "Policy violations are caught automatically in CI/CD"
\end{itemize}

\textbf{Path to Level 5:}
\begin{itemize}
    \item Implement ML-driven operations (AIOps)
    \item Add automated cost optimization and resource right-sizing
    \item Enable proactive issue detection with anomaly detection
    \item Implement automated incident response and remediation
    \item Build predictive capacity planning
    \item Create automated architecture recommendations
\end{itemize}

\subsection{Level 5: Optimizing (Self-Healing)}

\textbf{Characteristics:}
\begin{itemize}
    \item ML models predict and prevent failures
    \item Automated cost optimization adjusts resources dynamically
    \item Self-healing: automatic issue detection and remediation
    \item Continuous experimentation with A/B testing for pipeline improvements
    \item Predictive scaling based on forecast models
    \item Automated performance tuning
    \item Chaos engineering validates resilience continuously
\end{itemize}

\textbf{Indicators you're at this level:}
\begin{itemize}
    \item "Our system predicted and prevented an outage before it happened"
    \item "Cost optimization saved 40\% without manual intervention"
    \item "We run chaos experiments weekly to validate fault tolerance"
\end{itemize}

\textbf{Maintain excellence:}
\begin{itemize}
    \item Share learnings across industry (conferences, blog posts)
    \item Contribute to open source projects
    \item Continue investing in innovation (10-20\% of capacity)
    \item Avoid complacency: new failure modes always emerge
\end{itemize}

\section{Troubleshooting Guide}

Common pipeline issues and systematic approaches to resolution:

\subsection{Issue 1: Pipeline Running Slowly}

\textbf{Symptoms:}
\begin{itemize}
    \item Pipeline duration increased from 30 minutes to 3 hours
    \item Queries timing out
    \item Resource utilization (CPU, memory) at 100\%
\end{itemize}

\textbf{Diagnostic steps:}
\begin{enumerate}
    \item \textbf{Identify the bottleneck}
        \begin{itemize}
            \item Check task duration in orchestrator (Airflow) - which task is slow?
            \item Profile code with cProfile or py-spy
            \item Check database query performance with EXPLAIN ANALYZE
            \item Monitor resource utilization (CPU, memory, I/O, network)
        \end{itemize}

    \item \textbf{Common causes and fixes}
        \begin{itemize}
            \item \textbf{Data volume growth}: Input data 10x larger than before
                \begin{itemize}
                    \item Fix: Add data partitioning, use incremental processing
                    \item Fix: Scale up resources (more CPU, memory)
                \end{itemize}

            \item \textbf{Missing database indexes}: Full table scans
                \begin{itemize}
                    \item Fix: Add indexes on frequently filtered columns
                    \item Fix: Check query plan with EXPLAIN, identify sequential scans
                \end{itemize}

            \item \textbf{Data skew}: One partition has 90\% of data
                \begin{itemize}
                    \item Fix: Change partition key to higher cardinality field
                    \item Fix: Use salting technique to distribute hot keys
                \end{itemize}

            \item \textbf{External API slowness}: Third-party service degraded
                \begin{itemize}
                    \item Fix: Add caching layer (Redis, Memcached)
                    \item Fix: Batch API calls instead of one-by-one
                    \item Fix: Implement circuit breaker to fail fast
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Prevention:}
\begin{itemize}
    \item Monitor pipeline duration and alert on >20\% increase
    \item Track input data volume over time
    \item Load test pipelines before production deployment
    \item Set up automatic scaling for compute resources
\end{itemize}

\subsection{Issue 2: Data Quality Degradation}

\textbf{Symptoms:}
\begin{itemize}
    \item Dashboards showing nonsensical numbers (negative revenue, 150\% conversion rate)
    \item Null rate increased from 1\% to 30\%
    \item Downstream consumer reporting data issues
\end{itemize}

\textbf{Diagnostic steps:}
\begin{enumerate}
    \item \textbf{Identify when quality degraded}
        \begin{itemize}
            \item Check data quality metrics over time
            \item Identify exact timestamp when metrics dropped
            \item Correlate with deployments, config changes, upstream changes
        \end{itemize}

    \item \textbf{Trace lineage to root cause}
        \begin{itemize}
            \item Use lineage tracking to identify upstream dependencies
            \item Check each upstream source for quality issues
            \item Isolate to specific data source or transformation
        \end{itemize}

    \item \textbf{Common causes and fixes}
        \begin{itemize}
            \item \textbf{Upstream schema change}: Upstream added field or changed type
                \begin{itemize}
                    \item Fix: Implement schema validation at ingestion
                    \item Fix: Establish data contracts with upstream teams
                    \item Fix: Add backward compatibility checks in CI/CD
                \end{itemize}

            \item \textbf{Logic bug}: Code change introduced incorrect calculation
                \begin{itemize}
                    \item Fix: Rollback to previous version
                    \item Fix: Add unit tests covering this case
                    \item Fix: Reprocess historical data after fix
                \end{itemize}

            \item \textbf{Data drift}: Real-world distribution changed
                \begin{itemize}
                    \item Fix: Update validation rules to match new distribution
                    \item Fix: Retrain ML models on recent data
                    \item Fix: Investigate business context (e.g., pandemic behavior shift)
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Prevention:}
\begin{itemize}
    \item Implement comprehensive data quality gates
    \item Track quality metrics (completeness, accuracy, consistency) over time
    \item Alert on anomalies: sudden null rate increase, distribution shift
    \item Require data contracts for all cross-team dependencies
    \item Run data quality tests in CI/CD before deployment
\end{itemize}

\subsection{Issue 3: Pipeline Failures and Retries}

\textbf{Symptoms:}
\begin{itemize}
    \item Pipeline fails intermittently (works 80\% of time)
    \item Errors: "Connection timeout", "429 Rate Limit Exceeded"
    \item Manual retries usually succeed
\end{itemize}

\textbf{Diagnostic steps:}
\begin{enumerate}
    \item \textbf{Analyze failure patterns}
        \begin{itemize}
            \item Are failures random or clustered (e.g., all failures during peak hours)?
            \item What's the error message and stack trace?
            \item Which external dependencies are involved?
        \end{itemize}

    \item \textbf{Common causes and fixes}
        \begin{itemize}
            \item \textbf{Transient network issues}: Temporary connectivity loss
                \begin{itemize}
                    \item Fix: Implement exponential backoff retry (100ms, 200ms, 400ms, ...)
                    \item Fix: Set appropriate timeout values (not too aggressive)
                    \item Fix: Use idempotency to make retries safe
                \end{itemize}

            \item \textbf{Rate limiting}: Hitting API rate limits
                \begin{itemize}
                    \item Fix: Implement rate limiting on client side
                    \item Fix: Batch requests to stay under limits
                    \item Fix: Negotiate higher rate limits with provider
                \end{itemize}

            \item \textbf{Resource contention}: Competing with other workloads
                \begin{itemize}
                    \item Fix: Use resource quotas and priority classes
                    \item Fix: Schedule heavy workloads during off-peak hours
                    \item Fix: Scale up infrastructure during peak times
                \end{itemize}

            \item \textbf{Cascade failures}: Downstream service degraded
                \begin{itemize}
                    \item Fix: Implement circuit breaker pattern
                    \item Fix: Add fallback behavior (cached data, degraded mode)
                    \item Fix: Set aggressive timeouts to fail fast
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Prevention:}
\begin{itemize}
    \item Implement robust retry logic with exponential backoff
    \item Use circuit breakers for external dependencies
    \item Monitor external service health and latency
    \item Set up alerts for elevated error rates (>1\%)
    \item Test failure scenarios with chaos engineering
\end{itemize}

\subsection{Issue 4: Memory Issues and OOM Kills}

\textbf{Symptoms:}
\begin{itemize}
    \item Pipeline killed with "OutOfMemory" error
    \item Memory usage gradually increases over time (memory leak)
    \item System becomes unresponsive before crash
\end{itemize}

\textbf{Diagnostic steps:}
\begin{enumerate}
    \item \textbf{Profile memory usage}
        \begin{itemize}
            \item Use memory profiler (memory\_profiler for Python)
            \item Check container metrics (Kubernetes pod memory)
            \item Identify which operation consumes most memory
        \end{itemize}

    \item \textbf{Common causes and fixes}
        \begin{itemize}
            \item \textbf{Loading entire dataset into memory}
                \begin{itemize}
                    \item Fix: Use iterators/generators instead of loading all at once
                    \item Fix: Process data in chunks (pandas: chunksize=10000)
                    \item Fix: Use streaming frameworks (Spark Structured Streaming)
                \end{itemize}

            \item \textbf{Memory leak}: Objects not garbage collected
                \begin{itemize}
                    \item Fix: Use context managers (with statements) for resources
                    \item Fix: Explicitly delete large objects after use
                    \item Fix: Check for circular references preventing GC
                \end{itemize}

            \item \textbf{Spark memory issues}: Shuffle spilling to disk
                \begin{itemize}
                    \item Fix: Increase executor memory
                    \item Fix: Increase shuffle partitions to reduce partition size
                    \item Fix: Enable memory offheap storage
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Prevention:}
\begin{itemize}
    \item Set appropriate memory limits in Kubernetes
    \item Monitor memory usage trends over time
    \item Use VPA (Vertical Pod Autoscaler) for automatic right-sizing
    \item Load test with production data volumes
    \item Design for streaming/chunked processing from start
\end{itemize}

\subsection{Issue 5: Data Freshness SLA Violations}

\textbf{Symptoms:}
\begin{itemize}
    \item Data in dashboard is 2 hours old (SLA: <15 minutes)
    \item Pipeline is running on time but data appears stale
    \item Consumers reporting outdated data
\end{itemize}

\textbf{Diagnostic steps:}
\begin{enumerate}
    \item \textbf{Measure end-to-end latency}
        \begin{itemize}
            \item When was source event created?
            \item When did pipeline start processing it?
            \item When was result written to destination?
            \item When did consumer read it?
        \end{itemize}

    \item \textbf{Identify bottleneck in pipeline}
        \begin{itemize}
            \item Is source polling too infrequent?
            \item Is processing taking too long?
            \item Is there a batch window delaying data?
            \item Are consumers caching aggressively?
        \end{itemize}

    \item \textbf{Common causes and fixes}
        \begin{itemize}
            \item \textbf{Batch window too large}: Collecting 1-hour batches
                \begin{itemize}
                    \item Fix: Reduce batch window to 5 minutes
                    \item Fix: Switch to streaming for true real-time
                \end{itemize}

            \item \textbf{Infrequent polling}: Checking source every hour
                \begin{itemize}
                    \item Fix: Increase polling frequency
                    \item Fix: Switch to event-driven with webhooks/Kafka
                \end{itemize}

            \item \textbf{Slow processing}: Compute-intensive transformations
                \begin{itemize}
                    \item Fix: Optimize algorithms (caching, indexing)
                    \item Fix: Parallelize processing across multiple workers
                    \item Fix: Scale up compute resources
                \end{itemize}
        \end{itemize}
\end{enumerate}

\textbf{Prevention:}
\begin{itemize}
    \item Define clear freshness SLAs for each data product
    \item Monitor end-to-end latency with percentiles (p50, p95, p99)
    \item Alert on SLA violations (data >15 minutes old)
    \item Use event-driven architecture for true real-time
    \item Instrument every stage to measure latency contribution
\end{itemize}

\section{Key Takeaways}

\subsection{Event-Driven Architecture}

\begin{itemize}
    \item \textbf{Events are Facts}: Immutable, timestamped occurrences enable audit trails and replay
    \item \textbf{Partitioning Enables Scale}: Hash partitioning on high-cardinality keys distributes load
    \item \textbf{Windows Enable Aggregation}: Tumbling/sliding windows compute features on unbounded streams
    \item \textbf{Schemas Enable Evolution}: Versioned schemas allow backward-compatible changes
    \item \textbf{Exactly-Once is Hard}: Requires idempotent producers, transactional consumers, and deduplication
    \item \textbf{Late Events Happen}: Configure allowed lateness and watermarks for correctness
    \item \textbf{Monitor Everything}: Track lag, throughput, errors, partition distribution
\end{itemize}

\subsection{Data Mesh}

\begin{itemize}
    \item \textbf{Domain Ownership}: Domains own their data products end-to-end with clear accountability
    \item \textbf{Data as Product}: Treat datasets as products with SLAs, documentation, and quality guarantees
    \item \textbf{Self-Serve Infrastructure}: Central platform enables domain autonomy without reinventing wheels
    \item \textbf{Federated Governance}: Automated policy enforcement maintains compliance without bottlenecks
    \item \textbf{Bounded Contexts}: Clear domain boundaries prevent overlap and ownership conflicts
    \item \textbf{Lineage Tracking}: Cross-domain dependencies enable impact analysis and debugging
    \item \textbf{Federated Discovery}: Catalog enables easy discovery and consumption across organization
\end{itemize}

\subsection{Container Orchestration with Kubernetes}

\begin{itemize}
    \item \textbf{Containerization}: Package pipelines with all dependencies for consistent execution
    \item \textbf{Declarative Configuration}: Define desired state in YAML; Kubernetes maintains it
    \item \textbf{Auto-Scaling}: HPA adjusts replicas based on CPU/memory; VPA adjusts resource limits
    \item \textbf{Custom Operators}: Extend Kubernetes with domain-specific resources and controllers
    \item \textbf{Serverless Patterns}: Knative enables scale-to-zero with event-driven activation
    \item \textbf{Resource Isolation}: Namespaces, resource quotas, and network policies prevent interference
    \item \textbf{Health Checks}: Liveness and readiness probes enable self-healing infrastructure
\end{itemize}

\subsection{Hybrid Cloud and Compliance}

\begin{itemize}
    \item \textbf{Data Sovereignty}: Regional deployment ensures data residency compliance (GDPR, CCPA, LGPD)
    \item \textbf{Automated Compliance}: Validators prevent violations before data processing occurs
    \item \textbf{Cloud Agnostic}: Design abstractions work across AWS, GCP, Azure, on-prem
    \item \textbf{Regional Routing}: Automatically route data based on user location and regulations
    \item \textbf{Cross-Cloud Sync}: Secure, encrypted, audited data movement between clouds
    \item \textbf{Temporary Exports}: Time-limited cross-region transfers with audit trails
    \item \textbf{Right to Deletion}: Automated RTBF implementation across all regions and clouds
\end{itemize}

\subsection{Integration}

Event-driven architectures transform batch pipelines into real-time systems, enabling sub-second latency ML features. Data mesh solves organizational scaling challenges by distributing ownership while maintaining governance. Kubernetes provides the infrastructure automation that makes both patterns operational at scale. Hybrid cloud with compliance enables global deployment while meeting regulatory requirements. Together, they enable large organizations to build scalable, reliable data infrastructure that serves hundreds of data products across dozens of domains, deployed as containers with automatic scaling and self-healing capabilities, operating across multiple clouds while maintaining data sovereignty and regulatory compliance. Production systems require careful partitioning, schema management, governance automation, containerization, compliance validation, and comprehensive monitoring to maintain reliability and compliance at global scale.
