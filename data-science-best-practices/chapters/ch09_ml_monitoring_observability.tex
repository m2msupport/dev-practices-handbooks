\chapter{ML Monitoring and Observability}

\section{Introduction}

Production ML systems fail silently. A fraud detection model can degrade from 95\% to 65\% accuracy over weeks while still serving predictions with confidence. Data distributions shift, features become stale, and infrastructure degrades—all invisible without proper monitoring. The difference between reliable ML systems and those that erode business value is comprehensive observability.

\subsection{The Silent Degradation Problem}

Consider a credit scoring model deployed in January. By March, prediction latency has tripled, data drift affects 40\% of features, and accuracy has dropped 15\%—yet the system continues serving predictions. Without monitoring, this degradation goes unnoticed until business metrics collapse or regulatory audits reveal failures.

\subsection{Why ML Monitoring is Different}

Traditional software monitoring focuses on system metrics: CPU, memory, latency, errors. ML systems require additional layers:

\begin{itemize}
    \item \textbf{Model Performance}: Accuracy, precision, recall evolve over time
    \item \textbf{Data Quality}: Distribution shifts, missing features, invalid ranges
    \item \textbf{Prediction Drift}: Output distributions change independent of performance
    \item \textbf{Feature Importance}: Critical features lose predictive power
    \item \textbf{Business Metrics}: Model decisions impact revenue, cost, user satisfaction
\end{itemize}

\subsection{The Cost of Poor Monitoring}

Industry data reveals:
\begin{itemize}
    \item \textbf{73\% of ML models} experience undetected degradation in first 6 months
    \item \textbf{Silent failures} cost companies \$500K+ annually in lost revenue
    \item \textbf{Average detection time} for model drift is 45 days without monitoring
    \item \textbf{False alert fatigue} causes teams to ignore 60\% of monitoring alerts
\end{itemize}

\subsection{Chapter Overview}

This chapter provides production-grade monitoring systems:

\begin{enumerate}
    \item \textbf{Observability Framework}: Unified metrics, logs, traces, and model-specific signals
    \item \textbf{Performance Monitoring}: Custom metrics, alerting, and trend analysis
    \item \textbf{Data Drift Detection}: Statistical tests (KS, PSI, custom metrics)
    \item \textbf{Model Decay Detection}: Performance degradation and retraining triggers
    \item \textbf{Infrastructure Monitoring}: Latency, throughput, errors, resource usage
    \item \textbf{Alert Management}: Intelligent routing, escalation, and noise reduction
    \item \textbf{Dashboard Design}: Role-specific views for different stakeholders
    \item \textbf{Anomaly Detection}: Automated detection using statistical and ML methods
\end{enumerate}

\section{Comprehensive Observability Framework}

Modern ML systems require observability beyond traditional software monitoring. The four pillars of ML observability are: metrics (numerical measurements), logs (discrete events), traces (request flows), and model-specific signals (predictions, features, drift).

\subsection{The Three Pillars Plus One}

Traditional observability focuses on three pillars: metrics, logs, and traces. ML systems require a fourth pillar:

\begin{itemize}
    \item \textbf{Metrics}: Time-series data (latency, accuracy, throughput)
    \item \textbf{Logs}: Discrete events with context (predictions, errors, warnings)
    \item \textbf{Traces}: Request flows through distributed systems
    \item \textbf{Model Signals}: ML-specific data (feature distributions, prediction drift, model versions)
\end{itemize}

\subsection{ObservabilityStack: Unified Integration}

\begin{lstlisting}[language=Python, caption={Comprehensive Observability Stack Integration}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging
import json
from contextlib import contextmanager
import time

# Prometheus for metrics
from prometheus_client import (
    Counter, Gauge, Histogram, Summary,
    CollectorRegistry, push_to_gateway, start_http_server
)

# OpenTelemetry for tracing (Jaeger)
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource

# ELK integration (Elasticsearch, Logstash, Kibana)
from elasticsearch import Elasticsearch
import structlog

logger = logging.getLogger(__name__)

@dataclass
class ObservabilityConfig:
    """
    Configuration for observability stack.

    Attributes:
        prometheus_gateway: Prometheus pushgateway URL
        prometheus_port: Port for Prometheus metrics server
        jaeger_endpoint: Jaeger collector endpoint
        elasticsearch_hosts: Elasticsearch cluster hosts
        service_name: Name of service being monitored
        environment: Deployment environment (prod, staging, dev)
    """
    prometheus_gateway: Optional[str] = None
    prometheus_port: int = 8000
    jaeger_endpoint: Optional[str] = None
    elasticsearch_hosts: List[str] = field(default_factory=list)
    service_name: str = "ml-service"
    environment: str = "production"
    enable_metrics: bool = True
    enable_tracing: bool = True
    enable_logging: bool = True

class ObservabilityStack:
    """
    Comprehensive observability stack integrating metrics, logs, and traces.

    Integrates with:
    - Prometheus: Metrics collection and alerting
    - Grafana: Metrics visualization (via Prometheus)
    - Jaeger: Distributed tracing
    - ELK Stack: Centralized logging (Elasticsearch, Logstash, Kibana)

    Example:
        >>> config = ObservabilityConfig(
        ...     prometheus_gateway="localhost:9091",
        ...     jaeger_endpoint="http://localhost:14268/api/traces",
        ...     elasticsearch_hosts=["http://localhost:9200"]
        ... )
        >>> obs_stack = ObservabilityStack(config)
        >>> with obs_stack.trace_operation("predict"):
        ...     prediction = model.predict(features)
        ...     obs_stack.log_prediction(prediction, features)
    """

    def __init__(self, config: ObservabilityConfig):
        """
        Initialize observability stack.

        Args:
            config: Observability configuration
        """
        self.config = config

        # Initialize components
        if config.enable_metrics:
            self._setup_metrics()

        if config.enable_tracing:
            self._setup_tracing()

        if config.enable_logging:
            self._setup_logging()

        logger.info(
            f"ObservabilityStack initialized for {config.service_name} "
            f"({config.environment})"
        )

    def _setup_metrics(self):
        """Set up Prometheus metrics."""
        self.registry = CollectorRegistry()

        # Prediction metrics
        self.predictions_total = Counter(
            'ml_predictions_total',
            'Total predictions made',
            ['model_name', 'model_version', 'status'],
            registry=self.registry
        )

        self.prediction_latency = Histogram(
            'ml_prediction_latency_seconds',
            'Prediction latency distribution',
            ['model_name', 'model_version'],
            buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0],
            registry=self.registry
        )

        self.model_accuracy = Gauge(
            'ml_model_accuracy',
            'Current model accuracy',
            ['model_name', 'model_version', 'metric_type'],
            registry=self.registry
        )

        self.feature_drift_score = Gauge(
            'ml_feature_drift_score',
            'Feature drift score',
            ['model_name', 'feature_name', 'drift_method'],
            registry=self.registry
        )

        self.active_model_version = Gauge(
            'ml_active_model_version',
            'Currently active model version (encoded)',
            ['model_name'],
            registry=self.registry
        )

        # Infrastructure metrics
        self.memory_usage = Gauge(
            'ml_memory_usage_bytes',
            'Memory usage in bytes',
            ['component'],
            registry=self.registry
        )

        self.cache_hits = Counter(
            'ml_cache_hits_total',
            'Cache hit count',
            ['cache_type'],
            registry=self.registry
        )

        # Start Prometheus HTTP server
        if self.config.prometheus_port:
            try:
                start_http_server(
                    self.config.prometheus_port,
                    registry=self.registry
                )
                logger.info(
                    f"Prometheus metrics server started on "
                    f"port {self.config.prometheus_port}"
                )
            except OSError:
                logger.warning(
                    f"Port {self.config.prometheus_port} already in use"
                )

    def _setup_tracing(self):
        """Set up Jaeger distributed tracing."""
        # Create resource with service information
        resource = Resource.create({
            "service.name": self.config.service_name,
            "service.environment": self.config.environment
        })

        # Create tracer provider
        provider = TracerProvider(resource=resource)

        # Configure Jaeger exporter
        if self.config.jaeger_endpoint:
            jaeger_exporter = JaegerExporter(
                collector_endpoint=self.config.jaeger_endpoint,
            )
            provider.add_span_processor(
                BatchSpanProcessor(jaeger_exporter)
            )

        # Set global tracer provider
        trace.set_tracer_provider(provider)
        self.tracer = trace.get_tracer(__name__)

        logger.info("Jaeger tracing initialized")

    def _setup_logging(self):
        """Set up structured logging with ELK integration."""
        # Configure structured logging
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            cache_logger_on_first_use=True,
        )

        # Set up Elasticsearch client
        if self.config.elasticsearch_hosts:
            try:
                self.es_client = Elasticsearch(
                    self.config.elasticsearch_hosts
                )

                # Create index for ML logs
                index_name = f"ml-logs-{self.config.environment}"
                if not self.es_client.indices.exists(index=index_name):
                    self.es_client.indices.create(
                        index=index_name,
                        body={
                            "mappings": {
                                "properties": {
                                    "timestamp": {"type": "date"},
                                    "level": {"type": "keyword"},
                                    "message": {"type": "text"},
                                    "model_name": {"type": "keyword"},
                                    "model_version": {"type": "keyword"},
                                    "prediction_id": {"type": "keyword"},
                                    "features": {"type": "object"},
                                    "prediction": {"type": "object"}
                                }
                            }
                        }
                    )

                self.log_index = index_name
                logger.info(f"Elasticsearch logging to index: {index_name}")
            except Exception as e:
                logger.warning(f"Failed to connect to Elasticsearch: {e}")
                self.es_client = None
        else:
            self.es_client = None

        self.structured_logger = structlog.get_logger()

    @contextmanager
    def trace_operation(
        self,
        operation_name: str,
        attributes: Optional[Dict[str, Any]] = None
    ):
        """
        Trace an operation with distributed tracing.

        Args:
            operation_name: Name of operation to trace
            attributes: Additional attributes to attach to span

        Yields:
            Span object
        """
        if not self.config.enable_tracing:
            yield None
            return

        with self.tracer.start_as_current_span(operation_name) as span:
            # Add attributes
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, str(value))

            # Add service metadata
            span.set_attribute("service.name", self.config.service_name)
            span.set_attribute("environment", self.config.environment)

            try:
                yield span
            except Exception as e:
                # Record exception in trace
                span.set_status(trace.Status(trace.StatusCode.ERROR))
                span.record_exception(e)
                raise

    def record_prediction(
        self,
        model_name: str,
        model_version: str,
        features: Dict[str, Any],
        prediction: Any,
        latency: float,
        success: bool = True,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Record a prediction across all observability dimensions.

        Args:
            model_name: Name of model
            model_version: Model version
            features: Input features
            prediction: Model prediction
            latency: Prediction latency in seconds
            success: Whether prediction succeeded
            metadata: Additional metadata
        """
        prediction_id = f"{model_name}-{int(time.time() * 1000)}"

        # Record metrics
        if self.config.enable_metrics:
            status = 'success' if success else 'error'
            self.predictions_total.labels(
                model_name=model_name,
                model_version=model_version,
                status=status
            ).inc()

            self.prediction_latency.labels(
                model_name=model_name,
                model_version=model_version
            ).observe(latency)

        # Log to structured logging
        if self.config.enable_logging:
            log_data = {
                "event": "prediction",
                "prediction_id": prediction_id,
                "model_name": model_name,
                "model_version": model_version,
                "latency_seconds": latency,
                "success": success,
                "feature_count": len(features),
                "prediction": str(prediction)[:100],  # Truncate
                "metadata": metadata or {}
            }

            self.structured_logger.info(
                "Prediction recorded",
                **log_data
            )

            # Send to Elasticsearch
            if self.es_client:
                try:
                    self.es_client.index(
                        index=self.log_index,
                        document={
                            **log_data,
                            "timestamp": datetime.now().isoformat(),
                            "level": "info",
                            "features": features,
                            "prediction": prediction
                        }
                    )
                except Exception as e:
                    logger.warning(f"Failed to log to Elasticsearch: {e}")

    def record_drift(
        self,
        model_name: str,
        feature_name: str,
        drift_score: float,
        drift_method: str,
        drift_detected: bool
    ):
        """
        Record feature drift detection.

        Args:
            model_name: Name of model
            feature_name: Feature with drift
            drift_score: Drift score
            drift_method: Detection method used
            drift_detected: Whether drift was detected
        """
        # Record metric
        if self.config.enable_metrics:
            self.feature_drift_score.labels(
                model_name=model_name,
                feature_name=feature_name,
                drift_method=drift_method
            ).set(drift_score)

        # Log drift event
        if self.config.enable_logging and drift_detected:
            self.structured_logger.warning(
                "Feature drift detected",
                model_name=model_name,
                feature_name=feature_name,
                drift_score=drift_score,
                drift_method=drift_method
            )

    def record_accuracy(
        self,
        model_name: str,
        model_version: str,
        metric_type: str,
        value: float
    ):
        """
        Record model accuracy metric.

        Args:
            model_name: Name of model
            model_version: Model version
            metric_type: Type of metric (accuracy, precision, recall, etc.)
            value: Metric value
        """
        if self.config.enable_metrics:
            self.model_accuracy.labels(
                model_name=model_name,
                model_version=model_version,
                metric_type=metric_type
            ).set(value)

    def push_metrics(self):
        """Push metrics to Prometheus pushgateway."""
        if not self.config.enable_metrics or not self.config.prometheus_gateway:
            return

        try:
            push_to_gateway(
                self.config.prometheus_gateway,
                job=f'{self.config.service_name}-{self.config.environment}',
                registry=self.registry
            )
        except Exception as e:
            logger.error(f"Failed to push metrics to Prometheus: {e}")
\end{lstlisting}

\subsection{Integration with Grafana Dashboards}

Grafana consumes Prometheus metrics to create visualizations. Configure datasources and create dashboards programmatically:

\begin{lstlisting}[language=Python, caption={Grafana Dashboard Configuration}]
import requests
from typing import Dict, List

class GrafanaIntegration:
    """
    Programmatic Grafana dashboard creation.

    Example:
        >>> grafana = GrafanaIntegration("http://localhost:3000", "admin:admin")
        >>> grafana.create_ml_dashboard("fraud-model")
    """

    def __init__(self, grafana_url: str, api_key: str):
        """
        Initialize Grafana integration.

        Args:
            grafana_url: Grafana server URL
            api_key: Grafana API key or "user:password"
        """
        self.base_url = grafana_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def create_ml_dashboard(
        self,
        model_name: str,
        prometheus_datasource: str = "Prometheus"
    ) -> Dict:
        """
        Create comprehensive ML monitoring dashboard.

        Args:
            model_name: Name of model to monitor
            prometheus_datasource: Prometheus datasource name in Grafana

        Returns:
            Dashboard creation response
        """
        dashboard = {
            "dashboard": {
                "title": f"ML Monitoring: {model_name}",
                "tags": ["ml", "monitoring", model_name],
                "timezone": "browser",
                "panels": [
                    # Predictions per second
                    {
                        "id": 1,
                        "title": "Predictions per Second",
                        "type": "graph",
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
                        "targets": [{
                            "expr": f'rate(ml_predictions_total{{model_name="{model_name}"}}[5m])',
                            "legendFormat": "{{status}}"
                        }]
                    },
                    # Prediction latency
                    {
                        "id": 2,
                        "title": "Prediction Latency (p95, p99)",
                        "type": "graph",
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
                        "targets": [
                            {
                                "expr": f'histogram_quantile(0.95, ml_prediction_latency_seconds_bucket{{model_name="{model_name}"}})',
                                "legendFormat": "p95"
                            },
                            {
                                "expr": f'histogram_quantile(0.99, ml_prediction_latency_seconds_bucket{{model_name="{model_name}"}})',
                                "legendFormat": "p99"
                            }
                        ]
                    },
                    # Model accuracy
                    {
                        "id": 3,
                        "title": "Model Accuracy",
                        "type": "graph",
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
                        "targets": [{
                            "expr": f'ml_model_accuracy{{model_name="{model_name}"}}',
                            "legendFormat": "{{metric_type}}"
                        }]
                    },
                    # Feature drift heatmap
                    {
                        "id": 4,
                        "title": "Feature Drift Scores",
                        "type": "heatmap",
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
                        "targets": [{
                            "expr": f'ml_feature_drift_score{{model_name="{model_name}"}}',
                            "legendFormat": "{{feature_name}}"
                        }]
                    }
                ]
            },
            "overwrite": True
        }

        response = requests.post(
            f"{self.base_url}/api/dashboards/db",
            headers=self.headers,
            json=dashboard
        )
        response.raise_for_status()

        return response.json()
\end{lstlisting}

\section{Role-Based Dashboard Design}

Different stakeholders need different views of ML system health. Design dashboards for specific audiences.

\subsection{DashboardConfig: Role-Specific Views}

\begin{lstlisting}[language=Python, caption={Role-Based Dashboard Configuration}]
from dataclasses import dataclass
from typing import List, Dict, Any
from enum import Enum

class StakeholderRole(Enum):
    """Types of dashboard users."""
    DATA_SCIENTIST = "data_scientist"
    ML_ENGINEER = "ml_engineer"
    BUSINESS_STAKEHOLDER = "business"
    SRE_DEVOPS = "sre_devops"
    EXECUTIVE = "executive"

@dataclass
class DashboardPanel:
    """
    Individual dashboard panel configuration.

    Attributes:
        title: Panel title
        panel_type: Type of visualization (graph, stat, table, heatmap)
        query: Prometheus/data query
        description: Panel description
        priority: Display priority (1-10)
    """
    title: str
    panel_type: str
    query: str
    description: str
    priority: int = 5
    thresholds: Optional[Dict[str, float]] = None
    unit: str = "short"

class DashboardConfig:
    """
    Generate role-specific monitoring dashboards.

    Different roles need different metrics:
    - Data Scientists: Model performance, feature importance, drift
    - ML Engineers: Infrastructure, latency, errors, deployment status
    - Business: Business metrics, KPIs, ROI, user impact
    - SRE/DevOps: System health, resource usage, alerts
    - Executives: High-level KPIs, trends, business impact

    Example:
        >>> config = DashboardConfig("fraud-detection-model")
        >>> ds_dashboard = config.generate_dashboard(
        ...     StakeholderRole.DATA_SCIENTIST
        ... )
        >>> config.export_grafana(ds_dashboard)
    """

    def __init__(self, model_name: str):
        """
        Initialize dashboard configuration.

        Args:
            model_name: Name of model to monitor
        """
        self.model_name = model_name
        self._define_panels()

    def _define_panels(self):
        """Define all available dashboard panels."""
        # Data Scientist panels
        self.ds_panels = [
            DashboardPanel(
                title="Model Accuracy Trend",
                panel_type="graph",
                query=f'ml_model_accuracy{{model_name="{self.model_name}", metric_type="accuracy"}}',
                description="Model accuracy over time",
                priority=10,
                thresholds={"warning": 0.85, "critical": 0.75}
            ),
            DashboardPanel(
                title="Precision by Class",
                panel_type="graph",
                query=f'ml_model_accuracy{{model_name="{self.model_name}", metric_type="precision"}}',
                description="Per-class precision metrics",
                priority=9
            ),
            DashboardPanel(
                title="Feature Drift Detection",
                panel_type="heatmap",
                query=f'ml_feature_drift_score{{model_name="{self.model_name}"}}',
                description="Feature distribution drift over time",
                priority=10
            ),
            DashboardPanel(
                title="Prediction Distribution",
                panel_type="histogram",
                query=f'ml_prediction_distribution{{model_name="{self.model_name}"}}',
                description="Distribution of model predictions",
                priority=7
            ),
            DashboardPanel(
                title="Feature Importance Changes",
                panel_type="graph",
                query=f'ml_feature_importance{{model_name="{self.model_name}"}}',
                description="Feature importance over time",
                priority=8
            )
        ]

        # ML Engineer panels
        self.mle_panels = [
            DashboardPanel(
                title="Prediction Latency (p50, p95, p99)",
                panel_type="graph",
                query=f'histogram_quantile(0.95, ml_prediction_latency_seconds_bucket{{model_name="{self.model_name}"}})',
                description="Prediction latency percentiles",
                priority=10,
                unit="s",
                thresholds={"warning": 0.1, "critical": 0.5}
            ),
            DashboardPanel(
                title="Throughput (predictions/sec)",
                panel_type="stat",
                query=f'rate(ml_predictions_total{{model_name="{self.model_name}"}}[5m])',
                description="Prediction throughput",
                priority=10,
                unit="reqps"
            ),
            DashboardPanel(
                title="Error Rate",
                panel_type="graph",
                query=f'rate(ml_predictions_total{{model_name="{self.model_name}", status="error"}}[5m])',
                description="Prediction error rate",
                priority=10,
                thresholds={"warning": 0.01, "critical": 0.05}
            ),
            DashboardPanel(
                title="Model Version Status",
                panel_type="stat",
                query=f'ml_active_model_version{{model_name="{self.model_name}"}}',
                description="Currently deployed model version",
                priority=9
            ),
            DashboardPanel(
                title="Cache Hit Rate",
                panel_type="graph",
                query=f'rate(ml_cache_hits_total[5m])',
                description="Feature/prediction cache efficiency",
                priority=6
            ),
            DashboardPanel(
                title="Memory Usage",
                panel_type="graph",
                query=f'ml_memory_usage_bytes{{component="model"}}',
                description="Model memory consumption",
                priority=7,
                unit="bytes"
            )
        ]

        # Business Stakeholder panels
        self.business_panels = [
            DashboardPanel(
                title="Predictions Processed (24h)",
                panel_type="stat",
                query=f'increase(ml_predictions_total{{model_name="{self.model_name}"}}[24h])',
                description="Total predictions in last 24 hours",
                priority=10,
                unit="short"
            ),
            DashboardPanel(
                title="Model Accuracy (Current)",
                panel_type="gauge",
                query=f'ml_model_accuracy{{model_name="{self.model_name}", metric_type="accuracy"}}',
                description="Current model accuracy",
                priority=10,
                thresholds={"warning": 0.85, "critical": 0.75}
            ),
            DashboardPanel(
                title="Business Impact Score",
                panel_type="graph",
                query=f'ml_business_metric{{model_name="{self.model_name}", metric="revenue_impact"}}',
                description="Estimated business impact",
                priority=10,
                unit="currencyUSD"
            ),
            DashboardPanel(
                title="Uptime (7 days)",
                panel_type="stat",
                query=f'avg_over_time(up{{job="{self.model_name}"}}[7d]) * 100',
                description="Service availability percentage",
                priority=9,
                unit="percent"
            ),
            DashboardPanel(
                title="User Satisfaction Score",
                panel_type="graph",
                query=f'ml_user_feedback{{model_name="{self.model_name}"}}',
                description="User feedback on predictions",
                priority=8
            )
        ]

        # SRE/DevOps panels
        self.sre_panels = [
            DashboardPanel(
                title="Active Alerts",
                panel_type="table",
                query=f'ALERTS{{model_name="{self.model_name}", alertstate="firing"}}',
                description="Currently firing alerts",
                priority=10
            ),
            DashboardPanel(
                title="Error Budget Burn Rate",
                panel_type="graph",
                query=f'ml_error_budget_burn_rate{{model_name="{self.model_name}"}}',
                description="SLO error budget consumption rate",
                priority=10,
                thresholds={"warning": 1.0, "critical": 2.0}
            ),
            DashboardPanel(
                title="Resource Utilization",
                panel_type="graph",
                query=f'ml_resource_usage{{model_name="{self.model_name}"}}',
                description="CPU, memory, disk usage",
                priority=9,
                unit="percent"
            ),
            DashboardPanel(
                title="Request Queue Depth",
                panel_type="graph",
                query=f'ml_queue_depth{{model_name="{self.model_name}"}}',
                description="Prediction request queue size",
                priority=8
            ),
            DashboardPanel(
                title="Deployment Health",
                panel_type="stat",
                query=f'up{{job="{self.model_name}"}}',
                description="Service health status",
                priority=10
            )
        ]

        # Executive panels
        self.executive_panels = [
            DashboardPanel(
                title="Overall System Health",
                panel_type="gauge",
                query=f'ml_overall_health{{model_name="{self.model_name}"}}',
                description="Composite health score",
                priority=10,
                unit="percent"
            ),
            DashboardPanel(
                title="Monthly Predictions Trend",
                panel_type="graph",
                query=f'increase(ml_predictions_total{{model_name="{self.model_name}"}}[30d])',
                description="Prediction volume trend (30 days)",
                priority=9
            ),
            DashboardPanel(
                title="ROI from ML System",
                panel_type="stat",
                query=f'ml_business_metric{{model_name="{self.model_name}", metric="roi"}}',
                description="Return on investment",
                priority=10,
                unit="currencyUSD"
            ),
            DashboardPanel(
                title="Incidents (Last 30 Days)",
                panel_type="stat",
                query=f'count(ALERTS{{model_name="{self.model_name}", severity="critical"}}[30d])',
                description="Critical incidents in last month",
                priority=8
            )
        ]

    def generate_dashboard(
        self,
        role: StakeholderRole,
        include_common: bool = True
    ) -> Dict[str, Any]:
        """
        Generate dashboard configuration for specific role.

        Args:
            role: Stakeholder role
            include_common: Include common panels across all roles

        Returns:
            Dashboard configuration dictionary
        """
        # Select panels based on role
        role_panels = {
            StakeholderRole.DATA_SCIENTIST: self.ds_panels,
            StakeholderRole.ML_ENGINEER: self.mle_panels,
            StakeholderRole.BUSINESS_STAKEHOLDER: self.business_panels,
            StakeholderRole.SRE_DEVOPS: self.sre_panels,
            StakeholderRole.EXECUTIVE: self.executive_panels
        }

        panels = role_panels[role]

        # Sort by priority
        panels = sorted(panels, key=lambda p: p.priority, reverse=True)

        # Convert to Grafana format
        grafana_panels = []
        y_pos = 0

        for idx, panel in enumerate(panels):
            x_pos = (idx % 2) * 12  # 2 columns
            if idx % 2 == 0 and idx > 0:
                y_pos += 8

            grafana_panel = {
                "id": idx + 1,
                "title": panel.title,
                "type": panel.panel_type,
                "gridPos": {
                    "h": 8,
                    "w": 12,
                    "x": x_pos,
                    "y": y_pos
                },
                "targets": [{
                    "expr": panel.query,
                    "legendFormat": panel.title
                }],
                "description": panel.description,
                "fieldConfig": {
                    "defaults": {
                        "unit": panel.unit
                    }
                }
            }

            # Add thresholds if defined
            if panel.thresholds:
                grafana_panel["fieldConfig"]["defaults"]["thresholds"] = {
                    "mode": "absolute",
                    "steps": [
                        {"value": 0, "color": "green"},
                        {"value": panel.thresholds.get("warning", 0), "color": "yellow"},
                        {"value": panel.thresholds.get("critical", 0), "color": "red"}
                    ]
                }

            grafana_panels.append(grafana_panel)

        return {
            "title": f"{self.model_name} - {role.value.title()} Dashboard",
            "tags": ["ml", role.value, self.model_name],
            "panels": grafana_panels,
            "refresh": "30s",
            "time": {
                "from": "now-6h",
                "to": "now"
            }
        }

    def export_grafana(
        self,
        dashboard_config: Dict[str, Any],
        grafana_url: str,
        api_key: str
    ):
        """
        Export dashboard to Grafana.

        Args:
            dashboard_config: Dashboard configuration
            grafana_url: Grafana server URL
            api_key: Grafana API key
        """
        grafana = GrafanaIntegration(grafana_url, api_key)

        response = requests.post(
            f"{grafana_url}/api/dashboards/db",
            headers=grafana.headers,
            json={"dashboard": dashboard_config, "overwrite": True}
        )
        response.raise_for_status()

        logger.info(f"Dashboard exported: {dashboard_config['title']}")
\end{lstlisting}

\section{Automated Anomaly Detection}

Beyond threshold-based alerting, use statistical and machine learning methods to detect anomalies automatically.

\subsection{AnomalyDetector: Multi-Method Detection}

\begin{lstlisting}[language=Python, caption={Comprehensive Anomaly Detection System}]
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
from sklearn.svm import OneClassSVM
import logging

logger = logging.getLogger(__name__)

class AnomalyMethod(Enum):
    """Anomaly detection methods."""
    STATISTICAL = "statistical"  # Z-score, IQR
    ISOLATION_FOREST = "isolation_forest"
    ELLIPTIC_ENVELOPE = "elliptic_envelope"
    ONE_CLASS_SVM = "one_class_svm"
    EXPONENTIAL_SMOOTHING = "exponential_smoothing"
    DBSCAN = "dbscan"

@dataclass
class AnomalyResult:
    """
    Result of anomaly detection.

    Attributes:
        timestamp: When anomaly was detected
        metric_name: Name of metric with anomaly
        value: Anomalous value
        expected_range: Expected value range
        anomaly_score: Anomaly score (0-1)
        method: Detection method used
        severity: Anomaly severity
        context: Additional context
    """
    timestamp: datetime
    metric_name: str
    value: float
    expected_range: Tuple[float, float]
    anomaly_score: float
    method: AnomalyMethod
    severity: str  # 'low', 'medium', 'high'
    context: Dict[str, Any]

class AnomalyDetector:
    """
    Multi-method anomaly detection for ML metrics.

    Combines statistical methods and machine learning to detect
    anomalies in model performance, data quality, and infrastructure metrics.

    Methods:
    - Statistical: Z-score, IQR for univariate data
    - Isolation Forest: Tree-based anomaly detection
    - Elliptic Envelope: Gaussian distribution assumption
    - One-Class SVM: Support vector-based detection
    - Exponential Smoothing: Time-series anomalies

    Example:
        >>> detector = AnomalyDetector()
        >>> detector.fit(historical_metrics)
        >>> anomalies = detector.detect(current_metrics)
        >>> for anomaly in anomalies:
        ...     if anomaly.severity == 'high':
        ...         send_alert(anomaly)
    """

    def __init__(
        self,
        methods: List[AnomalyMethod] = None,
        contamination: float = 0.1,
        statistical_threshold: float = 3.0,
        min_samples: int = 100
    ):
        """
        Initialize anomaly detector.

        Args:
            methods: Anomaly detection methods to use
            contamination: Expected proportion of anomalies
            statistical_threshold: Z-score threshold for statistical method
            min_samples: Minimum samples needed for training
        """
        if methods is None:
            methods = [
                AnomalyMethod.STATISTICAL,
                AnomalyMethod.ISOLATION_FOREST
            ]

        self.methods = methods
        self.contamination = contamination
        self.statistical_threshold = statistical_threshold
        self.min_samples = min_samples

        # Model storage
        self.models: Dict[AnomalyMethod, Any] = {}
        self.statistical_params: Dict[str, Dict] = {}

        # Training data
        self.is_fitted = False

    def fit(self, X: pd.DataFrame):
        """
        Fit anomaly detectors on historical data.

        Args:
            X: Historical metrics (rows=samples, columns=metrics)
        """
        if len(X) < self.min_samples:
            raise ValueError(
                f"Insufficient samples for training: {len(X)} < {self.min_samples}"
            )

        logger.info(f"Fitting anomaly detectors on {len(X)} samples")

        # Fit statistical parameters
        if AnomalyMethod.STATISTICAL in self.methods:
            for column in X.columns:
                values = X[column].dropna()
                self.statistical_params[column] = {
                    'mean': values.mean(),
                    'std': values.std(),
                    'q1': values.quantile(0.25),
                    'q3': values.quantile(0.75),
                    'iqr': values.quantile(0.75) - values.quantile(0.25)
                }

        # Fit Isolation Forest
        if AnomalyMethod.ISOLATION_FOREST in self.methods:
            self.models[AnomalyMethod.ISOLATION_FOREST] = IsolationForest(
                contamination=self.contamination,
                random_state=42,
                n_estimators=100
            )
            self.models[AnomalyMethod.ISOLATION_FOREST].fit(X.fillna(0))

        # Fit Elliptic Envelope
        if AnomalyMethod.ELLIPTIC_ENVELOPE in self.methods:
            self.models[AnomalyMethod.ELLIPTIC_ENVELOPE] = EllipticEnvelope(
                contamination=self.contamination,
                random_state=42
            )
            self.models[AnomalyMethod.ELLIPTIC_ENVELOPE].fit(X.fillna(0))

        # Fit One-Class SVM
        if AnomalyMethod.ONE_CLASS_SVM in self.methods:
            self.models[AnomalyMethod.ONE_CLASS_SVM] = OneClassSVM(
                nu=self.contamination,
                gamma='auto'
            )
            self.models[AnomalyMethod.ONE_CLASS_SVM].fit(X.fillna(0))

        self.is_fitted = True
        logger.info("Anomaly detectors fitted successfully")

    def detect(
        self,
        X: pd.DataFrame,
        return_all: bool = False
    ) -> List[AnomalyResult]:
        """
        Detect anomalies in new data.

        Args:
            X: New metric data to check
            return_all: Return all detections (not just consensus)

        Returns:
            List of detected anomalies
        """
        if not self.is_fitted:
            raise ValueError("Detector not fitted. Call fit() first.")

        anomalies = []

        # Detect with each method
        detection_results = {}

        for method in self.methods:
            if method == AnomalyMethod.STATISTICAL:
                results = self._detect_statistical(X)
            elif method == AnomalyMethod.ISOLATION_FOREST:
                results = self._detect_isolation_forest(X)
            elif method == AnomalyMethod.ELLIPTIC_ENVELOPE:
                results = self._detect_elliptic_envelope(X)
            elif method == AnomalyMethod.ONE_CLASS_SVM:
                results = self._detect_one_class_svm(X)

            detection_results[method] = results

        # Combine results (consensus)
        if not return_all:
            anomalies = self._consensus_detection(detection_results, X)
        else:
            # Return all detections
            for method, results in detection_results.items():
                anomalies.extend(results)

        return anomalies

    def _detect_statistical(self, X: pd.DataFrame) -> List[AnomalyResult]:
        """Detect anomalies using statistical methods (Z-score, IQR)."""
        anomalies = []

        for column in X.columns:
            if column not in self.statistical_params:
                continue

            params = self.statistical_params[column]
            values = X[column].dropna()

            for idx, value in values.items():
                # Z-score method
                z_score = abs((value - params['mean']) / (params['std'] + 1e-10))

                # IQR method
                lower_bound = params['q1'] - 1.5 * params['iqr']
                upper_bound = params['q3'] + 1.5 * params['iqr']
                iqr_anomaly = value < lower_bound or value > upper_bound

                # Detect anomaly
                if z_score > self.statistical_threshold or iqr_anomaly:
                    severity = self._calculate_severity(z_score)

                    anomalies.append(AnomalyResult(
                        timestamp=datetime.now(),
                        metric_name=column,
                        value=value,
                        expected_range=(
                            params['mean'] - self.statistical_threshold * params['std'],
                            params['mean'] + self.statistical_threshold * params['std']
                        ),
                        anomaly_score=min(z_score / 10.0, 1.0),
                        method=AnomalyMethod.STATISTICAL,
                        severity=severity,
                        context={
                            'z_score': z_score,
                            'mean': params['mean'],
                            'std': params['std'],
                            'iqr_bounds': (lower_bound, upper_bound)
                        }
                    ))

        return anomalies

    def _detect_isolation_forest(self, X: pd.DataFrame) -> List[AnomalyResult]:
        """Detect anomalies using Isolation Forest."""
        model = self.models[AnomalyMethod.ISOLATION_FOREST]
        predictions = model.predict(X.fillna(0))
        scores = model.score_samples(X.fillna(0))

        anomalies = []

        for idx, (pred, score) in enumerate(zip(predictions, scores)):
            if pred == -1:  # Anomaly
                # Determine which features are anomalous
                row = X.iloc[idx]

                for col in X.columns:
                    severity = self._calculate_severity_from_score(-score)

                    anomalies.append(AnomalyResult(
                        timestamp=datetime.now(),
                        metric_name=col,
                        value=row[col],
                        expected_range=(np.nan, np.nan),  # IF doesn't provide range
                        anomaly_score=-score,
                        method=AnomalyMethod.ISOLATION_FOREST,
                        severity=severity,
                        context={
                            'isolation_score': score,
                            'row_index': idx
                        }
                    ))

        return anomalies

    def _detect_elliptic_envelope(self, X: pd.DataFrame) -> List[AnomalyResult]:
        """Detect anomalies using Elliptic Envelope."""
        model = self.models[AnomalyMethod.ELLIPTIC_ENVELOPE]
        predictions = model.predict(X.fillna(0))

        anomalies = []

        for idx, pred in enumerate(predictions):
            if pred == -1:  # Anomaly
                row = X.iloc[idx]

                for col in X.columns:
                    anomalies.append(AnomalyResult(
                        timestamp=datetime.now(),
                        metric_name=col,
                        value=row[col],
                        expected_range=(np.nan, np.nan),
                        anomaly_score=0.8,  # Binary detector
                        method=AnomalyMethod.ELLIPTIC_ENVELOPE,
                        severity='medium',
                        context={'row_index': idx}
                    ))

        return anomalies

    def _detect_one_class_svm(self, X: pd.DataFrame) -> List[AnomalyResult]:
        """Detect anomalies using One-Class SVM."""
        model = self.models[AnomalyMethod.ONE_CLASS_SVM]
        predictions = model.predict(X.fillna(0))

        anomalies = []

        for idx, pred in enumerate(predictions):
            if pred == -1:  # Anomaly
                row = X.iloc[idx]

                for col in X.columns:
                    anomalies.append(AnomalyResult(
                        timestamp=datetime.now(),
                        metric_name=col,
                        value=row[col],
                        expected_range=(np.nan, np.nan),
                        anomaly_score=0.75,
                        method=AnomalyMethod.ONE_CLASS_SVM,
                        severity='medium',
                        context={'row_index': idx}
                    ))

        return anomalies

    def _consensus_detection(
        self,
        detection_results: Dict[AnomalyMethod, List[AnomalyResult]],
        X: pd.DataFrame
    ) -> List[AnomalyResult]:
        """
        Combine detection results using consensus approach.

        Only report anomalies detected by multiple methods.
        """
        # Count detections per metric
        metric_detections = {}

        for method, results in detection_results.items():
            for result in results:
                key = result.metric_name
                if key not in metric_detections:
                    metric_detections[key] = []
                metric_detections[key].append(result)

        # Filter to consensus (detected by at least 2 methods or high severity)
        consensus_anomalies = []

        for metric, detections in metric_detections.items():
            if len(detections) >= 2 or any(d.severity == 'high' for d in detections):
                # Use the detection with highest score
                best_detection = max(detections, key=lambda d: d.anomaly_score)
                consensus_anomalies.append(best_detection)

        return consensus_anomalies

    def _calculate_severity(self, z_score: float) -> str:
        """Calculate anomaly severity from z-score."""
        if z_score > 5.0:
            return 'high'
        elif z_score > 3.5:
            return 'medium'
        else:
            return 'low'

    def _calculate_severity_from_score(self, score: float) -> str:
        """Calculate severity from anomaly score."""
        if score > 0.8:
            return 'high'
        elif score > 0.5:
            return 'medium'
        else:
            return 'low'
\end{lstlisting}

\subsection{Real-World Scenario: Alert Fatigue Elimination}

\subsubsection{The Problem}

A recommendation engine team deployed comprehensive monitoring with threshold-based alerts. Within a week, the on-call engineers received 500+ alerts per day:

\begin{itemize}
    \item \textbf{80\% were false positives}: Normal traffic spikes, cache warmup, deployment activities
    \item \textbf{Alert fatigue set in}: Team started ignoring alerts after day 3
    \item \textbf{Real issues missed}: A genuine model degradation (accuracy dropped 20\%) went unnoticed for 4 days
    \item \textbf{Team morale impacted}: Engineers requested to rotate off on-call duty
\end{itemize}

\subsubsection{The Solution}

Implemented intelligent anomaly detection and alert routing:

\begin{lstlisting}[language=Python, caption={Intelligent Alert System Implementation}]
# Initialize anomaly detector with historical data
anomaly_detector = AnomalyDetector(
    methods=[
        AnomalyMethod.STATISTICAL,
        AnomalyMethod.ISOLATION_FOREST,
        AnomalyMethod.EXPONENTIAL_SMOOTHING
    ],
    contamination=0.05,  # Expect 5% anomalies
    statistical_threshold=3.5  # Stricter than default
)

# Train on 30 days of historical metrics
historical_metrics = fetch_historical_metrics(days=30)
anomaly_detector.fit(historical_metrics)

# Enhanced alert manager with noise reduction
alert_manager = AlertManager(
    email_config=email_config,
    slack_webhook=slack_webhook
)

# Configure intelligent alert rules
alert_manager.add_rule(AlertRule(
    name="critical_anomalies",
    severity_levels=[AlertSeverity.CRITICAL],
    channels=[AlertChannel.PAGERDUTY, AlertChannel.SLACK],
    recipients=["oncall@company.com", "#incidents"],
    max_frequency=3,  # Max 3 alerts per hour
    frequency_window=timedelta(hours=1),
    suppress_similar=True  # Deduplicate similar alerts
))

alert_manager.add_rule(AlertRule(
    name="medium_anomalies",
    severity_levels=[AlertSeverity.WARNING],
    channels=[AlertChannel.SLACK],
    recipients=["#ml-monitoring"],
    max_frequency=10,
    frequency_window=timedelta(hours=1),
    suppress_similar=True
))

# Monitoring loop with intelligent detection
def intelligent_monitoring():
    """Monitoring with anomaly detection and noise reduction."""
    while True:
        try:
            # Fetch current metrics
            current_metrics = fetch_current_metrics()

            # Detect anomalies (consensus-based)
            anomalies = anomaly_detector.detect(
                current_metrics,
                return_all=False  # Only consensus anomalies
            )

            # Filter out known false positives
            anomalies = filter_known_patterns(anomalies)

            # Create alerts only for true anomalies
            for anomaly in anomalies:
                severity = {
                    'high': AlertSeverity.CRITICAL,
                    'medium': AlertSeverity.WARNING,
                    'low': AlertSeverity.INFO
                }[anomaly.severity]

                alert = Alert(
                    severity=severity,
                    metric_name=anomaly.metric_name,
                    message=f"Anomaly detected in {anomaly.metric_name}",
                    value=anomaly.value,
                    threshold=anomaly.expected_range[1],
                    timestamp=anomaly.timestamp,
                    context={
                        'detection_method': anomaly.method.value,
                        'anomaly_score': anomaly.anomaly_score,
                        'expected_range': anomaly.expected_range,
                        **anomaly.context
                    }
                )

                # Send through intelligent alert manager
                alert_manager.send_alert(alert)

            time.sleep(300)  # Check every 5 minutes

        except Exception as e:
            logger.error(f"Monitoring error: {e}")
            time.sleep(60)

def filter_known_patterns(anomalies: List[AnomalyResult]) -> List[AnomalyResult]:
    """Filter out known false positive patterns."""
    filtered = []

    for anomaly in anomalies:
        # Skip cache warmup anomalies (first 5 minutes after deployment)
        if is_recent_deployment() and time_since_deployment() < 300:
            continue

        # Skip known traffic spike patterns (daily batch jobs)
        if is_batch_job_time() and 'latency' in anomaly.metric_name:
            continue

        # Skip temporary spikes (single point anomalies)
        if is_transient_spike(anomaly):
            continue

        filtered.append(anomaly)

    return filtered
\end{lstlisting}

\subsubsection{Outcome}

After implementing intelligent anomaly detection:

\begin{itemize}
    \item \textbf{Alerts reduced 95\%}: From 500+/day to 20-25/day
    \item \textbf{False positive rate dropped to 15\%}: Down from 80\%
    \item \textbf{Actionable alerts}: 85\% of alerts led to meaningful investigation or action
    \item \textbf{Mean time to detection improved}: Real issues detected in 15 minutes (vs 4 days)
    \item \textbf{Team morale recovered}: Engineers actually trusted and acted on alerts
    \item \textbf{ROI quantified}: Prevented \$300K in potential losses from model degradation
\end{itemize}

\section{Model Performance Monitoring}

Performance monitoring tracks model quality metrics over time, detecting degradation before business impact.

\subsection{ModelMonitor: Core Monitoring System}

\begin{lstlisting}[language=Python, caption={Comprehensive Model Performance Monitor}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
from datetime import datetime, timedelta
from collections import defaultdict, deque
import logging
import json
import numpy as np
from prometheus_client import (
    Counter, Gauge, Histogram, Summary,
    CollectorRegistry, push_to_gateway
)

logger = logging.getLogger(__name__)

class MetricType(Enum):
    """Types of metrics to monitor."""
    COUNTER = "counter"  # Monotonically increasing
    GAUGE = "gauge"      # Can go up or down
    HISTOGRAM = "histogram"  # Distribution of values
    SUMMARY = "summary"  # Quantiles over sliding window

class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class MetricConfig:
    """
    Configuration for a monitored metric.

    Attributes:
        name: Metric identifier
        metric_type: Type of metric (counter, gauge, etc.)
        description: Human-readable description
        labels: Labels for metric dimensions
        thresholds: Alert thresholds by severity
        window_size: Time window for aggregation (seconds)
    """
    name: str
    metric_type: MetricType
    description: str
    labels: List[str] = field(default_factory=list)
    thresholds: Dict[AlertSeverity, float] = field(default_factory=dict)
    window_size: int = 3600  # 1 hour default

@dataclass
class Alert:
    """
    Monitoring alert with context.

    Attributes:
        severity: Alert severity level
        metric_name: Name of metric that triggered alert
        message: Alert description
        value: Current metric value
        threshold: Threshold that was breached
        timestamp: When alert was generated
        context: Additional context for debugging
    """
    severity: AlertSeverity
    metric_name: str
    message: str
    value: float
    threshold: float
    timestamp: datetime
    context: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert alert to dictionary."""
        return {
            "severity": self.severity.value,
            "metric_name": self.metric_name,
            "message": self.message,
            "value": self.value,
            "threshold": self.threshold,
            "timestamp": self.timestamp.isoformat(),
            "context": self.context
        }

class ModelMonitor:
    """
    Production-grade ML model monitoring system.

    Integrates with Prometheus for metrics collection and supports
    custom metrics, alerting, and trend analysis.

    Example:
        >>> monitor = ModelMonitor(
        ...     model_name="fraud_detector",
        ...     prometheus_gateway="localhost:9091"
        ... )
        >>> monitor.register_metric(MetricConfig(
        ...     name="prediction_accuracy",
        ...     metric_type=MetricType.GAUGE,
        ...     description="Model prediction accuracy",
        ...     thresholds={
        ...         AlertSeverity.WARNING: 0.85,
        ...         AlertSeverity.CRITICAL: 0.75
        ...     }
        ... ))
        >>> monitor.record_metric("prediction_accuracy", 0.82)
    """

    def __init__(
        self,
        model_name: str,
        model_version: str = "v1",
        prometheus_gateway: Optional[str] = None,
        alert_callback: Optional[Callable[[Alert], None]] = None,
        enable_push: bool = True
    ):
        """
        Initialize model monitor.

        Args:
            model_name: Name of model being monitored
            model_version: Model version identifier
            prometheus_gateway: Prometheus pushgateway address
            alert_callback: Function to call when alert is triggered
            enable_push: Whether to push metrics to Prometheus
        """
        self.model_name = model_name
        self.model_version = model_version
        self.prometheus_gateway = prometheus_gateway
        self.alert_callback = alert_callback
        self.enable_push = enable_push

        # Metric storage
        self.registry = CollectorRegistry()
        self.metrics: Dict[str, Any] = {}
        self.metric_configs: Dict[str, MetricConfig] = {}
        self.metric_history: Dict[str, deque] = defaultdict(
            lambda: deque(maxlen=10000)
        )

        # Alert management
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.alert_suppression: Dict[str, datetime] = {}

        # Performance counters
        self._setup_default_metrics()

        logger.info(
            f"Initialized ModelMonitor for {model_name} v{model_version}"
        )

    def _setup_default_metrics(self):
        """Set up default monitoring metrics."""
        # Prediction counter
        self.predictions_total = Counter(
            'model_predictions_total',
            'Total number of predictions',
            ['model_name', 'model_version', 'status'],
            registry=self.registry
        )

        # Prediction latency
        self.prediction_latency = Histogram(
            'model_prediction_latency_seconds',
            'Prediction latency in seconds',
            ['model_name', 'model_version'],
            buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],
            registry=self.registry
        )

        # Prediction confidence
        self.prediction_confidence = Summary(
            'model_prediction_confidence',
            'Distribution of prediction confidence scores',
            ['model_name', 'model_version'],
            registry=self.registry
        )

        # Active predictions
        self.active_predictions = Gauge(
            'model_active_predictions',
            'Number of predictions currently being processed',
            ['model_name', 'model_version'],
            registry=self.registry
        )

    def register_metric(self, config: MetricConfig):
        """
        Register a custom metric for monitoring.

        Args:
            config: Metric configuration
        """
        self.metric_configs[config.name] = config

        # Create Prometheus metric
        labels = ['model_name', 'model_version'] + config.labels

        if config.metric_type == MetricType.COUNTER:
            metric = Counter(
                f'model_{config.name}',
                config.description,
                labels,
                registry=self.registry
            )
        elif config.metric_type == MetricType.GAUGE:
            metric = Gauge(
                f'model_{config.name}',
                config.description,
                labels,
                registry=self.registry
            )
        elif config.metric_type == MetricType.HISTOGRAM:
            metric = Histogram(
                f'model_{config.name}',
                config.description,
                labels,
                registry=self.registry
            )
        else:  # SUMMARY
            metric = Summary(
                f'model_{config.name}',
                config.description,
                labels,
                registry=self.registry
            )

        self.metrics[config.name] = metric
        logger.info(f"Registered metric: {config.name}")

    def record_metric(
        self,
        metric_name: str,
        value: float,
        labels: Optional[Dict[str, str]] = None
    ):
        """
        Record a metric value.

        Args:
            metric_name: Name of metric to record
            value: Metric value
            labels: Optional label values
        """
        if metric_name not in self.metrics:
            logger.warning(f"Metric {metric_name} not registered")
            return

        # Store in history
        self.metric_history[metric_name].append({
            'timestamp': datetime.now(),
            'value': value,
            'labels': labels or {}
        })

        # Update Prometheus metric
        metric = self.metrics[metric_name]
        label_values = {
            'model_name': self.model_name,
            'model_version': self.model_version,
            **(labels or {})
        }

        config = self.metric_configs[metric_name]
        if config.metric_type == MetricType.COUNTER:
            metric.labels(**label_values).inc(value)
        elif config.metric_type == MetricType.GAUGE:
            metric.labels(**label_values).set(value)
        else:  # HISTOGRAM or SUMMARY
            metric.labels(**label_values).observe(value)

        # Check thresholds
        self._check_thresholds(metric_name, value)

        # Push to Prometheus if enabled
        if self.enable_push and self.prometheus_gateway:
            try:
                push_to_gateway(
                    self.prometheus_gateway,
                    job=f'model_monitor_{self.model_name}',
                    registry=self.registry
                )
            except Exception as e:
                logger.error(f"Failed to push to Prometheus: {e}")

    def _check_thresholds(self, metric_name: str, value: float):
        """
        Check if metric value breaches thresholds.

        Args:
            metric_name: Name of metric
            value: Current value
        """
        config = self.metric_configs.get(metric_name)
        if not config or not config.thresholds:
            return

        # Check from highest to lowest severity
        severities = [
            AlertSeverity.CRITICAL,
            AlertSeverity.ERROR,
            AlertSeverity.WARNING,
            AlertSeverity.INFO
        ]

        for severity in severities:
            if severity not in config.thresholds:
                continue

            threshold = config.thresholds[severity]

            # For performance metrics, breach is below threshold
            # For error metrics, breach is above threshold
            # Determine based on metric name conventions
            is_error_metric = any(
                term in metric_name.lower()
                for term in ['error', 'failure', 'latency', 'drift']
            )

            breached = (
                value > threshold if is_error_metric
                else value < threshold
            )

            if breached:
                self._trigger_alert(
                    severity,
                    metric_name,
                    value,
                    threshold,
                    is_error_metric
                )
                break  # Only trigger highest severity

    def _trigger_alert(
        self,
        severity: AlertSeverity,
        metric_name: str,
        value: float,
        threshold: float,
        is_error_metric: bool
    ):
        """
        Trigger a monitoring alert.

        Args:
            severity: Alert severity
            metric_name: Name of metric
            value: Current value
            threshold: Breached threshold
            is_error_metric: Whether this is an error-type metric
        """
        # Check alert suppression (prevent spam)
        suppression_key = f"{metric_name}_{severity.value}"
        if suppression_key in self.alert_suppression:
            last_alert = self.alert_suppression[suppression_key]
            if datetime.now() - last_alert < timedelta(minutes=15):
                return  # Suppress alert

        # Create alert
        direction = "above" if is_error_metric else "below"
        alert = Alert(
            severity=severity,
            metric_name=metric_name,
            message=(
                f"{metric_name} is {direction} threshold: "
                f"{value:.4f} (threshold: {threshold:.4f})"
            ),
            value=value,
            threshold=threshold,
            timestamp=datetime.now(),
            context={
                'model_name': self.model_name,
                'model_version': self.model_version,
                'history': list(self.metric_history[metric_name])[-10:]
            }
        )

        # Store alert
        self.active_alerts[metric_name] = alert
        self.alert_history.append(alert)
        self.alert_suppression[suppression_key] = datetime.now()

        # Log alert
        logger.log(
            logging.CRITICAL if severity == AlertSeverity.CRITICAL
            else logging.ERROR if severity == AlertSeverity.ERROR
            else logging.WARNING,
            f"ALERT: {alert.message}"
        )

        # Call alert callback
        if self.alert_callback:
            try:
                self.alert_callback(alert)
            except Exception as e:
                logger.error(f"Alert callback failed: {e}")

    def clear_alert(self, metric_name: str):
        """
        Clear an active alert.

        Args:
            metric_name: Name of metric
        """
        if metric_name in self.active_alerts:
            del self.active_alerts[metric_name]
            logger.info(f"Cleared alert for {metric_name}")

    def get_metric_history(
        self,
        metric_name: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """
        Get historical values for a metric.

        Args:
            metric_name: Name of metric
            start_time: Start of time range
            end_time: End of time range

        Returns:
            List of metric values with timestamps
        """
        if metric_name not in self.metric_history:
            return []

        history = list(self.metric_history[metric_name])

        if start_time:
            history = [
                h for h in history
                if h['timestamp'] >= start_time
            ]

        if end_time:
            history = [
                h for h in history
                if h['timestamp'] <= end_time
            ]

        return history

    def get_metrics_summary(self) -> Dict[str, Any]:
        """
        Get summary of all monitored metrics.

        Returns:
            Dictionary with metric summaries
        """
        summary = {
            'model_name': self.model_name,
            'model_version': self.model_version,
            'timestamp': datetime.now().isoformat(),
            'metrics': {},
            'active_alerts': len(self.active_alerts),
            'total_alerts': len(self.alert_history)
        }

        for metric_name, history in self.metric_history.items():
            if not history:
                continue

            values = [h['value'] for h in history]
            summary['metrics'][metric_name] = {
                'current': values[-1],
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values),
                'count': len(values)
            }

        return summary

    def record_prediction(
        self,
        latency: float,
        confidence: float,
        success: bool = True
    ):
        """
        Record a model prediction with standard metrics.

        Args:
            latency: Prediction latency in seconds
            confidence: Prediction confidence score
            success: Whether prediction succeeded
        """
        status = 'success' if success else 'error'

        self.predictions_total.labels(
            model_name=self.model_name,
            model_version=self.model_version,
            status=status
        ).inc()

        self.prediction_latency.labels(
            model_name=self.model_name,
            model_version=self.model_version
        ).observe(latency)

        self.prediction_confidence.labels(
            model_name=self.model_name,
            model_version=self.model_version
        ).observe(confidence)
\end{lstlisting}

\subsection{Custom Metrics and Alerting}

The ModelMonitor supports custom metrics with flexible thresholds:

\begin{lstlisting}[language=Python, caption={Custom Metrics Configuration}]
# Configure performance monitoring
monitor = ModelMonitor(
    model_name="fraud_detector",
    model_version="v2.1",
    prometheus_gateway="localhost:9091"
)

# Register accuracy metric
monitor.register_metric(MetricConfig(
    name="accuracy",
    metric_type=MetricType.GAUGE,
    description="Model prediction accuracy",
    thresholds={
        AlertSeverity.WARNING: 0.85,
        AlertSeverity.CRITICAL: 0.75
    },
    window_size=3600  # 1 hour
))

# Register precision and recall
monitor.register_metric(MetricConfig(
    name="precision",
    metric_type=MetricType.GAUGE,
    description="Precision for fraud class",
    labels=["class"],
    thresholds={
        AlertSeverity.WARNING: 0.80,
        AlertSeverity.CRITICAL: 0.70
    }
))

monitor.register_metric(MetricConfig(
    name="recall",
    metric_type=MetricType.GAUGE,
    description="Recall for fraud class",
    labels=["class"],
    thresholds={
        AlertSeverity.WARNING: 0.75,
        AlertSeverity.CRITICAL: 0.65
    }
))

# Register error rate
monitor.register_metric(MetricConfig(
    name="error_rate",
    metric_type=MetricType.GAUGE,
    description="Prediction error rate",
    thresholds={
        AlertSeverity.WARNING: 0.05,  # 5% errors
        AlertSeverity.CRITICAL: 0.10  # 10% errors
    }
))

# Record metrics during prediction
from contextlib import contextmanager
import time

@contextmanager
def monitor_prediction(monitor: ModelMonitor):
    """Context manager for monitoring predictions."""
    start_time = time.time()
    monitor.active_predictions.labels(
        model_name=monitor.model_name,
        model_version=monitor.model_version
    ).inc()

    try:
        yield
        success = True
    except Exception:
        success = False
        raise
    finally:
        latency = time.time() - start_time
        monitor.active_predictions.labels(
            model_name=monitor.model_name,
            model_version=monitor.model_version
        ).dec()

        # Record latency
        monitor.prediction_latency.labels(
            model_name=monitor.model_name,
            model_version=monitor.model_version
        ).observe(latency)

        # Record success/failure
        status = 'success' if success else 'error'
        monitor.predictions_total.labels(
            model_name=monitor.model_name,
            model_version=monitor.model_version,
            status=status
        ).inc()

# Usage in prediction pipeline
def make_prediction(features, monitor):
    """Make prediction with monitoring."""
    with monitor_prediction(monitor):
        prediction = model.predict(features)
        confidence = model.predict_proba(features).max()

        monitor.prediction_confidence.labels(
            model_name=monitor.model_name,
            model_version=monitor.model_version
        ).observe(confidence)

        return prediction
\end{lstlisting}

\section{Data Drift Detection}

Data drift occurs when input feature distributions change over time, degrading model performance.

\subsection{DriftDetector: Statistical Drift Detection}

\begin{lstlisting}[language=Python, caption={Comprehensive Drift Detection System}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
import pandas as pd
from scipy import stats
from scipy.spatial.distance import jensenshannon
import logging

logger = logging.getLogger(__name__)

class DriftType(Enum):
    """Types of drift detection methods."""
    KS_TEST = "kolmogorov_smirnov"  # For continuous features
    CHI_SQUARE = "chi_square"  # For categorical features
    PSI = "population_stability_index"  # For any features
    JS_DIVERGENCE = "jensen_shannon"  # For distributions
    WASSERSTEIN = "wasserstein"  # For continuous distributions

@dataclass
class DriftResult:
    """
    Result of drift detection analysis.

    Attributes:
        feature_name: Name of feature analyzed
        drift_detected: Whether drift was detected
        drift_score: Numeric drift score
        p_value: Statistical significance (if applicable)
        drift_type: Method used for detection
        reference_stats: Statistics of reference distribution
        current_stats: Statistics of current distribution
        threshold: Threshold used for detection
    """
    feature_name: str
    drift_detected: bool
    drift_score: float
    p_value: Optional[float]
    drift_type: DriftType
    reference_stats: Dict[str, float]
    current_stats: Dict[str, float]
    threshold: float

    def to_dict(self) -> Dict:
        """Convert to dictionary."""
        return {
            'feature_name': self.feature_name,
            'drift_detected': self.drift_detected,
            'drift_score': self.drift_score,
            'p_value': self.p_value,
            'drift_type': self.drift_type.value,
            'reference_stats': self.reference_stats,
            'current_stats': self.current_stats,
            'threshold': self.threshold
        }

class DriftDetector:
    """
    Statistical drift detection for ML features.

    Supports multiple drift detection methods:
    - Kolmogorov-Smirnov test for continuous features
    - Chi-square test for categorical features
    - Population Stability Index (PSI)
    - Jensen-Shannon divergence
    - Wasserstein distance

    Example:
        >>> detector = DriftDetector()
        >>> detector.fit(reference_data)
        >>> results = detector.detect_drift(current_data)
        >>> for result in results:
        ...     if result.drift_detected:
        ...         print(f"Drift in {result.feature_name}")
    """

    def __init__(
        self,
        categorical_features: Optional[List[str]] = None,
        ks_threshold: float = 0.05,
        chi_square_threshold: float = 0.05,
        psi_threshold: float = 0.2,
        js_threshold: float = 0.1,
        wasserstein_threshold: float = 0.1,
        n_bins: int = 10
    ):
        """
        Initialize drift detector.

        Args:
            categorical_features: List of categorical feature names
            ks_threshold: P-value threshold for KS test
            chi_square_threshold: P-value threshold for chi-square
            psi_threshold: PSI threshold (0.1=small, 0.2=medium drift)
            js_threshold: Jensen-Shannon divergence threshold
            wasserstein_threshold: Wasserstein distance threshold
            n_bins: Number of bins for discretization
        """
        self.categorical_features = categorical_features or []
        self.ks_threshold = ks_threshold
        self.chi_square_threshold = chi_square_threshold
        self.psi_threshold = psi_threshold
        self.js_threshold = js_threshold
        self.wasserstein_threshold = wasserstein_threshold
        self.n_bins = n_bins

        # Reference distribution storage
        self.reference_distributions: Dict[str, Dict] = {}
        self.is_fitted = False

    def fit(self, reference_data: pd.DataFrame):
        """
        Fit detector on reference data distribution.

        Args:
            reference_data: Reference dataset (training data)
        """
        logger.info("Fitting drift detector on reference data")

        for column in reference_data.columns:
            if column in self.categorical_features:
                # Store categorical distribution
                value_counts = reference_data[column].value_counts(
                    normalize=True
                )
                self.reference_distributions[column] = {
                    'type': 'categorical',
                    'distribution': value_counts.to_dict(),
                    'categories': set(value_counts.index)
                }
            else:
                # Store continuous distribution
                values = reference_data[column].dropna()
                self.reference_distributions[column] = {
                    'type': 'continuous',
                    'mean': float(values.mean()),
                    'std': float(values.std()),
                    'min': float(values.min()),
                    'max': float(values.max()),
                    'values': values.values,
                    'histogram': np.histogram(
                        values,
                        bins=self.n_bins
                    )
                }

        self.is_fitted = True
        logger.info(
            f"Fitted on {len(self.reference_distributions)} features"
        )

    def detect_drift(
        self,
        current_data: pd.DataFrame,
        methods: Optional[List[DriftType]] = None
    ) -> List[DriftResult]:
        """
        Detect drift in current data vs reference.

        Args:
            current_data: Current dataset to check for drift
            methods: Specific drift detection methods to use

        Returns:
            List of drift detection results
        """
        if not self.is_fitted:
            raise ValueError("Detector not fitted. Call fit() first.")

        if methods is None:
            methods = [DriftType.KS_TEST, DriftType.PSI]

        results = []

        for column in current_data.columns:
            if column not in self.reference_distributions:
                logger.warning(f"Column {column} not in reference data")
                continue

            ref_dist = self.reference_distributions[column]

            if ref_dist['type'] == 'categorical':
                # Categorical drift detection
                if DriftType.CHI_SQUARE in methods:
                    result = self._chi_square_test(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)

                if DriftType.PSI in methods:
                    result = self._psi_categorical(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)
            else:
                # Continuous drift detection
                if DriftType.KS_TEST in methods:
                    result = self._ks_test(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)

                if DriftType.PSI in methods:
                    result = self._psi_continuous(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)

                if DriftType.JS_DIVERGENCE in methods:
                    result = self._js_divergence(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)

                if DriftType.WASSERSTEIN in methods:
                    result = self._wasserstein_distance(
                        column,
                        current_data[column],
                        ref_dist
                    )
                    results.append(result)

        return results

    def _ks_test(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Kolmogorov-Smirnov test for continuous features.

        Tests null hypothesis that distributions are the same.
        """
        current_clean = current_values.dropna()
        reference_values = ref_dist['values']

        # Perform KS test
        statistic, p_value = stats.ks_2samp(
            reference_values,
            current_clean
        )

        drift_detected = p_value < self.ks_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=statistic,
            p_value=p_value,
            drift_type=DriftType.KS_TEST,
            reference_stats={
                'mean': ref_dist['mean'],
                'std': ref_dist['std']
            },
            current_stats={
                'mean': float(current_clean.mean()),
                'std': float(current_clean.std())
            },
            threshold=self.ks_threshold
        )

    def _chi_square_test(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Chi-square test for categorical features.

        Tests independence of distributions.
        """
        # Get current distribution
        current_counts = current_values.value_counts()
        ref_distribution = ref_dist['distribution']

        # Align categories
        all_categories = set(current_counts.index) | ref_dist['categories']

        observed = []
        expected = []
        total_current = len(current_values)

        for category in all_categories:
            observed.append(current_counts.get(category, 0))
            expected.append(
                ref_distribution.get(category, 0) * total_current
            )

        # Perform chi-square test
        observed = np.array(observed)
        expected = np.array(expected)

        # Add small constant to avoid division by zero
        expected = np.where(expected == 0, 0.001, expected)

        statistic, p_value = stats.chisquare(observed, expected)

        drift_detected = p_value < self.chi_square_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=statistic,
            p_value=p_value,
            drift_type=DriftType.CHI_SQUARE,
            reference_stats={'distribution': ref_distribution},
            current_stats={
                'distribution': current_counts.to_dict()
            },
            threshold=self.chi_square_threshold
        )

    def _psi_continuous(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Population Stability Index for continuous features.

        PSI = sum((current% - reference%) * ln(current% / reference%))
        """
        current_clean = current_values.dropna()

        # Use reference histogram bins
        ref_counts, ref_bins = ref_dist['histogram']

        # Bin current data with same bins
        current_counts, _ = np.histogram(
            current_clean,
            bins=ref_bins
        )

        # Calculate percentages
        ref_pct = ref_counts / ref_counts.sum()
        current_pct = current_counts / current_counts.sum()

        # Add small constant to avoid log(0)
        ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)
        current_pct = np.where(current_pct == 0, 0.0001, current_pct)

        # Calculate PSI
        psi = np.sum(
            (current_pct - ref_pct) * np.log(current_pct / ref_pct)
        )

        drift_detected = psi > self.psi_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=psi,
            p_value=None,
            drift_type=DriftType.PSI,
            reference_stats={
                'mean': ref_dist['mean'],
                'std': ref_dist['std']
            },
            current_stats={
                'mean': float(current_clean.mean()),
                'std': float(current_clean.std())
            },
            threshold=self.psi_threshold
        )

    def _psi_categorical(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Population Stability Index for categorical features.
        """
        current_pct = current_values.value_counts(normalize=True)
        ref_pct = pd.Series(ref_dist['distribution'])

        # Align indices
        all_categories = set(current_pct.index) | set(ref_pct.index)

        psi = 0.0
        for category in all_categories:
            current_p = current_pct.get(category, 0.0001)
            ref_p = ref_pct.get(category, 0.0001)

            psi += (current_p - ref_p) * np.log(current_p / ref_p)

        drift_detected = psi > self.psi_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=psi,
            p_value=None,
            drift_type=DriftType.PSI,
            reference_stats={'distribution': ref_dist['distribution']},
            current_stats={'distribution': current_pct.to_dict()},
            threshold=self.psi_threshold
        )

    def _js_divergence(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Jensen-Shannon divergence for continuous features.

        Symmetric measure of distribution similarity.
        """
        current_clean = current_values.dropna()

        # Use reference histogram bins
        ref_counts, ref_bins = ref_dist['histogram']
        current_counts, _ = np.histogram(
            current_clean,
            bins=ref_bins
        )

        # Normalize to probabilities
        ref_probs = ref_counts / ref_counts.sum()
        current_probs = current_counts / current_counts.sum()

        # Calculate JS divergence
        js_div = jensenshannon(ref_probs, current_probs)

        drift_detected = js_div > self.js_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=js_div,
            p_value=None,
            drift_type=DriftType.JS_DIVERGENCE,
            reference_stats={
                'mean': ref_dist['mean'],
                'std': ref_dist['std']
            },
            current_stats={
                'mean': float(current_clean.mean()),
                'std': float(current_clean.std())
            },
            threshold=self.js_threshold
        )

    def _wasserstein_distance(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> DriftResult:
        """
        Wasserstein distance (Earth Mover's Distance).

        Measures the minimum cost to transform one distribution
        into another.
        """
        current_clean = current_values.dropna()
        reference_values = ref_dist['values']

        # Calculate Wasserstein distance
        distance = stats.wasserstein_distance(
            reference_values,
            current_clean
        )

        # Normalize by reference std for interpretability
        normalized_distance = distance / (ref_dist['std'] + 1e-10)

        drift_detected = normalized_distance > self.wasserstein_threshold

        return DriftResult(
            feature_name=feature_name,
            drift_detected=drift_detected,
            drift_score=normalized_distance,
            p_value=None,
            drift_type=DriftType.WASSERSTEIN,
            reference_stats={
                'mean': ref_dist['mean'],
                'std': ref_dist['std']
            },
            current_stats={
                'mean': float(current_clean.mean()),
                'std': float(current_clean.std())
            },
            threshold=self.wasserstein_threshold
        )

    def get_drift_summary(
        self,
        results: List[DriftResult]
    ) -> Dict[str, Any]:
        """
        Generate summary of drift detection results.

        Args:
            results: List of drift detection results

        Returns:
            Summary dictionary with statistics
        """
        total_features = len(set(r.feature_name for r in results))
        drifted_features = len(
            set(r.feature_name for r in results if r.drift_detected)
        )

        # Group by drift type
        by_type = defaultdict(list)
        for result in results:
            by_type[result.drift_type].append(result)

        type_summaries = {}
        for drift_type, type_results in by_type.items():
            drifted = sum(1 for r in type_results if r.drift_detected)
            type_summaries[drift_type.value] = {
                'total_checked': len(type_results),
                'drifted': drifted,
                'drift_rate': drifted / len(type_results)
            }

        return {
            'total_features': total_features,
            'drifted_features': drifted_features,
            'drift_rate': drifted_features / total_features,
            'by_type': type_summaries,
            'drifted_feature_names': list(
                set(r.feature_name for r in results if r.drift_detected)
            )
        }
\end{lstlisting}

\subsection{Drift Detection in Practice}

\begin{lstlisting}[language=Python, caption={Implementing Drift Detection}]
# Initialize drift detector
drift_detector = DriftDetector(
    categorical_features=['country', 'product_category'],
    ks_threshold=0.05,
    psi_threshold=0.2
)

# Fit on training/reference data
drift_detector.fit(reference_data)

# Monitor production data periodically
def monitor_data_drift(current_batch: pd.DataFrame):
    """Monitor current batch for drift."""
    # Detect drift using multiple methods
    results = drift_detector.detect_drift(
        current_batch,
        methods=[
            DriftType.KS_TEST,
            DriftType.PSI,
            DriftType.JS_DIVERGENCE
        ]
    )

    # Get summary
    summary = drift_detector.get_drift_summary(results)

    # Log results
    logger.info(f"Drift Summary: {summary}")

    # Alert on significant drift
    if summary['drift_rate'] > 0.3:  # 30% of features drifting
        logger.warning(
            f"Significant drift detected: {summary['drift_rate']:.1%} "
            f"of features affected"
        )

        # Log specific drifted features
        for result in results:
            if result.drift_detected:
                logger.warning(
                    f"  {result.feature_name}: "
                    f"{result.drift_type.value} = {result.drift_score:.4f} "
                    f"(threshold: {result.threshold})"
                )

        # Trigger retraining if drift is severe
        if summary['drift_rate'] > 0.5:
            logger.critical("Severe drift detected - triggering retrain")
            trigger_model_retraining()

    return results, summary

# Schedule periodic drift checks
import schedule

def drift_check_job():
    """Scheduled drift check."""
    # Get recent production data
    current_batch = get_recent_production_data(hours=24)

    # Check for drift
    results, summary = monitor_data_drift(current_batch)

    # Store results for trend analysis
    store_drift_metrics(summary)

# Run drift check every 6 hours
schedule.every(6).hours.do(drift_check_job)
\end{lstlisting}

\section{Advanced Drift Detection Methods}

Beyond basic statistical tests, production ML systems require sophisticated drift detection capable of handling multi-dimensional features, concept drift, adversarial patterns, and causal analysis.

\subsection{Mathematical Foundation of Drift Detection}

Drift detection relies on measuring distributional divergence between reference distribution $P_{ref}$ and current distribution $P_{curr}$.

\subsubsection{Kolmogorov-Smirnov Statistic}

For continuous features, the KS statistic measures maximum distance between cumulative distribution functions:

\begin{equation}
D_{KS} = \sup_x |F_{ref}(x) - F_{curr}(x)|
\end{equation}

where $F_{ref}$ and $F_{curr}$ are empirical CDFs. Under null hypothesis (no drift):

\begin{equation}
D_{KS} \cdot \sqrt{\frac{n_{ref} \cdot n_{curr}}{n_{ref} + n_{curr}}} \sim K(\alpha)
\end{equation}

where $K(\alpha)$ is the Kolmogorov distribution with significance level $\alpha$.

\subsubsection{Population Stability Index (PSI)}

PSI quantifies shift in categorical or binned continuous distributions:

\begin{equation}
PSI = \sum_{i=1}^{k} (P_{curr,i} - P_{ref,i}) \cdot \ln\left(\frac{P_{curr,i}}{P_{ref,i}}\right)
\end{equation}

Interpretation thresholds:
\begin{itemize}
    \item $PSI < 0.1$: No significant drift
    \item $0.1 \leq PSI < 0.2$: Moderate drift, investigation needed
    \item $PSI \geq 0.2$: Significant drift, retraining recommended
\end{itemize}

\subsubsection{Maximum Mean Discrepancy (MMD)}

MMD measures distance between distributions in reproducing kernel Hilbert space:

\begin{equation}
MMD^2(P_{ref}, P_{curr}) = \mathbb{E}_{x,x' \sim P_{ref}}[k(x,x')] + \mathbb{E}_{y,y' \sim P_{curr}}[k(y,y')] - 2\mathbb{E}_{x \sim P_{ref}, y \sim P_{curr}}[k(x,y)]
\end{equation}

where $k(\cdot, \cdot)$ is a characteristic kernel (e.g., Gaussian RBF).

\subsection{Multi-Dimensional Drift Analysis}

\begin{lstlisting}[language=Python, caption={Advanced Multi-Dimensional Drift Detector}]
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
import numpy as np
import pandas as pd
from scipy import stats
from scipy.spatial.distance import jensenshannon
from sklearn.metrics.pairwise import rbf_kernel
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)

@dataclass
class MultiDimensionalDriftResult:
    """
    Comprehensive drift analysis result.

    Attributes:
        feature_name: Name of feature
        univariate_drift_score: Single feature drift score
        multivariate_drift_contribution: Contribution to joint drift
        correlation_changes: Changes in feature correlations
        drift_detected: Whether drift threshold exceeded
        p_value: Statistical significance
        confidence_interval: (lower, upper) bounds
        drift_severity: low/medium/high/critical
        correlated_features: Features with correlation changes
    """
    feature_name: str
    univariate_drift_score: float
    multivariate_drift_contribution: float
    correlation_changes: Dict[str, float]
    drift_detected: bool
    p_value: Optional[float]
    confidence_interval: Tuple[float, float]
    drift_severity: str
    correlated_features: Set[str]
    timestamp: datetime = field(default_factory=datetime.now)

class AdvancedDriftDetector:
    """
    Multi-dimensional drift detection with correlation analysis.

    Detects drift in:
    - Individual features (univariate)
    - Feature combinations (multivariate)
    - Correlation structure changes
    - Statistical significance with confidence intervals

    Uses multiple methods:
    - Kolmogorov-Smirnov test
    - Population Stability Index
    - Maximum Mean Discrepancy (MMD)
    - Chi-square test for correlations

    Example:
        >>> detector = AdvancedDriftDetector()
        >>> detector.fit(reference_data)
        >>> results = detector.detect_drift(
        ...     current_data,
        ...     analyze_correlations=True
        ... )
        >>> for result in results:
        ...     if result.drift_severity == 'critical':
        ...         trigger_alert(result)
    """

    def __init__(
        self,
        categorical_features: Optional[List[str]] = None,
        ks_alpha: float = 0.05,
        psi_threshold: float = 0.2,
        mmd_threshold: float = 0.1,
        correlation_threshold: float = 0.3,
        n_bins: int = 10,
        enable_multivariate: bool = True,
        confidence_level: float = 0.95
    ):
        """
        Initialize advanced drift detector.

        Args:
            categorical_features: List of categorical feature names
            ks_alpha: Significance level for KS test
            psi_threshold: PSI threshold for drift detection
            mmd_threshold: MMD threshold for drift detection
            correlation_threshold: Threshold for correlation changes
            n_bins: Number of bins for discretization
            enable_multivariate: Enable multivariate drift analysis
            confidence_level: Confidence level for intervals (0.95 = 95%)
        """
        self.categorical_features = categorical_features or []
        self.ks_alpha = ks_alpha
        self.psi_threshold = psi_threshold
        self.mmd_threshold = mmd_threshold
        self.correlation_threshold = correlation_threshold
        self.n_bins = n_bins
        self.enable_multivariate = enable_multivariate
        self.confidence_level = confidence_level

        # Reference distribution storage
        self.reference_distributions: Dict[str, Dict] = {}
        self.reference_correlations: Optional[np.ndarray] = None
        self.feature_names: List[str] = []
        self.is_fitted = False

    def fit(self, reference_data: pd.DataFrame):
        """
        Fit detector on reference data.

        Args:
            reference_data: Reference dataset (training data)
        """
        logger.info("Fitting advanced drift detector on reference data")

        self.feature_names = list(reference_data.columns)

        # Store individual feature distributions
        for column in reference_data.columns:
            if column in self.categorical_features:
                # Categorical distribution
                value_counts = reference_data[column].value_counts(
                    normalize=True
                )
                self.reference_distributions[column] = {
                    'type': 'categorical',
                    'distribution': value_counts.to_dict(),
                    'categories': set(value_counts.index),
                    'n_samples': len(reference_data)
                }
            else:
                # Continuous distribution
                values = reference_data[column].dropna()

                # Store raw values for KS test
                self.reference_distributions[column] = {
                    'type': 'continuous',
                    'values': values.values,
                    'mean': float(values.mean()),
                    'std': float(values.std()),
                    'min': float(values.min()),
                    'max': float(values.max()),
                    'n_samples': len(values)
                }

                # Compute histogram for PSI
                hist, bins = np.histogram(values, bins=self.n_bins)
                self.reference_distributions[column]['histogram'] = (hist, bins)

        # Compute correlation matrix for multivariate analysis
        if self.enable_multivariate:
            # Only use numerical features for correlation
            numerical_features = [
                col for col in reference_data.columns
                if col not in self.categorical_features
            ]

            if len(numerical_features) > 1:
                self.reference_correlations = reference_data[
                    numerical_features
                ].corr().values

        self.is_fitted = True
        logger.info(
            f"Fitted on {len(self.reference_distributions)} features"
        )

    def detect_drift(
        self,
        current_data: pd.DataFrame,
        analyze_correlations: bool = True,
        bootstrap_samples: int = 100
    ) -> List[MultiDimensionalDriftResult]:
        """
        Detect drift with multi-dimensional analysis.

        Args:
            current_data: Current dataset to check for drift
            analyze_correlations: Whether to analyze correlation changes
            bootstrap_samples: Number of bootstrap samples for CI

        Returns:
            List of drift detection results
        """
        if not self.is_fitted:
            raise ValueError("Detector not fitted. Call fit() first.")

        results = []

        # Detect correlation changes if enabled
        correlation_changes = {}
        if analyze_correlations and self.reference_correlations is not None:
            correlation_changes = self._detect_correlation_drift(current_data)

        # Analyze each feature
        for column in current_data.columns:
            if column not in self.reference_distributions:
                logger.warning(f"Column {column} not in reference data")
                continue

            ref_dist = self.reference_distributions[column]

            # Compute univariate drift
            if ref_dist['type'] == 'categorical':
                drift_score, p_value = self._categorical_drift(
                    column, current_data[column], ref_dist
                )
            else:
                drift_score, p_value = self._continuous_drift(
                    column, current_data[column], ref_dist
                )

            # Compute confidence interval via bootstrap
            ci_lower, ci_upper = self._bootstrap_confidence_interval(
                column,
                current_data[column],
                ref_dist,
                bootstrap_samples
            )

            # Compute multivariate contribution
            multivariate_contribution = 0.0
            if self.enable_multivariate:
                multivariate_contribution = self._compute_multivariate_contribution(
                    column, current_data
                )

            # Determine drift severity
            drift_detected = drift_score > self.psi_threshold
            severity = self._calculate_drift_severity(
                drift_score, p_value, multivariate_contribution
            )

            # Find correlated features with changes
            correlated_features = set()
            if column in correlation_changes:
                correlated_features = set(
                    feat for feat, change in correlation_changes[column].items()
                    if abs(change) > self.correlation_threshold
                )

            result = MultiDimensionalDriftResult(
                feature_name=column,
                univariate_drift_score=drift_score,
                multivariate_drift_contribution=multivariate_contribution,
                correlation_changes=correlation_changes.get(column, {}),
                drift_detected=drift_detected,
                p_value=p_value,
                confidence_interval=(ci_lower, ci_upper),
                drift_severity=severity,
                correlated_features=correlated_features
            )

            results.append(result)

        return results

    def _continuous_drift(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> Tuple[float, float]:
        """
        Detect drift in continuous feature using KS test and PSI.

        Returns:
            Tuple of (drift_score, p_value)
        """
        current_clean = current_values.dropna()
        reference_values = ref_dist['values']

        # Kolmogorov-Smirnov test
        ks_stat, ks_pvalue = stats.ks_2samp(
            reference_values,
            current_clean
        )

        # Population Stability Index
        ref_hist, ref_bins = ref_dist['histogram']
        curr_hist, _ = np.histogram(current_clean, bins=ref_bins)

        # Compute PSI
        ref_pct = ref_hist / ref_hist.sum()
        curr_pct = curr_hist / curr_hist.sum()

        # Avoid log(0)
        ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)
        curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)

        psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))

        # Use PSI as drift score, KS p-value for significance
        return psi, ks_pvalue

    def _categorical_drift(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict
    ) -> Tuple[float, float]:
        """
        Detect drift in categorical feature using PSI and chi-square.

        Returns:
            Tuple of (drift_score, p_value)
        """
        # Current distribution
        curr_counts = current_values.value_counts(normalize=True)
        ref_distribution = ref_dist['distribution']

        # Align categories
        all_categories = set(curr_counts.index) | ref_dist['categories']

        # Compute PSI
        psi = 0.0
        for category in all_categories:
            curr_p = curr_counts.get(category, 0.0001)
            ref_p = ref_distribution.get(category, 0.0001)
            psi += (curr_p - ref_p) * np.log(curr_p / ref_p)

        # Chi-square test for p-value
        observed = []
        expected = []
        total_current = len(current_values)

        for category in all_categories:
            observed.append(
                (current_values == category).sum()
            )
            expected.append(
                ref_distribution.get(category, 0.0001) * total_current
            )

        observed = np.array(observed)
        expected = np.array(expected)
        expected = np.where(expected == 0, 0.001, expected)

        chi2_stat, chi2_pvalue = stats.chisquare(observed, expected)

        return psi, chi2_pvalue

    def _detect_correlation_drift(
        self,
        current_data: pd.DataFrame
    ) -> Dict[str, Dict[str, float]]:
        """
        Detect changes in feature correlations.

        Returns:
            Dict mapping features to correlation changes
        """
        # Only use numerical features
        numerical_features = [
            col for col in current_data.columns
            if col not in self.categorical_features
        ]

        if len(numerical_features) <= 1:
            return {}

        # Compute current correlation matrix
        current_corr = current_data[numerical_features].corr().values

        # Compute correlation changes
        corr_changes = {}

        for i, feat1 in enumerate(numerical_features):
            corr_changes[feat1] = {}
            for j, feat2 in enumerate(numerical_features):
                if i != j:
                    change = abs(
                        current_corr[i, j] - self.reference_correlations[i, j]
                    )
                    corr_changes[feat1][feat2] = change

        return corr_changes

    def _bootstrap_confidence_interval(
        self,
        feature_name: str,
        current_values: pd.Series,
        ref_dist: Dict,
        n_samples: int
    ) -> Tuple[float, float]:
        """
        Compute confidence interval for drift score via bootstrap.

        Args:
            feature_name: Name of feature
            current_values: Current feature values
            ref_dist: Reference distribution
            n_samples: Number of bootstrap samples

        Returns:
            (lower_bound, upper_bound) confidence interval
        """
        current_clean = current_values.dropna().values
        n = len(current_clean)

        drift_scores = []

        for _ in range(n_samples):
            # Bootstrap resample
            bootstrap_sample = np.random.choice(
                current_clean, size=n, replace=True
            )

            # Compute drift score
            if ref_dist['type'] == 'categorical':
                # Use PSI for categorical
                ref_distribution = ref_dist['distribution']
                sample_counts = pd.Series(bootstrap_sample).value_counts(
                    normalize=True
                )

                psi = 0.0
                all_cats = set(sample_counts.index) | ref_dist['categories']
                for cat in all_cats:
                    curr_p = sample_counts.get(cat, 0.0001)
                    ref_p = ref_distribution.get(cat, 0.0001)
                    psi += (curr_p - ref_p) * np.log(curr_p / ref_p)

                drift_scores.append(psi)
            else:
                # Use PSI for continuous (binned)
                ref_hist, ref_bins = ref_dist['histogram']
                sample_hist, _ = np.histogram(bootstrap_sample, bins=ref_bins)

                ref_pct = ref_hist / ref_hist.sum()
                sample_pct = sample_hist / sample_hist.sum()

                ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)
                sample_pct = np.where(sample_pct == 0, 0.0001, sample_pct)

                psi = np.sum(
                    (sample_pct - ref_pct) * np.log(sample_pct / ref_pct)
                )

                drift_scores.append(psi)

        # Compute percentiles for confidence interval
        alpha = 1 - self.confidence_level
        lower = np.percentile(drift_scores, alpha / 2 * 100)
        upper = np.percentile(drift_scores, (1 - alpha / 2) * 100)

        return lower, upper

    def _compute_multivariate_contribution(
        self,
        feature_name: str,
        current_data: pd.DataFrame
    ) -> float:
        """
        Compute feature's contribution to multivariate drift using MMD.

        Returns:
            Multivariate drift contribution score
        """
        if not self.enable_multivariate:
            return 0.0

        # Use Maximum Mean Discrepancy (MMD) with RBF kernel
        # Simplified: use feature's correlation with other features

        numerical_features = [
            col for col in current_data.columns
            if col not in self.categorical_features
        ]

        if feature_name not in numerical_features or len(numerical_features) <= 1:
            return 0.0

        # Get feature index
        feat_idx = numerical_features.index(feature_name)

        # Compute correlation-based contribution
        # Features with changed correlations contribute more to multivariate drift
        current_corr = current_data[numerical_features].corr().values

        # Sum of absolute correlation changes
        contribution = np.sum(
            np.abs(current_corr[feat_idx] - self.reference_correlations[feat_idx])
        ) / len(numerical_features)

        return contribution

    def _calculate_drift_severity(
        self,
        drift_score: float,
        p_value: Optional[float],
        multivariate_contribution: float
    ) -> str:
        """
        Calculate drift severity level.

        Returns:
            Severity level: 'none', 'low', 'medium', 'high', 'critical'
        """
        # Base severity on PSI thresholds
        if drift_score < 0.1:
            base_severity = 'none'
        elif drift_score < 0.15:
            base_severity = 'low'
        elif drift_score < 0.25:
            base_severity = 'medium'
        else:
            base_severity = 'high'

        # Upgrade severity if p-value is very significant
        if p_value is not None and p_value < 0.001 and base_severity != 'none':
            severity_levels = ['none', 'low', 'medium', 'high', 'critical']
            current_idx = severity_levels.index(base_severity)
            base_severity = severity_levels[min(current_idx + 1, 4)]

        # Upgrade if multivariate contribution is high
        if multivariate_contribution > 0.3 and base_severity != 'none':
            severity_levels = ['none', 'low', 'medium', 'high', 'critical']
            current_idx = severity_levels.index(base_severity)
            base_severity = severity_levels[min(current_idx + 1, 4)]

        return base_severity

    def compute_mmd(
        self,
        X_ref: np.ndarray,
        X_curr: np.ndarray,
        gamma: float = 1.0
    ) -> float:
        """
        Compute Maximum Mean Discrepancy between distributions.

        Args:
            X_ref: Reference samples (n_samples, n_features)
            X_curr: Current samples (m_samples, n_features)
            gamma: RBF kernel bandwidth

        Returns:
            MMD^2 value
        """
        # Compute kernel matrices
        K_XX = rbf_kernel(X_ref, X_ref, gamma=gamma)
        K_YY = rbf_kernel(X_curr, X_curr, gamma=gamma)
        K_XY = rbf_kernel(X_ref, X_curr, gamma=gamma)

        # MMD^2 = E[k(x,x')] + E[k(y,y')] - 2*E[k(x,y)]
        mmd_squared = (
            K_XX.mean() + K_YY.mean() - 2 * K_XY.mean()
        )

        return max(0, mmd_squared)  # MMD^2 should be non-negative
\end{lstlisting}

\subsection{Concept Drift Detection with Adaptive Windowing}

Concept drift occurs when the relationship between features and target changes, even if feature distributions remain stable.

\begin{lstlisting}[language=Python, caption={Concept Drift Detection with ADWIN}]
from collections import deque
from typing import List, Optional, Tuple
import numpy as np
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ConceptDriftEvent:
    """
    Detected concept drift event.

    Attributes:
        timestamp: When drift was detected
        drift_magnitude: Magnitude of distribution change
        window_size_before: Size of window before drift
        window_size_after: Size of window after drift
        performance_drop: Drop in model performance
        changepoint_confidence: Confidence in changepoint (0-1)
    """
    timestamp: datetime
    drift_magnitude: float
    window_size_before: int
    window_size_after: int
    performance_drop: float
    changepoint_confidence: float

class ADWIN:
    """
    Adaptive Windowing (ADWIN) algorithm for concept drift detection.

    Detects changes in data streams using adaptive window resizing.
    When drift is detected, the window is shrunk to remove old data.

    Reference:
        Bifet & Gavaldà (2007). "Learning from Time-Changing Data with
        Adaptive Windowing."

    Example:
        >>> adwin = ADWIN(delta=0.002)
        >>> for value in data_stream:
        ...     drift_detected = adwin.add_element(value)
        ...     if drift_detected:
        ...         print("Concept drift detected!")
    """

    def __init__(self, delta: float = 0.002):
        """
        Initialize ADWIN.

        Args:
            delta: Confidence parameter (smaller = more sensitive)
        """
        self.delta = delta
        self.window: deque = deque()
        self.total = 0.0
        self.variance = 0.0
        self.width = 0

        # Track drift events
        self.drift_events: List[ConceptDriftEvent] = []

    def add_element(self, value: float) -> bool:
        """
        Add element to window and check for drift.

        Args:
            value: New value (e.g., error rate, accuracy)

        Returns:
            True if drift detected
        """
        # Add to window
        self.window.append(value)
        self.width += 1
        self.total += value

        # Update variance incrementally
        if self.width > 1:
            old_mean = (self.total - value) / (self.width - 1)
            new_mean = self.total / self.width
            self.variance += (value - old_mean) * (value - new_mean)

        # Check for drift
        drift_detected = self._detect_change()

        return drift_detected

    def _detect_change(self) -> bool:
        """
        Check if change occurred using ADWIN algorithm.

        Returns:
            True if change detected
        """
        if self.width < 2:
            return False

        # Try splitting window at different points
        for i in range(1, self.width):
            # Split into two sub-windows
            w0_size = i
            w1_size = self.width - i

            # Compute means
            w0_sum = sum(list(self.window)[:i])
            w1_sum = self.total - w0_sum

            w0_mean = w0_sum / w0_size
            w1_mean = w1_sum / w1_size

            # Compute difference
            diff = abs(w0_mean - w1_mean)

            # Compute threshold using Hoeffding bound
            m = 1.0 / w0_size + 1.0 / w1_size
            epsilon = np.sqrt(
                2.0 * m * np.log(2.0 / self.delta)
            )

            # Check if difference exceeds threshold
            if diff > epsilon:
                # Drift detected - remove old window
                drift_magnitude = diff

                # Shrink window (remove first i elements)
                for _ in range(i):
                    removed = self.window.popleft()
                    self.total -= removed

                self.width -= i

                # Record drift event
                self.drift_events.append(ConceptDriftEvent(
                    timestamp=datetime.now(),
                    drift_magnitude=drift_magnitude,
                    window_size_before=self.width + i,
                    window_size_after=self.width,
                    performance_drop=diff,
                    changepoint_confidence=min(diff / epsilon, 1.0)
                ))

                logger.warning(
                    f"Concept drift detected: magnitude={drift_magnitude:.4f}"
                )

                return True

        return False

    def reset(self):
        """Reset the detector."""
        self.window.clear()
        self.total = 0.0
        self.variance = 0.0
        self.width = 0

class ConceptDriftAnalyzer:
    """
    Comprehensive concept drift analysis with multiple detection methods.

    Combines:
    - ADWIN for adaptive windowing
    - CUSUM for cumulative sum control charts
    - Page-Hinkley test for sequential change detection

    Example:
        >>> analyzer = ConceptDriftAnalyzer()
        >>> for y_true, y_pred in predictions:
        ...     error = int(y_true != y_pred)
        ...     drift = analyzer.update(error)
        ...     if drift:
        ...         trigger_retraining()
    """

    def __init__(
        self,
        adwin_delta: float = 0.002,
        cusum_threshold: float = 50,
        ph_threshold: float = 10,
        ph_delta: float = 0.005
    ):
        """
        Initialize concept drift analyzer.

        Args:
            adwin_delta: ADWIN confidence parameter
            cusum_threshold: CUSUM detection threshold
            ph_threshold: Page-Hinkley detection threshold
            ph_delta: Page-Hinkley minimal change magnitude
        """
        # Initialize detectors
        self.adwin = ADWIN(delta=adwin_delta)

        # CUSUM parameters
        self.cusum_threshold = cusum_threshold
        self.cusum_pos = 0.0
        self.cusum_neg = 0.0
        self.cusum_mean = 0.0

        # Page-Hinkley parameters
        self.ph_threshold = ph_threshold
        self.ph_delta = ph_delta
        self.ph_sum = 0.0
        self.ph_min = 0.0

        # History
        self.values: List[float] = []
        self.drift_points: List[int] = []

    def update(self, value: float) -> Dict[str, bool]:
        """
        Update with new value and check all detectors.

        Args:
            value: New performance metric (e.g., 0=correct, 1=error)

        Returns:
            Dict with drift detection results from each method
        """
        self.values.append(value)
        n = len(self.values)

        results = {}

        # ADWIN detection
        results['adwin'] = self.adwin.add_element(value)

        # CUSUM detection
        results['cusum'] = self._cusum_update(value)

        # Page-Hinkley detection
        results['ph'] = self._page_hinkley_update(value)

        # Record drift point if any detector triggered
        if any(results.values()):
            self.drift_points.append(n - 1)
            logger.warning(
                f"Concept drift detected at sample {n}: {results}"
            )

        return results

    def _cusum_update(self, value: float) -> bool:
        """
        Update CUSUM detector.

        CUSUM tracks cumulative sum of deviations from mean.

        Returns:
            True if drift detected
        """
        # Update mean
        n = len(self.values)
        old_mean = self.cusum_mean
        self.cusum_mean += (value - self.cusum_mean) / n

        # Update CUSUM
        deviation = value - old_mean

        self.cusum_pos = max(0, self.cusum_pos + deviation)
        self.cusum_neg = max(0, self.cusum_neg - deviation)

        # Check thresholds
        if self.cusum_pos > self.cusum_threshold or \
           self.cusum_neg > self.cusum_threshold:
            # Reset CUSUM
            self.cusum_pos = 0.0
            self.cusum_neg = 0.0
            return True

        return False

    def _page_hinkley_update(self, value: float) -> bool:
        """
        Update Page-Hinkley test.

        Page-Hinkley test detects changes in mean of a sequence.

        Returns:
            True if drift detected
        """
        n = len(self.values)

        # Update cumulative sum
        mean = np.mean(self.values)
        self.ph_sum += value - mean - self.ph_delta

        # Update minimum
        self.ph_min = min(self.ph_min, self.ph_sum)

        # Check threshold
        if self.ph_sum - self.ph_min > self.ph_threshold:
            # Reset
            self.ph_sum = 0.0
            self.ph_min = 0.0
            return True

        return False

    def get_drift_points(self) -> List[int]:
        """Get indices where drift was detected."""
        return self.drift_points
\end{lstlisting}

\subsection{Adversarial Drift Detection}

Adversarial drift occurs when malicious actors intentionally manipulate inputs to evade detection or degrade model performance.

\begin{lstlisting}[language=Python, caption={Adversarial Drift Detector for Security Monitoring}]
from typing import List, Dict, Optional, Set
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
import logging

logger = logging.getLogger(__name__)

@dataclass
class AdversarialDriftResult:
    """
    Result of adversarial drift detection.

    Attributes:
        timestamp: Detection time
        anomaly_score: Overall anomaly score (0-1)
        suspicious_features: Features with adversarial patterns
        attack_type: Suspected attack type
        confidence: Detection confidence (0-1)
        recommended_action: Suggested response
    """
    timestamp: datetime
    anomaly_score: float
    suspicious_features: Set[str]
    attack_type: str
    confidence: float
    recommended_action: str

class AdversarialDriftDetector:
    """
    Detect adversarial drift and input manipulation attacks.

    Detects:
    - Out-of-distribution inputs (adversarial examples)
    - Feature manipulation patterns
    - Sudden distribution shifts in critical features
    - Coordinated attacks (multiple similar anomalies)

    Uses:
    - Isolation Forest for anomaly detection
    - Statistical process control for feature monitoring
    - Pattern matching for known attack signatures

    Example:
        >>> detector = AdversarialDriftDetector()
        >>> detector.fit(clean_training_data)
        >>> result = detector.detect_adversarial(
        ...     current_predictions, current_features
        ... )
        >>> if result.confidence > 0.8:
        ...     block_suspicious_requests()
    """

    def __init__(
        self,
        contamination: float = 0.01,
        suspicious_threshold: float = 0.7,
        min_attack_samples: int = 10,
        time_window: timedelta = timedelta(minutes=5)
    ):
        """
        Initialize adversarial drift detector.

        Args:
            contamination: Expected proportion of anomalies
            suspicious_threshold: Threshold for anomaly score
            min_attack_samples: Min samples to confirm coordinated attack
            time_window: Time window for attack detection
        """
        self.contamination = contamination
        self.suspicious_threshold = suspicious_threshold
        self.min_attack_samples = min_attack_samples
        self.time_window = time_window

        # Anomaly detectors
        self.isolation_forest: Optional[IsolationForest] = None
        self.elliptic_envelope: Optional[EllipticEnvelope] = None

        # Attack tracking
        self.recent_anomalies: deque = deque()
        self.attack_signatures: Dict[str, List[Dict]] = {}

        self.is_fitted = False

    def fit(self, clean_data: pd.DataFrame):
        """
        Fit detector on clean reference data.

        Args:
            clean_data: Clean training data without adversarial examples
        """
        logger.info("Fitting adversarial drift detector")

        # Fit Isolation Forest
        self.isolation_forest = IsolationForest(
            contamination=self.contamination,
            random_state=42,
            n_estimators=200
        )
        self.isolation_forest.fit(clean_data.fillna(0))

        # Fit Elliptic Envelope (assumes Gaussian)
        try:
            self.elliptic_envelope = EllipticEnvelope(
                contamination=self.contamination,
                random_state=42
            )
            self.elliptic_envelope.fit(clean_data.fillna(0))
        except Exception as e:
            logger.warning(f"Failed to fit Elliptic Envelope: {e}")
            self.elliptic_envelope = None

        # Store feature statistics for monitoring
        self.feature_stats = {}
        for col in clean_data.columns:
            values = clean_data[col].dropna()
            self.feature_stats[col] = {
                'mean': values.mean(),
                'std': values.std(),
                'min': values.min(),
                'max': values.max(),
                'q01': values.quantile(0.01),
                'q99': values.quantile(0.99)
            }

        self.is_fitted = True
        logger.info("Adversarial detector fitted successfully")

    def detect_adversarial(
        self,
        current_data: pd.DataFrame,
        return_details: bool = True
    ) -> AdversarialDriftResult:
        """
        Detect adversarial drift in current data.

        Args:
            current_data: Current data batch to check
            return_details: Whether to return detailed analysis

        Returns:
            Adversarial drift detection result
        """
        if not self.is_fitted:
            raise ValueError("Detector not fitted. Call fit() first.")

        # Detect anomalies with Isolation Forest
        if_scores = -self.isolation_forest.score_samples(
            current_data.fillna(0)
        )
        if_predictions = self.isolation_forest.predict(current_data.fillna(0))

        # Detect with Elliptic Envelope if available
        ee_predictions = None
        if self.elliptic_envelope:
            ee_predictions = self.elliptic_envelope.predict(
                current_data.fillna(0)
            )

        # Combine predictions (consensus)
        anomaly_mask = if_predictions == -1
        if ee_predictions is not None:
            anomaly_mask = anomaly_mask & (ee_predictions == -1)

        # Compute overall anomaly score
        anomaly_score = np.mean(if_scores[anomaly_mask]) if anomaly_mask.any() else 0.0

        # Identify suspicious features
        suspicious_features = self._identify_suspicious_features(
            current_data, anomaly_mask
        )

        # Detect attack type
        attack_type, confidence = self._classify_attack_type(
            current_data, anomaly_mask, suspicious_features
        )

        # Track recent anomalies
        self._update_anomaly_tracking(
            current_data, anomaly_mask, if_scores
        )

        # Determine recommended action
        recommended_action = self._recommend_action(
            anomaly_score, confidence, attack_type
        )

        result = AdversarialDriftResult(
            timestamp=datetime.now(),
            anomaly_score=anomaly_score,
            suspicious_features=suspicious_features,
            attack_type=attack_type,
            confidence=confidence,
            recommended_action=recommended_action
        )

        # Log if significant
        if confidence > self.suspicious_threshold:
            logger.warning(
                f"Potential adversarial attack detected: {attack_type} "
                f"(confidence={confidence:.2f})"
            )

        return result

    def _identify_suspicious_features(
        self,
        current_data: pd.DataFrame,
        anomaly_mask: np.ndarray
    ) -> Set[str]:
        """
        Identify features with suspicious patterns.

        Returns:
            Set of suspicious feature names
        """
        suspicious = set()

        if not anomaly_mask.any():
            return suspicious

        anomalous_data = current_data[anomaly_mask]

        for col in current_data.columns:
            stats = self.feature_stats.get(col)
            if not stats:
                continue

            values = anomalous_data[col].dropna()
            if len(values) == 0:
                continue

            # Check for out-of-range values
            out_of_range = (
                (values < stats['q01']).sum() +
                (values > stats['q99']).sum()
            ) / len(values)

            # Check for unusual clustering
            value_std = values.std()
            unusual_std = value_std < stats['std'] * 0.1  # Too clustered

            if out_of_range > 0.5 or unusual_std:
                suspicious.add(col)

        return suspicious

    def _classify_attack_type(
        self,
        current_data: pd.DataFrame,
        anomaly_mask: np.ndarray,
        suspicious_features: Set[str]
    ) -> Tuple[str, float]:
        """
        Classify type of adversarial attack.

        Returns:
            Tuple of (attack_type, confidence)
        """
        if not anomaly_mask.any():
            return "none", 0.0

        n_anomalies = anomaly_mask.sum()
        n_total = len(current_data)
        anomaly_rate = n_anomalies / n_total

        # Feature manipulation attack
        if len(suspicious_features) > 0:
            # Check if critical features are affected
            if anomaly_rate > 0.1:
                return "coordinated_feature_manipulation", 0.9
            else:
                return "targeted_feature_manipulation", 0.7

        # Adversarial example attack
        if anomaly_rate < 0.05 and n_anomalies >= self.min_attack_samples:
            return "adversarial_examples", 0.8

        # Distribution shift (may be adversarial)
        if anomaly_rate > 0.2:
            return "distribution_shift_attack", 0.6

        # Single anomalies (likely noise)
        if n_anomalies < self.min_attack_samples:
            return "isolated_anomalies", 0.3

        return "unknown_pattern", 0.5

    def _update_anomaly_tracking(
        self,
        current_data: pd.DataFrame,
        anomaly_mask: np.ndarray,
        scores: np.ndarray
    ):
        """Update tracking of recent anomalies."""
        now = datetime.now()

        # Add new anomalies
        for idx in np.where(anomaly_mask)[0]:
            self.recent_anomalies.append({
                'timestamp': now,
                'score': scores[idx],
                'features': current_data.iloc[idx].to_dict()
            })

        # Remove old anomalies outside time window
        cutoff = now - self.time_window
        while self.recent_anomalies and \
              self.recent_anomalies[0]['timestamp'] < cutoff:
            self.recent_anomalies.popleft()

    def _recommend_action(
        self,
        anomaly_score: float,
        confidence: float,
        attack_type: str
    ) -> str:
        """
        Recommend action based on detection results.

        Returns:
            Recommended action string
        """
        if confidence > 0.9:
            return "BLOCK_IMMEDIATELY"
        elif confidence > 0.7:
            return "ENHANCED_MONITORING"
        elif confidence > 0.5:
            return "LOG_AND_INVESTIGATE"
        else:
            return "MONITOR_ONLY"
\end{lstlisting}

\subsection{Causal Drift Analysis}

Identify root causes of drift and quantify impact on model performance.

\begin{lstlisting}[language=Python, caption={Causal Drift Analyzer with Root Cause Identification}]
from typing import List, Dict, Tuple, Set
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.ensemble import RandomForestRegressor
import networkx as nx
import logging

logger = logging.getLogger(__name__)

@dataclass
class CausalDriftResult:
    """
    Result of causal drift analysis.

    Attributes:
        root_causes: Primary features causing drift
        impact_scores: Impact of each feature on model performance
        causal_chain: Causal relationships between drifting features
        estimated_performance_impact: Estimated % drop in performance
        remediation_priority: Ordered list of features to address
    """
    root_causes: List[str]
    impact_scores: Dict[str, float]
    causal_chain: Dict[str, List[str]]
    estimated_performance_impact: float
    remediation_priority: List[Tuple[str, float]]

class CausalDriftAnalyzer:
    """
    Analyze causal relationships in drift and identify root causes.

    Uses:
    - Causal discovery algorithms
    - Feature importance for impact analysis
    - Granger causality for temporal relationships
    - Counterfactual analysis for impact estimation

    Example:
        >>> analyzer = CausalDriftAnalyzer()
        >>> analyzer.fit(reference_data, reference_performance)
        >>> result = analyzer.analyze_drift(
        ...     current_data,
        ...     current_performance,
        ...     drift_results
        ... )
        >>> print(f"Root causes: {result.root_causes}")
        >>> print(f"Fix priority: {result.remediation_priority}")
    """

    def __init__(
        self,
        min_impact_threshold: float = 0.05,
        causality_threshold: float = 0.1
    ):
        """
        Initialize causal drift analyzer.

        Args:
            min_impact_threshold: Minimum impact to consider
            causality_threshold: Threshold for causal relationships
        """
        self.min_impact_threshold = min_impact_threshold
        self.causality_threshold = causality_threshold

        # Reference data
        self.reference_data: Optional[pd.DataFrame] = None
        self.reference_performance: Optional[pd.Series] = None

        # Causal graph
        self.causal_graph: Optional[nx.DiGraph] = None

        self.is_fitted = False

    def fit(
        self,
        reference_data: pd.DataFrame,
        reference_performance: pd.Series
    ):
        """
        Fit analyzer on reference data.

        Args:
            reference_data: Reference features
            reference_performance: Reference model performance (e.g., accuracy)
        """
        logger.info("Fitting causal drift analyzer")

        self.reference_data = reference_data.copy()
        self.reference_performance = reference_performance.copy()

        # Build causal graph (simplified - use correlation as proxy)
        self.causal_graph = self._build_causal_graph(reference_data)

        self.is_fitted = True
        logger.info("Causal analyzer fitted")

    def analyze_drift(
        self,
        current_data: pd.DataFrame,
        current_performance: pd.Series,
        drift_results: List[MultiDimensionalDriftResult]
    ) -> CausalDriftResult:
        """
        Analyze causal structure of drift.

        Args:
            current_data: Current feature data
            current_performance: Current model performance
            drift_results: Drift detection results from AdvancedDriftDetector

        Returns:
            Causal drift analysis result
        """
        if not self.is_fitted:
            raise ValueError("Analyzer not fitted. Call fit() first.")

        # Identify features with drift
        drifted_features = [
            r.feature_name for r in drift_results
            if r.drift_detected
        ]

        if not drifted_features:
            return CausalDriftResult(
                root_causes=[],
                impact_scores={},
                causal_chain={},
                estimated_performance_impact=0.0,
                remediation_priority=[]
            )

        # Compute impact of each feature on performance
        impact_scores = self._compute_feature_impacts(
            current_data,
            current_performance,
            drifted_features
        )

        # Identify root causes (features that cause other drifts)
        root_causes = self._identify_root_causes(
            drifted_features,
            drift_results
        )

        # Build causal chain
        causal_chain = self._build_causal_chain(
            drifted_features,
            drift_results
        )

        # Estimate performance impact
        performance_impact = self._estimate_performance_impact(
            impact_scores,
            drift_results
        )

        # Prioritize remediation
        remediation_priority = sorted(
            impact_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )

        result = CausalDriftResult(
            root_causes=root_causes,
            impact_scores=impact_scores,
            causal_chain=causal_chain,
            estimated_performance_impact=performance_impact,
            remediation_priority=remediation_priority
        )

        logger.info(
            f"Causal analysis complete: {len(root_causes)} root causes, "
            f"{performance_impact:.1%} estimated impact"
        )

        return result

    def _build_causal_graph(
        self,
        data: pd.DataFrame
    ) -> nx.DiGraph:
        """
        Build causal graph from feature correlations.

        Note: This is simplified. In production, use proper causal
        discovery algorithms like PC, FCI, or LiNGAM.

        Returns:
            Directed graph of causal relationships
        """
        G = nx.DiGraph()

        # Add nodes
        for col in data.columns:
            G.add_node(col)

        # Add edges based on correlation (simplified)
        corr_matrix = data.corr()

        for i, col1 in enumerate(data.columns):
            for j, col2 in enumerate(data.columns):
                if i != j:
                    corr = abs(corr_matrix.iloc[i, j])
                    if corr > self.causality_threshold:
                        # Add directed edge (simplified causality)
                        G.add_edge(col1, col2, weight=corr)

        return G

    def _compute_feature_impacts(
        self,
        current_data: pd.DataFrame,
        current_performance: pd.Series,
        drifted_features: List[str]
    ) -> Dict[str, float]:
        """
        Compute impact of each drifted feature on performance.

        Uses permutation importance to estimate impact.

        Returns:
            Dict mapping features to impact scores
        """
        impact_scores = {}

        # Combine data and performance
        data_with_perf = current_data.copy()
        data_with_perf['_performance'] = current_performance

        # Train model to predict performance from features
        X = current_data[drifted_features].fillna(0)
        y = current_performance

        if len(X) < 10:
            # Not enough data
            return {feat: 0.0 for feat in drifted_features}

        try:
            # Train Random Forest to model performance
            rf = RandomForestRegressor(
                n_estimators=50,
                max_depth=5,
                random_state=42
            )
            rf.fit(X, y)

            # Get feature importances
            importances = rf.feature_importances_

            for feat, imp in zip(drifted_features, importances):
                impact_scores[feat] = float(imp)

        except Exception as e:
            logger.warning(f"Failed to compute impacts: {e}")
            # Fallback: use correlation with performance
            for feat in drifted_features:
                corr = abs(current_data[feat].corr(current_performance))
                impact_scores[feat] = corr if not np.isnan(corr) else 0.0

        return impact_scores

    def _identify_root_causes(
        self,
        drifted_features: List[str],
        drift_results: List[MultiDimensionalDriftResult]
    ) -> List[str]:
        """
        Identify root cause features (those that cause other drifts).

        Returns:
            List of root cause feature names
        """
        root_causes = []

        # Build correlation change graph
        for result in drift_results:
            if result.feature_name not in drifted_features:
                continue

            # Feature is root cause if it has many correlated features
            # that also drifted
            if len(result.correlated_features) >= 2:
                # Check if correlated features also drifted
                correlated_drifted = result.correlated_features.intersection(
                    set(drifted_features)
                )

                if len(correlated_drifted) >= 2:
                    root_causes.append(result.feature_name)

        # Also check causal graph for upstream features
        if self.causal_graph:
            for feat in drifted_features:
                # Features with high out-degree are potential root causes
                if self.causal_graph.out_degree(feat) >= 2:
                    if feat not in root_causes:
                        root_causes.append(feat)

        return root_causes

    def _build_causal_chain(
        self,
        drifted_features: List[str],
        drift_results: List[MultiDimensionalDriftResult]
    ) -> Dict[str, List[str]]:
        """
        Build causal chain showing drift propagation.

        Returns:
            Dict mapping features to downstream affected features
        """
        causal_chain = {}

        for result in drift_results:
            if result.feature_name not in drifted_features:
                continue

            # Find downstream features (those correlated and drifted)
            downstream = result.correlated_features.intersection(
                set(drifted_features)
            )

            if downstream:
                causal_chain[result.feature_name] = list(downstream)

        return causal_chain

    def _estimate_performance_impact(
        self,
        impact_scores: Dict[str, float],
        drift_results: List[MultiDimensionalDriftResult]
    ) -> float:
        """
        Estimate total performance impact from drift.

        Returns:
            Estimated percentage drop in performance
        """
        # Weighted sum of impacts and drift magnitudes
        total_impact = 0.0

        for result in drift_results:
            if result.feature_name in impact_scores:
                impact = impact_scores[result.feature_name]
                drift_magnitude = result.univariate_drift_score

                # Combined impact
                feature_impact = impact * drift_magnitude
                total_impact += feature_impact

        # Normalize and convert to percentage
        # (simplified model - in reality use actual performance data)
        estimated_drop = min(total_impact * 10, 100.0)  # Cap at 100%

        return estimated_drop
\end{lstlisting}

\subsection{Real-World Scenario: The Seasonal Drift Confusion}

\subsubsection{The Problem}

An e-commerce recommendation system deployed drift monitoring with PSI thresholds. Every quarter, the drift alarms triggered massive alerts:

\begin{itemize}
    \item \textbf{Q4 (Holiday Season)}: 85\% of features flagged as drifting
    \item \textbf{Alert Storm}: 1000+ drift alerts in first week of December
    \item \textbf{False Alarms}: All alerts were due to expected seasonal patterns
    \item \textbf{Team Response}: Ignored legitimate drift for 2 months
    \item \textbf{Real Drift Missed}: Actual drift from mobile app redesign went undetected
\end{itemize}

The root issue: Static reference distributions from Q2 compared against Q4 holiday shopping patterns.

\subsubsection{The Solution}

Implemented seasonal-aware drift detection with adaptive baselines:

\begin{lstlisting}[language=Python, caption={Seasonal-Aware Drift Detection}]
from typing import Dict, List, Optional
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class SeasonalDriftDetector:
    """
    Drift detector with seasonal pattern awareness.

    Maintains multiple reference distributions:
    - Overall baseline
    - Seasonal baselines (per season/month/quarter)
    - Day-of-week patterns
    - Holiday patterns

    Example:
        >>> detector = SeasonalDriftDetector()
        >>> detector.fit_seasonal(historical_data, timestamps)
        >>> result = detector.detect_drift(
        ...     current_data,
        ...     current_timestamp
        ... )  # Uses appropriate seasonal baseline
    """

    def __init__(
        self,
        seasonality_type: str = 'monthly',
        drift_threshold: float = 0.2
    ):
        """
        Initialize seasonal drift detector.

        Args:
            seasonality_type: 'monthly', 'quarterly', 'weekly'
            drift_threshold: PSI threshold for drift
        """
        self.seasonality_type = seasonality_type
        self.drift_threshold = drift_threshold

        # Seasonal baselines
        self.seasonal_baselines: Dict[str, Dict] = {}

        # Overall baseline (fallback)
        self.overall_baseline: Optional[Dict] = None

        self.is_fitted = False

    def fit_seasonal(
        self,
        historical_data: pd.DataFrame,
        timestamps: pd.Series
    ):
        """
        Fit detector with seasonal patterns.

        Args:
            historical_data: Historical feature data
            timestamps: Timestamps for each sample
        """
        logger.info("Fitting seasonal drift detector")

        # Fit overall baseline
        self.overall_baseline = self._compute_baseline(historical_data)

        # Fit seasonal baselines
        for period_key, period_data in self._split_by_season(
            historical_data, timestamps
        ).items():
            if len(period_data) >= 100:  # Minimum samples
                self.seasonal_baselines[period_key] = self._compute_baseline(
                    period_data
                )
                logger.info(
                    f"Fitted baseline for period {period_key}: "
                    f"{len(period_data)} samples"
                )

        self.is_fitted = True
        logger.info(
            f"Seasonal detector fitted with "
            f"{len(self.seasonal_baselines)} seasonal baselines"
        )

    def detect_drift(
        self,
        current_data: pd.DataFrame,
        current_timestamp: datetime
    ) -> Dict[str, Any]:
        """
        Detect drift using appropriate seasonal baseline.

        Args:
            current_data: Current data batch
            current_timestamp: Current time

        Returns:
            Drift detection results
        """
        if not self.is_fitted:
            raise ValueError("Detector not fitted")

        # Determine appropriate baseline
        period_key = self._get_period_key(current_timestamp)

        if period_key in self.seasonal_baselines:
            baseline = self.seasonal_baselines[period_key]
            baseline_type = f"seasonal_{period_key}"
        else:
            baseline = self.overall_baseline
            baseline_type = "overall"
            logger.warning(
                f"No seasonal baseline for {period_key}, "
                f"using overall baseline"
            )

        # Compute drift
        drift_scores = {}
        drifted_features = []

        for column in current_data.columns:
            if column not in baseline:
                continue

            # Compute PSI
            psi = self._compute_psi(
                current_data[column],
                baseline[column]
            )

            drift_scores[column] = psi

            if psi > self.drift_threshold:
                drifted_features.append(column)

        # Overall drift rate
        drift_rate = len(drifted_features) / len(drift_scores)

        result = {
            'timestamp': current_timestamp,
            'baseline_type': baseline_type,
            'period_key': period_key,
            'drift_scores': drift_scores,
            'drifted_features': drifted_features,
            'drift_rate': drift_rate,
            'seasonal_adjusted': period_key in self.seasonal_baselines
        }

        logger.info(
            f"Drift detection: {drift_rate:.1%} features drifted "
            f"(baseline: {baseline_type})"
        )

        return result

    def _split_by_season(
        self,
        data: pd.DataFrame,
        timestamps: pd.Series
    ) -> Dict[str, pd.DataFrame]:
        """
        Split data by seasonal periods.

        Returns:
            Dict mapping period keys to data
        """
        periods = {}

        for timestamp, row in zip(timestamps, data.iterrows()):
            period_key = self._get_period_key(timestamp)

            if period_key not in periods:
                periods[period_key] = []

            periods[period_key].append(row[1])

        # Convert to DataFrames
        return {
            key: pd.DataFrame(rows)
            for key, rows in periods.items()
        }

    def _get_period_key(self, timestamp: datetime) -> str:
        """Get period key for timestamp."""
        if self.seasonality_type == 'monthly':
            return f"month_{timestamp.month}"
        elif self.seasonality_type == 'quarterly':
            quarter = (timestamp.month - 1) // 3 + 1
            return f"quarter_{quarter}"
        elif self.seasonality_type == 'weekly':
            return f"weekday_{timestamp.weekday()}"
        else:
            return "overall"

    def _compute_baseline(
        self,
        data: pd.DataFrame
    ) -> Dict[str, Dict]:
        """Compute baseline distribution for data."""
        baseline = {}

        for column in data.columns:
            values = data[column].dropna()

            # Compute histogram
            hist, bins = np.histogram(values, bins=10)

            baseline[column] = {
                'histogram': hist,
                'bins': bins,
                'mean': values.mean(),
                'std': values.std()
            }

        return baseline

    def _compute_psi(
        self,
        current_values: pd.Series,
        baseline: Dict
    ) -> float:
        """Compute PSI between current and baseline."""
        # Bin current values using baseline bins
        current_hist, _ = np.histogram(
            current_values.dropna(),
            bins=baseline['bins']
        )

        ref_hist = baseline['histogram']

        # Compute PSI
        ref_pct = ref_hist / ref_hist.sum()
        curr_pct = current_hist / current_hist.sum()

        ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)
        curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)

        psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))

        return psi
\end{lstlisting}

\subsubsection{Outcome}

After implementing seasonal-aware drift detection:

\begin{itemize}
    \item \textbf{Alert reduction}: 95\% reduction in false seasonal alerts
    \item \textbf{Q4 holiday season}: Only 3 legitimate alerts (down from 1000+)
    \item \textbf{Real drift detected}: Mobile app redesign drift caught in 2 days
    \item \textbf{Adaptive baselines}: System learned 12 monthly patterns + holiday patterns
    \item \textbf{Team trust restored}: Engineers acted on alerts again
    \item \textbf{Business impact}: Prevented \$500K in lost revenue from undetected drift
\end{itemize}

\section{Business Impact Monitoring}

Technical metrics (accuracy, latency, drift) are necessary but insufficient. Production ML systems must track business outcomes and establish causal links between technical changes and business impact.

\subsection{Connecting Technical Metrics to Business Outcomes}

Business impact monitoring requires:
\begin{itemize}
    \item \textbf{Correlation analysis}: Identify relationships between technical and business metrics
    \item \textbf{Causal inference}: Distinguish correlation from causation
    \item \textbf{Attribution modeling}: Link business changes to specific model versions
    \item \textbf{Cost-benefit analysis}: Quantify ROI of ML systems
    \item \textbf{Fairness monitoring}: Ensure equitable outcomes across user segments
\end{itemize}

\subsection{BusinessMetricsTracker: Unified Tracking}

\begin{lstlisting}[language=Python, caption={Business Metrics Tracker with Correlation Analysis}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import pearsonr, spearmanr
from sklearn.linear_model import LinearRegression
import logging

logger = logging.getLogger(__name__)

@dataclass
class BusinessMetric:
    """
    Business metric definition.

    Attributes:
        name: Metric name
        value: Current value
        timestamp: When measured
        metadata: Additional context (user segment, region, etc.)
    """
    name: str
    value: float
    timestamp: datetime
    metadata: Dict[str, any] = field(default_factory=dict)

@dataclass
class MetricCorrelation:
    """
    Correlation between technical and business metrics.

    Attributes:
        technical_metric: Name of technical metric
        business_metric: Name of business metric
        correlation_coefficient: Correlation strength (-1 to 1)
        p_value: Statistical significance
        correlation_type: 'pearson' or 'spearman'
        lag_days: Time lag for maximum correlation
        causal_confidence: Confidence that relationship is causal (0-1)
    """
    technical_metric: str
    business_metric: str
    correlation_coefficient: float
    p_value: float
    correlation_type: str
    lag_days: int
    causal_confidence: float

class BusinessMetricsTracker:
    """
    Track business metrics and correlate with technical metrics.

    Monitors:
    - Revenue and conversions
    - User engagement (CTR, time on site, retention)
    - Customer satisfaction (NPS, ratings)
    - Cost metrics (infrastructure, support tickets)
    - Operational efficiency (processing time, errors)

    Analyzes:
    - Correlation with technical metrics (accuracy, latency, drift)
    - Time-lagged effects
    - Causal relationships using Granger causality
    - Attribution to model changes

    Example:
        >>> tracker = BusinessMetricsTracker()
        >>> tracker.record_business_metric(
        ...     "conversion_rate", 0.045, metadata={"segment": "premium"}
        ... )
        >>> tracker.record_technical_metric("model_accuracy", 0.92)
        >>> correlations = tracker.analyze_correlations()
        >>> for corr in correlations:
        ...     if corr.causal_confidence > 0.7:
        ...         print(f"{corr.technical_metric} -> {corr.business_metric}")
    """

    def __init__(
        self,
        correlation_threshold: float = 0.3,
        p_value_threshold: float = 0.05,
        max_lag_days: int = 7,
        min_samples: int = 30
    ):
        """
        Initialize business metrics tracker.

        Args:
            correlation_threshold: Minimum correlation to report
            p_value_threshold: Significance threshold
            max_lag_days: Maximum lag to test for delayed effects
            min_samples: Minimum samples for correlation analysis
        """
        self.correlation_threshold = correlation_threshold
        self.p_value_threshold = p_value_threshold
        self.max_lag_days = max_lag_days
        self.min_samples = min_samples

        # Metric storage
        self.business_metrics: Dict[str, List[BusinessMetric]] = {}
        self.technical_metrics: Dict[str, List[Tuple[datetime, float]]] = {}

        # Computed correlations
        self.correlations: List[MetricCorrelation] = []

    def record_business_metric(
        self,
        name: str,
        value: float,
        timestamp: Optional[datetime] = None,
        metadata: Optional[Dict] = None
    ):
        """
        Record a business metric.

        Args:
            name: Metric name (e.g., 'revenue', 'conversion_rate')
            value: Metric value
            timestamp: When metric was measured
            metadata: Additional context
        """
        if timestamp is None:
            timestamp = datetime.now()

        metric = BusinessMetric(
            name=name,
            value=value,
            timestamp=timestamp,
            metadata=metadata or {}
        )

        if name not in self.business_metrics:
            self.business_metrics[name] = []

        self.business_metrics[name].append(metric)

    def record_technical_metric(
        self,
        name: str,
        value: float,
        timestamp: Optional[datetime] = None
    ):
        """
        Record a technical metric.

        Args:
            name: Metric name (e.g., 'accuracy', 'latency_p95')
            value: Metric value
            timestamp: When metric was measured
        """
        if timestamp is None:
            timestamp = datetime.now()

        if name not in self.technical_metrics:
            self.technical_metrics[name] = []

        self.technical_metrics[name].append((timestamp, value))

    def analyze_correlations(
        self,
        lookback_days: int = 30
    ) -> List[MetricCorrelation]:
        """
        Analyze correlations between technical and business metrics.

        Args:
            lookback_days: Days of history to analyze

        Returns:
            List of significant correlations
        """
        correlations = []
        cutoff_time = datetime.now() - timedelta(days=lookback_days)

        # Analyze each business metric against each technical metric
        for biz_name, biz_metrics in self.business_metrics.items():
            for tech_name, tech_metrics in self.technical_metrics.items():
                # Filter by time window
                biz_recent = [
                    m for m in biz_metrics
                    if m.timestamp >= cutoff_time
                ]
                tech_recent = [
                    (ts, val) for ts, val in tech_metrics
                    if ts >= cutoff_time
                ]

                if len(biz_recent) < self.min_samples or \
                   len(tech_recent) < self.min_samples:
                    continue

                # Test different time lags
                best_corr = None
                best_lag = 0

                for lag in range(self.max_lag_days + 1):
                    corr = self._compute_correlation(
                        tech_recent,
                        biz_recent,
                        lag_days=lag
                    )

                    if corr and (
                        best_corr is None or
                        abs(corr.correlation_coefficient) >
                        abs(best_corr.correlation_coefficient)
                    ):
                        best_corr = corr
                        best_lag = lag

                # Only report significant correlations
                if best_corr and \
                   abs(best_corr.correlation_coefficient) >= self.correlation_threshold and \
                   best_corr.p_value <= self.p_value_threshold:

                    # Compute causal confidence
                    causal_conf = self._estimate_causality(
                        tech_recent,
                        biz_recent,
                        best_lag
                    )

                    best_corr.lag_days = best_lag
                    best_corr.causal_confidence = causal_conf

                    correlations.append(best_corr)

        self.correlations = correlations

        logger.info(
            f"Found {len(correlations)} significant correlations "
            f"between technical and business metrics"
        )

        return correlations

    def _compute_correlation(
        self,
        tech_metrics: List[Tuple[datetime, float]],
        biz_metrics: List[BusinessMetric],
        lag_days: int = 0
    ) -> Optional[MetricCorrelation]:
        """
        Compute correlation between technical and business metrics.

        Args:
            tech_metrics: List of (timestamp, value) for technical metric
            biz_metrics: List of business metrics
            lag_days: Time lag (business metric lags technical metric)

        Returns:
            MetricCorrelation or None if insufficient data
        """
        # Align time series
        tech_df = pd.DataFrame(
            tech_metrics,
            columns=['timestamp', 'tech_value']
        )
        tech_df['date'] = tech_df['timestamp'].dt.date

        biz_df = pd.DataFrame([
            {
                'timestamp': m.timestamp,
                'biz_value': m.value,
                'date': m.timestamp.date()
            }
            for m in biz_metrics
        ])

        # Apply lag
        if lag_days > 0:
            biz_df['date'] = biz_df['date'] - timedelta(days=lag_days)

        # Aggregate by date
        tech_daily = tech_df.groupby('date')['tech_value'].mean()
        biz_daily = biz_df.groupby('date')['biz_value'].mean()

        # Merge on date
        merged = pd.merge(
            tech_daily,
            biz_daily,
            left_index=True,
            right_index=True,
            how='inner'
        )

        if len(merged) < 10:  # Need minimum points
            return None

        # Compute Pearson correlation
        pearson_r, pearson_p = pearsonr(
            merged['tech_value'],
            merged['biz_value']
        )

        # Compute Spearman (rank-based, more robust)
        spearman_r, spearman_p = spearmanr(
            merged['tech_value'],
            merged['biz_value']
        )

        # Use Spearman if more significant
        if spearman_p < pearson_p:
            corr_coef = spearman_r
            p_value = spearman_p
            corr_type = 'spearman'
        else:
            corr_coef = pearson_r
            p_value = pearson_p
            corr_type = 'pearson'

        tech_name = tech_metrics[0][0] if tech_metrics else "unknown"
        biz_name = biz_metrics[0].name if biz_metrics else "unknown"

        return MetricCorrelation(
            technical_metric=tech_name,
            business_metric=biz_name,
            correlation_coefficient=corr_coef,
            p_value=p_value,
            correlation_type=corr_type,
            lag_days=0,  # Will be set by caller
            causal_confidence=0.0  # Will be computed separately
        )

    def _estimate_causality(
        self,
        tech_metrics: List[Tuple[datetime, float]],
        biz_metrics: List[BusinessMetric],
        lag_days: int
    ) -> float:
        """
        Estimate likelihood that technical metric causes business metric.

        Uses Granger causality test and temporal precedence.

        Args:
            tech_metrics: Technical metric time series
            biz_metrics: Business metric time series
            lag_days: Observed lag

        Returns:
            Causal confidence score (0-1)
        """
        # Temporal precedence: if lag > 0, tech metric precedes business
        temporal_score = min(lag_days / 3.0, 1.0)  # Max score at 3 days

        # Granger causality test (simplified)
        # Full implementation would use statsmodels.tsa.stattools.grangercausalitytests

        # For now, use heuristic:
        # - Strong correlation + temporal precedence = likely causal
        # - No lag or negative lag = less likely causal

        if lag_days > 0:
            causal_conf = 0.5 + temporal_score * 0.5
        elif lag_days == 0:
            causal_conf = 0.3  # Possible, but uncertain
        else:
            causal_conf = 0.1  # Unlikely to be causal

        return causal_conf

    def get_top_correlations(
        self,
        top_k: int = 10,
        min_causal_confidence: float = 0.5
    ) -> List[MetricCorrelation]:
        """
        Get top correlations sorted by strength and causal confidence.

        Args:
            top_k: Number of top correlations to return
            min_causal_confidence: Minimum causal confidence threshold

        Returns:
            Top k correlations
        """
        # Filter by causal confidence
        filtered = [
            c for c in self.correlations
            if c.causal_confidence >= min_causal_confidence
        ]

        # Sort by combined score: |correlation| * causal_confidence
        sorted_corrs = sorted(
            filtered,
            key=lambda c: abs(c.correlation_coefficient) * c.causal_confidence,
            reverse=True
        )

        return sorted_corrs[:top_k]

    def generate_insights(self) -> List[str]:
        """
        Generate human-readable insights from correlations.

        Returns:
            List of insight strings
        """
        insights = []

        top_corrs = self.get_top_correlations(top_k=5)

        for corr in top_corrs:
            direction = "increases" if corr.correlation_coefficient > 0 else "decreases"
            strength = "strongly" if abs(corr.correlation_coefficient) > 0.7 else "moderately"

            lag_str = ""
            if corr.lag_days > 0:
                lag_str = f" (with {corr.lag_days} day lag)"

            causal_str = ""
            if corr.causal_confidence > 0.7:
                causal_str = " This relationship appears causal."

            insight = (
                f"When {corr.technical_metric} improves, "
                f"{corr.business_metric} {strength} {direction}{lag_str}. "
                f"(r={corr.correlation_coefficient:.3f}, "
                f"p={corr.p_value:.4f}){causal_str}"
            )

            insights.append(insight)

        return insights
\end{lstlisting}

\subsection{CostMonitor: Infrastructure Cost Optimization}

\begin{lstlisting}[language=Python, caption={Cost Monitoring with Optimization Recommendations}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import numpy as np
import logging

logger = logging.getLogger(__name__)

@dataclass
class CostMetric:
    """
    Cost metric for ML infrastructure.

    Attributes:
        timestamp: When cost was measured
        compute_cost: Compute instance costs
        storage_cost: Data storage costs
        network_cost: Data transfer costs
        inference_cost: Per-prediction cost
        total_cost: Total cost
        predictions_served: Number of predictions
        cost_per_prediction: Unit economics
    """
    timestamp: datetime
    compute_cost: float
    storage_cost: float
    network_cost: float
    inference_cost: float
    total_cost: float
    predictions_served: int
    cost_per_prediction: float

@dataclass
class OptimizationRecommendation:
    """
    Cost optimization recommendation.

    Attributes:
        category: Type of optimization
        current_cost: Current monthly cost
        potential_savings: Estimated savings
        implementation_effort: Low/Medium/High
        recommendation: Description
        impact: Expected business impact
    """
    category: str
    current_cost: float
    potential_savings: float
    implementation_effort: str
    recommendation: str
    impact: str

class CostMonitor:
    """
    Monitor ML infrastructure costs and recommend optimizations.

    Tracks:
    - Compute costs (CPU, GPU, memory)
    - Storage costs (model artifacts, training data, logs)
    - Network costs (data transfer, API calls)
    - Per-prediction costs
    - Cost efficiency trends

    Recommends:
    - Instance type optimizations
    - Autoscaling adjustments
    - Batch size tuning
    - Model compression opportunities
    - Cache optimization

    Example:
        >>> monitor = CostMonitor(budget_monthly=10000)
        >>> monitor.record_costs(
        ...     compute=1500,
        ...     storage=300,
        ...     network=200,
        ...     predictions=1_000_000
        ... )
        >>> recommendations = monitor.get_optimization_recommendations()
        >>> for rec in recommendations:
        ...     print(f"{rec.category}: Save ${rec.potential_savings}/mo")
    """

    def __init__(
        self,
        budget_monthly: float,
        cost_per_prediction_target: float = 0.001,
        alert_threshold: float = 0.8
    ):
        """
        Initialize cost monitor.

        Args:
            budget_monthly: Monthly budget in dollars
            cost_per_prediction_target: Target cost per prediction
            alert_threshold: Alert when costs exceed this % of budget
        """
        self.budget_monthly = budget_monthly
        self.cost_per_prediction_target = cost_per_prediction_target
        self.alert_threshold = alert_threshold

        # Cost history
        self.cost_history: List[CostMetric] = []

        # Optimization recommendations
        self.recommendations: List[OptimizationRecommendation] = []

    def record_costs(
        self,
        compute: float,
        storage: float,
        network: float,
        predictions: int,
        timestamp: Optional[datetime] = None
    ):
        """
        Record infrastructure costs.

        Args:
            compute: Compute costs in dollars
            storage: Storage costs in dollars
            network: Network costs in dollars
            predictions: Number of predictions served
            timestamp: When costs were measured
        """
        if timestamp is None:
            timestamp = datetime.now()

        inference_cost = compute * 0.7  # Assume 70% of compute is inference

        total = compute + storage + network

        cost_per_pred = total / predictions if predictions > 0 else 0

        metric = CostMetric(
            timestamp=timestamp,
            compute_cost=compute,
            storage_cost=storage,
            network_cost=network,
            inference_cost=inference_cost,
            total_cost=total,
            predictions_served=predictions,
            cost_per_prediction=cost_per_pred
        )

        self.cost_history.append(metric)

        # Check budget
        monthly_projection = self._project_monthly_cost()
        if monthly_projection > self.budget_monthly * self.alert_threshold:
            logger.warning(
                f"Cost projection ${monthly_projection:.2f} exceeds "
                f"{self.alert_threshold * 100}% of budget "
                f"${self.budget_monthly:.2f}"
            )

    def _project_monthly_cost(self) -> float:
        """
        Project monthly cost based on recent trends.

        Returns:
            Projected monthly cost
        """
        if not self.cost_history:
            return 0.0

        # Use last 7 days
        recent = self.cost_history[-7:]
        daily_avg = np.mean([m.total_cost for m in recent])

        return daily_avg * 30

    def get_optimization_recommendations(self) -> List[OptimizationRecommendation]:
        """
        Generate cost optimization recommendations.

        Returns:
            List of recommendations sorted by potential savings
        """
        if len(self.cost_history) < 7:
            logger.warning("Insufficient data for recommendations")
            return []

        recommendations = []

        # Analyze recent costs
        recent = self.cost_history[-30:]  # Last 30 days

        avg_compute = np.mean([m.compute_cost for m in recent])
        avg_storage = np.mean([m.storage_cost for m in recent])
        avg_network = np.mean([m.network_cost for m in recent])
        avg_cost_per_pred = np.mean([m.cost_per_prediction for m in recent])

        # Recommendation 1: Compute optimization
        if avg_compute > avg_storage + avg_network:
            # Compute is dominant cost
            potential_savings = avg_compute * 0.3 * 30  # 30% savings

            recommendations.append(OptimizationRecommendation(
                category="Compute Optimization",
                current_cost=avg_compute * 30,
                potential_savings=potential_savings,
                implementation_effort="Medium",
                recommendation=(
                    "Compute is your largest cost driver. Consider: "
                    "(1) Rightsizing instances - current utilization suggests "
                    "over-provisioning, (2) Spot/preemptible instances for "
                    "batch workloads, (3) Auto-scaling policies to reduce "
                    "idle capacity during low-traffic periods."
                ),
                impact="Reduce monthly costs by ~30% without performance impact"
            ))

        # Recommendation 2: Storage optimization
        if avg_storage > self.budget_monthly * 0.15:  # >15% of budget
            potential_savings = avg_storage * 0.4 * 30

            recommendations.append(OptimizationRecommendation(
                category="Storage Optimization",
                current_cost=avg_storage * 30,
                potential_savings=potential_savings,
                implementation_effort="Low",
                recommendation=(
                    "Storage costs are high. Implement: (1) Lifecycle policies "
                    "to archive old training data to cold storage, (2) Delete "
                    "intermediate model checkpoints after final model is selected, "
                    "(3) Enable compression for logs and feature stores."
                ),
                impact="Reduce storage costs by 40% within 1 week"
            ))

        # Recommendation 3: Cost per prediction optimization
        if avg_cost_per_pred > self.cost_per_prediction_target:
            overhead_ratio = avg_cost_per_pred / self.cost_per_prediction_target
            potential_savings = (
                (avg_compute + avg_network) * 0.5 * 30
            )

            recommendations.append(OptimizationRecommendation(
                category="Inference Efficiency",
                current_cost=(avg_compute + avg_network) * 30,
                potential_savings=potential_savings,
                implementation_effort="High",
                recommendation=(
                    f"Cost per prediction (${avg_cost_per_pred:.4f}) is "
                    f"{overhead_ratio:.1f}x target (${self.cost_per_prediction_target:.4f}). "
                    "Optimize inference: (1) Implement request batching to amortize "
                    "overhead, (2) Cache frequent predictions, (3) Consider model "
                    "quantization or distillation to reduce compute requirements, "
                    "(4) Use faster model serving frameworks (TorchServe, TensorRT)."
                ),
                impact="Achieve target cost per prediction, improve profit margins"
            ))

        # Recommendation 4: Network optimization
        if avg_network > avg_compute * 0.3:
            potential_savings = avg_network * 0.5 * 30

            recommendations.append(OptimizationRecommendation(
                category="Network Optimization",
                current_cost=avg_network * 30,
                potential_savings=potential_savings,
                implementation_effort="Low",
                recommendation=(
                    "Network costs are unusually high. Implement: (1) Enable "
                    "response compression (gzip), (2) Deploy models closer to "
                    "users (CDN/edge deployment), (3) Cache feature data to "
                    "reduce feature store queries, (4) Use regional data transfer "
                    "where possible."
                ),
                impact="Reduce network costs by 50%"
            ))

        # Sort by potential savings
        recommendations.sort(key=lambda r: r.potential_savings, reverse=True)

        self.recommendations = recommendations

        return recommendations

    def get_cost_trends(
        self,
        days: int = 30
    ) -> Dict[str, any]:
        """
        Analyze cost trends over time.

        Args:
            days: Number of days to analyze

        Returns:
            Dictionary with trend analysis
        """
        cutoff = datetime.now() - timedelta(days=days)
        recent = [m for m in self.cost_history if m.timestamp >= cutoff]

        if len(recent) < 2:
            return {"error": "Insufficient data"}

        # Compute trends
        costs = np.array([m.total_cost for m in recent])
        predictions = np.array([m.predictions_served for m in recent])

        # Linear regression for trend
        X = np.arange(len(costs)).reshape(-1, 1)
        y = costs

        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(X, y)

        trend_slope = model.coef_[0]
        trend_direction = "increasing" if trend_slope > 0 else "decreasing"

        # Cost efficiency trend
        cost_per_pred = costs / (predictions + 1)
        efficiency_trend = np.polyfit(range(len(cost_per_pred)), cost_per_pred, 1)[0]

        return {
            "total_cost": costs.sum(),
            "avg_daily_cost": costs.mean(),
            "cost_trend": trend_direction,
            "cost_trend_slope": trend_slope,
            "total_predictions": predictions.sum(),
            "avg_cost_per_prediction": cost_per_pred.mean(),
            "efficiency_trend": "improving" if efficiency_trend < 0 else "degrading",
            "monthly_projection": self._project_monthly_cost(),
            "budget_utilization": self._project_monthly_cost() / self.budget_monthly
        }
\end{lstlisting}

\subsection{RevenueImpactAnalyzer: Attribution Modeling}

\begin{lstlisting}[language=Python, caption={Revenue Impact Analysis with Causal Attribution}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from scipy import stats
import logging

logger = logging.getLogger(__name__)

@dataclass
class ModelVersion:
    """
    Model version with deployment metadata.

    Attributes:
        version_id: Model version identifier
        deployed_at: Deployment timestamp
        replaced_at: When this version was replaced
        performance_metrics: Technical metrics (accuracy, etc.)
        description: Version description
    """
    version_id: str
    deployed_at: datetime
    replaced_at: Optional[datetime]
    performance_metrics: Dict[str, float]
    description: str

@dataclass
class RevenueAttribution:
    """
    Revenue attribution to model change.

    Attributes:
        model_version: Model version ID
        baseline_revenue: Revenue before change
        current_revenue: Revenue after change
        revenue_delta: Absolute change
        revenue_delta_pct: Percentage change
        confidence_interval: (lower, upper) bounds
        statistical_significance: P-value
        attribution_confidence: Confidence in causation (0-1)
        confounding_factors: Potential confounders identified
    """
    model_version: str
    baseline_revenue: float
    current_revenue: float
    revenue_delta: float
    revenue_delta_pct: float
    confidence_interval: Tuple[float, float]
    statistical_significance: float
    attribution_confidence: float
    confounding_factors: List[str]

class RevenueImpactAnalyzer:
    """
    Analyze revenue impact of model changes with causal attribution.

    Uses intervention analysis to isolate model effects from:
    - Seasonal trends
    - Marketing campaigns
    - External events
    - Product changes

    Methods:
    - Difference-in-differences estimation
    - Synthetic control
    - Interrupted time series analysis
    - A/B test analysis (when available)

    Example:
        >>> analyzer = RevenueImpactAnalyzer()
        >>> analyzer.register_model_deployment(
        ...     version_id="v2.3",
        ...     deployed_at=datetime(2024, 1, 15),
        ...     performance_metrics={"accuracy": 0.94}
        ... )
        >>> analyzer.record_revenue(100000, datetime(2024, 1, 16))
        >>> attribution = analyzer.analyze_revenue_impact("v2.3")
        >>> print(f"Revenue impact: ${attribution.revenue_delta:,.2f}")
    """

    def __init__(
        self,
        baseline_period_days: int = 14,
        evaluation_period_days: int = 14,
        min_confidence_for_attribution: float = 0.7
    ):
        """
        Initialize revenue impact analyzer.

        Args:
            baseline_period_days: Days before deployment for baseline
            evaluation_period_days: Days after deployment to evaluate
            min_confidence_for_attribution: Minimum confidence for causal claim
        """
        self.baseline_period_days = baseline_period_days
        self.evaluation_period_days = evaluation_period_days
        self.min_confidence_for_attribution = min_confidence_for_attribution

        # Data storage
        self.model_versions: Dict[str, ModelVersion] = {}
        self.revenue_history: List[Tuple[datetime, float]] = []
        self.external_events: List[Tuple[datetime, str]] = []

    def register_model_deployment(
        self,
        version_id: str,
        deployed_at: datetime,
        performance_metrics: Dict[str, float],
        description: str = ""
    ):
        """
        Register a model deployment.

        Args:
            version_id: Unique version identifier
            deployed_at: Deployment timestamp
            performance_metrics: Technical metrics
            description: Version description
        """
        # Mark previous version as replaced
        for prev_version in self.model_versions.values():
            if prev_version.replaced_at is None:
                prev_version.replaced_at = deployed_at

        self.model_versions[version_id] = ModelVersion(
            version_id=version_id,
            deployed_at=deployed_at,
            replaced_at=None,
            performance_metrics=performance_metrics,
            description=description
        )

        logger.info(f"Registered model version {version_id} deployed at {deployed_at}")

    def record_revenue(
        self,
        revenue: float,
        timestamp: Optional[datetime] = None
    ):
        """
        Record revenue observation.

        Args:
            revenue: Revenue amount
            timestamp: When revenue was measured
        """
        if timestamp is None:
            timestamp = datetime.now()

        self.revenue_history.append((timestamp, revenue))

    def register_external_event(
        self,
        event_name: str,
        timestamp: datetime
    ):
        """
        Register external event that may affect revenue.

        Args:
            event_name: Description of event (e.g., "Black Friday", "Marketing campaign")
            timestamp: When event occurred
        """
        self.external_events.append((timestamp, event_name))

    def analyze_revenue_impact(
        self,
        version_id: str
    ) -> RevenueAttribution:
        """
        Analyze revenue impact of model deployment using causal inference.

        Args:
            version_id: Model version to analyze

        Returns:
            Revenue attribution with confidence metrics
        """
        if version_id not in self.model_versions:
            raise ValueError(f"Model version {version_id} not found")

        version = self.model_versions[version_id]
        deployment_time = version.deployed_at

        # Extract baseline period (before deployment)
        baseline_start = deployment_time - timedelta(days=self.baseline_period_days)
        baseline_revenue = [
            rev for ts, rev in self.revenue_history
            if baseline_start <= ts < deployment_time
        ]

        # Extract evaluation period (after deployment)
        eval_end = deployment_time + timedelta(days=self.evaluation_period_days)
        eval_revenue = [
            rev for ts, rev in self.revenue_history
            if deployment_time <= ts < eval_end
        ]

        if len(baseline_revenue) < 7 or len(eval_revenue) < 7:
            raise ValueError("Insufficient data for analysis")

        # Compute baseline and current metrics
        baseline_mean = np.mean(baseline_revenue)
        current_mean = np.mean(eval_revenue)

        revenue_delta = current_mean - baseline_mean
        revenue_delta_pct = (revenue_delta / baseline_mean) * 100

        # Statistical significance (t-test)
        t_stat, p_value = stats.ttest_ind(baseline_revenue, eval_revenue)

        # Confidence interval (bootstrap)
        ci_lower, ci_upper = self._bootstrap_ci(
            baseline_revenue,
            eval_revenue
        )

        # Check for confounding factors
        confounders = self._identify_confounders(
            deployment_time,
            eval_end
        )

        # Estimate attribution confidence
        attribution_conf = self._estimate_attribution_confidence(
            p_value,
            confounders,
            revenue_delta_pct
        )

        attribution = RevenueAttribution(
            model_version=version_id,
            baseline_revenue=baseline_mean,
            current_revenue=current_mean,
            revenue_delta=revenue_delta,
            revenue_delta_pct=revenue_delta_pct,
            confidence_interval=(ci_lower, ci_upper),
            statistical_significance=p_value,
            attribution_confidence=attribution_conf,
            confounding_factors=confounders
        )

        logger.info(
            f"Revenue impact for {version_id}: "
            f"${revenue_delta:,.2f} ({revenue_delta_pct:+.1f}%), "
            f"p={p_value:.4f}, attribution_conf={attribution_conf:.2f}"
        )

        return attribution

    def _bootstrap_ci(
        self,
        baseline: List[float],
        current: List[float],
        n_iterations: int = 1000,
        confidence_level: float = 0.95
    ) -> Tuple[float, float]:
        """
        Compute confidence interval via bootstrap.

        Returns:
            (lower_bound, upper_bound) for revenue delta
        """
        deltas = []

        for _ in range(n_iterations):
            # Resample with replacement
            baseline_sample = np.random.choice(
                baseline,
                size=len(baseline),
                replace=True
            )
            current_sample = np.random.choice(
                current,
                size=len(current),
                replace=True
            )

            delta = current_sample.mean() - baseline_sample.mean()
            deltas.append(delta)

        # Compute percentiles
        alpha = 1 - confidence_level
        lower = np.percentile(deltas, alpha / 2 * 100)
        upper = np.percentile(deltas, (1 - alpha / 2) * 100)

        return lower, upper

    def _identify_confounders(
        self,
        deployment_time: datetime,
        eval_end: datetime
    ) -> List[str]:
        """
        Identify potential confounding factors.

        Returns:
            List of confounding events
        """
        confounders = []

        # Check for external events during evaluation period
        for event_time, event_name in self.external_events:
            if deployment_time <= event_time <= eval_end:
                confounders.append(event_name)

        # Check for seasonality (day of week, month)
        deployment_dow = deployment_time.weekday()
        if deployment_dow in [4, 5, 6]:  # Fri, Sat, Sun
            confounders.append("Weekend deployment")

        deployment_month = deployment_time.month
        if deployment_month in [11, 12]:  # Nov, Dec
            confounders.append("Holiday season")

        return confounders

    def _estimate_attribution_confidence(
        self,
        p_value: float,
        confounders: List[str],
        effect_size_pct: float
    ) -> float:
        """
        Estimate confidence that revenue change is due to model.

        Returns:
            Attribution confidence (0-1)
        """
        # Start with statistical significance
        if p_value < 0.01:
            base_confidence = 0.9
        elif p_value < 0.05:
            base_confidence = 0.7
        elif p_value < 0.1:
            base_confidence = 0.5
        else:
            base_confidence = 0.2

        # Reduce confidence for each confounder
        confounder_penalty = len(confounders) * 0.15
        base_confidence -= confounder_penalty

        # Increase confidence for large effect sizes
        if abs(effect_size_pct) > 10:
            base_confidence += 0.1

        return max(0.0, min(1.0, base_confidence))

    def generate_revenue_report(
        self,
        version_id: str
    ) -> str:
        """
        Generate human-readable revenue impact report.

        Args:
            version_id: Model version to analyze

        Returns:
            Formatted report string
        """
        attribution = self.analyze_revenue_impact(version_id)

        direction = "increase" if attribution.revenue_delta > 0 else "decrease"
        significance = "statistically significant" if \
            attribution.statistical_significance < 0.05 else "not significant"

        report = f"""
Revenue Impact Report: Model {attribution.model_version}
{'=' * 60}

Baseline Revenue (14 days before): ${attribution.baseline_revenue:,.2f}/day
Current Revenue (14 days after):   ${attribution.current_revenue:,.2f}/day

Revenue Change: ${attribution.revenue_delta:+,.2f}/day ({attribution.revenue_delta_pct:+.1f}%)
95% CI: [${attribution.confidence_interval[0]:,.2f}, ${attribution.confidence_interval[1]:,.2f}]

Statistical Significance: p={attribution.statistical_significance:.4f} ({significance})

Attribution Confidence: {attribution.attribution_confidence:.1%}
"""

        if attribution.confounding_factors:
            report += f"\nPotential Confounders:\n"
            for confounder in attribution.confounding_factors:
                report += f"  - {confounder}\n"

        if attribution.attribution_confidence >= self.min_confidence_for_attribution:
            report += f"\n✓ High confidence that model change caused revenue {direction}"
        else:
            report += f"\n⚠ Low confidence - confounders or insufficient data"

        return report
\end{lstlisting}

\subsection{FairnessMonitor: Bias Detection and Remediation}

\begin{lstlisting}[language=Python, caption={Comprehensive Fairness Monitoring System}]
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
from datetime import datetime
import numpy as np
import pandas as pd
from scipy import stats
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)

@dataclass
class FairnessMetric:
    """
    Fairness metric for a protected group.

    Attributes:
        metric_name: Name of fairness metric
        protected_attribute: Protected attribute (race, gender, age, etc.)
        reference_group: Reference group for comparison
        comparison_group: Group being compared
        reference_value: Metric value for reference group
        comparison_value: Metric value for comparison group
        disparity_ratio: Ratio of comparison to reference
        is_fair: Whether disparity is within acceptable bounds
        timestamp: When measured
    """
    metric_name: str
    protected_attribute: str
    reference_group: str
    comparison_group: str
    reference_value: float
    comparison_value: float
    disparity_ratio: float
    is_fair: bool
    timestamp: datetime

@dataclass
class BiasAlert:
    """
    Alert for detected bias.

    Attributes:
        severity: Alert severity (low/medium/high/critical)
        protected_attribute: Attribute with bias
        affected_groups: Groups affected
        fairness_violations: Fairness metrics violated
        recommendation: Remediation recommendation
        timestamp: When detected
    """
    severity: str
    protected_attribute: str
    affected_groups: List[str]
    fairness_violations: List[str]
    recommendation: str
    timestamp: datetime

class FairnessMonitor:
    """
    Monitor ML fairness across protected groups.

    Fairness metrics:
    - Demographic Parity: P(Y^=1|A=a) = P(Y^=1|A=b)
    - Equalized Odds: P(Y^=1|Y=y,A=a) = P(Y^=1|Y=y,A=b) for all y
    - Equal Opportunity: P(Y^=1|Y=1,A=a) = P(Y^=1|Y=1,A=b)
    - Predictive Parity: P(Y=1|Y^=1,A=a) = P(Y=1|Y^=1,A=b)
    - Calibration: P(Y=1|Y^=p,A=a) = p for all groups

    Example:
        >>> monitor = FairnessMonitor(
        ...     protected_attributes=["gender", "race", "age_group"]
        ... )
        >>> monitor.evaluate_fairness(
        ...     predictions=y_pred,
        ...     ground_truth=y_true,
        ...     protected_attributes_df=sensitive_features
        ... )
        >>> alerts = monitor.get_bias_alerts()
        >>> for alert in alerts:
        ...     if alert.severity == "critical":
        ...         trigger_remediation(alert)
    """

    def __init__(
        self,
        protected_attributes: List[str],
        fairness_threshold: float = 0.8,
        reference_groups: Optional[Dict[str, str]] = None
    ):
        """
        Initialize fairness monitor.

        Args:
            protected_attributes: List of protected attributes to monitor
            fairness_threshold: Minimum acceptable disparity ratio (0.8 = 80/20 rule)
            reference_groups: Reference group for each attribute
        """
        self.protected_attributes = protected_attributes
        self.fairness_threshold = fairness_threshold
        self.reference_groups = reference_groups or {}

        # Metrics storage
        self.fairness_metrics: List[FairnessMetric] = []
        self.bias_alerts: List[BiasAlert] = []

    def evaluate_fairness(
        self,
        predictions: np.ndarray,
        ground_truth: np.ndarray,
        protected_attributes_df: pd.DataFrame,
        timestamp: Optional[datetime] = None
    ):
        """
        Evaluate fairness across all protected attributes.

        Args:
            predictions: Model predictions (0/1 for binary classification)
            ground_truth: True labels
            protected_attributes_df: DataFrame with protected attributes
            timestamp: When evaluation occurred
        """
        if timestamp is None:
            timestamp = datetime.now()

        # Evaluate each protected attribute
        for attr in self.protected_attributes:
            if attr not in protected_attributes_df.columns:
                logger.warning(f"Protected attribute {attr} not found")
                continue

            # Compute fairness metrics
            metrics = self._compute_fairness_metrics(
                predictions,
                ground_truth,
                protected_attributes_df[attr],
                attr,
                timestamp
            )

            self.fairness_metrics.extend(metrics)

            # Check for violations
            violations = [m for m in metrics if not m.is_fair]

            if violations:
                # Create bias alert
                alert = self._create_bias_alert(
                    attr,
                    violations,
                    timestamp
                )
                self.bias_alerts.append(alert)

                logger.warning(
                    f"Bias detected in {attr}: {len(violations)} violations"
                )

    def _compute_fairness_metrics(
        self,
        predictions: np.ndarray,
        ground_truth: np.ndarray,
        protected_attr: pd.Series,
        attr_name: str,
        timestamp: datetime
    ) -> List[FairnessMetric]:
        """
        Compute fairness metrics for a protected attribute.

        Returns:
            List of fairness metrics
        """
        metrics = []

        # Get unique groups
        groups = protected_attr.unique()

        # Determine reference group
        reference_group = self.reference_groups.get(
            attr_name,
            groups[0]  # Default to first group
        )

        # Compute metrics for each group vs reference
        for group in groups:
            if group == reference_group:
                continue

            # Demographic Parity
            dp_metric = self._demographic_parity(
                predictions,
                protected_attr,
                reference_group,
                group,
                attr_name,
                timestamp
            )
            metrics.append(dp_metric)

            # Equal Opportunity
            eo_metric = self._equal_opportunity(
                predictions,
                ground_truth,
                protected_attr,
                reference_group,
                group,
                attr_name,
                timestamp
            )
            metrics.append(eo_metric)

            # Equalized Odds
            eodds_metric = self._equalized_odds(
                predictions,
                ground_truth,
                protected_attr,
                reference_group,
                group,
                attr_name,
                timestamp
            )
            metrics.append(eodds_metric)

        return metrics

    def _demographic_parity(
        self,
        predictions: np.ndarray,
        protected_attr: pd.Series,
        ref_group: str,
        comp_group: str,
        attr_name: str,
        timestamp: datetime
    ) -> FairnessMetric:
        """
        Compute demographic parity metric.

        Demographic parity: P(Y^=1|A=a) = P(Y^=1|A=b)

        Returns:
            FairnessMetric
        """
        # Positive rate for reference group
        ref_mask = protected_attr == ref_group
        ref_positive_rate = predictions[ref_mask].mean()

        # Positive rate for comparison group
        comp_mask = protected_attr == comp_group
        comp_positive_rate = predictions[comp_mask].mean()

        # Disparity ratio (avoid division by zero)
        if ref_positive_rate > 0:
            disparity_ratio = comp_positive_rate / ref_positive_rate
        else:
            disparity_ratio = 1.0 if comp_positive_rate == 0 else np.inf

        # Check if fair (within 80/20 rule)
        is_fair = (self.fairness_threshold <= disparity_ratio <= 1 / self.fairness_threshold)

        return FairnessMetric(
            metric_name="Demographic Parity",
            protected_attribute=attr_name,
            reference_group=ref_group,
            comparison_group=comp_group,
            reference_value=ref_positive_rate,
            comparison_value=comp_positive_rate,
            disparity_ratio=disparity_ratio,
            is_fair=is_fair,
            timestamp=timestamp
        )

    def _equal_opportunity(
        self,
        predictions: np.ndarray,
        ground_truth: np.ndarray,
        protected_attr: pd.Series,
        ref_group: str,
        comp_group: str,
        attr_name: str,
        timestamp: datetime
    ) -> FairnessMetric:
        """
        Compute equal opportunity metric.

        Equal opportunity: P(Y^=1|Y=1,A=a) = P(Y^=1|Y=1,A=b)
        (Equal true positive rates)

        Returns:
            FairnessMetric
        """
        # True positive rate for reference group
        ref_mask = (protected_attr == ref_group) & (ground_truth == 1)
        ref_tpr = predictions[ref_mask].mean() if ref_mask.sum() > 0 else 0

        # True positive rate for comparison group
        comp_mask = (protected_attr == comp_group) & (ground_truth == 1)
        comp_tpr = predictions[comp_mask].mean() if comp_mask.sum() > 0 else 0

        # Disparity ratio
        if ref_tpr > 0:
            disparity_ratio = comp_tpr / ref_tpr
        else:
            disparity_ratio = 1.0 if comp_tpr == 0 else np.inf

        is_fair = (self.fairness_threshold <= disparity_ratio <= 1 / self.fairness_threshold)

        return FairnessMetric(
            metric_name="Equal Opportunity",
            protected_attribute=attr_name,
            reference_group=ref_group,
            comparison_group=comp_group,
            reference_value=ref_tpr,
            comparison_value=comp_tpr,
            disparity_ratio=disparity_ratio,
            is_fair=is_fair,
            timestamp=timestamp
        )

    def _equalized_odds(
        self,
        predictions: np.ndarray,
        ground_truth: np.ndarray,
        protected_attr: pd.Series,
        ref_group: str,
        comp_group: str,
        attr_name: str,
        timestamp: datetime
    ) -> FairnessMetric:
        """
        Compute equalized odds metric.

        Equalized odds: Equal TPR and FPR across groups

        Returns:
            FairnessMetric (average of TPR and FPR disparity)
        """
        # TPR disparity
        ref_tpr_mask = (protected_attr == ref_group) & (ground_truth == 1)
        comp_tpr_mask = (protected_attr == comp_group) & (ground_truth == 1)

        ref_tpr = predictions[ref_tpr_mask].mean() if ref_tpr_mask.sum() > 0 else 0
        comp_tpr = predictions[comp_tpr_mask].mean() if comp_tpr_mask.sum() > 0 else 0

        # FPR disparity
        ref_fpr_mask = (protected_attr == ref_group) & (ground_truth == 0)
        comp_fpr_mask = (protected_attr == comp_group) & (ground_truth == 0)

        ref_fpr = predictions[ref_fpr_mask].mean() if ref_fpr_mask.sum() > 0 else 0
        comp_fpr = predictions[comp_fpr_mask].mean() if comp_fpr_mask.sum() > 0 else 0

        # Combined disparity (average)
        tpr_disparity = comp_tpr / ref_tpr if ref_tpr > 0 else 1.0
        fpr_disparity = comp_fpr / ref_fpr if ref_fpr > 0 else 1.0

        # Use worse of the two
        disparity_ratio = min(tpr_disparity, fpr_disparity)

        is_fair = (self.fairness_threshold <= disparity_ratio <= 1 / self.fairness_threshold)

        return FairnessMetric(
            metric_name="Equalized Odds",
            protected_attribute=attr_name,
            reference_group=ref_group,
            comparison_group=comp_group,
            reference_value=(ref_tpr + ref_fpr) / 2,
            comparison_value=(comp_tpr + comp_fpr) / 2,
            disparity_ratio=disparity_ratio,
            is_fair=is_fair,
            timestamp=timestamp
        )

    def _create_bias_alert(
        self,
        attr_name: str,
        violations: List[FairnessMetric],
        timestamp: datetime
    ) -> BiasAlert:
        """
        Create bias alert from violations.

        Args:
            attr_name: Protected attribute with violations
            violations: List of fairness metric violations
            timestamp: When detected

        Returns:
            BiasAlert
        """
        # Determine severity
        num_violations = len(violations)
        worst_disparity = min(v.disparity_ratio for v in violations)

        if worst_disparity < 0.5 or num_violations >= 3:
            severity = "critical"
        elif worst_disparity < 0.7 or num_violations >= 2:
            severity = "high"
        elif worst_disparity < 0.8:
            severity = "medium"
        else:
            severity = "low"

        # Affected groups
        affected_groups = list(set(v.comparison_group for v in violations))

        # Violated metrics
        violated_metrics = list(set(v.metric_name for v in violations))

        # Generate recommendation
        recommendation = self._generate_remediation_recommendation(
            attr_name,
            violations
        )

        return BiasAlert(
            severity=severity,
            protected_attribute=attr_name,
            affected_groups=affected_groups,
            fairness_violations=violated_metrics,
            recommendation=recommendation,
            timestamp=timestamp
        )

    def _generate_remediation_recommendation(
        self,
        attr_name: str,
        violations: List[FairnessMetric]
    ) -> str:
        """
        Generate remediation recommendation.

        Returns:
            Recommendation string
        """
        # Determine which metrics are violated
        demographic_parity_violated = any(
            v.metric_name == "Demographic Parity" for v in violations
        )
        equal_opportunity_violated = any(
            v.metric_name == "Equal Opportunity" for v in violations
        )

        recommendations = []

        if demographic_parity_violated:
            recommendations.append(
                "1. Re-balance training data to ensure equal representation"
            )
            recommendations.append(
                "2. Apply threshold optimization per group"
            )

        if equal_opportunity_violated:
            recommendations.append(
                "3. Investigate feature importance for affected groups"
            )
            recommendations.append(
                "4. Consider fairness constraints during training (fair learning)"
            )

        recommendations.append(
            "5. Implement post-processing bias mitigation (equalized odds post-processing)"
        )
        recommendations.append(
            "6. Increase model explainability to identify bias sources"
        )

        return "\n".join(recommendations)

    def get_bias_alerts(
        self,
        min_severity: str = "low"
    ) -> List[BiasAlert]:
        """
        Get bias alerts filtered by severity.

        Args:
            min_severity: Minimum severity to return

        Returns:
            List of bias alerts
        """
        severity_order = {"low": 0, "medium": 1, "high": 2, "critical": 3}
        min_level = severity_order[min_severity]

        return [
            alert for alert in self.bias_alerts
            if severity_order[alert.severity] >= min_level
        ]

    def get_fairness_summary(self) -> Dict[str, any]:
        """
        Get summary of fairness status.

        Returns:
            Summary dictionary
        """
        if not self.fairness_metrics:
            return {"status": "No fairness data"}

        total_metrics = len(self.fairness_metrics)
        fair_metrics = sum(1 for m in self.fairness_metrics if m.is_fair)

        # Group by protected attribute
        by_attribute = defaultdict(list)
        for metric in self.fairness_metrics:
            by_attribute[metric.protected_attribute].append(metric)

        attribute_status = {}
        for attr, metrics in by_attribute.items():
            fair = sum(1 for m in metrics if m.is_fair)
            total = len(metrics)
            attribute_status[attr] = {
                "fair_metrics": fair,
                "total_metrics": total,
                "fairness_rate": fair / total,
                "status": "FAIR" if fair == total else "BIASED"
            }

        return {
            "overall_fairness_rate": fair_metrics / total_metrics,
            "total_metrics": total_metrics,
            "fair_metrics": fair_metrics,
            "biased_metrics": total_metrics - fair_metrics,
            "by_attribute": attribute_status,
            "total_alerts": len(self.bias_alerts),
            "critical_alerts": len([a for a in self.bias_alerts if a.severity == "critical"])
        }
\end{lstlisting}

\subsection{Real-World Scenario: The Vanity Metric Trap}

\subsubsection{The Problem}

A content recommendation ML team optimized for model accuracy as their primary metric. After months of iteration, they achieved impressive improvements:

\begin{itemize}
    \item \textbf{Accuracy improved}: 89\% → 94\% (+5.6\%)
    \item \textbf{AUC-ROC improved}: 0.91 → 0.96 (+5.5\%)
    \item \textbf{Precision improved}: 0.87 → 0.93 (+6.9\%)
    \item \textbf{Team celebrated}: Awards given, bonuses paid
\end{itemize}

However, 3 months after deployment:

\begin{itemize}
    \item \textbf{User engagement DOWN 12\%}: Average session time decreased
    \item \textbf{Revenue DOWN 8\%}: Subscription conversions dropped
    \item \textbf{User satisfaction DOWN}: NPS score dropped from 42 to 31
    \item \textbf{Churn UP 15\%}: Users leaving platform at higher rate
\end{itemize}

\textbf{Root cause analysis revealed}:
\begin{itemize}
    \item Model optimized for click prediction, not engagement quality
    \item Higher accuracy achieved by recommending "clickbait" content
    \item Users clicked more (improving accuracy) but were less satisfied
    \item Content quality degraded, harming long-term retention
    \item No business metric tracking connected to model improvements
\end{itemize}

\subsubsection{The Solution}

Implemented comprehensive business impact monitoring:

\begin{lstlisting}[language=Python, caption={Business-Aligned Monitoring Implementation}]
# Initialize business metrics tracker
business_tracker = BusinessMetricsTracker(
    correlation_threshold=0.3,
    p_value_threshold=0.05
)

# Track both technical AND business metrics
def record_recommendation_metrics(
    user_id: str,
    recommendations: List[str],
    model_accuracy: float,
    timestamp: datetime
):
    """Record comprehensive metrics for each recommendation session."""

    # Technical metrics
    business_tracker.record_technical_metric(
        "model_accuracy",
        model_accuracy,
        timestamp
    )

    # Business metrics (measured later when user behavior is observed)
    engagement_time = measure_user_engagement(user_id, timestamp)
    business_tracker.record_business_metric(
        "engagement_minutes",
        engagement_time,
        timestamp,
        metadata={"user_id": user_id}
    )

    content_quality_rating = get_content_ratings(recommendations)
    business_tracker.record_business_metric(
        "content_quality_score",
        content_quality_rating,
        timestamp
    )

    subscription_converted = check_subscription_conversion(user_id, timestamp)
    business_tracker.record_business_metric(
        "conversion",
        1.0 if subscription_converted else 0.0,
        timestamp
    )

# Analyze correlations weekly
def weekly_business_analysis():
    """Analyze correlations between technical and business metrics."""

    correlations = business_tracker.analyze_correlations(lookback_days=30)

    # Print insights
    insights = business_tracker.generate_insights()
    for insight in insights:
        print(insight)

    # Alert on negative correlations
    for corr in correlations:
        if corr.correlation_coefficient < -0.3 and \
           corr.causal_confidence > 0.6:
            # Technical metric improving but business metric declining
            send_alert(
                severity="CRITICAL",
                message=f"Improving {corr.technical_metric} is harming "
                        f"{corr.business_metric}! Correlation: "
                        f"{corr.correlation_coefficient:.3f}"
            )

# New model evaluation criteria
def evaluate_model_holistically(
    model_version: str,
    technical_metrics: Dict[str, float],
    business_metrics: Dict[str, float]
):
    """Evaluate model on BOTH technical and business metrics."""

    # Traditional technical evaluation
    if technical_metrics["accuracy"] < 0.90:
        return False, "Accuracy too low"

    # NEW: Business impact analysis
    revenue_analyzer = RevenueImpactAnalyzer()
    revenue_impact = revenue_analyzer.analyze_revenue_impact(model_version)

    if revenue_impact.revenue_delta < 0:
        # Revenue decreased
        if revenue_impact.attribution_confidence > 0.7:
            return False, (
                f"Model improves technical metrics but HARMS revenue by "
                f"${abs(revenue_impact.revenue_delta):,.2f}/day"
            )

    # Check user engagement
    if business_metrics.get("engagement_minutes", 0) < baseline_engagement:
        return False, "User engagement decreased"

    # Check fairness
    fairness_monitor = FairnessMonitor(
        protected_attributes=["age_group", "gender", "region"]
    )
    fairness_summary = fairness_monitor.get_fairness_summary()

    if fairness_summary.get("critical_alerts", 0) > 0:
        return False, "Fairness violations detected"

    return True, "Model improves technical AND business metrics"

# Updated success metrics
success_metrics = {
    # Technical (necessary but not sufficient)
    "accuracy": {"target": "> 0.90", "weight": 0.2},
    "latency_p95": {"target": "< 100ms", "weight": 0.1},

    # Business (primary goals)
    "engagement_minutes": {"target": "> 25min/session", "weight": 0.3},
    "conversion_rate": {"target": "> 4.5%", "weight": 0.3},
    "user_satisfaction_nps": {"target": "> 40", "weight": 0.2},
    "content_quality_score": {"target": "> 4.0/5.0", "weight": 0.2},

    # Fairness (constraint)
    "demographic_parity": {"target": "> 0.8", "required": True}
}
\end{lstlisting}

\subsubsection{Outcome}

After implementing business-aligned monitoring:

\begin{itemize}
    \item \textbf{New optimization target}: Balanced accuracy with engagement quality
    \item \textbf{Model v2.0 deployed}:
        \begin{itemize}
            \item Accuracy: 92\% (slightly lower than v1.5's 94\%)
            \item Engagement: +18\% vs v1.5
            \item Revenue: +12\% vs v1.5
            \item NPS: 45 (up from 31)
        \end{itemize}
    \item \textbf{Early warning system}: Detected inverse correlations in A/B tests
    \item \textbf{Holistic evaluation}: Technical metrics are necessary but not sufficient
    \item \textbf{Team alignment}: Data scientists now measured on business impact, not just technical metrics
    \item \textbf{ROI quantified}: Model v2.0 generated \$2.3M additional annual revenue
\end{itemize}

\textbf{Key lesson}: \textit{Optimizing for technical metrics without monitoring business impact can harm the business. Always track the full causal chain from technical changes to business outcomes.}

\section{Production-Ready Monitoring Systems}

Enterprise ML systems require monitoring infrastructure that is scalable, reliable, and integrated with existing operational tools. This section provides production-grade implementations suitable for large-scale deployments.

\subsection{Enterprise ModelMonitor with Scalable Architecture}

\begin{lstlisting}[language=Python, caption={Production-Grade Model Monitor with Async Processing}]
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading
from queue import Queue, PriorityQueue
import logging
from collections import defaultdict
import json

logger = logging.getLogger(__name__)

class MonitoringPriority(Enum):
    """Priority levels for monitoring tasks."""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4

@dataclass
class MonitoringTask:
    """
    Task for model monitoring.

    Attributes:
        task_id: Unique task identifier
        priority: Task priority
        model_name: Name of model to monitor
        metric_type: Type of metric to compute
        data_batch: Data for monitoring
        timestamp: When task was created
        callback: Optional callback function
    """
    task_id: str
    priority: MonitoringPriority
    model_name: str
    metric_type: str
    data_batch: Any
    timestamp: datetime = field(default_factory=datetime.now)
    callback: Optional[Callable] = None

    def __lt__(self, other):
        """Compare tasks by priority for priority queue."""
        return self.priority.value < other.priority.value

@dataclass
class MonitoringMetric:
    """
    Computed monitoring metric.

    Attributes:
        model_name: Model name
        metric_name: Metric name
        value: Metric value
        timestamp: When computed
        tags: Additional tags
    """
    model_name: str
    metric_name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)

class ModelMonitor:
    """
    Enterprise-grade model monitoring system with scalable architecture.

    Features:
    - Async processing for high throughput
    - Batching for efficiency
    - Priority queue for critical metrics
    - Thread pool for parallel processing
    - Metric aggregation and buffering
    - Integration with observability stack

    Architecture:
    - Producer: Receives monitoring requests
    - Queue: Priority-based task queue
    - Workers: Async workers processing tasks
    - Aggregator: Batches metrics for efficient storage
    - Exporter: Sends metrics to backend (Prometheus, etc.)

    Example:
        >>> monitor = ModelMonitor(
        ...     num_workers=4,
        ...     batch_size=100,
        ...     flush_interval=30
        ... )
        >>> await monitor.start()
        >>> await monitor.track_prediction(
        ...     model_name="fraud-detector",
        ...     prediction=0.85,
        ...     latency_ms=45,
        ...     features={"amount": 100}
        ... )
    """

    def __init__(
        self,
        num_workers: int = 4,
        batch_size: int = 100,
        flush_interval: int = 30,
        max_queue_size: int = 10000,
        observability_stack: Optional[Any] = None
    ):
        """
        Initialize model monitor.

        Args:
            num_workers: Number of async workers
            batch_size: Batch size for aggregation
            flush_interval: Flush interval in seconds
            max_queue_size: Maximum queue size
            observability_stack: ObservabilityStack instance
        """
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.max_queue_size = max_queue_size
        self.observability_stack = observability_stack

        # Task queue
        self.task_queue: PriorityQueue = PriorityQueue(maxsize=max_queue_size)

        # Metric buffer for batching
        self.metric_buffer: List[MonitoringMetric] = []
        self.buffer_lock = threading.Lock()

        # Worker management
        self.workers: List[asyncio.Task] = []
        self.running = False

        # Statistics
        self.stats = {
            'tasks_processed': 0,
            'tasks_failed': 0,
            'metrics_exported': 0,
            'queue_full_events': 0
        }

        logger.info(
            f"ModelMonitor initialized with {num_workers} workers, "
            f"batch_size={batch_size}, flush_interval={flush_interval}s"
        )

    async def start(self):
        """Start monitoring system."""
        if self.running:
            logger.warning("Monitor already running")
            return

        self.running = True

        # Start worker tasks
        for i in range(self.num_workers):
            worker = asyncio.create_task(self._worker(i))
            self.workers.append(worker)

        # Start flush task
        flush_task = asyncio.create_task(self._flush_loop())
        self.workers.append(flush_task)

        logger.info(f"Started {self.num_workers} workers and flush loop")

    async def stop(self):
        """Stop monitoring system gracefully."""
        logger.info("Stopping ModelMonitor...")

        self.running = False

        # Wait for workers to finish
        for worker in self.workers:
            worker.cancel()

        await asyncio.gather(*self.workers, return_exceptions=True)

        # Flush remaining metrics
        await self._flush_metrics()

        logger.info("ModelMonitor stopped")

    async def track_prediction(
        self,
        model_name: str,
        prediction: Any,
        latency_ms: float,
        features: Dict[str, Any],
        ground_truth: Optional[Any] = None,
        priority: MonitoringPriority = MonitoringPriority.NORMAL
    ):
        """
        Track a model prediction.

        Args:
            model_name: Model name
            prediction: Model prediction
            latency_ms: Prediction latency in milliseconds
            features: Input features
            ground_truth: Ground truth label (if available)
            priority: Task priority
        """
        task = MonitoringTask(
            task_id=f"{model_name}-{datetime.now().timestamp()}",
            priority=priority,
            model_name=model_name,
            metric_type="prediction",
            data_batch={
                'prediction': prediction,
                'latency_ms': latency_ms,
                'features': features,
                'ground_truth': ground_truth
            }
        )

        try:
            self.task_queue.put_nowait(task)
        except:
            self.stats['queue_full_events'] += 1
            logger.warning(f"Queue full, dropping task for {model_name}")

    async def track_batch_predictions(
        self,
        model_name: str,
        predictions: List[Any],
        latencies_ms: List[float],
        features_batch: List[Dict[str, Any]],
        ground_truths: Optional[List[Any]] = None
    ):
        """
        Track batch of predictions efficiently.

        Args:
            model_name: Model name
            predictions: List of predictions
            latencies_ms: List of latencies
            features_batch: List of feature dicts
            ground_truths: Optional ground truth labels
        """
        task = MonitoringTask(
            task_id=f"{model_name}-batch-{datetime.now().timestamp()}",
            priority=MonitoringPriority.NORMAL,
            model_name=model_name,
            metric_type="batch_prediction",
            data_batch={
                'predictions': predictions,
                'latencies_ms': latencies_ms,
                'features_batch': features_batch,
                'ground_truths': ground_truths
            }
        )

        try:
            self.task_queue.put_nowait(task)
        except:
            self.stats['queue_full_events'] += 1

    async def _worker(self, worker_id: int):
        """
        Async worker processing monitoring tasks.

        Args:
            worker_id: Worker identifier
        """
        logger.info(f"Worker {worker_id} started")

        while self.running:
            try:
                # Get task from queue with timeout
                task = await asyncio.wait_for(
                    asyncio.to_thread(self.task_queue.get, timeout=1),
                    timeout=2
                )

                # Process task
                await self._process_task(task)

                self.stats['tasks_processed'] += 1

            except asyncio.TimeoutError:
                # No tasks available, continue
                continue
            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}")
                self.stats['tasks_failed'] += 1

        logger.info(f"Worker {worker_id} stopped")

    async def _process_task(self, task: MonitoringTask):
        """
        Process a monitoring task.

        Args:
            task: MonitoringTask to process
        """
        if task.metric_type == "prediction":
            await self._process_prediction(task)
        elif task.metric_type == "batch_prediction":
            await self._process_batch_prediction(task)
        else:
            logger.warning(f"Unknown task type: {task.metric_type}")

        # Execute callback if provided
        if task.callback:
            try:
                await asyncio.to_thread(task.callback, task)
            except Exception as e:
                logger.error(f"Callback error: {e}")

    async def _process_prediction(self, task: MonitoringTask):
        """Process single prediction monitoring."""
        data = task.data_batch
        timestamp = task.timestamp

        # Create metrics
        metrics = []

        # Latency metric
        metrics.append(MonitoringMetric(
            model_name=task.model_name,
            metric_name="prediction_latency_ms",
            value=data['latency_ms'],
            timestamp=timestamp,
            tags={'model': task.model_name}
        ))

        # Prediction count
        metrics.append(MonitoringMetric(
            model_name=task.model_name,
            metric_name="predictions_total",
            value=1.0,
            timestamp=timestamp,
            tags={'model': task.model_name}
        ))

        # Accuracy metric (if ground truth available)
        if data.get('ground_truth') is not None:
            correct = int(data['prediction'] == data['ground_truth'])
            metrics.append(MonitoringMetric(
                model_name=task.model_name,
                metric_name="prediction_accuracy",
                value=float(correct),
                timestamp=timestamp,
                tags={'model': task.model_name}
            ))

        # Add to buffer
        await self._buffer_metrics(metrics)

        # Send to observability stack if available
        if self.observability_stack:
            await asyncio.to_thread(
                self.observability_stack.record_prediction,
                task.model_name,
                "v1.0",  # Version tracking would be added
                data['features'],
                data['prediction'],
                data['latency_ms'] / 1000.0,  # Convert to seconds
                success=True
            )

    async def _process_batch_prediction(self, task: MonitoringTask):
        """Process batch of predictions efficiently."""
        data = task.data_batch
        timestamp = task.timestamp

        predictions = data['predictions']
        latencies = data['latencies_ms']
        ground_truths = data.get('ground_truths')

        # Aggregate metrics
        avg_latency = sum(latencies) / len(latencies)
        total_predictions = len(predictions)

        metrics = [
            MonitoringMetric(
                model_name=task.model_name,
                metric_name="batch_avg_latency_ms",
                value=avg_latency,
                timestamp=timestamp,
                tags={'model': task.model_name, 'batch_size': str(total_predictions)}
            ),
            MonitoringMetric(
                model_name=task.model_name,
                metric_name="predictions_total",
                value=float(total_predictions),
                timestamp=timestamp,
                tags={'model': task.model_name}
            )
        ]

        # Batch accuracy
        if ground_truths:
            correct = sum(
                1 for pred, truth in zip(predictions, ground_truths)
                if pred == truth
            )
            accuracy = correct / len(predictions)

            metrics.append(MonitoringMetric(
                model_name=task.model_name,
                metric_name="batch_accuracy",
                value=accuracy,
                timestamp=timestamp,
                tags={'model': task.model_name}
            ))

        await self._buffer_metrics(metrics)

    async def _buffer_metrics(self, metrics: List[MonitoringMetric]):
        """
        Add metrics to buffer for batched export.

        Args:
            metrics: List of metrics to buffer
        """
        with self.buffer_lock:
            self.metric_buffer.extend(metrics)

            # Flush if buffer exceeds batch size
            if len(self.metric_buffer) >= self.batch_size:
                await self._flush_metrics()

    async def _flush_loop(self):
        """Periodically flush metrics buffer."""
        while self.running:
            await asyncio.sleep(self.flush_interval)
            await self._flush_metrics()

    async def _flush_metrics(self):
        """Flush buffered metrics to storage/export."""
        with self.buffer_lock:
            if not self.metric_buffer:
                return

            metrics_to_export = self.metric_buffer.copy()
            self.metric_buffer.clear()

        # Export metrics
        await self._export_metrics(metrics_to_export)

        self.stats['metrics_exported'] += len(metrics_to_export)

        logger.debug(f"Flushed {len(metrics_to_export)} metrics")

    async def _export_metrics(self, metrics: List[MonitoringMetric]):
        """
        Export metrics to backend systems.

        Args:
            metrics: Metrics to export
        """
        # Group by model for efficient export
        by_model = defaultdict(list)
        for metric in metrics:
            by_model[metric.model_name].append(metric)

        # Export to various backends (implement based on your stack)
        # Example: Prometheus, InfluxDB, CloudWatch, etc.

        for model_name, model_metrics in by_model.items():
            # Aggregate metrics
            aggregated = self._aggregate_metrics(model_metrics)

            # Export (example - would integrate with actual backend)
            logger.debug(
                f"Exporting {len(model_metrics)} metrics for {model_name}: "
                f"{aggregated}"
            )

    def _aggregate_metrics(
        self,
        metrics: List[MonitoringMetric]
    ) -> Dict[str, float]:
        """
        Aggregate metrics for export.

        Args:
            metrics: List of metrics

        Returns:
            Aggregated metric values
        """
        aggregated = defaultdict(list)

        for metric in metrics:
            aggregated[metric.metric_name].append(metric.value)

        # Compute aggregates
        result = {}
        for metric_name, values in aggregated.items():
            if len(values) > 0:
                result[f"{metric_name}_avg"] = sum(values) / len(values)
                result[f"{metric_name}_sum"] = sum(values)
                result[f"{metric_name}_count"] = len(values)

        return result

    def get_stats(self) -> Dict[str, Any]:
        """Get monitoring system statistics."""
        return {
            **self.stats,
            'queue_size': self.task_queue.qsize(),
            'buffer_size': len(self.metric_buffer),
            'workers_active': len(self.workers),
            'running': self.running
        }
\end{lstlisting}

\subsection{Enhanced AlertManager with Intelligent Routing}

\begin{lstlisting}[language=Python, caption={Production Alert Manager with Escalation and SLA Tracking}]
from typing import Dict, List, Optional, Set, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import logging
from collections import defaultdict, deque

logger = logging.getLogger(__name__)

class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = 1
    WARNING = 2
    ERROR = 3
    CRITICAL = 4

class AlertChannel(Enum):
    """Alert delivery channels."""
    EMAIL = "email"
    SLACK = "slack"
    PAGERDUTY = "pagerduty"
    SERVICENOW = "servicenow"
    WEBHOOK = "webhook"

class AlertStatus(Enum):
    """Alert lifecycle status."""
    OPEN = "open"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"

@dataclass
class Alert:
    """
    Alert definition.

    Attributes:
        alert_id: Unique identifier
        severity: Alert severity
        metric_name: Metric that triggered alert
        message: Alert message
        value: Current metric value
        threshold: Threshold that was exceeded
        model_name: Model name
        timestamp: When alert was created
        status: Current alert status
        context: Additional context
    """
    alert_id: str
    severity: AlertSeverity
    metric_name: str
    message: str
    value: float
    threshold: float
    model_name: str
    timestamp: datetime = field(default_factory=datetime.now)
    status: AlertStatus = AlertStatus.OPEN
    context: Dict[str, Any] = field(default_factory=dict)
    acknowledged_at: Optional[datetime] = None
    resolved_at: Optional[datetime] = None
    escalation_level: int = 0

@dataclass
class AlertRule:
    """
    Alert routing rule.

    Attributes:
        name: Rule name
        severity_levels: Severities this rule applies to
        channels: Delivery channels
        recipients: Recipients (emails, Slack channels, etc.)
        max_frequency: Max alerts per window
        frequency_window: Time window for frequency limit
        suppress_similar: Suppress similar alerts
        escalation_delay: Delay before escalation (seconds)
        sla_response_time: SLA response time (seconds)
    """
    name: str
    severity_levels: List[AlertSeverity]
    channels: List[AlertChannel]
    recipients: List[str]
    max_frequency: int = 10
    frequency_window: timedelta = timedelta(hours=1)
    suppress_similar: bool = True
    escalation_delay: int = 300  # 5 minutes
    sla_response_time: int = 900  # 15 minutes

@dataclass
class EscalationPolicy:
    """
    Alert escalation policy.

    Attributes:
        name: Policy name
        levels: List of escalation levels with recipients
        escalation_intervals: Time between escalation levels (seconds)
    """
    name: str
    levels: List[Dict[str, Any]]  # [{'channels': [...], 'recipients': [...]}]
    escalation_intervals: List[int]  # Time between levels in seconds

class AlertManager:
    """
    Enterprise alert management system with intelligent routing.

    Features:
    - Rule-based routing
    - Frequency limiting and deduplication
    - Multi-channel delivery (Email, Slack, PagerDuty, ServiceNow)
    - Escalation policies
    - SLA tracking
    - Alert suppression and grouping
    - Alert lifecycle management

    Example:
        >>> manager = AlertManager()
        >>> manager.add_rule(AlertRule(
        ...     name="critical_alerts",
        ...     severity_levels=[AlertSeverity.CRITICAL],
        ...     channels=[AlertChannel.PAGERDUTY, AlertChannel.SLACK],
        ...     recipients=["oncall@company.com", "#incidents"]
        ... ))
        >>> await manager.send_alert(alert)
    """

    def __init__(
        self,
        email_config: Optional[Dict] = None,
        slack_webhook: Optional[str] = None,
        pagerduty_api_key: Optional[str] = None,
        servicenow_config: Optional[Dict] = None
    ):
        """
        Initialize alert manager.

        Args:
            email_config: Email configuration
            slack_webhook: Slack webhook URL
            pagerduty_api_key: PagerDuty API key
            servicenow_config: ServiceNow configuration
        """
        # Configuration
        self.email_config = email_config
        self.slack_webhook = slack_webhook
        self.pagerduty_api_key = pagerduty_api_key
        self.servicenow_config = servicenow_config

        # Routing rules
        self.rules: List[AlertRule] = []

        # Escalation policies
        self.escalation_policies: Dict[str, EscalationPolicy] = {}

        # Alert history
        self.alerts: Dict[str, Alert] = {}
        self.alert_history: deque = deque(maxlen=10000)

        # Frequency tracking
        self.alert_counts: Dict[str, deque] = defaultdict(
            lambda: deque(maxlen=1000)
        )

        # SLA tracking
        self.sla_violations: List[Dict] = []

        # Suppression tracking
        self.suppressed_alerts: Set[str] = set()

        logger.info("AlertManager initialized")

    def add_rule(self, rule: AlertRule):
        """
        Add alert routing rule.

        Args:
            rule: AlertRule to add
        """
        self.rules.append(rule)
        logger.info(f"Added alert rule: {rule.name}")

    def add_escalation_policy(self, policy: EscalationPolicy):
        """
        Add escalation policy.

        Args:
            policy: EscalationPolicy to add
        """
        self.escalation_policies[policy.name] = policy
        logger.info(f"Added escalation policy: {policy.name}")

    async def send_alert(self, alert: Alert) -> bool:
        """
        Send alert through configured channels.

        Args:
            alert: Alert to send

        Returns:
            True if alert was sent, False if suppressed
        """
        # Check if alert should be suppressed
        if self._should_suppress(alert):
            alert.status = AlertStatus.SUPPRESSED
            self.suppressed_alerts.add(alert.alert_id)
            logger.info(f"Alert suppressed: {alert.alert_id}")
            return False

        # Store alert
        self.alerts[alert.alert_id] = alert
        self.alert_history.append(alert)

        # Find matching rules
        matching_rules = self._find_matching_rules(alert)

        if not matching_rules:
            logger.warning(f"No matching rules for alert: {alert.alert_id}")
            return False

        # Send through channels
        for rule in matching_rules:
            # Check frequency limits
            if not self._check_frequency_limit(alert, rule):
                logger.info(
                    f"Alert {alert.alert_id} exceeds frequency limit for rule {rule.name}"
                )
                continue

            # Send to channels
            await self._deliver_alert(alert, rule)

            # Track frequency
            self._track_alert_frequency(alert, rule)

        # Schedule escalation if needed
        for rule in matching_rules:
            if rule.escalation_delay > 0:
                asyncio.create_task(self._schedule_escalation(alert, rule))

        # Track SLA
        self._track_sla(alert, matching_rules)

        logger.info(
            f"Alert sent: {alert.alert_id} (severity={alert.severity.name})"
        )

        return True

    def _should_suppress(self, alert: Alert) -> bool:
        """
        Check if alert should be suppressed.

        Args:
            alert: Alert to check

        Returns:
            True if alert should be suppressed
        """
        # Check for similar recent alerts
        recent_cutoff = datetime.now() - timedelta(minutes=5)

        for existing_alert in reversed(self.alert_history):
            if existing_alert.timestamp < recent_cutoff:
                break

            # Check similarity
            if (existing_alert.model_name == alert.model_name and
                existing_alert.metric_name == alert.metric_name and
                existing_alert.severity == alert.severity and
                existing_alert.status != AlertStatus.RESOLVED):

                # Similar alert exists
                return True

        return False

    def _find_matching_rules(self, alert: Alert) -> List[AlertRule]:
        """
        Find rules matching alert.

        Args:
            alert: Alert

        Returns:
            List of matching rules
        """
        matching = []

        for rule in self.rules:
            if alert.severity in rule.severity_levels:
                matching.append(rule)

        return matching

    def _check_frequency_limit(
        self,
        alert: Alert,
        rule: AlertRule
    ) -> bool:
        """
        Check if alert exceeds frequency limit.

        Args:
            alert: Alert
            rule: Routing rule

        Returns:
            True if within limit, False if exceeded
        """
        key = f"{rule.name}:{alert.model_name}:{alert.metric_name}"
        recent_alerts = self.alert_counts[key]

        # Remove old alerts outside window
        cutoff = datetime.now() - rule.frequency_window
        while recent_alerts and recent_alerts[0] < cutoff:
            recent_alerts.popleft()

        # Check count
        return len(recent_alerts) < rule.max_frequency

    def _track_alert_frequency(self, alert: Alert, rule: AlertRule):
        """Track alert for frequency limiting."""
        key = f"{rule.name}:{alert.model_name}:{alert.metric_name}"
        self.alert_counts[key].append(alert.timestamp)

    async def _deliver_alert(self, alert: Alert, rule: AlertRule):
        """
        Deliver alert through configured channels.

        Args:
            alert: Alert to deliver
            rule: Routing rule
        """
        tasks = []

        for channel in rule.channels:
            if channel == AlertChannel.EMAIL:
                tasks.append(self._send_email(alert, rule))
            elif channel == AlertChannel.SLACK:
                tasks.append(self._send_slack(alert, rule))
            elif channel == AlertChannel.PAGERDUTY:
                tasks.append(self._send_pagerduty(alert, rule))
            elif channel == AlertChannel.SERVICENOW:
                tasks.append(self._send_servicenow(alert, rule))

        # Execute all deliveries concurrently
        await asyncio.gather(*tasks, return_exceptions=True)

    async def _send_email(self, alert: Alert, rule: AlertRule):
        """Send alert via email."""
        if not self.email_config:
            return

        # Implementation would use SMTP or email service
        logger.info(
            f"Sending email alert to {rule.recipients}: {alert.message}"
        )

        # Example implementation (pseudocode)
        # await send_email(
        #     to=rule.recipients,
        #     subject=f"[{alert.severity.name}] {alert.metric_name}",
        #     body=self._format_alert_email(alert)
        # )

    async def _send_slack(self, alert: Alert, rule: AlertRule):
        """Send alert via Slack."""
        if not self.slack_webhook:
            return

        import aiohttp

        # Format Slack message
        payload = {
            "text": f":warning: *{alert.severity.name}* Alert",
            "attachments": [{
                "color": self._severity_color(alert.severity),
                "fields": [
                    {"title": "Model", "value": alert.model_name, "short": True},
                    {"title": "Metric", "value": alert.metric_name, "short": True},
                    {"title": "Value", "value": str(alert.value), "short": True},
                    {"title": "Threshold", "value": str(alert.threshold), "short": True},
                    {"title": "Message", "value": alert.message, "short": False}
                ],
                "footer": f"Alert ID: {alert.alert_id}",
                "ts": int(alert.timestamp.timestamp())
            }]
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.slack_webhook, json=payload) as resp:
                    if resp.status != 200:
                        logger.error(f"Slack webhook failed: {resp.status}")
        except Exception as e:
            logger.error(f"Failed to send Slack alert: {e}")

    async def _send_pagerduty(self, alert: Alert, rule: AlertRule):
        """Send alert via PagerDuty."""
        if not self.pagerduty_api_key:
            return

        import aiohttp

        # PagerDuty Events API v2
        url = "https://events.pagerduty.com/v2/enqueue"

        payload = {
            "routing_key": self.pagerduty_api_key,
            "event_action": "trigger",
            "dedup_key": f"{alert.model_name}:{alert.metric_name}",
            "payload": {
                "summary": alert.message,
                "severity": alert.severity.name.lower(),
                "source": alert.model_name,
                "custom_details": {
                    "metric": alert.metric_name,
                    "value": alert.value,
                    "threshold": alert.threshold,
                    **alert.context
                }
            }
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as resp:
                    if resp.status != 202:
                        logger.error(f"PagerDuty failed: {resp.status}")
                    else:
                        logger.info(f"PagerDuty alert sent: {alert.alert_id}")
        except Exception as e:
            logger.error(f"Failed to send PagerDuty alert: {e}")

    async def _send_servicenow(self, alert: Alert, rule: AlertRule):
        """Create ServiceNow incident."""
        if not self.servicenow_config:
            return

        import aiohttp

        # ServiceNow Table API
        instance = self.servicenow_config.get('instance')
        username = self.servicenow_config.get('username')
        password = self.servicenow_config.get('password')

        url = f"https://{instance}.service-now.com/api/now/table/incident"

        # Map severity to ServiceNow impact/urgency
        severity_map = {
            AlertSeverity.CRITICAL: {'impact': '1', 'urgency': '1'},
            AlertSeverity.ERROR: {'impact': '2', 'urgency': '2'},
            AlertSeverity.WARNING: {'impact': '3', 'urgency': '3'},
            AlertSeverity.INFO: {'impact': '3', 'urgency': '3'}
        }

        mapping = severity_map[alert.severity]

        payload = {
            "short_description": f"{alert.model_name}: {alert.message}",
            "description": self._format_servicenow_description(alert),
            "impact": mapping['impact'],
            "urgency": mapping['urgency'],
            "category": "ML Model Monitoring",
            "subcategory": alert.metric_name,
            "assignment_group": self.servicenow_config.get('assignment_group', 'ML Ops')
        }

        try:
            auth = aiohttp.BasicAuth(username, password)
            headers = {'Content-Type': 'application/json'}

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    url,
                    json=payload,
                    auth=auth,
                    headers=headers
                ) as resp:
                    if resp.status == 201:
                        result = await resp.json()
                        incident_number = result['result']['number']
                        logger.info(
                            f"ServiceNow incident created: {incident_number}"
                        )
                    else:
                        logger.error(f"ServiceNow failed: {resp.status}")
        except Exception as e:
            logger.error(f"Failed to create ServiceNow incident: {e}")

    async def _schedule_escalation(self, alert: Alert, rule: AlertRule):
        """
        Schedule alert escalation.

        Args:
            alert: Alert
            rule: Routing rule
        """
        await asyncio.sleep(rule.escalation_delay)

        # Check if alert was acknowledged/resolved
        if alert.status in [AlertStatus.ACKNOWLEDGED, AlertStatus.RESOLVED]:
            return

        # Escalate
        alert.escalation_level += 1

        logger.warning(
            f"Escalating alert {alert.alert_id} to level {alert.escalation_level}"
        )

        # Send to escalation channels (implement based on policy)
        # This would trigger additional notifications to management, etc.

    def _track_sla(self, alert: Alert, rules: List[AlertRule]):
        """
        Track SLA for alert response.

        Args:
            alert: Alert
            rules: Matching rules
        """
        for rule in rules:
            if rule.sla_response_time > 0:
                # Schedule SLA check
                asyncio.create_task(self._check_sla(alert, rule))

    async def _check_sla(self, alert: Alert, rule: AlertRule):
        """Check if SLA was met."""
        await asyncio.sleep(rule.sla_response_time)

        # Check if alert was acknowledged within SLA
        if alert.status == AlertStatus.OPEN:
            # SLA violation
            self.sla_violations.append({
                'alert_id': alert.alert_id,
                'rule': rule.name,
                'sla_time': rule.sla_response_time,
                'timestamp': datetime.now()
            })

            logger.error(
                f"SLA violation: Alert {alert.alert_id} not acknowledged "
                f"within {rule.sla_response_time}s"
            )

    def acknowledge_alert(self, alert_id: str):
        """
        Acknowledge an alert.

        Args:
            alert_id: Alert ID
        """
        if alert_id in self.alerts:
            alert = self.alerts[alert_id]
            alert.status = AlertStatus.ACKNOWLEDGED
            alert.acknowledged_at = datetime.now()

            logger.info(f"Alert acknowledged: {alert_id}")

    def resolve_alert(self, alert_id: str):
        """
        Resolve an alert.

        Args:
            alert_id: Alert ID
        """
        if alert_id in self.alerts:
            alert = self.alerts[alert_id]
            alert.status = AlertStatus.RESOLVED
            alert.resolved_at = datetime.now()

            logger.info(f"Alert resolved: {alert_id}")

    def _severity_color(self, severity: AlertSeverity) -> str:
        """Get color for severity level."""
        colors = {
            AlertSeverity.INFO: "good",
            AlertSeverity.WARNING: "warning",
            AlertSeverity.ERROR: "danger",
            AlertSeverity.CRITICAL: "danger"
        }
        return colors.get(severity, "warning")

    def _format_alert_email(self, alert: Alert) -> str:
        """Format alert for email."""
        return f"""
Alert: {alert.message}

Model: {alert.model_name}
Metric: {alert.metric_name}
Severity: {alert.severity.name}

Current Value: {alert.value}
Threshold: {alert.threshold}

Timestamp: {alert.timestamp}
Alert ID: {alert.alert_id}

Context:
{json.dumps(alert.context, indent=2)}
"""

    def _format_servicenow_description(self, alert: Alert) -> str:
        """Format alert description for ServiceNow."""
        return f"""
ML Model Alert

Model Name: {alert.model_name}
Metric: {alert.metric_name}
Severity: {alert.severity.name}

Alert Message:
{alert.message}

Details:
- Current Value: {alert.value}
- Threshold: {alert.threshold}
- Timestamp: {alert.timestamp}

Alert ID: {alert.alert_id}

Additional Context:
{json.dumps(alert.context, indent=2)}
"""

    def get_alert_stats(self) -> Dict[str, Any]:
        """Get alert statistics."""
        total_alerts = len(self.alert_history)
        by_severity = defaultdict(int)
        by_status = defaultdict(int)

        for alert in self.alert_history:
            by_severity[alert.severity.name] += 1
            by_status[alert.status.name] += 1

        return {
            'total_alerts': total_alerts,
            'active_alerts': len([
                a for a in self.alerts.values()
                if a.status == AlertStatus.OPEN
            ]),
            'by_severity': dict(by_severity),
            'by_status': dict(by_status),
            'sla_violations': len(self.sla_violations),
            'suppressed_alerts': len(self.suppressed_alerts)
        }
\end{lstlisting}

\subsection{Configuration Examples and Deployment Patterns}

\subsubsection{High-Availability Monitoring Configuration}

\begin{lstlisting}[language=Python, caption={Production Monitoring Configuration}]
# config/monitoring_production.yaml
monitoring:
  # Model Monitor Configuration
  model_monitor:
    num_workers: 8  # Scale based on traffic
    batch_size: 500
    flush_interval: 30  # seconds
    max_queue_size: 50000

    # Observability integration
    observability:
      prometheus_gateway: "prometheus-pushgateway.monitoring.svc:9091"
      prometheus_port: 8000
      jaeger_endpoint: "http://jaeger-collector.monitoring.svc:14268/api/traces"
      elasticsearch_hosts:
        - "http://elasticsearch.monitoring.svc:9200"
      service_name: "ml-prediction-service"
      environment: "production"

  # Alert Manager Configuration
  alert_manager:
    # Channel configurations
    email:
      smtp_host: "smtp.company.com"
      smtp_port: 587
      from_addr: "ml-alerts@company.com"
      use_tls: true

    slack:
      webhook_url: "${SLACK_WEBHOOK_URL}"  # From environment
      default_channel: "#ml-alerts"

    pagerduty:
      api_key: "${PAGERDUTY_API_KEY}"
      service_id: "PXXXXXX"

    servicenow:
      instance: "company"
      username: "${SERVICENOW_USER}"
      password: "${SERVICENOW_PASS}"
      assignment_group: "ML Operations"

    # Alert Rules
    rules:
      - name: "critical_model_alerts"
        severity_levels: ["CRITICAL"]
        channels: ["pagerduty", "slack", "servicenow"]
        recipients:
          - "oncall-ml@company.com"
          - "#incidents"
        max_frequency: 5
        frequency_window_hours: 1
        suppress_similar: true
        escalation_delay_seconds: 300
        sla_response_time_seconds: 900

      - name: "error_alerts"
        severity_levels: ["ERROR"]
        channels: ["slack", "email"]
        recipients:
          - "ml-team@company.com"
          - "#ml-monitoring"
        max_frequency: 10
        frequency_window_hours: 1
        suppress_similar: true
        escalation_delay_seconds: 600
        sla_response_time_seconds: 1800

      - name: "warning_alerts"
        severity_levels: ["WARNING"]
        channels: ["slack"]
        recipients: ["#ml-monitoring"]
        max_frequency: 20
        frequency_window_hours: 1
        suppress_similar: true

    # Escalation Policies
    escalation_policies:
      - name: "critical_escalation"
        levels:
          - channels: ["pagerduty", "slack"]
            recipients: ["oncall-ml@company.com", "#incidents"]
          - channels: ["pagerduty", "slack", "email"]
            recipients: ["ml-manager@company.com", "#leadership"]
          - channels: ["pagerduty", "email"]
            recipients: ["vp-engineering@company.com"]
        escalation_intervals_seconds: [300, 600, 900]

  # High Availability Configuration
  ha:
    # Run multiple monitor instances
    num_instances: 3

    # Use distributed queue (Redis, RabbitMQ, Kafka)
    queue_backend: "redis"
    redis_url: "redis://redis-cluster.monitoring.svc:6379"

    # Leader election for coordination
    leader_election: true
    leader_election_backend: "etcd"
    etcd_endpoints:
      - "http://etcd-1.monitoring.svc:2379"
      - "http://etcd-2.monitoring.svc:2379"
      - "http://etcd-3.monitoring.svc:2379"

    # Heartbeat configuration
    heartbeat_interval_seconds: 30
    heartbeat_timeout_seconds: 90

  # Performance Tuning
  performance:
    # Async processing
    event_loop: "uvloop"  # Faster event loop

    # Connection pooling
    http_pool_size: 100
    db_pool_size: 20

    # Caching
    enable_caching: true
    cache_backend: "redis"
    cache_ttl_seconds: 300

    # Batching
    batch_processing: true
    batch_timeout_ms: 100
    max_batch_size: 1000
\end{lstlisting}

\subsubsection{Kubernetes Deployment for High Availability}

\begin{lstlisting}[language=YAML, caption={Kubernetes Deployment for ML Monitoring}]
# k8s/monitoring-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-monitoring
  namespace: ml-platform
  labels:
    app: ml-monitoring
    component: monitoring
spec:
  replicas: 3  # High availability
  selector:
    matchLabels:
      app: ml-monitoring
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero downtime
  template:
    metadata:
      labels:
        app: ml-monitoring
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      # Anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: ml-monitoring
              topologyKey: kubernetes.io/hostname

      containers:
      - name: monitoring
        image: company/ml-monitoring:v1.2.0
        imagePullPolicy: Always

        # Resource limits for production
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "2000m"
            memory: "4Gi"

        env:
        - name: ENVIRONMENT
          value: "production"
        - name: NUM_WORKERS
          value: "8"
        - name: BATCH_SIZE
          value: "500"

        # Secrets from Kubernetes secrets
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: ml-monitoring-secrets
              key: slack-webhook
        - name: PAGERDUTY_API_KEY
          valueFrom:
            secretKeyRef:
              name: ml-monitoring-secrets
              key: pagerduty-key

        # Health checks
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

        ports:
        - containerPort: 8080
          name: http
        - containerPort: 8000
          name: metrics

        volumeMounts:
        - name: config
          mountPath: /etc/monitoring/config.yaml
          subPath: config.yaml
        - name: cache
          mountPath: /var/cache/monitoring

      volumes:
      - name: config
        configMap:
          name: ml-monitoring-config
      - name: cache
        emptyDir: {}
---
# Service for monitoring
apiVersion: v1
kind: Service
metadata:
  name: ml-monitoring
  namespace: ml-platform
spec:
  type: ClusterIP
  selector:
    app: ml-monitoring
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  - port: 8000
    targetPort: 8000
    name: metrics
---
# HorizontalPodAutoscaler for auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-monitoring-hpa
  namespace: ml-platform
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-monitoring
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
\end{lstlisting}

\subsection{Performance Considerations at Scale}

\subsubsection{Optimization Strategies}

\begin{enumerate}
    \item \textbf{Batching}: Process metrics in batches to reduce overhead
    \begin{itemize}
        \item Typical batch size: 100-1000 metrics
        \item Flush interval: 10-60 seconds
        \item Trade-off: Latency vs throughput
    \end{itemize}

    \item \textbf{Async Processing}: Use async I/O for non-blocking operations
    \begin{itemize}
        \item Event loop: uvloop for 2-4x performance boost
        \item Concurrent workers: Match to CPU cores (4-16 workers)
        \item Connection pooling: Reuse HTTP/DB connections
    \end{itemize}

    \item \textbf{Sampling}: Sample high-volume metrics
    \begin{itemize}
        \item Reservoir sampling for uniform distribution
        \item Stratified sampling for important subsets
        \item Typical sample rate: 1-10\% for very high volume
    \end{itemize}

    \item \textbf{Distributed Architecture}: Scale horizontally
    \begin{itemize}
        \item Multiple monitor instances with shared queue
        \item Use Redis/Kafka for distributed queue
        \item Leader election for coordination tasks
    \end{itemize}

    \item \textbf{Caching}: Cache computed metrics
    \begin{itemize}
        \item Cache aggregations for dashboards
        \item TTL: 1-5 minutes for most metrics
        \item Use Redis for distributed cache
    \end{itemize}
\end{enumerate}

\subsubsection{Scalability Benchmarks}

Expected performance at different scales:

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metric} & \textbf{Small} & \textbf{Medium} & \textbf{Large} \\
\hline
Predictions/sec & 100 & 10,000 & 100,000 \\
Monitor instances & 1 & 3 & 10 \\
Workers per instance & 4 & 8 & 16 \\
Batch size & 100 & 500 & 1000 \\
Flush interval (s) & 60 & 30 & 10 \\
Queue size & 1,000 & 10,000 & 100,000 \\
Memory per instance & 512MB & 2GB & 4GB \\
CPU per instance & 0.5 cores & 1 core & 2 cores \\
\hline
\end{tabular}
\caption{Scalability configuration by system size}
\end{table}

\section{Performance Tracking and Model Decay}

Model performance degrades over time due to data drift, concept drift, or changing patterns. Detecting decay early enables timely retraining.

\subsection{PerformanceTracker: Sliding Window Analysis}

\begin{lstlisting}[language=Python, caption={Performance Tracking with Decay Detection}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque
import numpy as np
from scipy import stats
import logging

logger = logging.getLogger(__name__)

@dataclass
class PerformanceWindow:
    """
    Performance metrics for a time window.

    Attributes:
        start_time: Window start
        end_time: Window end
        metrics: Dictionary of metric values
        sample_count: Number of samples in window
    """
    start_time: datetime
    end_time: datetime
    metrics: Dict[str, float]
    sample_count: int

@dataclass
class DecayDetectionResult:
    """
    Result of model decay analysis.

    Attributes:
        metric_name: Name of metric analyzed
        decay_detected: Whether decay was detected
        current_value: Current metric value
        baseline_value: Baseline/reference value
        change_percent: Percentage change from baseline
        trend: Trend direction ('improving', 'stable', 'declining')
        confidence: Statistical confidence (0-1)
        trigger_retrain: Whether retraining should be triggered
    """
    metric_name: str
    decay_detected: bool
    current_value: float
    baseline_value: float
    change_percent: float
    trend: str
    confidence: float
    trigger_retrain: bool

class PerformanceTracker:
    """
    Track model performance with sliding window analysis.

    Detects performance decay and triggers retraining when needed.

    Example:
        >>> tracker = PerformanceTracker(
        ...     window_size=timedelta(days=7),
        ...     decay_threshold=0.05
        ... )
        >>> tracker.record_prediction(
        ...     y_true=1,
        ...     y_pred=1,
        ...     y_prob=0.95
        ... )
        >>> decay_result = tracker.check_decay()
    """

    def __init__(
        self,
        window_size: timedelta = timedelta(days=7),
        slide_interval: timedelta = timedelta(hours=1),
        decay_threshold: float = 0.05,
        min_samples: int = 100,
        retrain_threshold: float = 0.10
    ):
        """
        Initialize performance tracker.

        Args:
            window_size: Size of sliding window
            slide_interval: How often to compute metrics
            decay_threshold: Threshold for decay detection (5% drop)
            min_samples: Minimum samples for reliable metrics
            retrain_threshold: Threshold for triggering retrain (10% drop)
        """
        self.window_size = window_size
        self.slide_interval = slide_interval
        self.decay_threshold = decay_threshold
        self.min_samples = min_samples
        self.retrain_threshold = retrain_threshold

        # Prediction storage
        self.predictions: deque = deque()

        # Performance windows
        self.windows: List[PerformanceWindow] = []

        # Baseline metrics (from initial period)
        self.baseline_metrics: Optional[Dict[str, float]] = None
        self.baseline_set = False

        # Last computation time
        self.last_computation = datetime.now()

    def record_prediction(
        self,
        y_true: Any,
        y_pred: Any,
        y_prob: Optional[np.ndarray] = None,
        metadata: Optional[Dict] = None
    ):
        """
        Record a prediction for tracking.

        Args:
            y_true: Ground truth label
            y_pred: Predicted label
            y_prob: Prediction probabilities (if available)
            metadata: Additional metadata
        """
        self.predictions.append({
            'timestamp': datetime.now(),
            'y_true': y_true,
            'y_pred': y_pred,
            'y_prob': y_prob,
            'metadata': metadata or {}
        })

        # Compute metrics if interval elapsed
        if datetime.now() - self.last_computation >= self.slide_interval:
            self._compute_window_metrics()

    def _compute_window_metrics(self):
        """Compute metrics for current window."""
        now = datetime.now()
        window_start = now - self.window_size

        # Filter predictions in window
        window_preds = [
            p for p in self.predictions
            if p['timestamp'] >= window_start
        ]

        if len(window_preds) < self.min_samples:
            logger.debug(
                f"Insufficient samples in window: {len(window_preds)}"
            )
            return

        # Extract arrays
        y_true = np.array([p['y_true'] for p in window_preds])
        y_pred = np.array([p['y_pred'] for p in window_preds])

        # Compute metrics
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score,
            f1_score, roc_auc_score
        )

        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(
                y_true, y_pred, average='weighted', zero_division=0
            ),
            'recall': recall_score(
                y_true, y_pred, average='weighted', zero_division=0
            ),
            'f1': f1_score(
                y_true, y_pred, average='weighted', zero_division=0
            )
        }

        # Add AUC if probabilities available
        if window_preds[0]['y_prob'] is not None:
            y_prob = np.array([p['y_prob'] for p in window_preds])
            try:
                metrics['auc'] = roc_auc_score(
                    y_true, y_prob, average='weighted', multi_class='ovr'
                )
            except ValueError:
                pass  # Not enough classes

        # Create window
        window = PerformanceWindow(
            start_time=window_start,
            end_time=now,
            metrics=metrics,
            sample_count=len(window_preds)
        )

        self.windows.append(window)
        self.last_computation = now

        # Set baseline if first window
        if not self.baseline_set and len(self.windows) >= 3:
            # Use average of first 3 windows as baseline
            self._set_baseline()

        # Clean old windows
        self._clean_old_windows()

        logger.debug(f"Computed window metrics: {metrics}")

    def _set_baseline(self):
        """Set baseline metrics from initial windows."""
        baseline_windows = self.windows[:3]

        metric_names = baseline_windows[0].metrics.keys()
        self.baseline_metrics = {}

        for metric_name in metric_names:
            values = [
                w.metrics[metric_name]
                for w in baseline_windows
            ]
            self.baseline_metrics[metric_name] = np.mean(values)

        self.baseline_set = True
        logger.info(f"Baseline metrics set: {self.baseline_metrics}")

    def _clean_old_windows(self):
        """Remove windows older than needed for analysis."""
        # Keep last 30 days of windows
        cutoff = datetime.now() - timedelta(days=30)
        self.windows = [
            w for w in self.windows
            if w.end_time >= cutoff
        ]

        # Clean old predictions
        cutoff_preds = datetime.now() - self.window_size
        while self.predictions and self.predictions[0]['timestamp'] < cutoff_preds:
            self.predictions.popleft()

    def check_decay(self) -> List[DecayDetectionResult]:
        """
        Check for performance decay.

        Returns:
            List of decay detection results
        """
        if not self.baseline_set or not self.windows:
            return []

        # Get recent window metrics
        recent_window = self.windows[-1]

        results = []

        for metric_name, baseline_value in self.baseline_metrics.items():
            current_value = recent_window.metrics[metric_name]

            # Calculate change
            change_percent = (
                (current_value - baseline_value) / baseline_value
            )

            # Determine trend
            if len(self.windows) >= 5:
                recent_values = [
                    w.metrics[metric_name]
                    for w in self.windows[-5:]
                ]
                trend, confidence = self._analyze_trend(recent_values)
            else:
                trend = 'unknown'
                confidence = 0.0

            # Detect decay (performance drop)
            decay_detected = change_percent < -self.decay_threshold
            trigger_retrain = change_percent < -self.retrain_threshold

            result = DecayDetectionResult(
                metric_name=metric_name,
                decay_detected=decay_detected,
                current_value=current_value,
                baseline_value=baseline_value,
                change_percent=change_percent,
                trend=trend,
                confidence=confidence,
                trigger_retrain=trigger_retrain
            )

            results.append(result)

            # Log significant changes
            if decay_detected:
                logger.warning(
                    f"Performance decay detected for {metric_name}: "
                    f"{change_percent:.1%} drop "
                    f"(current: {current_value:.4f}, "
                    f"baseline: {baseline_value:.4f})"
                )

            if trigger_retrain:
                logger.critical(
                    f"Retraining threshold exceeded for {metric_name}"
                )

        return results

    def _analyze_trend(
        self,
        values: List[float]
    ) -> Tuple[str, float]:
        """
        Analyze trend in metric values.

        Args:
            values: List of metric values over time

        Returns:
            Tuple of (trend direction, confidence)
        """
        x = np.arange(len(values))
        y = np.array(values)

        # Linear regression
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)

        # Determine trend
        if abs(slope) < 0.001:  # Nearly flat
            trend = 'stable'
        elif slope > 0:
            trend = 'improving'
        else:
            trend = 'declining'

        # Confidence is R-squared
        confidence = r_value ** 2

        return trend, confidence

    def get_performance_summary(self) -> Dict[str, Any]:
        """
        Get summary of performance tracking.

        Returns:
            Dictionary with performance statistics
        """
        if not self.windows:
            return {'status': 'no_data'}

        recent_window = self.windows[-1]

        summary = {
            'timestamp': recent_window.end_time.isoformat(),
            'window_size_days': self.window_size.days,
            'sample_count': recent_window.sample_count,
            'current_metrics': recent_window.metrics,
            'baseline_metrics': self.baseline_metrics,
            'total_windows': len(self.windows),
            'total_predictions': len(self.predictions)
        }

        # Add decay information if baseline set
        if self.baseline_set:
            decay_results = self.check_decay()
            summary['decay_results'] = [
                {
                    'metric': r.metric_name,
                    'decay_detected': r.decay_detected,
                    'change_percent': r.change_percent,
                    'trend': r.trend,
                    'trigger_retrain': r.trigger_retrain
                }
                for r in decay_results
            ]

        return summary

    def should_retrain(self) -> bool:
        """
        Determine if model should be retrained.

        Returns:
            True if retraining is recommended
        """
        if not self.baseline_set:
            return False

        decay_results = self.check_decay()

        # Retrain if any metric exceeds threshold
        return any(r.trigger_retrain for r in decay_results)
\end{lstlisting}

\subsection{Automated Retraining Triggers}

\begin{lstlisting}[language=Python, caption={Retraining Pipeline with Triggers}]
from typing import Optional
from pathlib import Path
import joblib

class AutoRetrainingPipeline:
    """
    Automated model retraining pipeline.

    Monitors performance and triggers retraining when needed.
    """

    def __init__(
        self,
        model_class,
        performance_tracker: PerformanceTracker,
        drift_detector: DriftDetector,
        model_monitor: ModelMonitor
    ):
        """
        Initialize retraining pipeline.

        Args:
            model_class: Model class to instantiate for retraining
            performance_tracker: Performance tracking system
            drift_detector: Drift detection system
            model_monitor: Model monitoring system
        """
        self.model_class = model_class
        self.performance_tracker = performance_tracker
        self.drift_detector = drift_detector
        self.model_monitor = model_monitor

        self.current_model = None
        self.retraining_in_progress = False
        self.last_retrain_time: Optional[datetime] = None
        self.min_retrain_interval = timedelta(days=7)

    def check_retraining_triggers(
        self,
        current_data: pd.DataFrame
    ) -> Tuple[bool, List[str]]:
        """
        Check if retraining should be triggered.

        Args:
            current_data: Current production data

        Returns:
            Tuple of (should_retrain, reasons)
        """
        reasons = []

        # Check if minimum interval has passed
        if self.last_retrain_time:
            time_since_retrain = datetime.now() - self.last_retrain_time
            if time_since_retrain < self.min_retrain_interval:
                return False, ["Minimum retrain interval not reached"]

        # Check performance decay
        if self.performance_tracker.should_retrain():
            reasons.append("Performance decay threshold exceeded")

        # Check data drift
        drift_results = self.drift_detector.detect_drift(current_data)
        drift_summary = self.drift_detector.get_drift_summary(drift_results)

        if drift_summary['drift_rate'] > 0.5:  # 50% of features
            reasons.append(
                f"Significant data drift: {drift_summary['drift_rate']:.1%}"
            )

        # Check alert status
        if len(self.model_monitor.active_alerts) > 3:
            reasons.append("Multiple active alerts")

        should_retrain = len(reasons) > 0

        return should_retrain, reasons

    def trigger_retraining(
        self,
        training_data: pd.DataFrame,
        validation_data: pd.DataFrame,
        reasons: List[str]
    ):
        """
        Trigger model retraining.

        Args:
            training_data: Data for retraining
            validation_data: Data for validation
            reasons: Reasons for retraining
        """
        if self.retraining_in_progress:
            logger.warning("Retraining already in progress")
            return

        self.retraining_in_progress = True

        logger.info(f"Triggering retraining. Reasons: {reasons}")

        try:
            # Extract features and targets
            X_train = training_data.drop('target', axis=1)
            y_train = training_data['target']
            X_val = validation_data.drop('target', axis=1)
            y_val = validation_data['target']

            # Train new model
            logger.info("Training new model")
            new_model = self.model_class()
            new_model.fit(X_train, y_train)

            # Validate new model
            val_score = new_model.score(X_val, y_val)
            logger.info(f"New model validation score: {val_score:.4f}")

            # Compare with current model
            if self.current_model:
                current_score = self.current_model.score(X_val, y_val)
                logger.info(
                    f"Current model validation score: {current_score:.4f}"
                )

                # Only replace if new model is better
                if val_score <= current_score:
                    logger.warning(
                        "New model not better than current model"
                    )
                    self.retraining_in_progress = False
                    return

            # Replace model
            self.current_model = new_model
            self.last_retrain_time = datetime.now()

            # Save model
            model_path = self._save_model(new_model)
            logger.info(f"Model saved to {model_path}")

            # Reset performance tracker baseline
            self.performance_tracker.baseline_set = False
            self.performance_tracker.windows = []

            # Clear active alerts
            self.model_monitor.active_alerts.clear()

            logger.info("Retraining completed successfully")

        except Exception as e:
            logger.error(f"Retraining failed: {e}")
            raise
        finally:
            self.retraining_in_progress = False

    def _save_model(self, model) -> Path:
        """Save model with timestamp."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = Path(f"models/model_{timestamp}.pkl")
        model_path.parent.mkdir(exist_ok=True)

        joblib.dump(model, model_path)

        return model_path

    def monitor_and_retrain(
        self,
        current_data: pd.DataFrame,
        training_data_fn: Callable[[], Tuple[pd.DataFrame, pd.DataFrame]]
    ):
        """
        Main monitoring loop with automatic retraining.

        Args:
            current_data: Current production data
            training_data_fn: Function to fetch training/validation data
        """
        # Check triggers
        should_retrain, reasons = self.check_retraining_triggers(
            current_data
        )

        if should_retrain:
            logger.warning(
                f"Retraining triggered. Reasons: {reasons}"
            )

            # Fetch training data
            training_data, validation_data = training_data_fn()

            # Trigger retraining
            self.trigger_retraining(
                training_data,
                validation_data,
                reasons
            )
        else:
            logger.info("No retraining needed")

# Usage
pipeline = AutoRetrainingPipeline(
    model_class=RandomForestClassifier,
    performance_tracker=tracker,
    drift_detector=drift_detector,
    model_monitor=monitor
)

# In production loop
def monitoring_loop():
    """Main monitoring loop."""
    while True:
        # Get current data
        current_data = fetch_recent_data()

        # Check and retrain if needed
        pipeline.monitor_and_retrain(
            current_data,
            training_data_fn=fetch_training_data
        )

        # Sleep
        time.sleep(3600)  # Check every hour
\end{lstlisting}

\section{Infrastructure and Operational Monitoring}

Beyond model metrics, infrastructure health is critical for reliable ML systems.

\subsection{AlertManager: Intelligent Alert Routing}

\begin{lstlisting}[language=Python, caption={Alert Management System}]
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import requests
import logging

logger = logging.getLogger(__name__)

class AlertChannel(Enum):
    """Alert delivery channels."""
    EMAIL = "email"
    SLACK = "slack"
    PAGERDUTY = "pagerduty"
    WEBHOOK = "webhook"
    LOG = "log"

@dataclass
class AlertRule:
    """
    Rule for alert routing and escalation.

    Attributes:
        name: Rule identifier
        severity_levels: Severities this rule applies to
        channels: Delivery channels
        recipients: List of recipients (emails, slack channels, etc.)
        escalation_delay: Time before escalating
        max_frequency: Maximum alerts per time period
        suppress_similar: Whether to suppress similar alerts
    """
    name: str
    severity_levels: List[AlertSeverity]
    channels: List[AlertChannel]
    recipients: List[str]
    escalation_delay: Optional[timedelta] = None
    max_frequency: int = 10
    frequency_window: timedelta = timedelta(hours=1)
    suppress_similar: bool = True

class AlertManager:
    """
    Intelligent alert management with routing and escalation.

    Prevents alert fatigue through deduplication, rate limiting,
    and intelligent routing.

    Example:
        >>> alert_mgr = AlertManager()
        >>> alert_mgr.add_rule(AlertRule(
        ...     name="critical_alerts",
        ...     severity_levels=[AlertSeverity.CRITICAL],
        ...     channels=[AlertChannel.PAGERDUTY, AlertChannel.SLACK],
        ...     recipients=["oncall@company.com", "#incidents"]
        ... ))
        >>> alert_mgr.send_alert(alert)
    """

    def __init__(
        self,
        email_config: Optional[Dict] = None,
        slack_webhook: Optional[str] = None,
        pagerduty_key: Optional[str] = None
    ):
        """
        Initialize alert manager.

        Args:
            email_config: SMTP configuration for email
            slack_webhook: Slack webhook URL
            pagerduty_key: PagerDuty integration key
        """
        self.email_config = email_config
        self.slack_webhook = slack_webhook
        self.pagerduty_key = pagerduty_key

        # Alert rules
        self.rules: List[AlertRule] = []

        # Alert history for rate limiting
        self.alert_history: Dict[str, List[datetime]] = defaultdict(list)

        # Suppressed alerts
        self.suppressed_alerts: Dict[str, Alert] = {}

    def add_rule(self, rule: AlertRule):
        """
        Add alert routing rule.

        Args:
            rule: Alert rule configuration
        """
        self.rules.append(rule)
        logger.info(f"Added alert rule: {rule.name}")

    def send_alert(self, alert: Alert):
        """
        Send alert through configured channels.

        Args:
            alert: Alert to send
        """
        # Find matching rules
        matching_rules = [
            rule for rule in self.rules
            if alert.severity in rule.severity_levels
        ]

        if not matching_rules:
            logger.warning(
                f"No matching rules for alert: {alert.metric_name}"
            )
            return

        for rule in matching_rules:
            # Check rate limiting
            if not self._check_rate_limit(rule, alert):
                logger.info(
                    f"Alert rate limited for rule {rule.name}"
                )
                continue

            # Check suppression
            if rule.suppress_similar and self._is_suppressed(alert):
                logger.info(
                    f"Alert suppressed (similar recent alert): "
                    f"{alert.metric_name}"
                )
                continue

            # Send through channels
            for channel in rule.channels:
                try:
                    if channel == AlertChannel.EMAIL:
                        self._send_email(alert, rule.recipients)
                    elif channel == AlertChannel.SLACK:
                        self._send_slack(alert, rule.recipients)
                    elif channel == AlertChannel.PAGERDUTY:
                        self._send_pagerduty(alert)
                    elif channel == AlertChannel.LOG:
                        self._send_log(alert)
                except Exception as e:
                    logger.error(
                        f"Failed to send alert via {channel.value}: {e}"
                    )

            # Record alert
            rule_key = f"{rule.name}_{alert.metric_name}"
            self.alert_history[rule_key].append(datetime.now())

            # Store for suppression
            if rule.suppress_similar:
                self.suppressed_alerts[alert.metric_name] = alert

    def _check_rate_limit(
        self,
        rule: AlertRule,
        alert: Alert
    ) -> bool:
        """
        Check if alert exceeds rate limit.

        Args:
            rule: Alert rule
            alert: Alert to check

        Returns:
            True if alert should be sent
        """
        rule_key = f"{rule.name}_{alert.metric_name}"

        # Get recent alerts
        cutoff = datetime.now() - rule.frequency_window
        recent_alerts = [
            ts for ts in self.alert_history.get(rule_key, [])
            if ts >= cutoff
        ]

        # Update history
        self.alert_history[rule_key] = recent_alerts

        # Check limit
        return len(recent_alerts) < rule.max_frequency

    def _is_suppressed(self, alert: Alert) -> bool:
        """
        Check if similar alert was recently sent.

        Args:
            alert: Alert to check

        Returns:
            True if alert should be suppressed
        """
        if alert.metric_name not in self.suppressed_alerts:
            return False

        previous = self.suppressed_alerts[alert.metric_name]

        # Suppress if within 15 minutes and similar severity
        time_diff = alert.timestamp - previous.timestamp
        similar_severity = alert.severity == previous.severity

        return time_diff < timedelta(minutes=15) and similar_severity

    def _send_email(self, alert: Alert, recipients: List[str]):
        """Send alert via email."""
        if not self.email_config:
            logger.warning("Email not configured")
            return

        # Create message
        msg = MIMEMultipart()
        msg['From'] = self.email_config['from']
        msg['To'] = ', '.join(recipients)
        msg['Subject'] = (
            f"[{alert.severity.value.upper()}] {alert.metric_name}"
        )

        # Create body
        body = f"""
ML Monitoring Alert

Severity: {alert.severity.value}
Metric: {alert.metric_name}
Message: {alert.message}

Current Value: {alert.value:.4f}
Threshold: {alert.threshold:.4f}
Timestamp: {alert.timestamp}

Context:
{json.dumps(alert.context, indent=2)}
        """

        msg.attach(MIMEText(body, 'plain'))

        # Send
        with smtplib.SMTP(
            self.email_config['host'],
            self.email_config['port']
        ) as server:
            if self.email_config.get('use_tls'):
                server.starttls()

            if 'username' in self.email_config:
                server.login(
                    self.email_config['username'],
                    self.email_config['password']
                )

            server.send_message(msg)

        logger.info(f"Email sent to {recipients}")

    def _send_slack(self, alert: Alert, channels: List[str]):
        """Send alert to Slack."""
        if not self.slack_webhook:
            logger.warning("Slack not configured")
            return

        # Severity emoji
        emoji_map = {
            AlertSeverity.INFO: ':information_source:',
            AlertSeverity.WARNING: ':warning:',
            AlertSeverity.ERROR: ':x:',
            AlertSeverity.CRITICAL: ':rotating_light:'
        }

        # Create payload
        payload = {
            "text": f"{emoji_map[alert.severity]} *ML Monitoring Alert*",
            "blocks": [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": f"{alert.severity.value.upper()}: {alert.metric_name}"
                    }
                },
                {
                    "type": "section",
                    "fields": [
                        {
                            "type": "mrkdwn",
                            "text": f"*Message:*\n{alert.message}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Current Value:*\n{alert.value:.4f}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Threshold:*\n{alert.threshold:.4f}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Time:*\n{alert.timestamp}"
                        }
                    ]
                }
            ]
        }

        # Send to webhook
        response = requests.post(
            self.slack_webhook,
            json=payload
        )
        response.raise_for_status()

        logger.info(f"Slack alert sent")

    def _send_pagerduty(self, alert: Alert):
        """Send alert to PagerDuty."""
        if not self.pagerduty_key:
            logger.warning("PagerDuty not configured")
            return

        # Only page for ERROR and CRITICAL
        if alert.severity not in [AlertSeverity.ERROR, AlertSeverity.CRITICAL]:
            return

        payload = {
            "routing_key": self.pagerduty_key,
            "event_action": "trigger",
            "payload": {
                "summary": alert.message,
                "severity": alert.severity.value,
                "source": alert.metric_name,
                "custom_details": {
                    "value": alert.value,
                    "threshold": alert.threshold,
                    "context": alert.context
                }
            }
        }

        response = requests.post(
            "https://events.pagerduty.com/v2/enqueue",
            json=payload
        )
        response.raise_for_status()

        logger.info("PagerDuty alert sent")

    def _send_log(self, alert: Alert):
        """Log alert."""
        level_map = {
            AlertSeverity.INFO: logging.INFO,
            AlertSeverity.WARNING: logging.WARNING,
            AlertSeverity.ERROR: logging.ERROR,
            AlertSeverity.CRITICAL: logging.CRITICAL
        }

        logger.log(
            level_map[alert.severity],
            f"ALERT: {alert.message} "
            f"(value={alert.value:.4f}, threshold={alert.threshold:.4f})"
        )
\end{lstlisting}

\section{Real-World Scenario: Silent Model Degradation}

\subsection{The Problem}

A credit scoring model was deployed in January 2024. By March, business teams noticed a 20\% increase in default rates among approved loans, costing the company \$2M in losses. Investigation revealed:

\begin{itemize}
    \item Model accuracy dropped from 89\% to 72\%
    \item Data drift affected 45\% of features due to economic changes
    \item Prediction latency increased 3x due to infrastructure issues
    \item No monitoring detected these issues for 8 weeks
\end{itemize}

\subsection{The Solution}

Implementing comprehensive monitoring would have caught this early:

\begin{lstlisting}[language=Python, caption={Complete Monitoring Implementation}]
# Initialize monitoring systems
model_monitor = ModelMonitor(
    model_name="credit_scoring",
    model_version="v1.0",
    prometheus_gateway="localhost:9091",
    alert_callback=lambda alert: alert_manager.send_alert(alert)
)

# Register critical metrics
model_monitor.register_metric(MetricConfig(
    name="accuracy",
    metric_type=MetricType.GAUGE,
    description="Model accuracy on recent predictions",
    thresholds={
        AlertSeverity.WARNING: 0.85,  # Alert at 85%
        AlertSeverity.CRITICAL: 0.75   # Critical at 75%
    }
))

model_monitor.register_metric(MetricConfig(
    name="default_rate",
    metric_type=MetricType.GAUGE,
    description="Rate of defaults among approved loans",
    thresholds={
        AlertSeverity.WARNING: 0.15,  # Alert at 15%
        AlertSeverity.CRITICAL: 0.20   # Critical at 20%
    }
))

# Initialize drift detection
drift_detector = DriftDetector(
    categorical_features=['employment_type', 'loan_purpose'],
    ks_threshold=0.05,
    psi_threshold=0.15  # Stricter threshold
)
drift_detector.fit(training_data)

# Initialize performance tracking
performance_tracker = PerformanceTracker(
    window_size=timedelta(days=7),
    decay_threshold=0.03,  # 3% decay triggers alert
    retrain_threshold=0.10  # 10% decay triggers retrain
)

# Configure alert manager
alert_manager = AlertManager(
    email_config=email_config,
    slack_webhook=slack_webhook
)

alert_manager.add_rule(AlertRule(
    name="critical_performance",
    severity_levels=[AlertSeverity.CRITICAL],
    channels=[AlertChannel.SLACK, AlertChannel.EMAIL],
    recipients=["ml-team@company.com", "#ml-alerts"]
))

alert_manager.add_rule(AlertRule(
    name="warning_performance",
    severity_levels=[AlertSeverity.WARNING],
    channels=[AlertChannel.SLACK],
    recipients=["#ml-monitoring"]
))

# Main monitoring loop
def production_monitoring():
    """Production monitoring with all systems."""
    while True:
        try:
            # Fetch recent predictions with ground truth
            recent_data = fetch_recent_predictions(hours=24)

            # Check drift
            drift_results = drift_detector.detect_drift(recent_data)
            drift_summary = drift_detector.get_drift_summary(drift_results)

            if drift_summary['drift_rate'] > 0.3:
                logger.warning(
                    f"Drift detected in {drift_summary['drift_rate']:.1%} "
                    f"of features"
                )

                # Log to monitoring system
                model_monitor.record_metric(
                    "drift_rate",
                    drift_summary['drift_rate']
                )

            # Update performance tracker
            for _, row in recent_data.iterrows():
                if 'ground_truth' in row:  # Only if labels available
                    performance_tracker.record_prediction(
                        y_true=row['ground_truth'],
                        y_pred=row['prediction'],
                        y_prob=row.get('probability')
                    )

            # Check for decay
            decay_results = performance_tracker.check_decay()

            for result in decay_results:
                model_monitor.record_metric(
                    result.metric_name,
                    result.current_value
                )

            # Check if retraining needed
            if performance_tracker.should_retrain():
                logger.critical("Model retraining required")

                # Trigger automated retraining
                trigger_retraining_pipeline()

            # Compute and log business metrics
            business_metrics = compute_business_metrics(recent_data)
            for metric_name, value in business_metrics.items():
                model_monitor.record_metric(metric_name, value)

            # Sleep
            time.sleep(3600)  # Check every hour

        except Exception as e:
            logger.error(f"Monitoring loop error: {e}")
            time.sleep(300)  # Retry after 5 minutes

# Start monitoring
if __name__ == "__main__":
    production_monitoring()
\end{lstlisting}

\subsection{Outcome}

With comprehensive monitoring:
\begin{itemize}
    \item \textbf{Week 2}: Drift detected in 3 key features (economic indicators changed)
    \item \textbf{Week 3}: Performance decay alert triggered (accuracy dropped to 85\%)
    \item \textbf{Week 4}: Automated retraining initiated, new model deployed
    \item \textbf{Impact}: Prevented \$1.8M in losses, maintained model performance
\end{itemize}

\section{Observability Best Practices}

\subsection{SLO and SLI Definition}

Define Service Level Objectives and Indicators for ML systems:

\begin{lstlisting}[language=Python, caption={SLO/SLI Implementation}]
from dataclasses import dataclass
from typing import Dict, List
from enum import Enum

class SLIType(Enum):
    """Types of Service Level Indicators."""
    AVAILABILITY = "availability"
    LATENCY = "latency"
    ACCURACY = "accuracy"
    THROUGHPUT = "throughput"
    ERROR_RATE = "error_rate"

@dataclass
class SLI:
    """
    Service Level Indicator.

    Measurable metric of service quality.
    """
    name: str
    sli_type: SLIType
    description: str
    measurement_window: timedelta
    target_value: float

    def __post_init__(self):
        self.measurements: List[float] = []

    def record(self, value: float):
        """Record SLI measurement."""
        self.measurements.append(value)

    def compute(self) -> float:
        """Compute current SLI value."""
        if not self.measurements:
            return 0.0

        if self.sli_type == SLIType.AVAILABILITY:
            # Availability: % of successful requests
            return np.mean(self.measurements)
        elif self.sli_type == SLIType.LATENCY:
            # Latency: 95th percentile
            return np.percentile(self.measurements, 95)
        elif self.sli_type == SLIType.ACCURACY:
            # Accuracy: mean accuracy
            return np.mean(self.measurements)
        elif self.sli_type == SLIType.THROUGHPUT:
            # Throughput: requests per second
            return len(self.measurements) / self.measurement_window.total_seconds()
        else:  # ERROR_RATE
            # Error rate: % of errors
            return np.mean(self.measurements)

@dataclass
class SLO:
    """
    Service Level Objective.

    Target for SLI performance.
    """
    name: str
    sli: SLI
    objective: float  # Target value
    time_period: timedelta  # Evaluation period

    def is_met(self) -> bool:
        """Check if SLO is met."""
        current_value = self.sli.compute()

        # For latency and error rate, lower is better
        if self.sli.sli_type in [SLIType.LATENCY, SLIType.ERROR_RATE]:
            return current_value <= self.objective
        else:
            return current_value >= self.objective

    def error_budget(self) -> float:
        """
        Calculate remaining error budget.

        Error budget = allowed failures before SLO breach
        """
        current_value = self.sli.compute()

        if self.sli.sli_type in [SLIType.LATENCY, SLIType.ERROR_RATE]:
            budget = self.objective - current_value
        else:
            budget = current_value - self.objective

        return budget

# Define SLOs for ML system
def define_ml_slos() -> List[SLO]:
    """Define SLOs for ML prediction service."""
    slos = []

    # Availability SLO: 99.9% uptime
    availability_sli = SLI(
        name="prediction_availability",
        sli_type=SLIType.AVAILABILITY,
        description="Percentage of successful predictions",
        measurement_window=timedelta(days=30),
        target_value=0.999
    )
    slos.append(SLO(
        name="99.9% Availability",
        sli=availability_sli,
        objective=0.999,
        time_period=timedelta(days=30)
    ))

    # Latency SLO: 95th percentile < 100ms
    latency_sli = SLI(
        name="prediction_latency_p95",
        sli_type=SLIType.LATENCY,
        description="95th percentile prediction latency",
        measurement_window=timedelta(days=7),
        target_value=0.100  # 100ms
    )
    slos.append(SLO(
        name="P95 Latency < 100ms",
        sli=latency_sli,
        objective=0.100,
        time_period=timedelta(days=7)
    ))

    # Accuracy SLO: > 85% accuracy
    accuracy_sli = SLI(
        name="model_accuracy",
        sli_type=SLIType.ACCURACY,
        description="Model prediction accuracy",
        measurement_window=timedelta(days=7),
        target_value=0.85
    )
    slos.append(SLO(
        name="Accuracy > 85%",
        sli=accuracy_sli,
        objective=0.85,
        time_period=timedelta(days=7)
    ))

    # Error rate SLO: < 0.1% errors
    error_sli = SLI(
        name="error_rate",
        sli_type=SLIType.ERROR_RATE,
        description="Prediction error rate",
        measurement_window=timedelta(days=30),
        target_value=0.001
    )
    slos.append(SLO(
        name="Error Rate < 0.1%",
        sli=error_sli,
        objective=0.001,
        time_period=timedelta(days=30)
    ))

    return slos

# Monitor SLOs
class SLOMonitor:
    """Monitor SLOs and trigger alerts on breach."""

    def __init__(self, slos: List[SLO], alert_manager: AlertManager):
        self.slos = slos
        self.alert_manager = alert_manager

    def check_slos(self):
        """Check all SLOs and alert on breach."""
        for slo in self.slos:
            if not slo.is_met():
                error_budget = slo.error_budget()

                # Create alert
                alert = Alert(
                    severity=AlertSeverity.CRITICAL,
                    metric_name=slo.name,
                    message=f"SLO breach: {slo.name}",
                    value=slo.sli.compute(),
                    threshold=slo.objective,
                    timestamp=datetime.now(),
                    context={
                        'sli_name': slo.sli.name,
                        'error_budget': error_budget,
                        'time_period': str(slo.time_period)
                    }
                )

                self.alert_manager.send_alert(alert)
\end{lstlisting}

\section{Enterprise Monitoring Scenarios and Lessons Learned}

This section presents five detailed enterprise scenarios demonstrating critical monitoring challenges, their business impact, and comprehensive solutions. Each scenario includes incident timelines, before/after configurations, quantified metrics, and actionable lessons learned.

\subsection{Scenario 1: The Silent Model Death}

\subsubsection{The Problem}

TechBank deployed a fraud detection model in January 2023 achieving 96\% accuracy. Without comprehensive monitoring, the model's performance gradually degraded over six months to 71\% accuracy—unnoticed until a major fraud ring exploited the weakness, resulting in \$12.4M in fraudulent transactions during Q2. The incident occurred because the team relied solely on monthly batch evaluations rather than continuous monitoring. Customer trust eroded as legitimate transactions were increasingly flagged (false positive rate rose from 2\% to 18\%), while actual fraud slipped through. The fraud pattern had shifted toward synthetic identity fraud, a pattern the model had minimal training data for, yet no drift detection was in place to identify this distributional change.

\subsubsection{Incident Timeline}

\begin{lstlisting}[language=bash, caption={Silent Model Death Timeline}]
Month 1 (Jan 2023):
  Day 1:  Model deployed with 96% accuracy, 2% FPR
  Day 15: First signs of drift (undetected)
  Day 30: Monthly evaluation: 95% accuracy (within tolerance)

Month 2 (Feb 2023):
  Day 15: Drift accelerates as fraud patterns evolve
  Day 28: Monthly evaluation: 92% accuracy (concerning but not alarming)

Month 3 (Mar 2023):
  Day 10: Major fraud ring begins testing model boundaries
  Day 20: False positive rate hits 8% (customer complaints increase)
  Day 31: Monthly evaluation: 87% accuracy (emergency meeting called)

Month 4 (Apr 2023):
  Day 5:  Fraud ring fully exploits model weaknesses
  Day 15: Customer support overwhelmed with complaints
  Day 25: \$4.2M in fraud losses detected
  Day 30: Monthly evaluation: 81% accuracy

Month 5 (May 2023):
  Day 10: Board demands explanation for rising fraud losses
  Day 15: Emergency audit reveals systematic model failure
  Day 20: Total fraud losses reach \$8.7M
  Day 31: Model accuracy: 75%

Month 6 (Jun 2023):
  Day 5:  Comprehensive monitoring finally implemented
  Day 7:  Real-time monitoring reveals catastrophic drift
  Day 10: Model rollback initiated, emergency retraining begins
  Day 15: New model deployed with continuous monitoring
  Day 30: Total fraud losses: \$12.4M, reputation damage severe
\end{lstlisting}

\subsubsection{Before Configuration}

\begin{lstlisting}[language=yaml, caption={Inadequate Monitoring Configuration}]
# TechBank's original monitoring (inadequate)
monitoring:
  evaluation_frequency: monthly  # Too infrequent!
  metrics:
    - accuracy  # Only aggregate metric

  alerts:
    accuracy_threshold: 0.85  # Too permissive
    notification: email  # Slow response

  data_validation: none  # No drift detection!

  logging:
    level: ERROR  # Missing valuable signals

  dashboard: none  # No visibility between monthly reports
\end{lstlisting}

\subsubsection{After Configuration}

\begin{lstlisting}[language=yaml, caption={Comprehensive Monitoring Configuration}]
# TechBank's improved monitoring system
monitoring:
  evaluation_frequency: realtime  # Continuous evaluation

  metrics:
    # Performance metrics (per-class)
    - accuracy_overall
    - precision_per_fraud_type  # Granular tracking
    - recall_per_fraud_type
    - f1_score_per_fraud_type
    - false_positive_rate
    - false_negative_rate

    # Business metrics
    - fraud_loss_amount
    - customer_friction_rate
    - legitimate_transaction_decline_rate

  drift_detection:
    methods:
      - kolmogorov_smirnov  # Feature drift
      - psi  # Population stability
      - concept_drift_adwin  # Performance drift

    features:
      - transaction_amount
      - merchant_category
      - transaction_velocity
      - device_fingerprint
      - all_engineered_features

    check_frequency: hourly
    window_size: 7_days

  alerts:
    # Multi-level alert system
    warning:
      accuracy_drop: 0.02  # 2% drop triggers warning
      drift_pvalue: 0.05
      response_time: 1_hour

    critical:
      accuracy_drop: 0.05  # 5% drop triggers critical
      fraud_loss_spike: 50000  # \$50K in 24h
      false_positive_spike: 0.05  # 5% increase
      response_time: 15_minutes
      escalation:
        - level1: ml_team_slack
        - level2: engineering_leads (30min)
        - level3: vp_engineering (1hour)
        - level4: cto (2hours)

  observability:
    tracing: enabled  # Full request tracing
    logging:
      level: INFO
      structured: true
      include_predictions: true
      include_feature_values: true

    dashboards:
      - real_time_performance
      - drift_analysis
      - business_impact
      - fraud_type_breakdown

    retention:
      metrics: 90_days
      logs: 30_days
      predictions: 365_days  # For regulatory compliance

  automated_response:
    performance_degradation:
      - trigger_retraining_pipeline
      - increase_human_review_percentage
      - alert_fraud_investigation_team

    severe_drift:
      - rollback_to_previous_model
      - initiate_emergency_retraining
      - activate_rule_based_backup_system
\end{lstlisting}

\subsubsection{Quantified Business Impact}

\begin{lstlisting}[language=bash, caption={Silent Model Death Impact Analysis}]
FINANCIAL IMPACT:
  Direct fraud losses:              \$12,400,000
  Customer compensation:            \$   890,000
  Emergency response costs:         \$   450,000
  Regulatory fines:                 \$ 1,200,000
  Total direct cost:                \$14,940,000

  Indirect costs (estimated):
    Brand reputation damage:        \$ 8,500,000
    Customer churn (2,400 accounts):\$ 3,600,000
    Increased fraud insurance:      \$   420,000
    Total indirect cost:            \$12,520,000

  TOTAL INCIDENT COST:              \$27,460,000

OPERATIONAL IMPACT:
  Customer support tickets:         47,000 (300% increase)
  Average resolution time:          8.5 hours (vs. 2.1 hours baseline)
  Fraud investigation hours:        12,400 hours
  Engineering response hours:       3,200 hours
  Executive time consumed:          180 hours

RECOVERY METRICS (Post-Implementation):
  Time to detect degradation:       6 months → 2 hours
  Time to incident response:        N/A → 15 minutes
  Model retraining frequency:       Ad-hoc → Automated weekly
  False positive rate:              18% → 2.3%
  Fraud detection accuracy:         71% → 97.5%

  Prevented losses (6 months post-fix): \$18,200,000
  Monitoring system ROI:            4,200%
\end{lstlisting}

\subsubsection{Key Learnings}

\begin{itemize}
    \item \textbf{Real-time monitoring is non-negotiable}: Monthly evaluations are insufficient for production ML systems; degradation can occur rapidly
    \item \textbf{Track business metrics, not just technical metrics}: Technical accuracy remained ``acceptable'' while business impact was catastrophic
    \item \textbf{Granular metrics reveal hidden failures}: Aggregate accuracy masked per-fraud-type failures
    \item \textbf{Automated response saves millions}: Manual incident response delays amplify losses exponentially
    \item \textbf{Multi-method drift detection is essential}: Single detection methods miss nuanced distributional shifts
    \item \textbf{Compliance and audit trails matter}: Lack of prediction logging complicated regulatory response
\end{itemize}

\subsubsection{Prevention Strategies}

\begin{lstlisting}[language=Python, caption={Early Warning System Implementation}]
from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta
import numpy as np

@dataclass
class EarlyWarningConfig:
    """
    Configuration for early warning system preventing silent degradation.
    """
    # Performance thresholds
    min_acceptable_accuracy: float = 0.92
    max_acceptable_fpr: float = 0.05
    max_acceptable_fnr: float = 0.08

    # Drift thresholds
    drift_check_frequency: timedelta = timedelta(hours=1)
    drift_pvalue_threshold: float = 0.05
    max_feature_drift_count: int = 3  # Max features drifting simultaneously

    # Business metric thresholds
    max_daily_fraud_loss: float = 50000.0
    max_customer_complaint_rate: float = 0.02

    # Evaluation windows
    performance_window: timedelta = timedelta(hours=24)
    drift_reference_window: timedelta = timedelta(days=7)
    business_impact_window: timedelta = timedelta(hours=6)

class SilentDegradationPrevention:
    """
    Comprehensive system to prevent silent model degradation.

    Implements multi-layered monitoring with automatic escalation
    and emergency response protocols.
    """

    def __init__(self, config: EarlyWarningConfig):
        self.config = config
        self.performance_history: List[Dict] = []
        self.drift_alerts: List[Dict] = []
        self.business_impact_history: List[Dict] = []

    def continuous_health_check(
        self,
        predictions: np.ndarray,
        actuals: np.ndarray,
        features: np.ndarray,
        business_metrics: Dict[str, float]
    ) -> Dict[str, any]:
        """
        Perform comprehensive health check every evaluation cycle.

        Returns:
            Health status with specific degradation indicators
        """
        health_status = {
            'timestamp': datetime.now(),
            'overall_health': 'healthy',
            'alerts': [],
            'metrics': {}
        }

        # 1. Performance degradation check
        accuracy = np.mean(predictions == actuals)
        fpr = self._calculate_fpr(predictions, actuals)
        fnr = self._calculate_fnr(predictions, actuals)

        health_status['metrics']['accuracy'] = accuracy
        health_status['metrics']['fpr'] = fpr
        health_status['metrics']['fnr'] = fnr

        if accuracy < self.config.min_acceptable_accuracy:
            health_status['overall_health'] = 'critical'
            health_status['alerts'].append({
                'type': 'performance_degradation',
                'severity': 'critical',
                'metric': 'accuracy',
                'value': accuracy,
                'threshold': self.config.min_acceptable_accuracy,
                'action': 'IMMEDIATE_ESCALATION'
            })

        if fpr > self.config.max_acceptable_fpr:
            health_status['overall_health'] = 'warning'
            health_status['alerts'].append({
                'type': 'false_positive_spike',
                'severity': 'warning',
                'metric': 'fpr',
                'value': fpr,
                'threshold': self.config.max_acceptable_fpr,
                'action': 'INVESTIGATE_CUSTOMER_IMPACT'
            })

        if fnr > self.config.max_acceptable_fnr:
            health_status['overall_health'] = 'critical'
            health_status['alerts'].append({
                'type': 'false_negative_spike',
                'severity': 'critical',
                'metric': 'fnr',
                'value': fnr,
                'threshold': self.config.max_acceptable_fnr,
                'action': 'EVALUATE_FRAUD_EXPOSURE'
            })

        # 2. Drift detection check
        drift_results = self._check_feature_drift(features)
        if drift_results['drifted_features_count'] > self.config.max_feature_drift_count:
            health_status['overall_health'] = 'warning'
            health_status['alerts'].append({
                'type': 'multi_feature_drift',
                'severity': 'warning',
                'drifted_features': drift_results['drifted_features'],
                'action': 'TRIGGER_RETRAINING_EVALUATION'
            })

        # 3. Business impact check
        if business_metrics.get('fraud_loss_24h', 0) > self.config.max_daily_fraud_loss:
            health_status['overall_health'] = 'critical'
            health_status['alerts'].append({
                'type': 'fraud_loss_threshold_breach',
                'severity': 'critical',
                'value': business_metrics['fraud_loss_24h'],
                'threshold': self.config.max_daily_fraud_loss,
                'action': 'ACTIVATE_INCIDENT_RESPONSE'
            })

        # 4. Trend analysis (degradation velocity)
        degradation_velocity = self._calculate_degradation_velocity()
        if degradation_velocity < -0.01:  # Losing 1% accuracy per day
            health_status['overall_health'] = 'warning'
            health_status['alerts'].append({
                'type': 'accelerating_degradation',
                'severity': 'warning',
                'velocity': degradation_velocity,
                'projected_failure': self._project_failure_date(degradation_velocity),
                'action': 'SCHEDULE_URGENT_RETRAINING'
            })

        # Store for historical analysis
        self.performance_history.append(health_status)

        return health_status

    def _calculate_fpr(self, predictions: np.ndarray, actuals: np.ndarray) -> float:
        """Calculate false positive rate."""
        negatives = actuals == 0
        if negatives.sum() == 0:
            return 0.0
        false_positives = (predictions[negatives] == 1).sum()
        return false_positives / negatives.sum()

    def _calculate_fnr(self, predictions: np.ndarray, actuals: np.ndarray) -> float:
        """Calculate false negative rate."""
        positives = actuals == 1
        if positives.sum() == 0:
            return 0.0
        false_negatives = (predictions[positives] == 0).sum()
        return false_negatives / positives.sum()

    def _check_feature_drift(self, current_features: np.ndarray) -> Dict:
        """Check for feature drift using multiple methods."""
        # Simplified drift detection (would use reference distribution in practice)
        drifted_features = []
        # Implementation would use KS test, PSI, etc.
        return {
            'drifted_features_count': len(drifted_features),
            'drifted_features': drifted_features
        }

    def _calculate_degradation_velocity(self) -> float:
        """Calculate rate of performance degradation."""
        if len(self.performance_history) < 24:  # Need 24 hours of data
            return 0.0

        recent = self.performance_history[-24:]
        accuracies = [h['metrics']['accuracy'] for h in recent]

        # Simple linear regression slope
        x = np.arange(len(accuracies))
        slope = np.polyfit(x, accuracies, 1)[0]
        return slope

    def _project_failure_date(self, velocity: float) -> datetime:
        """Project when model will reach failure threshold."""
        current_accuracy = self.performance_history[-1]['metrics']['accuracy']
        hours_to_failure = (current_accuracy - self.config.min_acceptable_accuracy) / abs(velocity)
        return datetime.now() + timedelta(hours=hours_to_failure)
\end{lstlisting}

\subsection{Scenario 2: The Alert Storm Crisis}

\subsubsection{The Problem}

RetailAI deployed a recommendation model with enthusiastic monitoring—tracking 240 metrics with aggressive thresholds. Within the first week, the system generated 18,500 alerts, averaging 110 alerts per hour. The ML team, initially responsive, experienced severe alert fatigue within three days. By week two, they began ignoring alerts entirely, missing a critical data pipeline failure that caused \$340K in lost revenue over 48 hours. The root cause was overly sensitive thresholds combined with correlated metrics (e.g., alerting on both ``latency\_p95'' and ``latency\_p99'' which move together), seasonal patterns triggering false alarms (traffic spikes every day at 9 AM treated as anomalies), and lack of alert deduplication allowing the same issue to generate hundreds of redundant notifications.

\subsubsection{Incident Timeline}

\begin{lstlisting}[language=bash, caption={Alert Storm Crisis Timeline}]
Day 1 (Monday):
  09:00: Model deployed with comprehensive monitoring
  09:15: First alert: latency spike (legitimate morning traffic)
  09:30: 47 alerts received (all legitimate traffic patterns)
  12:00: 134 total alerts; team investigates each one
  18:00: 312 alerts; team working overtime
  23:59: 876 alerts on Day 1

Day 2 (Tuesday):
  08:00: Team arrives to 243 overnight alerts
  10:00: Engineering lead requests "less sensitive" thresholds
  12:00: Quick threshold adjustment (doubled all thresholds)
  14:00: Alerts continue (now missing real issues)
  18:00: Team morale declining; 1,450 cumulative alerts

Day 3 (Wednesday):
  09:00: Team begins ignoring most alerts
  11:30: Critical data pipeline failure occurs (MISSED)
  14:00: Recommendation quality degrading (unnoticed)
  16:00: Customer support reports "weird recommendations"
  18:00: ML team investigates; discovers pipeline failure
  20:00: 2 hours to fix; \$28K revenue lost

Day 4-5 (Thu-Fri):
  - Alert fatigue complete; team checks alerts once daily
  - Multiple minor issues go undetected
  - Friday: emergency meeting scheduled

Weekend:
  - Data pipeline failure continues undetected
  - Recommendation model serving stale features
  - Customer engagement drops 23%

Day 8 (Monday):
  09:00: Weekly business review reveals revenue drop
  10:00: Deep investigation discovers 48-hour outage
  12:00: Total revenue impact: \$340,000
  14:00: Emergency task force formed

Week 2:
  - Complete monitoring system overhaul
  - Implementation of intelligent alert management
  - Alert volume reduction: 18,500/week → 47/week
\end{lstlisting}

\subsubsection{Before Configuration}

\begin{lstlisting}[language=yaml, caption={Alert Storm Configuration (Problematic)}]
# RetailAI's original configuration (generated alert storm)
alerting:
  metrics_count: 240  # Too many metrics!

  thresholds:
    # Overly sensitive thresholds
    latency_p50_ms: 45  # Too strict
    latency_p95_ms: 120
    latency_p99_ms: 200
    error_rate: 0.001  # 0.1% triggers alert

    # Correlated metrics (redundant alerts)
    cpu_usage: 0.7
    cpu_per_request: 0.0001

    # No seasonal awareness
    traffic_rate_change: 0.15  # 15% change = alert

  alert_rules:
    - name: "High latency P50"
      frequency: every_1_minute  # Too frequent!
      cooldown: none  # No suppression!

    - name: "High latency P95"
      frequency: every_1_minute
      cooldown: none

    # ... 238 more similar rules

  deduplication: disabled  # Same issue → multiple alerts

  correlation_analysis: disabled  # Can't group related alerts

  channels:
    - slack: "#ml-alerts"  # Single channel for everything
    - email: "ml-team@retailai.com"
    - pagerduty: "all"  # Everything is P1!
\end{lstlisting}

\subsubsection{After Configuration}

\begin{lstlisting}[language=yaml, caption={Intelligent Alert Management Configuration}]
# RetailAI's improved configuration (intelligent filtering)
alerting:
  metrics_count: 52  # Focused on actionable metrics

  # Tiered alert severity
  severity_levels:
    P1_critical:  # Immediate business impact
      metrics:
        - recommendation_serving_failure_rate
        - data_pipeline_health
        - revenue_impacting_errors
      response_time: 15_minutes
      channels: [pagerduty, slack_urgent, sms]

    P2_warning:  # Degraded performance
      metrics:
        - latency_p99  # Only P99, not P50/P95/P99
        - model_accuracy_drop
        - feature_drift_severe
      response_time: 2_hours
      channels: [slack_warnings, email]

    P3_info:  # Informational
      metrics:
        - traffic_patterns
        - resource_utilization
      response_time: next_business_day
      channels: [slack_info]

  thresholds:
    # Adaptive thresholds with seasonal awareness
    latency_p99_ms:
      baseline: 200
      seasonal_multipliers:
        weekday_morning: 1.5  # Expect higher latency 8-10 AM
        weekend: 0.8  # Lower traffic
        holiday: 2.0  # Black Friday, etc.

    error_rate:
      baseline: 0.01  # 1% is realistic threshold
      sustained_duration: 5_minutes  # Must persist

  alert_rules:
    # Intelligent grouping and deduplication
    - name: "Recommendation serving degraded"
      conditions:
        - metric: latency_p99_ms
          operator: ">"
          threshold: adaptive  # Uses seasonal multipliers
          duration: 5_minutes  # Sustained issue
        OR:
        - metric: error_rate
          operator: ">"
          threshold: 0.02
          duration: 3_minutes

      frequency: every_5_minutes
      cooldown: 30_minutes  # Suppress duplicates

      correlation:
        group_with:
          - latency_alerts
          - performance_alerts
        suppress_if_parent_active: true

    - name: "Data pipeline failure"
      conditions:
        - metric: pipeline_freshness_minutes
          operator: ">"
          threshold: 15

      severity: P1
      frequency: every_2_minutes
      max_alerts_per_hour: 3  # Rate limiting

      auto_actions:
        - trigger_pipeline_restart
        - collect_diagnostic_logs
        - create_incident_ticket

  deduplication:
    enabled: true
    window: 30_minutes
    fingerprint_fields:
      - alert_name
      - affected_service
      - root_cause_signature

  correlation_analysis:
    enabled: true
    correlation_window: 10_minutes

    rules:
      # Group related infrastructure alerts
      - pattern: "high_cpu_* AND high_latency_*"
        action: create_single_alert
        title: "Infrastructure performance degradation"

      # Suppress downstream alerts when root cause identified
      - pattern: "data_pipeline_failure"
        suppress:
          - feature_freshness_alert
          - recommendation_quality_alert
        reason: "Known dependency on pipeline"

  intelligent_routing:
    business_hours:
      weekday: "08:00-18:00"
      weekend: "10:00-16:00"

    routing_rules:
      - if: severity == "P1" AND business_hours
        then: [pagerduty, slack_urgent, email]

      - if: severity == "P1" AND NOT business_hours
        then: [pagerduty_oncall_only]

      - if: severity == "P2" AND business_hours
        then: [slack_warnings]
        else: [email_summary]  # Batched email

  alert_quality_metrics:
    track:
      - alert_response_rate
      - false_positive_rate
      - time_to_resolution
      - alert_fatigue_score

    feedback_loop:
      - if: alert_ignored_rate > 0.3
        then: increase_threshold_by_20_percent

      - if: alert_false_positive_rate > 0.5
        then: disable_rule_and_notify_team
\end{lstlisting}

\subsubsection{Quantified Business Impact}

\begin{lstlisting}[language=bash, caption={Alert Storm Impact Analysis}]
ALERT VOLUME IMPACT:
  Week 1 alerts:                    18,500
  Week 1 actionable alerts:         12 (0.06%)
  Week 1 false positives:           18,488 (99.94%)

  Engineering time wasted:
    Initial investigation (Day 1-3):  96 hours
    Ongoing alert triage (Week 1):    180 hours
    Total engineering cost:           \$68,400 (@ \$250/hour loaded)

MISSED INCIDENT IMPACT:
  Data pipeline outage duration:    48 hours
  Recommendations served with stale data: 34.2M

  Revenue impact:
    Lost purchases (engagement drop): \$287,000
    Customer support overhead:        \$ 23,000
    Emergency remediation:            \$ 30,000
    Total revenue impact:             \$340,000

TEAM IMPACT:
  ML engineer burnout:              3 engineers (2 requested transfer)
  On-call satisfaction score:       2.1/10 → 1.3/10
  Average sleep hours during week:  4.2 hours
  Weekend work hours:               38 hours (unpaid)

POST-FIX IMPROVEMENTS:
  Alert volume reduction:           18,500/week → 47/week (99.7% reduction)
  False positive rate:              99.94% → 8.5%
  Time to detect real issues:       48 hours → 3.2 minutes
  Engineering time on alerts:       180 hours/week → 4 hours/week
  On-call satisfaction:             1.3/10 → 7.8/10

  Incidents prevented (6 months):   23 major, 67 minor
  Estimated value of prevention:    \$2,100,000

  ROI of intelligent alerting:     520%
\end{lstlisting}

\subsubsection{Key Learnings}

\begin{itemize}
    \item \textbf{More alerts $\neq$ better monitoring}: Quality over quantity; 52 actionable metrics better than 240 noisy ones
    \item \textbf{Alert fatigue is a critical failure mode}: Teams will ignore alerts when overwhelmed, including critical ones
    \item \textbf{Correlation and deduplication are essential}: Related issues should generate one grouped alert, not hundreds
    \item \textbf{Seasonal patterns must be learned}: Traffic spikes at predictable times shouldn't trigger alerts
    \item \textbf{Severity tiering prevents desensitization}: Not everything is P1; inappropriate escalation destroys alert credibility
    \item \textbf{Feedback loops improve alert quality}: Track which alerts are acted upon; disable low-value alerts automatically
    \item \textbf{Team wellbeing impacts incident response}: Burned-out teams make mistakes and miss critical signals
\end{itemize}

\subsubsection{Prevention Strategies}

\begin{lstlisting}[language=Python, caption={Intelligent Alert Manager with Fatigue Prevention}]
from dataclasses import dataclass
from typing import List, Dict, Set
from datetime import datetime, timedelta
from collections import defaultdict
import hashlib

@dataclass
class Alert:
    """Alert with fingerprinting for deduplication."""
    name: str
    severity: str  # P1, P2, P3
    metric: str
    value: float
    threshold: float
    timestamp: datetime
    metadata: Dict = None

    def fingerprint(self) -> str:
        """Generate fingerprint for deduplication."""
        components = f"{self.name}:{self.metric}:{self.metadata.get('service', 'unknown')}"
        return hashlib.md5(components.encode()).hexdigest()

class AlertFatiguePreventionSystem:
    """
    Intelligent alert management system preventing alert fatigue.

    Features:
    - Deduplication within time windows
    - Alert correlation and grouping
    - Adaptive threshold adjustment
    - Alert quality tracking
    - Automatic rule tuning
    """

    def __init__(self):
        self.alert_history: List[Alert] = []
        self.active_alerts: Dict[str, Alert] = {}  # fingerprint -> alert
        self.suppressed_alerts: Set[str] = set()
        self.alert_quality_metrics = defaultdict(lambda: {
            'sent': 0,
            'acknowledged': 0,
            'resolved': 0,
            'ignored': 0,
            'false_positive': 0
        })

    def process_alert(self, alert: Alert) -> Dict[str, any]:
        """
        Process incoming alert with intelligent filtering.

        Returns:
            Processing decision and actions taken
        """
        result = {
            'should_send': False,
            'reason': None,
            'actions': [],
            'grouped_with': None
        }

        # 1. Deduplication check
        fingerprint = alert.fingerprint()
        if fingerprint in self.active_alerts:
            existing_alert = self.active_alerts[fingerprint]
            time_since_last = (alert.timestamp - existing_alert.timestamp).total_seconds()

            if time_since_last < 1800:  # 30 minutes cooldown
                result['reason'] = 'duplicate_suppressed'
                result['actions'].append(f"Suppressed duplicate of {existing_alert.name}")
                return result

        # 2. Correlation analysis
        correlated_alerts = self._find_correlated_alerts(alert)
        if correlated_alerts:
            result['should_send'] = True
            result['grouped_with'] = [a.name for a in correlated_alerts]
            result['actions'].append(
                f"Grouped with {len(correlated_alerts)} related alerts"
            )
            # Create grouped alert instead of individual ones
            grouped_alert = self._create_grouped_alert(alert, correlated_alerts)
            alert = grouped_alert

        # 3. Severity validation
        if not self._validate_severity(alert):
            result['reason'] = 'severity_downgraded'
            result['actions'].append(f"Downgraded from {alert.severity} to P3")
            alert.severity = 'P3'

        # 4. Rate limiting per metric
        if self._is_rate_limited(alert):
            result['reason'] = 'rate_limited'
            result['actions'].append("Rate limit exceeded for this metric")
            return result

        # 5. Alert quality check
        alert_performance = self.alert_quality_metrics[alert.name]
        if alert_performance['sent'] > 10:
            false_positive_rate = (
                alert_performance['false_positive'] / alert_performance['sent']
            )
            if false_positive_rate > 0.5:
                result['reason'] = 'low_quality_alert'
                result['actions'].append(
                    f"Alert has {false_positive_rate:.1%} false positive rate; "
                    f"consider tuning threshold"
                )
                # Auto-disable if consistently poor
                if false_positive_rate > 0.8 and alert_performance['sent'] > 50:
                    result['actions'].append("AUTO-DISABLED due to poor quality")
                    return result

        # 6. Seasonal pattern check
        if self._is_expected_pattern(alert):
            result['reason'] = 'expected_seasonal_pattern'
            result['actions'].append("Matches known traffic pattern; not anomalous")
            return result

        # Alert passes all filters
        result['should_send'] = True
        result['reason'] = 'actionable_alert'

        # Store for deduplication and quality tracking
        self.active_alerts[fingerprint] = alert
        self.alert_quality_metrics[alert.name]['sent'] += 1
        self.alert_history.append(alert)

        return result

    def _find_correlated_alerts(self, alert: Alert) -> List[Alert]:
        """Find alerts correlated with this one."""
        correlated = []

        # Look for alerts in last 10 minutes
        recent_window = alert.timestamp - timedelta(minutes=10)
        recent_alerts = [
            a for a in self.alert_history
            if a.timestamp >= recent_window
        ]

        # Correlation patterns
        correlation_patterns = {
            'high_latency': ['high_cpu', 'high_memory', 'high_error_rate'],
            'data_pipeline_failure': ['feature_freshness', 'recommendation_quality'],
            'high_traffic': ['high_latency', 'resource_saturation']
        }

        for pattern_key, related_metrics in correlation_patterns.items():
            if pattern_key in alert.metric:
                for recent_alert in recent_alerts:
                    if any(rm in recent_alert.metric for rm in related_metrics):
                        correlated.append(recent_alert)

        return correlated

    def _create_grouped_alert(self, primary: Alert, related: List[Alert]) -> Alert:
        """Create grouped alert from correlated alerts."""
        return Alert(
            name=f"Grouped: {primary.name} and {len(related)} related",
            severity=primary.severity,
            metric="grouped_alert",
            value=primary.value,
            threshold=primary.threshold,
            timestamp=primary.timestamp,
            metadata={
                'primary_alert': primary.name,
                'related_alerts': [a.name for a in related],
                'investigation_priority': 'high'
            }
        )

    def _validate_severity(self, alert: Alert) -> bool:
        """Validate alert severity is appropriate."""
        # P1 should only be for business-impacting issues
        if alert.severity == 'P1':
            p1_metrics = [
                'serving_failure',
                'data_pipeline_down',
                'revenue_impacting'
            ]
            return any(m in alert.metric for m in p1_metrics)
        return True

    def _is_rate_limited(self, alert: Alert) -> bool:
        """Check if alert exceeds rate limits."""
        # Count alerts of this type in last hour
        one_hour_ago = alert.timestamp - timedelta(hours=1)
        recent_count = sum(
            1 for a in self.alert_history
            if a.name == alert.name and a.timestamp >= one_hour_ago
        )

        # Rate limits by severity
        rate_limits = {'P1': 10, 'P2': 20, 'P3': 50}
        limit = rate_limits.get(alert.severity, 50)

        return recent_count >= limit

    def _is_expected_pattern(self, alert: Alert) -> bool:
        """Check if alert matches known seasonal patterns."""
        # Example: Morning traffic spike
        if 'traffic' in alert.metric or 'latency' in alert.metric:
            hour = alert.timestamp.hour
            if 8 <= hour <= 10:  # Morning peak
                return True

        # Would include more sophisticated pattern matching in practice
        return False

    def record_alert_outcome(self, alert_name: str, outcome: str):
        """
        Record what happened to an alert for quality tracking.

        Args:
            alert_name: Name of the alert
            outcome: One of 'acknowledged', 'resolved', 'ignored', 'false_positive'
        """
        if outcome in self.alert_quality_metrics[alert_name]:
            self.alert_quality_metrics[alert_name][outcome] += 1

    def get_alert_quality_report(self) -> Dict[str, any]:
        """Generate report on alert quality."""
        report = {}

        for alert_name, metrics in self.alert_quality_metrics.items():
            if metrics['sent'] > 0:
                report[alert_name] = {
                    'total_sent': metrics['sent'],
                    'acknowledgment_rate': metrics['acknowledged'] / metrics['sent'],
                    'false_positive_rate': metrics['false_positive'] / metrics['sent'],
                    'ignore_rate': metrics['ignored'] / metrics['sent'],
                    'quality_score': self._calculate_quality_score(metrics)
                }

        return report

    def _calculate_quality_score(self, metrics: Dict) -> float:
        """Calculate overall quality score for an alert (0-100)."""
        if metrics['sent'] == 0:
            return 0.0

        # Positive factors
        ack_rate = metrics['acknowledged'] / metrics['sent']
        resolution_rate = metrics['resolved'] / metrics['sent']

        # Negative factors
        fp_rate = metrics['false_positive'] / metrics['sent']
        ignore_rate = metrics['ignored'] / metrics['sent']

        # Weighted score
        score = (
            (ack_rate * 40) +
            (resolution_rate * 40) -
            (fp_rate * 50) -
            (ignore_rate * 30)
        )

        return max(0, min(100, score))
\end{lstlisting}

\subsection{Scenario 3: The Compliance Blind Spot}

\subsubsection{The Problem}

FinServe Corp operated an AI-driven loan approval system processing 15,000 applications daily. During a routine regulatory audit in Q3 2023, examiners requested prediction logs and model decision explanations for a random sample of 500 loan decisions from the previous quarter. The ML team discovered they had only retained high-level aggregate metrics—individual prediction logs, feature values, and model versions were not stored. They could not demonstrate that protected characteristics (race, gender, age) didn't influence decisions, couldn't reproduce specific predictions, and had no audit trail showing which model version made each decision. The regulatory finding resulted in \$4.7M in fines, a six-month restriction on new AI deployments, mandatory third-party auditing, and severe reputational damage in financial services press.

\subsubsection{Incident Timeline}

\begin{lstlisting}[language=bash, caption={Compliance Blind Spot Timeline}]
Q2 2023:
  April 1:   Loan approval model deployed (v3.2)
  April 15:  15,000 applications/day processed
  May 1:     Model update to v3.3 (no decision audit trail)
  May 20:    Model update to v3.4
  June 10:   Model update to v3.5 (current version)
  June 30:   Q2 ends; 1.35M loan decisions made

Q3 2023:
  July 15:   Regulatory audit notice received
  July 18:   Initial document request for Q2 decisions
  July 22:   ML team realizes prediction logs not retained

  July 25:   Emergency meeting with legal/compliance
              Discovery: No way to reproduce Q2 predictions
              Discovery: Don't know which model version made which decision
              Discovery: Can't prove fairness compliance

  July 28:   Disclosure to regulators: "Data not available"
  Aug 1:     Regulatory escalation; formal investigation

  Aug 5:     Attempted reconstruction from application logs (failed)
  Aug 10:    Attempted statistical approximation (rejected by regulators)
  Aug 15:    External audit firm engaged (\$380K emergency contract)

  Aug 25:    Regulatory finding: "Material compliance deficiency"
  Aug 30:    Preliminary fine estimate: \$3-7M

  Sept 5:    Emergency compliance remediation begins
  Sept 15:   New monitoring system design approved
  Sept 30:   Comprehensive audit trail system deployed

Q4 2023:
  Oct 15:    Final regulatory fine: \$4.7M
  Oct 20:    6-month AI deployment freeze imposed
  Nov 1:     Mandatory quarterly third-party audits required
  Nov 15:    News coverage damages reputation
  Dec 31:    Full compliance system operational

2024:
  Jan-Mar:   Intensive regulatory oversight
  April 1:   AI deployment freeze lifted (with conditions)
  June 30:   First successful compliance audit
\end{lstlisting}

\subsubsection{Before Configuration}

\begin{lstlisting}[language=yaml, caption={Non-Compliant Monitoring Configuration}]
# FinServe's original configuration (compliance blind spot)
monitoring:
  metrics:
    # Only aggregate metrics retained
    - daily_approval_rate
    - daily_average_score
    - daily_application_volume

  retention:
    aggregate_metrics: 90_days
    prediction_logs: NONE  # Critical gap!

  logging:
    level: ERROR  # Only errors logged
    prediction_details: false  # No individual decisions recorded
    feature_values: false  # No input data stored
    model_version: false  # Can't track which version decided

  fairness:
    monitoring: false  # No fairness tracking!

  auditability:
    decision_reproduction: impossible  # Can't reproduce decisions
    explanation_logs: none  # No explanations stored

  compliance:
    regulatory_reporting: manual  # No automated compliance reporting
    audit_trail: incomplete  # Missing critical elements
\end{lstlisting}

\subsubsection{After Configuration}

\begin{lstlisting}[language=yaml, caption={Compliance-Ready Monitoring Configuration}]
# FinServe's compliant configuration (comprehensive audit trail)
monitoring:
  # Comprehensive decision logging
  prediction_logging:
    enabled: true
    log_every_prediction: true  # Every single decision

    capture:
      - prediction_id  # Unique identifier
      - timestamp
      - model_name
      - model_version
      - model_artifact_hash  # Cryptographic verification
      - input_features  # All feature values
      - feature_engineering_version
      - prediction_score
      - prediction_class
      - confidence_intervals
      - explanation  # SHAP values, feature importance
      - processing_latency
      - request_metadata

    # Protected characteristics handling
    protected_attributes:
      capture_separately: true  # Segregated storage
      access_control: strict_rbac
      audit_access: true  # Log who accessed protected data
      attributes:
        - age
        - gender
        - race
        - marital_status
        - zip_code  # Proxy for protected classes

  retention:
    # Regulatory requirement: 7 years
    prediction_logs: 2555_days  # 7 years
    aggregate_metrics: 365_days
    model_artifacts: 2555_days
    training_data_snapshots: 2555_days

    # Immutable storage
    storage_type: append_only_s3
    encryption: AES_256
    versioning: enabled
    access_logging: comprehensive

  fairness_monitoring:
    enabled: true
    check_frequency: daily

    metrics:
      # Disparate Impact Analysis
      - demographic_parity_ratio
      - equal_opportunity_difference
      - average_odds_difference
      - statistical_parity_difference

    protected_groups:
      - age_groups: ["18-25", "26-35", "36-50", "51-65", "65+"]
      - gender: ["male", "female", "non_binary"]
      - ethnicity: [...] # Per regulatory requirements

    thresholds:
      four_fifths_rule: 0.8  # 80% rule for disparate impact
      statistical_significance: 0.05

    reporting:
      frequency: monthly
      recipients: [compliance_team, regulators]
      format: regulatory_standard_template

  auditability:
    # Complete decision reproduction capability
    prediction_reproduction:
      enabled: true
      store_model_artifacts: true
      store_feature_engineering_code: true
      store_preprocessing_state: true

    # Automated compliance reporting
    compliance_reports:
      - type: model_fairness_analysis
        frequency: quarterly
        template: federal_standard_template

      - type: decision_distribution_analysis
        frequency: monthly
        breakdown_by: [protected_groups, decision_outcome]

      - type: adverse_action_explanations
        frequency: on_demand
        include: [feature_contributions, counterfactuals]

  explainability:
    # Explanation for every decision
    method: shap_tree_explainer

    capture:
      - global_feature_importance
      - individual_prediction_shap_values
      - counterfactual_examples  # What would change decision
      - confidence_intervals

    human_readable:
      generate: true
      template: "Your application was {decision} because: {top_3_factors}"

  model_governance:
    # Track model lineage
    model_registry:
      track_versions: true
      track_training_data: true
      track_hyperparameters: true
      track_performance_metrics: true
      require_approval_before_deployment: true

    change_management:
      require_testing_on_holdout: true
      require_fairness_validation: true
      require_compliance_review: true

  access_control:
    # Strict RBAC for compliance
    roles:
      - ml_engineer: [read_models, deploy_to_staging]
      - compliance_officer: [read_all_predictions, generate_reports]
      - auditor: [read_only_everything]
      - data_scientist: [read_aggregate_only]

    audit_all_access: true
    alert_on_bulk_export: true  # Detect potential data breach

  automated_reporting:
    # Regulatory reporting automation
    quarterly_compliance_report:
      generate_automatically: true
      include:
        - model_performance_summary
        - fairness_analysis_all_groups
        - adverse_action_statistics
        - model_change_log
        - incident_summary

      delivery:
        - compliance_team
        - legal_department
        - regulatory_portal_api  # Direct submission
\end{lstlisting}

\subsubsection{Quantified Business Impact}

\begin{lstlisting}[language=bash, caption={Compliance Blind Spot Impact Analysis}]
REGULATORY PENALTIES:
  Direct fine:                      \$4,700,000
  Third-party audit costs:          \$  380,000
  Ongoing audit requirements:       \$  780,000 (2 years)
  Legal fees:                       \$  450,000
  Total regulatory cost:            \$6,310,000

BUSINESS IMPACT:
  AI deployment freeze (6 months):
    Delayed product launches:       3 initiatives
    Lost competitive advantage:     \$2,100,000 (estimated)
    Engineering productivity loss:  \$  890,000

  Reputational damage:
    Customer trust impact:          15% increase in application abandonment
    Lost business opportunities:    \$1,200,000 (partnerships canceled)
    Brand recovery marketing:       \$  340,000

  Total business impact:            \$4,530,000

REMEDIATION COSTS:
  Compliance system development:    \$  560,000
  Infrastructure upgrades:          \$  280,000
  Storage costs (7-year retention): \$   42,000/year
  Compliance team expansion:        \$  420,000/year (3 FTEs)
  Training and process updates:     \$  180,000
  Total remediation:                \$1,482,000 (Year 1)

TOTAL INCIDENT COST:                \$12,322,000

POST-REMEDIATION BENEFITS:
  Regulatory audit pass rate:       100% (3 consecutive audits)
  Time to generate compliance report: 3 weeks → 2 hours (automated)
  Audit preparation cost:           \$200K → \$15K per audit

  Risk mitigation value:
    Prevented future fines:         \$8,000,000 (estimated)
    Competitive advantage restored: Partnership growth resumed
    Customer trust recovered:       Application abandonment: 15% → 7%

  ROI of compliance monitoring:     340% (3-year horizon)
\end{lstlisting}

\subsubsection{Key Learnings}

\begin{itemize}
    \item \textbf{Compliance is not optional}: Regulatory requirements must be designed into monitoring from day one
    \item \textbf{Aggregate metrics are insufficient}: Individual prediction logs are essential for auditability
    \item \textbf{Reproducibility is a regulatory requirement}: Must be able to exactly reproduce any historical decision
    \item \textbf{Fairness monitoring prevents discrimination}: Continuous tracking of protected group outcomes is mandatory
    \item \textbf{Retention policies have legal implications}: Understand regulatory requirements before setting retention periods
    \item \textbf{Automated reporting saves millions}: Manual compliance processes are expensive and error-prone
    \item \textbf{Model versioning is critical}: Must track which model version made each decision
    \item \textbf{Explainability is both technical and legal}: Store explanations in human-readable and machine-readable formats
\end{itemize}

\subsubsection{Prevention Strategies}

\begin{lstlisting}[language=Python, caption={Compliance-Ready Prediction Logger}]
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional
from datetime import datetime
import hashlib
import json
import boto3
from enum import Enum

class DecisionOutcome(Enum):
    """Loan decision outcomes."""
    APPROVED = "approved"
    DENIED = "denied"
    MANUAL_REVIEW = "manual_review"

@dataclass
class PredictionLog:
    """
    Comprehensive prediction log for regulatory compliance.

    Captures everything needed to reproduce and explain a decision.
    """
    # Unique identifiers
    prediction_id: str
    application_id: str
    timestamp: datetime

    # Model information
    model_name: str
    model_version: str
    model_artifact_hash: str  # SHA-256 of model file
    feature_engineering_version: str

    # Input data
    input_features: Dict[str, any]
    feature_names: List[str]

    # Protected attributes (segregated)
    protected_attributes_ref: Optional[str]  # Reference to segregated storage

    # Prediction outputs
    prediction_score: float
    prediction_class: str
    decision_outcome: DecisionOutcome
    confidence_interval_lower: float
    confidence_interval_upper: float

    # Explainability
    feature_importance: Dict[str, float]  # SHAP values
    top_contributing_features: List[tuple]  # [(feature, contribution), ...]
    human_readable_explanation: str
    counterfactual_example: Optional[Dict[str, any]]

    # Processing metadata
    processing_latency_ms: float
    preprocessing_time_ms: float
    inference_time_ms: float

    # Audit trail
    request_source: str
    user_id: Optional[str]
    session_id: Optional[str]

class CompliancePredictionLogger:
    """
    Production-grade prediction logger for regulatory compliance.

    Features:
    - Immutable append-only logging
    - Segregated protected attribute storage
    - Cryptographic verification
    - Automated compliance reporting
    - Decision reproduction capability
    """

    def __init__(
        self,
        s3_bucket: str,
        protected_attributes_bucket: str,
        encryption_key_id: str
    ):
        self.s3_client = boto3.client('s3')
        self.s3_bucket = s3_bucket
        self.protected_attributes_bucket = protected_attributes_bucket
        self.encryption_key_id = encryption_key_id

    def log_prediction(
        self,
        prediction_log: PredictionLog,
        protected_attributes: Optional[Dict[str, any]] = None
    ) -> str:
        """
        Log prediction with full audit trail.

        Returns:
            S3 object key for the logged prediction
        """
        # 1. Store protected attributes separately
        if protected_attributes:
            protected_ref = self._store_protected_attributes(
                prediction_log.prediction_id,
                protected_attributes
            )
            prediction_log.protected_attributes_ref = protected_ref

        # 2. Serialize prediction log
        log_dict = asdict(prediction_log)
        log_dict['timestamp'] = log_dict['timestamp'].isoformat()
        log_json = json.dumps(log_dict, indent=2)

        # 3. Generate cryptographic hash for verification
        log_hash = hashlib.sha256(log_json.encode()).hexdigest()
        log_dict['log_hash'] = log_hash

        # 4. Store in immutable S3 with encryption
        s3_key = self._generate_s3_key(prediction_log)

        self.s3_client.put_object(
            Bucket=self.s3_bucket,
            Key=s3_key,
            Body=json.dumps(log_dict, indent=2),
            ServerSideEncryption='aws:kms',
            SSEKMSKeyId=self.encryption_key_id,
            Metadata={
                'prediction_id': prediction_log.prediction_id,
                'model_version': prediction_log.model_version,
                'decision_outcome': prediction_log.decision_outcome.value,
                'log_hash': log_hash
            }
        )

        # 5. Also log to audit database for querying
        self._store_in_audit_database(log_dict)

        return s3_key

    def _store_protected_attributes(
        self,
        prediction_id: str,
        protected_attributes: Dict[str, any]
    ) -> str:
        """
        Store protected attributes in segregated, access-controlled storage.

        Returns:
            Reference ID for protected attributes
        """
        protected_ref = f"protected/{prediction_id}/{datetime.now().isoformat()}.json"

        # Enhanced encryption and access controls for protected data
        self.s3_client.put_object(
            Bucket=self.protected_attributes_bucket,
            Key=protected_ref,
            Body=json.dumps(protected_attributes, indent=2),
            ServerSideEncryption='aws:kms',
            SSEKMSKeyId=self.encryption_key_id,
            Metadata={
                'prediction_id': prediction_id,
                'data_classification': 'protected_attributes',
                'requires_audit_trail': 'true'
            },
            # Strict access control
            ACL='private'
        )

        # Log access to protected attributes
        self._audit_protected_attribute_access(prediction_id, 'WRITE')

        return protected_ref

    def _generate_s3_key(self, prediction_log: PredictionLog) -> str:
        """Generate S3 key with year/month/day partitioning for efficiency."""
        dt = prediction_log.timestamp
        return (
            f"predictions/"
            f"year={dt.year}/"
            f"month={dt.month:02d}/"
            f"day={dt.day:02d}/"
            f"{prediction_log.prediction_id}.json"
        )

    def _store_in_audit_database(self, log_dict: Dict):
        """Store in queryable database for compliance reporting."""
        # Would insert into PostgreSQL, DynamoDB, etc.
        pass

    def _audit_protected_attribute_access(self, prediction_id: str, action: str):
        """Audit all access to protected attributes."""
        audit_log = {
            'timestamp': datetime.now().isoformat(),
            'prediction_id': prediction_id,
            'action': action,
            'accessor': 'system',  # Would capture actual user in production
            'purpose': 'compliance_logging'
        }
        # Store audit log
        pass

    def reproduce_prediction(
        self,
        prediction_id: str,
        include_protected_attributes: bool = False
    ) -> Dict[str, any]:
        """
        Reproduce a historical prediction exactly.

        Essential for regulatory compliance and audits.
        """
        # 1. Retrieve prediction log
        prediction_log = self._retrieve_prediction_log(prediction_id)

        # 2. Load exact model version used
        model = self._load_model_version(
            prediction_log['model_name'],
            prediction_log['model_version'],
            prediction_log['model_artifact_hash']
        )

        # 3. Retrieve feature engineering version
        feature_engineer = self._load_feature_engineering_version(
            prediction_log['feature_engineering_version']
        )

        # 4. Reconstruct exact input
        input_features = prediction_log['input_features']

        # 5. Reproduce prediction
        reproduced_score = model.predict_proba([input_features])[0][1]

        # 6. Verify reproduction matches original
        original_score = prediction_log['prediction_score']
        if abs(reproduced_score - original_score) > 1e-6:
            raise ValueError(
                f"Reproduction mismatch: original={original_score}, "
                f"reproduced={reproduced_score}"
            )

        # 7. Include protected attributes if authorized
        if include_protected_attributes:
            self._audit_protected_attribute_access(prediction_id, 'READ')
            protected_attrs = self._retrieve_protected_attributes(
                prediction_log['protected_attributes_ref']
            )
            prediction_log['protected_attributes'] = protected_attrs

        return {
            'original_prediction': prediction_log,
            'reproduced_score': reproduced_score,
            'verification_status': 'EXACT_MATCH',
            'reproduction_timestamp': datetime.now().isoformat()
        }

    def _retrieve_prediction_log(self, prediction_id: str) -> Dict:
        """Retrieve prediction log from S3."""
        # Query audit database to find S3 key, then retrieve
        pass

    def _load_model_version(
        self,
        model_name: str,
        model_version: str,
        expected_hash: str
    ):
        """Load exact model version and verify hash."""
        # Load from model registry
        # Verify SHA-256 hash matches
        pass

    def _load_feature_engineering_version(self, version: str):
        """Load exact feature engineering code version."""
        # Load from version control or artifact store
        pass

    def _retrieve_protected_attributes(self, ref: str) -> Dict:
        """Retrieve protected attributes (with audit trail)."""
        response = self.s3_client.get_object(
            Bucket=self.protected_attributes_bucket,
            Key=ref
        )
        return json.loads(response['Body'].read())

    def generate_compliance_report(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, any]:
        """
        Generate comprehensive compliance report for date range.

        Includes fairness analysis, decision distribution, etc.
        """
        # Query all predictions in date range
        # Analyze fairness metrics across protected groups
        # Generate regulatory-compliant report
        pass
\end{lstlisting}

\subsection{Scenario 4: The Cross-Team Coordination Failure}

\subsubsection{The Problem}

MedTech AI deployed a patient risk stratification model used by clinicians, IT operations, and ML engineers. At 2:47 AM on a Saturday, the model began producing anomalous risk scores—incorrectly flagging low-risk patients as high-risk. Automated alerts fired to three different teams, but unclear ownership led to a 14-hour incident during which 2,847 patients received unnecessary urgent care notifications. The root cause was a data pipeline change by the data engineering team that wasn't communicated to ML ops. No single team owned the end-to-end incident response. The ML team thought IT was investigating, IT assumed ML was handling it, and clinical informatics wasn't informed until patients started complaining. The lack of a RACI matrix and unified incident command structure cost \$680K in operational waste and risked patient safety.

\subsubsection{Incident Timeline}

\begin{lstlisting}[language=bash, caption={Cross-Team Coordination Failure Timeline}]
Saturday, 2:47 AM:
  - Data pipeline update deployed (feature scaling changed)
  - Model serving continues with changed inputs
  - Risk scores begin drifting upward

Saturday, 2:52 AM:
  - Automated alert: "Feature drift detected" → ML Team Slack (no response, 2 AM)
  - Automated alert: "Prediction distribution shift" → Data Team (no on-call)
  - Automated alert: "API latency spike" → IT Ops PagerDuty (acknowledged)

Saturday, 3:15 AM:
  - IT ops engineer investigates latency alert
  - Determines "within acceptable range"
  - Closes alert without investigating model outputs

Saturday, 6:30 AM:
  - First batch of urgent care notifications sent (847 patients)
  - No escalation; automated system working as designed

Saturday, 8:00 AM:
  - Clinical staff begins shift
  - Notice unusual volume of high-risk patients
  - Assume seasonal illness outbreak

Saturday, 10:30 AM:
  - ML engineer sees Slack alerts from 2:52 AM
  - Begins investigation independently
  - Doesn't realize IT already investigated (different systems)

Saturday, 11:45 AM:
  - Second batch notifications sent (1,200 patients)
  - Hospital call center overwhelmed
  - Still no coordination between teams

Saturday, 1:00 PM:
  - Clinical informaticist notices pattern
  - Suspects model issue but doesn't know who to contact
  - Emails ML team lead (out of office, weekend)

Saturday, 2:30 PM:
  - Hospital administrator escalates to CTO
  - CTO initiates emergency bridge call
  - First time all teams on same call

Saturday, 3:00 PM:
  - Root cause identified: data pipeline change
  - Realization that three teams investigated independently
  - Model rollback initiated

Saturday, 4:45 PM:
  - Previous model version deployed
  - Notifications stopped
  - Total duration: 14 hours, 2,847 patients affected

Sunday-Monday:
  - Patient follow-up and apology calls
  - Incident post-mortem reveals coordination failures
  - RACI matrix and unified incident command established
\end{lstlisting}

\subsubsection{Before Configuration}

\begin{lstlisting}[language=yaml, caption={Fragmented Monitoring Configuration}]
# MedTech's original configuration (no coordination)
monitoring:
  # Multiple disconnected monitoring systems
  systems:
    ml_monitoring:
      owner: ml_team
      alerts: ml_team_slack
      escalation: none  # No cross-team escalation!

    infrastructure_monitoring:
      owner: it_ops
      alerts: pagerduty
      escalation: it_manager → vp_it

    data_pipeline_monitoring:
      owner: data_engineering
      alerts: data_team_email
      escalation: none  # Email only, no urgency!

    clinical_alerts:
      owner: clinical_informatics
      alerts: clinical_dashboard
      escalation: hospital_admin (business hours only!)

  incident_response:
    ownership: UNCLEAR  # Critical gap!
    communication_channels: FRAGMENTED
    escalation_path: UNDEFINED
    coordination: none

  change_management:
    notification: optional  # Teams not required to notify!
    impact_analysis: none
    rollback_authority: unclear
\end{lstlisting}

\subsubsection{After Configuration}

\begin{lstlisting}[language=yaml, caption={Unified Incident Response Configuration}]
# MedTech's improved configuration (coordinated response)
monitoring:
  # Unified monitoring with clear ownership
  unified_observability:
    platform: datadog  # Single source of truth
    dashboards:
      - executive_overview  # Real-time system health
      - ml_model_health  # Model-specific metrics
      - infrastructure_health  # System resources
      - clinical_impact  # Patient-facing metrics

  incident_management:
    # RACI Matrix for ML model incidents
    raci:
      model_performance_degradation:
        responsible: ml_engineer_on_call
        accountable: ml_team_lead
        consulted: [data_engineering, clinical_informatics]
        informed: [it_ops, hospital_admin]

      data_pipeline_issues:
        responsible: data_engineer_on_call
        accountable: data_engineering_lead
        consulted: [ml_team, it_ops]
        informed: [clinical_informatics]

      infrastructure_failures:
        responsible: it_ops_on_call
        accountable: it_manager
        consulted: [ml_team, data_engineering]
        informed: [all_stakeholders]

      clinical_safety_incidents:
        responsible: clinical_informaticist_on_call
        accountable: chief_medical_informatics_officer
        consulted: [ml_team, hospital_admin]
        informed: [legal, compliance, all_teams]

  unified_alerting:
    # All alerts go to unified incident channel
    primary_channel: "#incident-response-ml"
    participants:
      - ml_team_on_call
      - data_engineering_on_call
      - it_ops_on_call
      - clinical_informatics_on_call

    alert_routing:
      # Severity-based routing with cross-team visibility
      P1_critical:
        notify:
          - pagerduty: [ml_oncall, it_oncall, data_oncall, clinical_oncall]
          - slack: "#incident-response-ml"
          - sms: [team_leads, vp_engineering, chief_medical_officer]
        auto_escalate: 15_minutes  # If not acknowledged

      P2_major:
        notify:
          - slack: "#incident-response-ml"
          - pagerduty: [primary_oncall_only]
        auto_escalate: 1_hour

  incident_command_structure:
    # Unified incident commander
    commander_rotation:
      - week_1: ml_lead
      - week_2: data_engineering_lead
      - week_3: it_manager
      - week_4: clinical_informatics_lead

    commander_authority:
      - declare_incident
      - assign_roles
      - make_rollback_decisions
      - communicate_with_stakeholders
      - authorize_emergency_changes

    incident_roles:
      - commander: Leads response, makes decisions
      - technical_lead: Investigates root cause
      - communications: Updates stakeholders
      - scribe: Documents timeline and decisions

  change_management:
    # Mandatory change notifications
    notification_required: true
    notification_channels: ["#ml-changes", "#data-changes", "#infra-changes"]

    change_categories:
      high_risk:  # Model, pipeline, infrastructure changes
        require:
          - impact_analysis
          - affected_teams_approval
          - rollback_plan
          - staged_rollout
        notify:
          - all_stakeholders
          - incident_commander

      medium_risk:  # Configuration, feature changes
        require:
          - impact_analysis
          - rollback_plan
        notify:
          - affected_teams

      low_risk:  # Monitoring, dashboards
        require:
          - documentation
        notify:
          - team_channel

  communication_protocols:
    # Structured communication during incidents
    status_updates:
      frequency: every_30_minutes
      channels: ["#incident-response-ml", email_executives]
      template: |
        INCIDENT UPDATE
        Status: {status}
        Impact: {affected_patients} patients
        Root Cause: {current_hypothesis}
        Next Steps: {action_items}
        ETA Resolution: {estimate}

    escalation_triggers:
      - duration > 1_hour AND no_root_cause_identified
      - patient_safety_risk
      - >1000_patients_affected
      - public_visibility_risk

  post_incident_review:
    required: true
    timeline: within_48_hours
    attendees: [all_incident_participants, team_leads, executives]
    deliverables:
      - incident_timeline
      - root_cause_analysis
      - action_items_with_owners
      - prevention_strategies
\end{lstlisting}

\subsubsection{Quantified Business Impact}

\begin{lstlisting}[language=bash, caption={Cross-Team Coordination Impact Analysis}]
OPERATIONAL IMPACT:
  Incident duration:                14 hours
  Patients incorrectly flagged:    2,847
  Unnecessary urgent care visits:  342 patients
  Emergency department volume spike: +28%

FINANCIAL IMPACT:
  Unnecessary care delivery:        \$487,000
  Call center overtime:             \$ 34,000
  Clinical staff overtime:          \$ 68,000
  Patient compensation/goodwill:    \$ 91,000
  Total operational cost:           \$680,000

TEAM IMPACT:
  Engineering hours (weekend):      147 hours (across 3 teams)
  Duplicate investigation effort:   87 hours (wasted on parallel work)
  Post-incident remediation:        340 hours
  Average time to coordinate:       11.5 hours (should be <30 min)

PATIENT SAFETY RISK:
  Severity: HIGH
  Actual harm: None (fortunately)
  Potential harm: Delayed care for actual high-risk patients
  Regulatory reporting: Required (patient safety event)

POST-FIX IMPROVEMENTS:
  Time to unified incident response: 11.5 hours → 8 minutes
  Incident commander assignment:     N/A → Automatic (via rotation)
  Cross-team communication:          Ad-hoc → Structured (#incident-response-ml)
  Duplicate investigation:           87 hours → 0 hours

  Incidents resolved (6 months):
    With old system: Avg 6.4 hours, 3 teams working independently
    With new system: Avg 42 minutes, coordinated response

  Cost savings (6 months):
    Prevented operational waste:    \$2,100,000
    Reduced engineering time:       \$  340,000
    Avoided patient safety events:  \$4,500,000 (estimated liability)

  ROI of unified incident management: 920%
\end{lstlisting}

\subsubsection{Key Learnings}

\begin{itemize}
    \item \textbf{RACI matrices prevent ownership confusion}: Every incident type needs clear Responsible, Accountable, Consulted, Informed roles
    \item \textbf{Unified incident channels are essential}: Fragmented communication leads to duplicate work and delayed resolution
    \item \textbf{Incident command structure saves time}: Pre-assigned commanders make decisions faster than consensus-seeking
    \item \textbf{Cross-team visibility prevents blind spots}: All stakeholders must see all alerts, even if not primary responder
    \item \textbf{Change notification is mandatory, not optional}: Unannounced changes are a leading cause of production incidents
    \item \textbf{Automated escalation prevents delays}: If incident not acknowledged in 15 minutes, auto-escalate to leadership
    \item \textbf{Patient safety requires special protocols}: Clinical ML systems need clinical stakeholders in incident response
\end{itemize}

\subsubsection{Prevention Strategies}

\begin{lstlisting}[language=Python, caption={Unified Incident Coordinator}]
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from enum import Enum

class IncidentSeverity(Enum):
    """Incident severity levels."""
    P1_CRITICAL = "p1_critical"  # Patient safety, system down
    P2_MAJOR = "p2_major"  # Degraded performance, >100 patients affected
    P3_MINOR = "p3_minor"  # Minor issues, <100 patients affected

class IncidentStatus(Enum):
    """Incident lifecycle status."""
    DETECTED = "detected"
    ACKNOWLEDGED = "acknowledged"
    INVESTIGATING = "investigating"
    MITIGATING = "mitigating"
    RESOLVED = "resolved"
    CLOSED = "closed"

@dataclass
class IncidentRole:
    """Roles in incident response."""
    commander: str  # Leads response
    technical_lead: str  # Investigates root cause
    communications: str  # Stakeholder updates
    scribe: str  # Documents timeline

@dataclass
class Incident:
    """
    Unified incident representation.
    """
    incident_id: str
    title: str
    severity: IncidentSeverity
    status: IncidentStatus
    detected_at: datetime
    affected_systems: List[str]
    affected_patients: int
    root_cause_hypothesis: Optional[str]
    roles: IncidentRole

    timeline: List[Dict] = None
    action_items: List[Dict] = None

    def __post_init__(self):
        if self.timeline is None:
            self.timeline = []
        if self.action_items is None:
            self.action_items = []

class UnifiedIncidentCoordinator:
    """
    Unified incident management system coordinating cross-team response.

    Features:
    - RACI-based role assignment
    - Automated escalation
    - Cross-team communication
    - Incident command structure
    - Post-incident analysis
    """

    def __init__(self, config: Dict):
        self.config = config
        self.active_incidents: Dict[str, Incident] = {}
        self.incident_commanders_rotation = config['commander_rotation']
        self.raci_matrix = config['raci_matrix']

    def declare_incident(
        self,
        title: str,
        severity: IncidentSeverity,
        affected_systems: List[str],
        initial_observations: str
    ) -> Incident:
        """
        Declare new incident with automatic role assignment.

        Returns:
            Incident object with assigned roles
        """
        incident_id = f"INC-{datetime.now().strftime('%Y%m%d%H%M%S')}"

        # Determine incident commander from rotation
        commander = self._get_current_commander()

        # Assign roles based on affected systems and RACI matrix
        roles = self._assign_roles(affected_systems, severity)

        incident = Incident(
            incident_id=incident_id,
            title=title,
            severity=severity,
            status=IncidentStatus.DETECTED,
            detected_at=datetime.now(),
            affected_systems=affected_systems,
            affected_patients=0,  # Updated as we learn more
            root_cause_hypothesis=None,
            roles=roles
        )

        # Add to timeline
        incident.timeline.append({
            'timestamp': datetime.now(),
            'event': 'INCIDENT_DECLARED',
            'details': initial_observations,
            'actor': 'monitoring_system'
        })

        # Store active incident
        self.active_incidents[incident_id] = incident

        # Notify all stakeholders
        self._notify_stakeholders(incident)

        # Schedule automatic escalation if not acknowledged
        self._schedule_auto_escalation(incident)

        return incident

    def _get_current_commander(self) -> str:
        """Determine current incident commander from rotation."""
        week_of_year = datetime.now().isocalendar()[1]
        rotation_index = week_of_year % len(self.incident_commanders_rotation)
        return self.incident_commanders_rotation[rotation_index]

    def _assign_roles(
        self,
        affected_systems: List[str],
        severity: IncidentSeverity
    ) -> IncidentRole:
        """
        Assign incident roles based on RACI matrix and affected systems.
        """
        # Determine primary responsible party
        if 'ml_model' in affected_systems:
            technical_lead = self._get_oncall('ml_team')
        elif 'data_pipeline' in affected_systems:
            technical_lead = self._get_oncall('data_engineering')
        elif 'infrastructure' in affected_systems:
            technical_lead = self._get_oncall('it_ops')
        else:
            technical_lead = self._get_oncall('ml_team')  # Default

        return IncidentRole(
            commander=self._get_current_commander(),
            technical_lead=technical_lead,
            communications=self._get_oncall('communications'),
            scribe='incident_bot'  # Automated scribe
        )

    def _get_oncall(self, team: str) -> str:
        """Get current on-call person for team."""
        # Would integrate with PagerDuty API
        return f"{team}_oncall"

    def _notify_stakeholders(self, incident: Incident):
        """
        Notify all stakeholders based on severity and RACI matrix.
        """
        notifications = []

        if incident.severity == IncidentSeverity.P1_CRITICAL:
            # P1: Page everyone
            notifications.extend([
                ('pagerduty', incident.roles.commander),
                ('pagerduty', incident.roles.technical_lead),
                ('sms', 'vp_engineering'),
                ('sms', 'chief_medical_officer'),
                ('slack', '#incident-response-ml'),
                ('email', 'executives@medtech.com')
            ])
        elif incident.severity == IncidentSeverity.P2_MAJOR:
            notifications.extend([
                ('pagerduty', incident.roles.technical_lead),
                ('slack', '#incident-response-ml'),
                ('email', incident.roles.commander)
            ])
        else:  # P3
            notifications.extend([
                ('slack', '#incident-response-ml'),
                ('email', incident.roles.technical_lead)
            ])

        # Send notifications (implementation would use actual services)
        for channel, recipient in notifications:
            self._send_notification(channel, recipient, incident)

    def _send_notification(self, channel: str, recipient: str, incident: Incident):
        """Send notification via specified channel."""
        message = f"""
INCIDENT DECLARED: {incident.incident_id}
Title: {incident.title}
Severity: {incident.severity.value}
Commander: {incident.roles.commander}
Technical Lead: {incident.roles.technical_lead}
Affected Systems: {', '.join(incident.affected_systems)}

Join response: #incident-response-ml
        """.strip()
        # Implementation would send via Slack, PagerDuty, etc.
        pass

    def _schedule_auto_escalation(self, incident: Incident):
        """
        Schedule automatic escalation if not acknowledged.
        """
        escalation_time = timedelta(minutes=15)

        # Would use async scheduler
        # If incident still DETECTED after 15 min, auto-escalate
        def check_and_escalate():
            if incident.status == IncidentStatus.DETECTED:
                self._escalate_incident(incident)

        # Schedule check (simplified; would use actual scheduler)
        pass

    def _escalate_incident(self, incident: Incident):
        """Escalate incident to executive level."""
        incident.timeline.append({
            'timestamp': datetime.now(),
            'event': 'AUTO_ESCALATED',
            'details': 'No acknowledgment within 15 minutes',
            'actor': 'system'
        })

        # Notify executives
        executives = ['vp_engineering', 'cto', 'chief_medical_officer']
        for exec_role in executives:
            self._send_notification('sms', exec_role, incident)
            self._send_notification('pagerduty', exec_role, incident)

    def update_incident(
        self,
        incident_id: str,
        status: Optional[IncidentStatus] = None,
        affected_patients: Optional[int] = None,
        root_cause_hypothesis: Optional[str] = None,
        action_item: Optional[Dict] = None
    ):
        """
        Update incident with new information.
        """
        incident = self.active_incidents[incident_id]

        if status:
            incident.status = status
            incident.timeline.append({
                'timestamp': datetime.now(),
                'event': f'STATUS_CHANGED_TO_{status.value}',
                'actor': incident.roles.commander
            })

        if affected_patients is not None:
            incident.affected_patients = affected_patients

        if root_cause_hypothesis:
            incident.root_cause_hypothesis = root_cause_hypothesis
            incident.timeline.append({
                'timestamp': datetime.now(),
                'event': 'ROOT_CAUSE_IDENTIFIED',
                'details': root_cause_hypothesis,
                'actor': incident.roles.technical_lead
            })

        if action_item:
            incident.action_items.append(action_item)

        # Send status update to stakeholders
        self._send_status_update(incident)

    def _send_status_update(self, incident: Incident):
        """Send structured status update to stakeholders."""
        duration = datetime.now() - incident.detected_at
        eta_resolution = "Unknown"  # Would calculate based on progress

        update_message = f"""
INCIDENT UPDATE: {incident.incident_id}
Status: {incident.status.value}
Duration: {duration}
Impact: {incident.affected_patients} patients affected
Root Cause: {incident.root_cause_hypothesis or 'Under investigation'}
Next Steps: {self._format_action_items(incident.action_items)}
ETA Resolution: {eta_resolution}
        """.strip()

        # Send to incident channel
        self._send_notification('slack', '#incident-response-ml', incident)

    def _format_action_items(self, action_items: List[Dict]) -> str:
        """Format action items for status update."""
        if not action_items:
            return "None"
        return '\n'.join([
            f"- {item['description']} (Owner: {item.get('owner', 'Unassigned')})"
            for item in action_items[-3:]  # Last 3 items
        ])

    def generate_post_incident_report(self, incident_id: str) -> Dict:
        """
        Generate comprehensive post-incident report.

        Required within 48 hours of incident resolution.
        """
        incident = self.active_incidents[incident_id]

        return {
            'incident_id': incident.incident_id,
            'title': incident.title,
            'severity': incident.severity.value,
            'duration': str(datetime.now() - incident.detected_at),
            'affected_patients': incident.affected_patients,
            'root_cause': incident.root_cause_hypothesis,
            'timeline': incident.timeline,
            'action_items': incident.action_items,
            'lessons_learned': self._extract_lessons_learned(incident),
            'prevention_strategies': self._generate_prevention_strategies(incident)
        }

    def _extract_lessons_learned(self, incident: Incident) -> List[str]:
        """Extract lessons learned from incident."""
        # Would analyze timeline for patterns
        return [
            "Add monitoring for similar scenarios",
            "Improve cross-team communication",
            "Update runbooks with new findings"
        ]

    def _generate_prevention_strategies(self, incident: Incident) -> List[Dict]:
        """Generate actionable prevention strategies."""
        return [
            {
                'strategy': 'Enhanced monitoring',
                'owner': incident.roles.technical_lead,
                'deadline': 'Within 2 weeks'
            },
            {
                'strategy': 'Update change management process',
                'owner': 'engineering_leads',
                'deadline': 'Within 1 week'
            }
        ]
\end{lstlisting}

\subsection{Scenario 5: The Cost Explosion}

\subsubsection{The Problem}

CloudRec operated a large-scale recommendation model serving 50M requests daily. Over six months, their monitoring infrastructure costs grew from \$18K/month to \$247K/month—exceeding the entire ML team's budget and threatening to make the model economically unviable. The explosion was caused by logging every prediction with full feature vectors (850 features × 50M predictions/day = 42.5B feature values/day), storing high-cardinality metrics in Prometheus (3.2M unique time series), retaining raw logs for 90 days in Elasticsearch (187 TB), and running 24/7 drift detection on all 850 features every 5 minutes. The monitoring cost (\$247K/month) approached the model's business value (\$380K/month in incremental revenue). Leadership demanded either cost reduction or model shutdown. An emergency cost optimization reduced monitoring costs by 91\% while improving detection effectiveness.

\subsubsection{Incident Timeline}

\begin{lstlisting}[language=bash, caption={Cost Explosion Timeline}]
Month 1 (Jan):
  - Comprehensive monitoring deployed
  - Initial cost: \$18,000/month (seemed reasonable)
  - Logging: All predictions with full features

Month 2 (Feb):
  - Traffic grows 15%
  - Monitoring cost: \$24,000 (+33%)
  - No cost alerts configured

Month 3 (Mar):
  - Added 200 new features to model
  - Drift detection running on all 850 features
  - Monitoring cost: \$58,000 (+142%)
  - First warning from finance team

Month 4 (Apr):
  - Retention increased to 90 days for "compliance"
  - Elasticsearch cluster: 12 nodes → 28 nodes
  - Prometheus cardinality explosion (high-dimensional labels)
  - Monitoring cost: \$127,000 (+119%)
  - CFO escalation

Month 5 (May):
  - Emergency meeting: monitoring costs unsustainable
  - Attempt to reduce costs by downgrading instance types
  - Performance degradation; reverted
  - Monitoring cost: \$198,000 (+56%)
  - Model ROI now negative

Month 6 (Jun):
  - Monitoring cost: \$247,000 (+25%)
  - Total 6-month monitoring cost: \$672,000
  - Model incremental revenue: \$2.1M (6 months)
  - Monitoring = 32% of model value!
  - Leadership ultimatum: Fix or shutdown

Week 1 (Emergency Response):
  - Cost optimization task force formed
  - Audit of all monitoring costs
  - Identification of 7 cost drivers

Week 2-3 (Implementation):
  - Sampling strategy: Log 1% of predictions (full), 100% (minimal)
  - Feature reduction: Drift detection on 47 critical features only
  - Tiered retention: 7 days full logs, 30 days aggregates
  - Metrics cardinality reduction: 3.2M → 45K time series
  - Smart batching and compression

Week 4 (Results):
  - New monitoring cost: \$22,000/month (91% reduction!)
  - Detection effectiveness: Maintained (no degradation)
  - False positive rate: 12% → 3% (better quality!)
\end{lstlisting}

\subsubsection{Before Configuration}

\begin{lstlisting}[language=yaml, caption={Cost-Inefficient Monitoring Configuration}]
# CloudRec's original configuration (unsustainable costs)
monitoring:
  prediction_logging:
    sample_rate: 1.0  # Log 100% of predictions!
    feature_logging: full  # All 850 features per prediction
    storage: elasticsearch
    retention: 90_days  # 187 TB of logs!

    daily_volume:
      predictions: 50_000_000
      features_per_prediction: 850
      feature_values_logged: 42_500_000_000  # 42.5 billion/day!

  metrics:
    # High-cardinality metrics
    labels:
      - user_id  # 15M unique users!
      - item_id  # 2M unique items!
      - feature_1...feature_850  # Per-feature metrics

    prometheus:
      time_series: 3_200_000  # Unsustainable!
      retention: 30_days
      storage: 24_TB

  drift_detection:
    features: all_850_features  # Expensive!
    frequency: every_5_minutes  # 288 checks/day per feature
    method: kolmogorov_smirnov  # Requires all data points

  infrastructure:
    elasticsearch:
      nodes: 28
      instance_type: r5.4xlarge  # \$1.008/hour × 28 = \$21,081/month
      storage: 187_TB

    prometheus:
      nodes: 8
      instance_type: r5.2xlarge
      storage: 24_TB

  total_monthly_cost: \$247,000  # UNSUSTAINABLE!
\end{lstlisting}

\subsubsection{After Configuration}

\begin{lstlisting}[language=yaml, caption={Cost-Optimized Monitoring Configuration}]
# CloudRec's optimized configuration (91% cost reduction)
monitoring:
  prediction_logging:
    # Tiered sampling strategy
    sampling:
      full_logging:  # Complete feature vectors
        sample_rate: 0.01  # 1% of predictions
        selection: stratified  # Ensure representative sample

      minimal_logging:  # Essential fields only
        sample_rate: 1.0  # 100% of predictions
        fields: [prediction_id, timestamp, score, label, model_version]

      error_logging:  # Always log errors
        sample_rate: 1.0
        condition: error_occurred OR low_confidence

    storage:
      hot_tier:
        backend: elasticsearch
        retention: 7_days
        data: full_sampled_predictions

      warm_tier:
        backend: s3
        retention: 30_days
        data: aggregated_metrics
        compression: gzip

      cold_tier:
        backend: s3_glacier
        retention: 365_days
        data: compliance_required_only
        compression: zstd

    daily_volume_reduction:
      before: 42.5B feature values
      after: 500M feature values (99% reduction!)

  metrics:
    # Low-cardinality metrics only
    labels:
      # NO user_id or item_id labels!
      - model_version
      - prediction_outcome: [approved, denied, manual_review]  # 3 values
      - confidence_bucket: [high, medium, low]  # 3 values
      - feature_drift_status: [none, warning, critical]  # 3 values

    prometheus:
      time_series: 45_000  # 98.6% reduction!
      retention: 15_days  # Reduced from 30
      storage: 380_GB  # From 24 TB!

    aggregation:
      # Pre-aggregate before storage
      percentiles: [p50, p95, p99]
      windows: [1min, 5min, 1hour, 1day]

  drift_detection:
    # Smart feature selection
    features:
      critical: 47_features  # Top features by importance
      frequency: every_1_hour  # From every 5 min

      secondary: 150_features  # Medium importance
      frequency: every_6_hours

      tertiary: remaining_features
      frequency: daily

    method: psi  # Lighter than KS test
    sample_size: 10_000  # Don't need all data

  infrastructure:
    # Right-sized infrastructure
    elasticsearch:
      nodes: 6  # From 28!
      instance_type: r5.xlarge  # From 4xlarge
      storage: 12_TB  # From 187 TB
      cost: \$4,320/month  # From \$21,081

    prometheus:
      nodes: 2  # From 8
      instance_type: r5.large
      storage: 380_GB
      cost: \$720/month

    additional_optimizations:
      - compression: zstd_level_9
      - batching: 1000_predictions_per_batch
      - async_processing: true
      - intelligent_caching: redis (1hr TTL for aggregates)

  total_monthly_cost: \$22,000  # 91% REDUCTION!

  cost_per_million_predictions:
    before: \$4.94
    after: \$0.44
    reduction: 91%
\end{lstlisting}

\subsubsection{Quantified Business Impact}

\begin{lstlisting}[language=bash, caption={Cost Explosion Impact Analysis}]
MONITORING COSTS (6 months):
  Month 1: \$  18,000
  Month 2: \$  24,000
  Month 3: \$  58,000
  Month 4: \$ 127,000
  Month 5: \$ 198,000
  Month 6: \$ 247,000
  Total:   \$ 672,000

MODEL ECONOMICS (Before Optimization):
  Model incremental revenue:        \$380,000/month
  Monitoring cost:                  \$247,000/month
  Net value:                        \$133,000/month (65% eaten by monitoring!)
  ROI: Questionable

POST-OPTIMIZATION (Month 7+):
  Monitoring cost:                  \$ 22,000/month (91% reduction)
  Model incremental revenue:        \$380,000/month (unchanged)
  Net value:                        \$358,000/month
  ROI: 1,627% (excellent!)

  Annual savings:                   \$2,700,000

DETECTION EFFECTIVENESS:
  Drift detection coverage:
    Before: 850 features @ 5min intervals = Expensive overkill
    After: 47 critical features @ 1hr intervals = Effective

  Incident detection time:
    Before: 5 minutes (but drowning in data)
    After: 15 minutes (actionable insights)

  False positive rate:
    Before: 12% (noise from over-monitoring)
    After: 3% (focused on what matters)

BUSINESS OUTCOME:
  Model viability:                  Threatened → Secure
  Monitoring ROI:                   Negative → 1,627%
  Detection quality:                Maintained
  Team productivity:                Improved (less noise)

  Opportunity cost prevented:
    Model would have been shut down
    Lost revenue: \$380K/month × 12 months = \$4.56M
    Cost optimization saved the model!
\end{lstlisting}

\subsubsection{Key Learnings}

\begin{itemize}
    \item \textbf{Monitor monitoring costs}: Track monitoring spend as a percentage of model business value
    \item \textbf{Sampling is essential at scale}: You don't need 100\% of predictions; 1\% statistically valid sample suffices
    \item \textbf{Feature selection for drift detection}: Monitor the 20\% of features that drive 80\% of model performance
    \item \textbf{Tiered storage reduces costs dramatically}: Hot/warm/cold tiers match data access patterns to cost
    \item \textbf{High-cardinality labels are expensive}: Never use user\_id or item\_id as Prometheus labels
    \item \textbf{More monitoring $\neq$ better monitoring}: Over-monitoring creates noise and explodes costs
    \item \textbf{Aggregation before storage saves millions}: Store percentiles and aggregates, not raw data points
    \item \textbf{Cost optimization often improves quality}: Reducing noise improved detection effectiveness
\end{itemize}

\subsubsection{Prevention Strategies}

\begin{lstlisting}[language=Python, caption={Cost-Aware Monitoring System}]
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime
import random
import numpy as np

@dataclass
class MonitoringBudget:
    """
    Monitoring budget configuration.
    """
    monthly_budget: float  # Maximum monthly spend
    cost_per_gb_storage: float = 0.023  # S3 pricing
    cost_per_gb_elasticsearch: float = 0.15  # Elasticsearch pricing
    cost_per_million_metrics: float = 2.50  # Prometheus pricing

class CostAwareMonitoringSystem:
    """
    Monitoring system with built-in cost awareness and optimization.

    Features:
    - Intelligent sampling
    - Tiered storage
    - Cost tracking
    - Automatic cost optimization
    - Budget alerts
    """

    def __init__(self, budget: MonitoringBudget):
        self.budget = budget
        self.current_month_cost = 0.0
        self.prediction_count = 0
        self.storage_gb = 0.0

    def should_log_prediction(
        self,
        prediction: Dict,
        model_confidence: float
    ) -> tuple[bool, str]:
        """
        Intelligent sampling decision with cost awareness.

        Returns:
            (should_log, log_level) where log_level is 'full', 'minimal', or 'skip'
        """
        # 1. Always log errors and low-confidence predictions
        if prediction.get('error') or model_confidence < 0.7:
            return (True, 'full')

        # 2. Check if approaching budget limit
        budget_utilization = self.current_month_cost / self.budget.monthly_budget
        if budget_utilization > 0.9:
            # Approaching budget limit; reduce sampling
            sample_rate = 0.001  # 0.1%
        elif budget_utilization > 0.75:
            sample_rate = 0.005  # 0.5%
        else:
            sample_rate = 0.01  # 1%

        # 3. Stratified sampling for representative coverage
        if self._stratified_sample(prediction, sample_rate):
            return (True, 'full')

        # 4. Minimal logging for all predictions (for counting/aggregates)
        return (True, 'minimal')

    def _stratified_sample(self, prediction: Dict, base_rate: float) -> bool:
        """
        Stratified sampling ensuring coverage of all important segments.
        """
        # Ensure representation across outcome classes
        outcome = prediction.get('outcome')
        outcome_rates = {
            'approved': base_rate,
            'denied': base_rate * 2,  # 2x sampling for denied (less common)
            'manual_review': base_rate * 5  # 5x for manual review (rare but important)
        }

        sample_rate = outcome_rates.get(outcome, base_rate)
        return random.random() < sample_rate

    def log_prediction(
        self,
        prediction: Dict,
        log_level: str
    ) -> Dict[str, any]:
        """
        Log prediction with cost tracking.

        Returns:
            Logging metadata including cost impact
        """
        self.prediction_count += 1

        if log_level == 'full':
            # Full logging: prediction + all features
            log_size_bytes = self._calculate_log_size(prediction, include_features=True)
            storage_cost = self._estimate_storage_cost(log_size_bytes)

            log_data = {
                'prediction_id': prediction['id'],
                'timestamp': datetime.now(),
                'model_version': prediction['model_version'],
                'features': prediction['features'],  # All 850 features
                'prediction_score': prediction['score'],
                'outcome': prediction['outcome'],
                'confidence': prediction['confidence']
            }

        elif log_level == 'minimal':
            # Minimal logging: essential fields only
            log_size_bytes = self._calculate_log_size(prediction, include_features=False)
            storage_cost = self._estimate_storage_cost(log_size_bytes)

            log_data = {
                'prediction_id': prediction['id'],
                'timestamp': datetime.now(),
                'model_version': prediction['model_version'],
                'prediction_score': prediction['score'],
                'outcome': prediction['outcome']
                # No features!
            }
        else:
            return {'logged': False, 'reason': 'skipped'}

        # Track costs
        self.current_month_cost += storage_cost
        self.storage_gb += log_size_bytes / (1024 ** 3)

        # Store using tiered storage
        self._store_with_tiering(log_data, log_level)

        return {
            'logged': True,
            'log_level': log_level,
            'size_bytes': log_size_bytes,
            'cost_usd': storage_cost,
            'cumulative_cost': self.current_month_cost
        }

    def _calculate_log_size(self, prediction: Dict, include_features: bool) -> int:
        """Calculate log size in bytes."""
        base_size = 200  # Minimal fields

        if include_features:
            feature_count = len(prediction.get('features', {}))
            feature_size = feature_count * 8  # 8 bytes per float
            return base_size + feature_size

        return base_size

    def _estimate_storage_cost(self, size_bytes: float) -> float:
        """Estimate storage cost for this log entry."""
        size_gb = size_bytes / (1024 ** 3)

        # Hot tier: Elasticsearch (7 days)
        hot_cost = size_gb * self.budget.cost_per_gb_elasticsearch * (7/30)

        # Warm tier: S3 (30 days)
        warm_cost = size_gb * self.budget.cost_per_gb_storage * (30/30)

        return hot_cost + warm_cost

    def _store_with_tiering(self, log_data: Dict, log_level: str):
        """Store log data using tiered storage strategy."""
        if log_level == 'full':
            # Hot tier: Elasticsearch for fast querying (7 days)
            # self.elasticsearch.index(log_data)

            # Warm tier: Compress and move to S3 after 7 days
            # (Automated by lifecycle policy)
            pass
        else:
            # Minimal logs go directly to S3 (cheaper)
            # self.s3.put_object(compressed(log_data))
            pass

    def track_drift_detection_costs(
        self,
        features_monitored: int,
        check_frequency_hours: int
    ) -> Dict[str, float]:
        """
        Calculate and track drift detection costs.

        Returns:
            Cost breakdown for drift detection
        """
        # Compute cost per check
        daily_checks = 24 / check_frequency_hours
        monthly_checks = daily_checks * 30

        # Each check processes sample data
        sample_size = 10_000  # Don't need all data
        data_processed_gb = (features_monitored * sample_size * 8) / (1024 ** 3)

        # Compute costs (simplified)
        compute_cost_per_check = 0.01  # Rough estimate
        monthly_compute_cost = monthly_checks * compute_cost_per_check

        storage_cost = data_processed_gb * self.budget.cost_per_gb_storage

        total_cost = monthly_compute_cost + storage_cost

        return {
            'features_monitored': features_monitored,
            'daily_checks': daily_checks,
            'monthly_checks': monthly_checks,
            'monthly_compute_cost': monthly_compute_cost,
            'storage_cost': storage_cost,
            'total_monthly_cost': total_cost
        }

    def optimize_feature_monitoring(
        self,
        all_features: List[str],
        feature_importance: Dict[str, float],
        budget_allocation: float
    ) -> Dict[str, any]:
        """
        Select optimal subset of features to monitor within budget.

        Uses feature importance to prioritize critical features.
        """
        # Sort features by importance
        sorted_features = sorted(
            all_features,
            key=lambda f: feature_importance.get(f, 0),
            reverse=True
        )

        # Calculate cost per feature
        cost_per_feature = self.track_drift_detection_costs(
            features_monitored=1,
            check_frequency_hours=1
        )['total_monthly_cost']

        # Determine how many features we can afford
        affordable_features = int(budget_allocation / cost_per_feature)

        # Tiered monitoring strategy
        critical_features = sorted_features[:min(50, affordable_features)]
        secondary_features = sorted_features[50:min(200, affordable_features * 3)]
        tertiary_features = sorted_features[200:]

        return {
            'critical_features': {
                'features': critical_features,
                'frequency_hours': 1,
                'count': len(critical_features)
            },
            'secondary_features': {
                'features': secondary_features,
                'frequency_hours': 6,
                'count': len(secondary_features)
            },
            'tertiary_features': {
                'features': tertiary_features,
                'frequency_hours': 24,
                'count': len(tertiary_features)
            },
            'estimated_monthly_cost': self._calculate_tiered_cost(
                len(critical_features),
                len(secondary_features),
                len(tertiary_features)
            ),
            'coverage': (len(critical_features) + len(secondary_features)) / len(all_features)
        }

    def _calculate_tiered_cost(
        self,
        critical_count: int,
        secondary_count: int,
        tertiary_count: int
    ) -> float:
        """Calculate total cost for tiered monitoring strategy."""
        cost_critical = self.track_drift_detection_costs(critical_count, 1)['total_monthly_cost']
        cost_secondary = self.track_drift_detection_costs(secondary_count, 6)['total_monthly_cost']
        cost_tertiary = self.track_drift_detection_costs(tertiary_count, 24)['total_monthly_cost']

        return cost_critical + cost_secondary + cost_tertiary

    def get_cost_report(self) -> Dict[str, any]:
        """
        Generate comprehensive cost report.
        """
        budget_utilization = self.current_month_cost / self.budget.monthly_budget

        return {
            'current_month_cost': self.current_month_cost,
            'monthly_budget': self.budget.monthly_budget,
            'budget_utilization': budget_utilization,
            'predictions_logged': self.prediction_count,
            'cost_per_million_predictions': (
                self.current_month_cost / self.prediction_count * 1_000_000
                if self.prediction_count > 0 else 0
            ),
            'storage_gb': self.storage_gb,
            'alert_status': 'CRITICAL' if budget_utilization > 0.9 else 'OK'
        }
\end{lstlisting}

\subsection{Post-Mortem Analysis Framework}

After every significant monitoring incident, conduct a structured post-mortem to extract maximum learning value and prevent recurrence. This framework applies to all five scenarios above and future incidents.

\subsubsection{Post-Mortem Template}

\begin{lstlisting}[language=yaml, caption={Structured Post-Mortem Template}]
incident_post_mortem:
  metadata:
    incident_id: "INC-20231015-001"
    incident_date: "2023-10-15"
    severity: "P1_CRITICAL"
    duration_hours: 14
    participants: [ml_team, data_engineering, it_ops, clinical_informatics]
    post_mortem_date: "2023-10-17"

  executive_summary:
    # 2-3 sentences summarizing incident and impact
    what_happened: >
      Patient risk model produced anomalous scores for 14 hours,
      affecting 2,847 patients due to uncoordinated cross-team response.

    business_impact: >
      $680K operational waste, patient safety risk, regulatory reporting required.

    root_cause: >
      Data pipeline change deployed without ML team notification,
      combined with unclear incident ownership.

  timeline:
    # Chronological event sequence
    detection:
      timestamp: "2023-10-15 02:47:00"
      detected_by: "automated_monitoring"
      first_alert: "Feature drift detected"

    response:
      first_human_acknowledgment: "2023-10-15 03:15:00"
      time_to_acknowledgment: "28 minutes"
      coordinator_assigned: "2023-10-15 14:30:00"  # 11.7 hours delay!

    resolution:
      root_cause_identified: "2023-10-15 15:00:00"
      mitigation_started: "2023-10-15 15:30:00"
      incident_resolved: "2023-10-15 16:45:00"
      total_duration: "14 hours"

  root_cause_analysis:
    # Five Whys methodology
    problem: "Anomalous patient risk scores"

    why_1: "Data pipeline changed feature scaling"
    why_2: "Change deployed without ML team notification"
    why_3: "No mandatory change notification process"
    why_4: "Teams operate in silos with separate tools"
    why_5: "No cross-team coordination framework (ROOT CAUSE)"

    contributing_factors:
      - "Unclear incident ownership (RACI not defined)"
      - "Fragmented alerting across 3 systems"
      - "No incident commander role"
      - "Weekend deployment with insufficient coverage"

  impact_quantification:
    patient_impact:
      patients_affected: 2847
      unnecessary_interventions: 342
      patient_safety_events: 0  # Fortunately none

    financial_impact:
      operational_cost: 680000
      engineering_hours: 147
      patient_compensation: 91000

    reputational_impact:
      severity: "MEDIUM"
      regulatory_reporting: true

  what_went_well:
    - "Automated monitoring detected issue within 5 minutes"
    - "IT ops responded quickly to their alert"
    - "Once coordinated, resolution was fast (75 minutes)"

  what_went_wrong:
    - "No cross-team coordination for 11.7 hours"
    - "Duplicate investigation effort (3 teams independently)"
    - "Change deployed without impact assessment"
    - "Clinical team not included in technical incident response"

  action_items:
    high_priority:
      - action: "Implement RACI matrix for all incident types"
        owner: "engineering_director"
        deadline: "2023-10-24"
        status: "IN_PROGRESS"

      - action: "Deploy unified incident response channel"
        owner: "ml_lead"
        deadline: "2023-10-20"
        status: "COMPLETED"

      - action: "Mandatory change notification process"
        owner: "data_engineering_lead"
        deadline: "2023-10-22"
        status: "IN_PROGRESS"

    medium_priority:
      - action: "Establish incident commander rotation"
        owner: "engineering_director"
        deadline: "2023-10-31"

      - action: "Cross-team incident response training"
        owner: "ml_lead"
        deadline: "2023-11-15"

  prevention_strategies:
    immediate:
      - "Create #incident-response-ml Slack channel (all teams)"
      - "Document incident commander responsibilities"
      - "Add mandatory change notification to deployment pipeline"

    short_term:
      - "Implement RACI matrix across all ML systems"
      - "Unified monitoring dashboard (all teams see same data)"
      - "Automated change impact analysis"

    long_term:
      - "Chaos engineering exercises quarterly"
      - "Cross-team incident response drills"
      - "Automated rollback on detected anomalies"

  lessons_learned:
    technical:
      - "Cross-team visibility prevents duplicate work"
      - "Automated detection is worthless without coordinated response"
      - "Change management must include downstream impact analysis"

    organizational:
      - "Clear ownership (RACI) essential for incidents"
      - "Weekend deployments need explicit on-call coverage"
      - "Clinical stakeholders must be in technical incident response"

    cultural:
      - "Silos are dangerous; unified teams respond faster"
      - "Blame-free post-mortems encourage honest analysis"
      - "Document everything during incidents for learning"

  metrics_to_track:
    # Metrics for preventing recurrence
    - metric: "Time to incident acknowledgment"
      target: "< 15 minutes"
      current: "28 minutes → 8 minutes (improved)"

    - metric: "Cross-team coordination time"
      target: "< 30 minutes"
      current: "11.7 hours → 8 minutes (improved)"

    - metric: "Change notification compliance"
      target: "100%"
      current: "0% → 100% (mandated)"

  follow_up:
    review_date: "2023-11-15"
    review_participants: [all_incident_participants]
    success_criteria:
      - "All high-priority action items completed"
      - "RACI matrix operational and tested"
      - "No similar incidents in 30 days"
\end{lstlisting}

\subsubsection{Post-Mortem Best Practices}

\begin{itemize}
    \item \textbf{Blameless culture is essential}: Focus on systems and processes, not individuals; people don't cause incidents, systems do
    \item \textbf{Conduct within 48 hours}: Memory fades quickly; strike while details are fresh
    \item \textbf{Include all participants}: Every team that touched the incident should contribute
    \item \textbf{Use Five Whys to find root cause}: Keep asking "why" until you reach systemic issues
    \item \textbf{Quantify everything}: Business impact, technical impact, time spent—numbers drive action
    \item \textbf{Action items need owners and deadlines}: Vague commitments accomplish nothing
    \item \textbf{Track action item completion}: Post-mortems are worthless if learnings aren't implemented
    \item \textbf{Share broadly}: Other teams can learn from your incidents
    \item \textbf{Celebrate improvements}: Acknowledge when post-mortem actions prevent future incidents
    \item \textbf{Review post-mortem effectiveness}: Quarterly review of whether action items actually prevented recurrence
\end{itemize}

\section{Observability Patterns for ML Systems}

Modern ML systems require comprehensive observability beyond traditional application monitoring. This section presents production-ready patterns for distributed tracing, structured logging, custom metrics, health checking, and performance profiling tailored specifically for machine learning workloads.

\subsection{Distributed Tracing with Model Inference Correlation}

Distributed tracing tracks requests as they flow through microservices, capturing timing information and dependencies. For ML systems, this includes model inference paths, feature pipeline execution, and downstream service calls.

\begin{lstlisting}[language=Python, caption={Production TracingManager with OpenTelemetry}]
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource
from contextlib import contextmanager
from typing import Dict, Optional, List
from datetime import datetime
import hashlib
import json

class TracingManager:
    """
    Production-grade distributed tracing for ML inference pipelines.

    Features:
    - Request flow analysis across microservices
    - Model inference correlation tracking
    - Feature pipeline dependency mapping
    - Performance bottleneck identification
    - Automatic error capture and tagging
    """

    def __init__(
        self,
        service_name: str,
        jaeger_endpoint: str = "localhost:6831",
        environment: str = "production"
    ):
        # Configure OpenTelemetry tracer
        resource = Resource(attributes={
            "service.name": service_name,
            "service.version": "1.0.0",
            "deployment.environment": environment
        })

        provider = TracerProvider(resource=resource)

        # Configure Jaeger exporter
        jaeger_exporter = JaegerExporter(
            agent_host_name=jaeger_endpoint.split(':')[0],
            agent_port=int(jaeger_endpoint.split(':')[1]),
        )

        # Batch span processor for performance
        span_processor = BatchSpanProcessor(jaeger_exporter)
        provider.add_span_processor(span_processor)

        trace.set_tracer_provider(provider)
        self.tracer = trace.get_tracer(__name__)

        # Trace correlation cache
        self.active_traces: Dict[str, trace.Span] = {}

    @contextmanager
    def trace_inference(
        self,
        model_name: str,
        model_version: str,
        request_id: str,
        user_id: Optional[str] = None,
        **attributes
    ):
        """
        Trace a complete ML inference request.

        Creates parent span for entire inference flow and enables
        correlation with downstream operations.

        Args:
            model_name: Name of model being invoked
            model_version: Version of model
            request_id: Unique request identifier
            user_id: Optional user identifier
            **attributes: Additional attributes to attach
        """
        with self.tracer.start_as_current_span(
            f"ml.inference.{model_name}",
            kind=trace.SpanKind.SERVER
        ) as span:
            # Add standard ML attributes
            span.set_attribute("ml.model.name", model_name)
            span.set_attribute("ml.model.version", model_version)
            span.set_attribute("ml.request.id", request_id)

            if user_id:
                span.set_attribute("ml.user.id", hashlib.sha256(user_id.encode()).hexdigest()[:16])

            # Add custom attributes
            for key, value in attributes.items():
                span.set_attribute(f"ml.{key}", str(value))

            # Store for correlation
            self.active_traces[request_id] = span

            try:
                yield span
            except Exception as e:
                # Capture errors
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise
            finally:
                # Cleanup
                self.active_traces.pop(request_id, None)

    @contextmanager
    def trace_feature_pipeline(
        self,
        request_id: str,
        pipeline_stage: str,
        feature_count: int,
        **attributes
    ):
        """
        Trace feature pipeline execution.

        Tracks feature engineering, preprocessing, and validation
        as child spans of the parent inference request.
        """
        with self.tracer.start_as_current_span(
            f"ml.feature_pipeline.{pipeline_stage}",
            kind=trace.SpanKind.INTERNAL
        ) as span:
            span.set_attribute("ml.request.id", request_id)
            span.set_attribute("ml.pipeline.stage", pipeline_stage)
            span.set_attribute("ml.features.count", feature_count)

            for key, value in attributes.items():
                span.set_attribute(f"ml.{key}", str(value))

            try:
                yield span
            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise

    @contextmanager
    def trace_model_execution(
        self,
        request_id: str,
        model_name: str,
        input_shape: tuple,
        **attributes
    ):
        """
        Trace actual model inference execution.

        Captures model-specific metrics like input shape,
        output shape, and inference latency.
        """
        with self.tracer.start_as_current_span(
            f"ml.model.predict.{model_name}",
            kind=trace.SpanKind.INTERNAL
        ) as span:
            span.set_attribute("ml.request.id", request_id)
            span.set_attribute("ml.model.name", model_name)
            span.set_attribute("ml.input.shape", str(input_shape))

            for key, value in attributes.items():
                span.set_attribute(f"ml.{key}", str(value))

            start_time = datetime.now()

            try:
                yield span
            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise
            finally:
                # Record inference latency
                latency_ms = (datetime.now() - start_time).total_seconds() * 1000
                span.set_attribute("ml.inference.latency_ms", latency_ms)

    @contextmanager
    def trace_external_service(
        self,
        request_id: str,
        service_name: str,
        operation: str,
        **attributes
    ):
        """
        Trace calls to external services (databases, APIs, etc.).

        Enables dependency mapping and bottleneck identification.
        """
        with self.tracer.start_as_current_span(
            f"external.{service_name}.{operation}",
            kind=trace.SpanKind.CLIENT
        ) as span:
            span.set_attribute("ml.request.id", request_id)
            span.set_attribute("peer.service", service_name)
            span.set_attribute("service.operation", operation)

            for key, value in attributes.items():
                span.set_attribute(key, str(value))

            try:
                yield span
            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise

    def add_event(
        self,
        request_id: str,
        event_name: str,
        attributes: Optional[Dict] = None
    ):
        """
        Add event to active trace.

        Events mark significant points in request processing without
        creating separate spans.
        """
        if request_id in self.active_traces:
            span = self.active_traces[request_id]
            span.add_event(event_name, attributes=attributes or {})

    def get_trace_context(self, request_id: str) -> Optional[Dict]:
        """
        Get trace context for propagation to downstream services.

        Returns:
            Dictionary with trace_id and span_id for correlation
        """
        if request_id in self.active_traces:
            span = self.active_traces[request_id]
            context = span.get_span_context()
            return {
                "trace_id": format(context.trace_id, '032x'),
                "span_id": format(context.span_id, '016x'),
                "trace_flags": context.trace_flags
            }
        return None


# Example usage demonstrating full request flow tracing
class MLServiceWithTracing:
    """Example ML service with comprehensive tracing."""

    def __init__(self):
        self.tracing = TracingManager(
            service_name="fraud-detection-service",
            environment="production"
        )

    async def predict(self, request_data: Dict) -> Dict:
        """
        Make prediction with full distributed tracing.
        """
        request_id = request_data['request_id']
        user_id = request_data.get('user_id')

        # Start parent trace for entire request
        with self.tracing.trace_inference(
            model_name="fraud_detector",
            model_version="v3.2",
            request_id=request_id,
            user_id=user_id,
            transaction_amount=request_data.get('amount', 0)
        ):
            # Trace feature pipeline
            with self.tracing.trace_feature_pipeline(
                request_id=request_id,
                pipeline_stage="preprocessing",
                feature_count=47
            ):
                features = await self._preprocess(request_data)

            # Trace feature store lookup
            with self.tracing.trace_external_service(
                request_id=request_id,
                service_name="feature_store",
                operation="batch_get_features"
            ):
                enriched_features = await self._get_features(user_id)

            # Add event for feature validation
            self.tracing.add_event(
                request_id=request_id,
                event_name="features_validated",
                attributes={"feature_count": len(enriched_features)}
            )

            # Trace model execution
            with self.tracing.trace_model_execution(
                request_id=request_id,
                model_name="fraud_detector",
                input_shape=(1, 47)
            ) as span:
                prediction = await self._model_predict(enriched_features)

                # Record prediction metadata
                span.set_attribute("ml.prediction.score", prediction['score'])
                span.set_attribute("ml.prediction.class", prediction['class'])

            return prediction

    async def _preprocess(self, data: Dict) -> Dict:
        # Preprocessing implementation
        return {}

    async def _get_features(self, user_id: str) -> Dict:
        # Feature store lookup
        return {}

    async def _model_predict(self, features: Dict) -> Dict:
        # Model inference
        return {'score': 0.85, 'class': 'legitimate'}
\end{lstlisting}

\subsection{Structured Logging with Correlation IDs}

Structured logging enables efficient searching, filtering, and correlation across distributed systems. For ML systems, logs capture model decisions, feature values, and business context.

\begin{lstlisting}[language=Python, caption={Production StructuredLogger with Metadata Enrichment}]
import logging
import json
from datetime import datetime
from typing import Dict, Optional, Any
from contextlib import contextmanager
import threading
import hashlib

class StructuredLogger:
    """
    Production-grade structured logging for ML systems.

    Features:
    - JSON-formatted logs for machine parsing
    - Automatic correlation ID propagation
    - Sensitive data masking
    - Log level enrichment
    - Context managers for scoped logging
    - Integration with log aggregation (ELK, Splunk)
    """

    def __init__(
        self,
        service_name: str,
        environment: str = "production",
        log_level: str = "INFO"
    ):
        self.service_name = service_name
        self.environment = environment

        # Configure Python logger
        self.logger = logging.getLogger(service_name)
        self.logger.setLevel(getattr(logging, log_level))

        # JSON formatter
        handler = logging.StreamHandler()
        handler.setFormatter(self._JSONFormatter())
        self.logger.addHandler(handler)

        # Thread-local storage for correlation context
        self.context = threading.local()

        # Sensitive field patterns
        self.sensitive_fields = {
            'password', 'secret', 'token', 'api_key', 'ssn',
            'credit_card', 'cvv', 'account_number'
        }

    class _JSONFormatter(logging.Formatter):
        """Custom JSON formatter for structured logs."""

        def format(self, record: logging.LogRecord) -> str:
            log_data = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
            }

            # Add extra fields
            if hasattr(record, 'extra'):
                log_data.update(record.extra)

            return json.dumps(log_data)

    @contextmanager
    def correlation_context(
        self,
        request_id: str,
        user_id: Optional[str] = None,
        **context_fields
    ):
        """
        Establish correlation context for all logs within scope.

        All logs emitted within this context will include correlation ID
        and additional context fields.
        """
        # Store context in thread-local storage
        old_context = getattr(self.context, 'fields', {})

        self.context.fields = {
            'request_id': request_id,
            'service': self.service_name,
            'environment': self.environment,
        }

        if user_id:
            # Hash user_id for privacy
            self.context.fields['user_id_hash'] = hashlib.sha256(
                user_id.encode()
            ).hexdigest()[:16]

        self.context.fields.update(context_fields)

        try:
            yield
        finally:
            self.context.fields = old_context

    def _enrich_log_data(self, **kwargs) -> Dict:
        """Enrich log data with correlation context and metadata."""
        log_data = {}

        # Add correlation context if available
        if hasattr(self.context, 'fields'):
            log_data.update(self.context.fields)

        # Add provided fields
        log_data.update(kwargs)

        # Mask sensitive data
        log_data = self._mask_sensitive_data(log_data)

        return log_data

    def _mask_sensitive_data(self, data: Dict) -> Dict:
        """Recursively mask sensitive fields."""
        masked = {}

        for key, value in data.items():
            key_lower = key.lower()

            # Check if field is sensitive
            if any(sensitive in key_lower for sensitive in self.sensitive_fields):
                masked[key] = "***REDACTED***"
            elif isinstance(value, dict):
                masked[key] = self._mask_sensitive_data(value)
            elif isinstance(value, list):
                masked[key] = [
                    self._mask_sensitive_data(item) if isinstance(item, dict) else item
                    for item in value
                ]
            else:
                masked[key] = value

        return masked

    def info(self, message: str, **kwargs):
        """Log info level message with structured data."""
        extra_data = self._enrich_log_data(**kwargs)
        self.logger.info(message, extra={'extra': extra_data})

    def warning(self, message: str, **kwargs):
        """Log warning level message with structured data."""
        extra_data = self._enrich_log_data(**kwargs)
        self.logger.warning(message, extra={'extra': extra_data})

    def error(self, message: str, exception: Optional[Exception] = None, **kwargs):
        """Log error level message with structured data and exception."""
        extra_data = self._enrich_log_data(**kwargs)

        if exception:
            extra_data['exception_type'] = type(exception).__name__
            extra_data['exception_message'] = str(exception)
            extra_data['exception_stacktrace'] = self._format_stacktrace(exception)

        self.logger.error(message, extra={'extra': extra_data})

    def _format_stacktrace(self, exception: Exception) -> str:
        """Format exception stacktrace for logging."""
        import traceback
        return ''.join(traceback.format_exception(
            type(exception), exception, exception.__traceback__
        ))

    def log_prediction(
        self,
        model_name: str,
        model_version: str,
        prediction: Any,
        confidence: float,
        latency_ms: float,
        **context
    ):
        """
        Log ML prediction with full context.

        Specialized logging for model predictions including
        model metadata, prediction details, and business context.
        """
        self.info(
            f"Model prediction: {model_name}",
            model_name=model_name,
            model_version=model_version,
            prediction_class=str(prediction),
            prediction_confidence=confidence,
            inference_latency_ms=latency_ms,
            **context
        )

    def log_drift_detection(
        self,
        feature_name: str,
        drift_score: float,
        threshold: float,
        drift_detected: bool,
        **context
    ):
        """Log drift detection results."""
        level = "warning" if drift_detected else "info"
        message = f"Drift {'DETECTED' if drift_detected else 'check'}: {feature_name}"

        log_func = self.warning if drift_detected else self.info
        log_func(
            message,
            feature_name=feature_name,
            drift_score=drift_score,
            drift_threshold=threshold,
            drift_detected=drift_detected,
            **context
        )

    def log_model_performance(
        self,
        model_name: str,
        metric_name: str,
        metric_value: float,
        window: str,
        **context
    ):
        """Log model performance metrics."""
        self.info(
            f"Model performance: {model_name} {metric_name}",
            model_name=model_name,
            metric_name=metric_name,
            metric_value=metric_value,
            evaluation_window=window,
            **context
        )

    def log_feature_validation(
        self,
        validation_passed: bool,
        failed_features: List[str],
        **context
    ):
        """Log feature validation results."""
        if validation_passed:
            self.info(
                "Feature validation passed",
                validation_result="PASS",
                **context
            )
        else:
            self.warning(
                "Feature validation failed",
                validation_result="FAIL",
                failed_features=failed_features,
                failed_count=len(failed_features),
                **context
            )


# Example usage showing correlation across request lifecycle
class MLPredictionService:
    """Example service with structured logging."""

    def __init__(self):
        self.logger = StructuredLogger(
            service_name="ml-prediction-service",
            environment="production"
        )

    async def handle_request(self, request_data: Dict) -> Dict:
        """Handle prediction request with structured logging."""
        request_id = request_data['request_id']
        user_id = request_data.get('user_id')

        # Establish correlation context for entire request
        with self.logger.correlation_context(
            request_id=request_id,
            user_id=user_id,
            transaction_amount=request_data.get('amount')
        ):
            self.logger.info(
                "Received prediction request",
                endpoint="/api/v1/predict",
                method="POST"
            )

            try:
                # Feature extraction
                features = await self._extract_features(request_data)
                self.logger.info(
                    "Features extracted",
                    feature_count=len(features),
                    extraction_duration_ms=12.5
                )

                # Validate features
                validation_result = self._validate_features(features)
                self.logger.log_feature_validation(
                    validation_passed=validation_result['valid'],
                    failed_features=validation_result.get('failed', [])
                )

                # Model prediction
                start_time = datetime.now()
                prediction = await self._predict(features)
                latency_ms = (datetime.now() - start_time).total_seconds() * 1000

                # Log prediction
                self.logger.log_prediction(
                    model_name="fraud_detector",
                    model_version="v3.2",
                    prediction=prediction['class'],
                    confidence=prediction['confidence'],
                    latency_ms=latency_ms,
                    feature_count=len(features)
                )

                self.logger.info(
                    "Request completed successfully",
                    status="success",
                    total_duration_ms=latency_ms + 12.5
                )

                return prediction

            except Exception as e:
                self.logger.error(
                    "Request failed",
                    exception=e,
                    status="error"
                )
                raise

    async def _extract_features(self, data: Dict) -> Dict:
        return {}

    def _validate_features(self, features: Dict) -> Dict:
        return {'valid': True}

    async def _predict(self, features: Dict) -> Dict:
        return {'class': 'legitimate', 'confidence': 0.92}
\end{lstlisting}

\subsection{Health Checking with Deep Model Validation}

Health checks verify system readiness and detect issues before they impact users. For ML systems, this includes model availability, dependency health, and prediction quality validation.

\begin{lstlisting}[language=Python, caption={Comprehensive HealthChecker for ML Systems}]
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import numpy as np

class HealthStatus(Enum):
    """Health check status levels."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

@dataclass
class HealthCheckResult:
    """Result of a health check."""
    component: str
    status: HealthStatus
    message: str
    latency_ms: float
    timestamp: datetime
    details: Optional[Dict] = None

class HealthChecker:
    """
    Comprehensive health checking for ML systems.

    Features:
    - Deep model validation (loading, inference, accuracy)
    - Dependency health checks (databases, feature stores, APIs)
    - Resource availability checks (memory, disk, GPU)
    - Liveness probes (service is running)
    - Readiness probes (service can accept traffic)
    - Startup probes (service initialization complete)
    """

    def __init__(self, service_name: str):
        self.service_name = service_name
        self.checks: Dict[str, Callable] = {}
        self.check_history: List[HealthCheckResult] = []
        self.last_check_time: Optional[datetime] = None

    def register_check(
        self,
        name: str,
        check_func: Callable,
        check_type: str = "readiness"
    ):
        """
        Register a health check function.

        Args:
            name: Unique name for this check
            check_func: Async function that performs the check
            check_type: One of 'liveness', 'readiness', 'startup'
        """
        self.checks[name] = {
            'func': check_func,
            'type': check_type
        }

    async def check_all(self) -> Dict[str, any]:
        """
        Execute all registered health checks.

        Returns:
            Comprehensive health status with individual check results
        """
        results = []
        start_time = datetime.now()

        # Execute all checks in parallel
        check_tasks = [
            self._execute_check(name, check['func'])
            for name, check in self.checks.items()
        ]

        results = await asyncio.gather(*check_tasks, return_exceptions=True)

        # Process results
        check_results = []
        for result in results:
            if isinstance(result, Exception):
                check_results.append(HealthCheckResult(
                    component="unknown",
                    status=HealthStatus.UNHEALTHY,
                    message=f"Check failed: {str(result)}",
                    latency_ms=0,
                    timestamp=datetime.now()
                ))
            else:
                check_results.append(result)

        # Store history
        self.check_history.extend(check_results)
        self.last_check_time = datetime.now()

        # Determine overall health
        overall_status = self._compute_overall_health(check_results)

        total_latency = (datetime.now() - start_time).total_seconds() * 1000

        return {
            'service': self.service_name,
            'status': overall_status.value,
            'timestamp': datetime.now().isoformat(),
            'checks': [
                {
                    'component': r.component,
                    'status': r.status.value,
                    'message': r.message,
                    'latency_ms': r.latency_ms,
                    'details': r.details
                }
                for r in check_results
            ],
            'total_checks': len(check_results),
            'healthy_checks': sum(1 for r in check_results if r.status == HealthStatus.HEALTHY),
            'total_latency_ms': total_latency
        }

    async def _execute_check(
        self,
        name: str,
        check_func: Callable
    ) -> HealthCheckResult:
        """Execute individual health check with timing."""
        start_time = datetime.now()

        try:
            result = await check_func()
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000

            return HealthCheckResult(
                component=name,
                status=result.get('status', HealthStatus.HEALTHY),
                message=result.get('message', 'Check passed'),
                latency_ms=latency_ms,
                timestamp=datetime.now(),
                details=result.get('details')
            )
        except Exception as e:
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000

            return HealthCheckResult(
                component=name,
                status=HealthStatus.UNHEALTHY,
                message=f"Check failed: {str(e)}",
                latency_ms=latency_ms,
                timestamp=datetime.now()
            )

    def _compute_overall_health(
        self,
        results: List[HealthCheckResult]
    ) -> HealthStatus:
        """Compute overall health from individual check results."""
        if any(r.status == HealthStatus.UNHEALTHY for r in results):
            return HealthStatus.UNHEALTHY
        elif any(r.status == HealthStatus.DEGRADED for r in results):
            return HealthStatus.DEGRADED
        else:
            return HealthStatus.HEALTHY

    # Predefined health checks for ML systems

    async def check_model_loaded(self, model: any) -> Dict:
        """Check if model is loaded and ready."""
        if model is None:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': 'Model not loaded'
            }

        # Verify model has required methods
        required_methods = ['predict', 'predict_proba']
        missing_methods = [m for m in required_methods if not hasattr(model, m)]

        if missing_methods:
            return {
                'status': HealthStatus.DEGRADED,
                'message': f'Model missing methods: {missing_methods}'
            }

        return {
            'status': HealthStatus.HEALTHY,
            'message': 'Model loaded and operational',
            'details': {
                'model_type': type(model).__name__,
                'methods_available': [m for m in required_methods if hasattr(model, m)]
            }
        }

    async def check_model_inference(
        self,
        model: any,
        test_input: np.ndarray,
        expected_shape: tuple
    ) -> Dict:
        """
        Perform test inference to validate model health.

        Runs a smoke test prediction to ensure model can
        produce valid outputs.
        """
        try:
            # Execute test prediction
            start_time = datetime.now()
            prediction = model.predict(test_input)
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000

            # Validate output
            if prediction is None:
                return {
                    'status': HealthStatus.UNHEALTHY,
                    'message': 'Model returned None prediction'
                }

            if hasattr(prediction, 'shape') and prediction.shape != expected_shape:
                return {
                    'status': HealthStatus.DEGRADED,
                    'message': f'Unexpected output shape: {prediction.shape}, expected: {expected_shape}'
                }

            # Check for NaN or Inf values
            if hasattr(prediction, '__iter__'):
                if np.any(np.isnan(prediction)) or np.any(np.isinf(prediction)):
                    return {
                        'status': HealthStatus.DEGRADED,
                        'message': 'Model output contains NaN or Inf values'
                    }

            return {
                'status': HealthStatus.HEALTHY,
                'message': 'Model inference successful',
                'details': {
                    'test_latency_ms': latency_ms,
                    'output_shape': str(prediction.shape if hasattr(prediction, 'shape') else 'scalar')
                }
            }

        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Model inference failed: {str(e)}'
            }

    async def check_feature_store(
        self,
        feature_store_client: any,
        test_entity_id: str
    ) -> Dict:
        """Check feature store connectivity and responsiveness."""
        try:
            start_time = datetime.now()
            features = await feature_store_client.get_features(test_entity_id)
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000

            if features is None or len(features) == 0:
                return {
                    'status': HealthStatus.DEGRADED,
                    'message': 'Feature store returned empty features'
                }

            # Check latency threshold
            if latency_ms > 1000:  # 1 second threshold
                return {
                    'status': HealthStatus.DEGRADED,
                    'message': f'Feature store slow: {latency_ms:.0f}ms',
                    'details': {'latency_ms': latency_ms}
                }

            return {
                'status': HealthStatus.HEALTHY,
                'message': 'Feature store operational',
                'details': {
                    'latency_ms': latency_ms,
                    'features_count': len(features)
                }
            }

        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Feature store unreachable: {str(e)}'
            }

    async def check_database(
        self,
        db_client: any,
        test_query: str
    ) -> Dict:
        """Check database connectivity."""
        try:
            start_time = datetime.now()
            await db_client.execute(test_query)
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000

            if latency_ms > 500:  # 500ms threshold
                return {
                    'status': HealthStatus.DEGRADED,
                    'message': f'Database slow: {latency_ms:.0f}ms',
                    'details': {'latency_ms': latency_ms}
                }

            return {
                'status': HealthStatus.HEALTHY,
                'message': 'Database operational',
                'details': {'latency_ms': latency_ms}
            }

        except Exception as e:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Database unreachable: {str(e)}'
            }

    async def check_memory_usage(self, threshold_percent: float = 90.0) -> Dict:
        """Check system memory usage."""
        import psutil

        memory = psutil.virtual_memory()
        percent_used = memory.percent

        if percent_used > threshold_percent:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'High memory usage: {percent_used:.1f}%',
                'details': {
                    'percent_used': percent_used,
                    'available_gb': memory.available / (1024**3),
                    'total_gb': memory.total / (1024**3)
                }
            }
        elif percent_used > threshold_percent * 0.8:  # 80% of threshold
            return {
                'status': HealthStatus.DEGRADED,
                'message': f'Elevated memory usage: {percent_used:.1f}%',
                'details': {'percent_used': percent_used}
            }

        return {
            'status': HealthStatus.HEALTHY,
            'message': f'Memory usage normal: {percent_used:.1f}%',
            'details': {'percent_used': percent_used}
        }

    async def check_disk_space(self, path: str = "/", threshold_percent: float = 90.0) -> Dict:
        """Check disk space availability."""
        import psutil

        disk = psutil.disk_usage(path)
        percent_used = disk.percent

        if percent_used > threshold_percent:
            return {
                'status': HealthStatus.UNHEALTHY,
                'message': f'Low disk space: {percent_used:.1f}% used',
                'details': {
                    'percent_used': percent_used,
                    'free_gb': disk.free / (1024**3),
                    'total_gb': disk.total / (1024**3)
                }
            }
        elif percent_used > threshold_percent * 0.8:
            return {
                'status': HealthStatus.DEGRADED,
                'message': f'Disk space running low: {percent_used:.1f}% used',
                'details': {'percent_used': percent_used}
            }

        return {
            'status': HealthStatus.HEALTHY,
            'message': f'Disk space adequate: {percent_used:.1f}% used',
            'details': {'percent_used': percent_used}
        }


# Example usage
class MLServiceWithHealthChecks:
    """Example ML service with comprehensive health checking."""

    def __init__(self, model, feature_store, database):
        self.model = model
        self.feature_store = feature_store
        self.database = database

        # Initialize health checker
        self.health = HealthChecker("fraud-detection-service")

        # Register health checks
        self._register_health_checks()

    def _register_health_checks(self):
        """Register all health checks for this service."""

        # Liveness checks (is service running?)
        self.health.register_check(
            "memory_usage",
            lambda: self.health.check_memory_usage(threshold_percent=90),
            check_type="liveness"
        )

        self.health.register_check(
            "disk_space",
            lambda: self.health.check_disk_space(threshold_percent=90),
            check_type="liveness"
        )

        # Readiness checks (can service handle traffic?)
        self.health.register_check(
            "model_loaded",
            lambda: self.health.check_model_loaded(self.model),
            check_type="readiness"
        )

        test_input = np.array([[1.0] * 47])  # 47 features
        self.health.register_check(
            "model_inference",
            lambda: self.health.check_model_inference(
                self.model, test_input, expected_shape=(1,)
            ),
            check_type="readiness"
        )

        self.health.register_check(
            "feature_store",
            lambda: self.health.check_feature_store(
                self.feature_store, test_entity_id="test_user_123"
            ),
            check_type="readiness"
        )

        self.health.register_check(
            "database",
            lambda: self.health.check_database(
                self.database, test_query="SELECT 1"
            ),
            check_type="readiness"
        )

    async def health_check_endpoint(self) -> Dict:
        """HTTP endpoint for health checks."""
        return await self.health.check_all()
\end{lstlisting}

\subsection{Performance Profiling with Bottleneck Identification}

Performance profiling identifies bottlenecks in ML inference pipelines, enabling targeted optimization. This includes CPU profiling, memory analysis, and latency breakdown across pipeline stages.

\begin{lstlisting}[language=Python, caption={PerformanceProfiler for ML Pipelines}]
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime
import time
import psutil
import numpy as np
from contextlib import contextmanager
import cProfile
import pstats
from io import StringIO

@dataclass
class ProfileResult:
    """Result of a performance profiling session."""
    stage: str
    duration_ms: float
    cpu_percent: float
    memory_mb: float
    timestamp: datetime
    details: Optional[Dict] = None

class PerformanceProfiler:
    """
    Performance profiling for ML inference pipelines.

    Features:
    - Stage-by-stage latency breakdown
    - CPU and memory usage tracking
    - Bottleneck identification
    - Optimization recommendations
    - Historical performance tracking
    """

    def __init__(self):
        self.profiles: List[ProfileResult] = []
        self.stage_history: Dict[str, List[float]] = {}

    @contextmanager
    def profile_stage(self, stage_name: str, **context):
        """
        Profile a specific pipeline stage.

        Tracks execution time, CPU, and memory usage.
        """
        # Capture initial state
        process = psutil.Process()
        start_time = time.perf_counter()
        start_cpu_percent = process.cpu_percent()
        start_memory_mb = process.memory_info().rss / (1024 * 1024)

        try:
            yield
        finally:
            # Capture final state
            end_time = time.perf_counter()
            duration_ms = (end_time - start_time) * 1000

            # CPU percentage during this stage
            cpu_percent = process.cpu_percent()

            # Memory used during this stage
            end_memory_mb = process.memory_info().rss / (1024 * 1024)
            memory_delta_mb = end_memory_mb - start_memory_mb

            # Create profile result
            result = ProfileResult(
                stage=stage_name,
                duration_ms=duration_ms,
                cpu_percent=cpu_percent,
                memory_mb=memory_delta_mb,
                timestamp=datetime.now(),
                details=context
            )

            # Store result
            self.profiles.append(result)

            # Update stage history
            if stage_name not in self.stage_history:
                self.stage_history[stage_name] = []
            self.stage_history[stage_name].append(duration_ms)

    def profile_function(self, func: Callable, *args, **kwargs) -> Dict:
        """
        Profile a function with cProfile for detailed analysis.

        Returns detailed profiling statistics including:
        - Function call counts
        - Cumulative time per function
        - Top time-consuming functions
        """
        profiler = cProfile.Profile()
        profiler.enable()

        # Execute function
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        duration_ms = (time.perf_counter() - start_time) * 1000

        profiler.disable()

        # Capture statistics
        stats_stream = StringIO()
        stats = pstats.Stats(profiler, stream=stats_stream)
        stats.strip_dirs()
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # Top 20 functions

        return {
            'duration_ms': duration_ms,
            'result': result,
            'profile_stats': stats_stream.getvalue(),
            'top_functions': self._extract_top_functions(stats)
        }

    def _extract_top_functions(self, stats: pstats.Stats, n: int = 10) -> List[Dict]:
        """Extract top N time-consuming functions."""
        top_functions = []

        for func, (cc, nc, tt, ct, callers) in list(stats.stats.items())[:n]:
            top_functions.append({
                'function': f"{func[0]}:{func[1]}:{func[2]}",
                'call_count': nc,
                'total_time': tt,
                'cumulative_time': ct,
                'time_per_call': tt / nc if nc > 0 else 0
            })

        return top_functions

    def identify_bottlenecks(self, threshold_ms: float = 100) -> Dict:
        """
        Identify bottlenecks in the inference pipeline.

        Args:
            threshold_ms: Stages exceeding this duration are flagged

        Returns:
            Analysis with bottleneck identification and recommendations
        """
        if not self.profiles:
            return {'bottlenecks': [], 'message': 'No profiling data available'}

        # Aggregate statistics by stage
        stage_stats = {}
        for stage, durations in self.stage_history.items():
            stage_stats[stage] = {
                'mean_ms': np.mean(durations),
                'p50_ms': np.percentile(durations, 50),
                'p95_ms': np.percentile(durations, 95),
                'p99_ms': np.percentile(durations, 99),
                'max_ms': np.max(durations),
                'count': len(durations)
            }

        # Identify bottlenecks
        bottlenecks = []
        total_time = sum(stats['mean_ms'] for stats in stage_stats.values())

        for stage, stats in stage_stats.items():
            percentage_of_total = (stats['mean_ms'] / total_time * 100) if total_time > 0 else 0

            if stats['mean_ms'] > threshold_ms or percentage_of_total > 20:
                bottlenecks.append({
                    'stage': stage,
                    'mean_latency_ms': stats['mean_ms'],
                    'p95_latency_ms': stats['p95_ms'],
                    'percentage_of_total': percentage_of_total,
                    'severity': 'high' if percentage_of_total > 40 else 'medium',
                    'recommendation': self._get_optimization_recommendation(stage, stats)
                })

        # Sort by impact
        bottlenecks.sort(key=lambda x: x['percentage_of_total'], reverse=True)

        return {
            'total_pipeline_time_ms': total_time,
            'bottlenecks': bottlenecks,
            'stage_breakdown': stage_stats,
            'optimization_priority': [b['stage'] for b in bottlenecks]
        }

    def _get_optimization_recommendation(self, stage: str, stats: Dict) -> str:
        """Generate optimization recommendation for a stage."""
        mean_ms = stats['mean_ms']

        if 'preprocessing' in stage.lower():
            if mean_ms > 100:
                return "Consider vectorizing operations, using NumPy, or caching preprocessed results"
        elif 'feature' in stage.lower():
            if mean_ms > 200:
                return "Optimize feature store queries; consider caching or batch retrieval"
        elif 'model' in stage.lower() or 'inference' in stage.lower():
            if mean_ms > 500:
                return "Model inference bottleneck; consider model optimization (quantization, pruning) or GPU acceleration"
        elif 'postprocess' in stage.lower():
            if mean_ms > 50:
                return "Optimize postprocessing logic; avoid unnecessary transformations"

        return "Profile this stage in detail to identify specific bottlenecks"

    def get_performance_report(self) -> Dict:
        """Generate comprehensive performance report."""
        if not self.profiles:
            return {'message': 'No profiling data available'}

        recent_profiles = self.profiles[-100:]  # Last 100 profiling sessions

        # Overall statistics
        total_durations = [p.duration_ms for p in recent_profiles]
        total_cpu = [p.cpu_percent for p in recent_profiles]
        total_memory = [p.memory_mb for p in recent_profiles]

        return {
            'summary': {
                'total_profiles': len(self.profiles),
                'recent_profiles': len(recent_profiles),
                'mean_duration_ms': np.mean(total_durations),
                'p95_duration_ms': np.percentile(total_durations, 95),
                'mean_cpu_percent': np.mean(total_cpu),
                'mean_memory_mb': np.mean(total_memory)
            },
            'stage_breakdown': {
                stage: {
                    'mean_ms': np.mean(durations),
                    'p95_ms': np.percentile(durations, 95),
                    'count': len(durations)
                }
                for stage, durations in self.stage_history.items()
            },
            'bottlenecks': self.identify_bottlenecks()
        }


# Example usage
class MLServiceWithProfiling:
    """Example ML service with performance profiling."""

    def __init__(self):
        self.profiler = PerformanceProfiler()

    async def predict_with_profiling(self, request_data: Dict) -> Dict:
        """Make prediction with comprehensive profiling."""

        # Profile preprocessing
        with self.profiler.profile_stage("preprocessing", request_id=request_data['request_id']):
            features = await self._preprocess(request_data)

        # Profile feature store lookup
        with self.profiler.profile_stage("feature_store_lookup"):
            enriched_features = await self._get_features(request_data.get('user_id'))

        # Profile model inference
        with self.profiler.profile_stage("model_inference"):
            prediction = await self._model_predict(enriched_features)

        # Profile postprocessing
        with self.profiler.profile_stage("postprocessing"):
            result = await self._postprocess(prediction)

        return result

    async def _preprocess(self, data: Dict) -> Dict:
        # Simulate preprocessing
        time.sleep(0.05)  # 50ms
        return {}

    async def _get_features(self, user_id: str) -> Dict:
        # Simulate feature store lookup
        time.sleep(0.15)  # 150ms (bottleneck!)
        return {}

    async def _model_predict(self, features: Dict) -> Dict:
        # Simulate model inference
        time.sleep(0.08)  # 80ms
        return {'score': 0.85}

    async def _postprocess(self, prediction: Dict) -> Dict:
        # Simulate postprocessing
        time.sleep(0.02)  # 20ms
        return prediction

    def get_performance_insights(self) -> Dict:
        """Get performance insights and optimization recommendations."""
        return self.profiler.get_performance_report()
\end{lstlisting}

\subsection{Custom Metrics Collection}

Custom metrics capture domain-specific indicators for ML systems, including business metrics, model-specific metrics, and operational metrics.

\begin{lstlisting}[language=Python, caption={CustomMetricsCollector for ML Systems}]
from prometheus_client import Counter, Histogram, Gauge, Summary
from typing import Dict, List, Optional
from datetime import datetime
import numpy as np

class CustomMetricsCollector:
    """
    Custom metrics collection for ML systems.

    Integrates with Prometheus for metrics export and provides
    specialized ML and business metric tracking.
    """

    def __init__(self, service_name: str):
        self.service_name = service_name

        # Model performance metrics
        self.prediction_counter = Counter(
            'ml_predictions_total',
            'Total number of predictions made',
            ['model_name', 'model_version', 'prediction_class']
        )

        self.prediction_latency = Histogram(
            'ml_prediction_latency_seconds',
            'Prediction latency distribution',
            ['model_name', 'stage'],
            buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
        )

        self.prediction_confidence = Histogram(
            'ml_prediction_confidence',
            'Prediction confidence score distribution',
            ['model_name'],
            buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]
        )

        self.model_accuracy_gauge = Gauge(
            'ml_model_accuracy',
            'Current model accuracy (sliding window)',
            ['model_name', 'window']
        )

        # Business metrics
        self.business_conversion_rate = Gauge(
            'ml_business_conversion_rate',
            'Conversion rate influenced by ML',
            ['model_name', 'segment']
        )

        self.business_revenue_impact = Counter(
            'ml_business_revenue_impact_total',
            'Revenue impact attributed to ML (in cents)',
            ['model_name', 'outcome']
        )

        # Data quality metrics
        self.feature_null_rate = Gauge(
            'ml_feature_null_rate',
            'Percentage of null values in features',
            ['feature_name']
        )

        self.drift_score = Gauge(
            'ml_drift_score',
            'Feature drift score',
            ['feature_name', 'method']
        )

        # Operational metrics
        self.error_counter = Counter(
            'ml_errors_total',
            'Total errors by type',
            ['model_name', 'error_type']
        )

        self.cache_hit_rate = Gauge(
            'ml_cache_hit_rate',
            'Cache hit rate for features/predictions',
            ['cache_type']
        )

    def record_prediction(
        self,
        model_name: str,
        model_version: str,
        prediction_class: str,
        confidence: float,
        latency_seconds: float,
        stage_latencies: Optional[Dict[str, float]] = None
    ):
        """
        Record a prediction with all associated metrics.
        """
        # Increment prediction counter
        self.prediction_counter.labels(
            model_name=model_name,
            model_version=model_version,
            prediction_class=prediction_class
        ).inc()

        # Record overall latency
        self.prediction_latency.labels(
            model_name=model_name,
            stage='total'
        ).observe(latency_seconds)

        # Record stage-specific latencies
        if stage_latencies:
            for stage, latency in stage_latencies.items():
                self.prediction_latency.labels(
                    model_name=model_name,
                    stage=stage
                ).observe(latency)

        # Record confidence distribution
        self.prediction_confidence.labels(
            model_name=model_name
        ).observe(confidence)

    def update_model_accuracy(
        self,
        model_name: str,
        accuracy: float,
        window: str = '1h'
    ):
        """Update current model accuracy gauge."""
        self.model_accuracy_gauge.labels(
            model_name=model_name,
            window=window
        ).set(accuracy)

    def record_business_conversion(
        self,
        model_name: str,
        segment: str,
        conversion_rate: float
    ):
        """Record business conversion rate."""
        self.business_conversion_rate.labels(
            model_name=model_name,
            segment=segment
        ).set(conversion_rate)

    def record_revenue_impact(
        self,
        model_name: str,
        outcome: str,
        revenue_cents: int
    ):
        """
        Record revenue impact in cents.

        Args:
            model_name: Name of model
            outcome: 'positive' or 'negative'
            revenue_cents: Revenue impact in cents (to avoid floating point)
        """
        self.business_revenue_impact.labels(
            model_name=model_name,
            outcome=outcome
        ).inc(revenue_cents)

    def update_feature_quality(
        self,
        feature_name: str,
        null_rate: float
    ):
        """Update feature null rate."""
        self.feature_null_rate.labels(
            feature_name=feature_name
        ).set(null_rate)

    def record_drift(
        self,
        feature_name: str,
        method: str,
        drift_score: float
    ):
        """Record drift detection score."""
        self.drift_score.labels(
            feature_name=feature_name,
            method=method
        ).set(drift_score)

    def record_error(
        self,
        model_name: str,
        error_type: str
    ):
        """Record an error occurrence."""
        self.error_counter.labels(
            model_name=model_name,
            error_type=error_type
        ).inc()

    def update_cache_metrics(
        self,
        cache_type: str,
        hit_rate: float
    ):
        """Update cache hit rate."""
        self.cache_hit_rate.labels(
            cache_type=cache_type
        ).set(hit_rate)


# Example integration
class MLServiceWithMetrics:
    """Example ML service with comprehensive metrics collection."""

    def __init__(self):
        self.metrics = CustomMetricsCollector("fraud-detection-service")

    async def predict(self, request_data: Dict) -> Dict:
        """Make prediction with metrics collection."""
        model_name = "fraud_detector"
        model_version = "v3.2"

        start_time = datetime.now()
        stage_times = {}

        try:
            # Preprocessing
            preprocess_start = datetime.now()
            features = await self._preprocess(request_data)
            stage_times['preprocessing'] = (datetime.now() - preprocess_start).total_seconds()

            # Model inference
            inference_start = datetime.now()
            prediction = await self._model_predict(features)
            stage_times['inference'] = (datetime.now() - inference_start).total_seconds()

            # Postprocessing
            postprocess_start = datetime.now()
            result = await self._postprocess(prediction)
            stage_times['postprocessing'] = (datetime.now() - postprocess_start).total_seconds()

            # Calculate total latency
            total_latency = (datetime.now() - start_time).total_seconds()

            # Record metrics
            self.metrics.record_prediction(
                model_name=model_name,
                model_version=model_version,
                prediction_class=result['class'],
                confidence=result['confidence'],
                latency_seconds=total_latency,
                stage_latencies=stage_times
            )

            # Record business impact
            if result['class'] == 'fraud':
                self.metrics.record_revenue_impact(
                    model_name=model_name,
                    outcome='positive',
                    revenue_cents=int(request_data.get('amount', 0) * 100)
                )

            return result

        except Exception as e:
            # Record error
            self.metrics.record_error(
                model_name=model_name,
                error_type=type(e).__name__
            )
            raise

    async def _preprocess(self, data: Dict) -> Dict:
        return {}

    async def _model_predict(self, features: Dict) -> Dict:
        return {'score': 0.85}

    async def _postprocess(self, prediction: Dict) -> Dict:
        return {'class': 'legitimate', 'confidence': 0.92}
\end{lstlisting}

\section{Incident Management Framework}

Comprehensive incident management ensures rapid detection, coordinated response, and continuous improvement. This framework adapts SRE principles for ML systems with automated workflows and runbook automation.

\subsection{Automated Incident Detection}

Automated incident detection uses multi-signal analysis to identify issues before they impact users, with severity classification and intelligent priority scoring.

\begin{lstlisting}[language=Python, caption={Automated IncidentDetector with ML-Specific Signals}]
from dataclasses import dataclass
from typing import Dict, List, Optional, Callable
from datetime import datetime, timedelta
from enum import Enum
import asyncio

class IncidentSeverity(Enum):
    """Incident severity levels aligned with SRE practices."""
    SEV1_CRITICAL = "sev1"  # Service down, data loss, security breach
    SEV2_HIGH = "sev2"  # Major degradation, significant user impact
    SEV3_MEDIUM = "sev3"  # Minor degradation, limited impact
    SEV4_LOW = "sev4"  # No user impact, monitoring/alerting issues

@dataclass
class IncidentSignal:
    """A signal indicating potential incident."""
    signal_type: str
    value: float
    threshold: float
    severity: IncidentSeverity
    timestamp: datetime
    metadata: Dict

@dataclass
class Incident:
    """Detected incident."""
    incident_id: str
    title: str
    severity: IncidentSeverity
    signals: List[IncidentSignal]
    detected_at: datetime
    priority_score: float
    affected_components: List[str]
    estimated_user_impact: str
    runbook_id: Optional[str] = None

class IncidentDetector:
    """
    Automated incident detection for ML systems.

    Features:
    - Multi-signal analysis
    - Severity classification
    - Priority scoring
    - Automatic runbook selection
    - Impact estimation
    """

    def __init__(self):
        self.active_incidents: Dict[str, Incident] = {}
        self.signal_thresholds = self._initialize_thresholds()
        self.detection_rules = self._initialize_detection_rules()

    def _initialize_thresholds(self) -> Dict:
        """Initialize detection thresholds for various signals."""
        return {
            # Model performance signals
            'model_accuracy_drop': {
                'sev1_threshold': 0.15,  # 15% drop = critical
                'sev2_threshold': 0.08,  # 8% drop = high
                'sev3_threshold': 0.03   # 3% drop = medium
            },
            'model_latency_p99': {
                'sev1_threshold': 5000,  # 5s = critical
                'sev2_threshold': 2000,  # 2s = high
                'sev3_threshold': 1000   # 1s = medium
            },
            'error_rate': {
                'sev1_threshold': 0.10,  # 10% = critical
                'sev2_threshold': 0.05,  # 5% = high
                'sev3_threshold': 0.02   # 2% = medium
            },

            # Data quality signals
            'feature_null_rate': {
                'sev1_threshold': 0.50,  # 50% = critical
                'sev2_threshold': 0.20,  # 20% = high
                'sev3_threshold': 0.10   # 10% = medium
            },
            'drift_score': {
                'sev1_threshold': 0.8,   # Severe drift
                'sev2_threshold': 0.5,   # Moderate drift
                'sev3_threshold': 0.3    # Minor drift
            },

            # Infrastructure signals
            'memory_usage': {
                'sev1_threshold': 0.95,  # 95% = critical
                'sev2_threshold': 0.85,  # 85% = high
                'sev3_threshold': 0.75   # 75% = medium
            },
            'cpu_usage': {
                'sev1_threshold': 0.95,
                'sev2_threshold': 0.85,
                'sev3_threshold': 0.75
            },

            # Business impact signals
            'conversion_rate_drop': {
                'sev1_threshold': 0.30,  # 30% drop = critical
                'sev2_threshold': 0.15,  # 15% drop = high
                'sev3_threshold': 0.08   # 8% drop = medium
            },
            'revenue_impact_hourly': {
                'sev1_threshold': 10000,  # $10k/hour loss
                'sev2_threshold': 5000,   # $5k/hour loss
                'sev3_threshold': 1000    # $1k/hour loss
            }
        }

    def _initialize_detection_rules(self) -> List[Dict]:
        """Initialize multi-signal detection rules."""
        return [
            {
                'name': 'model_degradation',
                'signals': ['model_accuracy_drop', 'error_rate'],
                'logic': 'AND',  # Both signals must trigger
                'runbook_id': 'RB001_MODEL_DEGRADATION'
            },
            {
                'name': 'data_pipeline_failure',
                'signals': ['feature_null_rate', 'drift_score'],
                'logic': 'OR',  # Either signal triggers
                'runbook_id': 'RB002_DATA_PIPELINE'
            },
            {
                'name': 'infrastructure_overload',
                'signals': ['memory_usage', 'cpu_usage', 'model_latency_p99'],
                'logic': 'MAJORITY',  # 2 out of 3 must trigger
                'runbook_id': 'RB003_INFRASTRUCTURE'
            },
            {
                'name': 'business_impact',
                'signals': ['conversion_rate_drop', 'revenue_impact_hourly'],
                'logic': 'OR',
                'runbook_id': 'RB004_BUSINESS_IMPACT'
            }
        ]

    async def analyze_signals(self, current_metrics: Dict) -> Optional[Incident]:
        """
        Analyze current metrics to detect incidents.

        Args:
            current_metrics: Dictionary of current metric values

        Returns:
            Incident if detected, None otherwise
        """
        signals: List[IncidentSignal] = []

        # Evaluate each metric against thresholds
        for metric_name, metric_value in current_metrics.items():
            if metric_name in self.signal_thresholds:
                signal = self._evaluate_metric(metric_name, metric_value)
                if signal:
                    signals.append(signal)

        # If no signals, no incident
        if not signals:
            return None

        # Apply detection rules
        for rule in self.detection_rules:
            if self._rule_matches(rule, signals):
                # Create incident
                incident = self._create_incident(rule, signals)
                return incident

        # Individual signal incident (no rule matched)
        if signals:
            # Create incident for most severe signal
            most_severe = max(signals, key=lambda s: self._severity_to_int(s.severity))
            return self._create_incident_from_signal(most_severe)

        return None

    def _evaluate_metric(self, metric_name: str, value: float) -> Optional[IncidentSignal]:
        """Evaluate a single metric against thresholds."""
        thresholds = self.signal_thresholds[metric_name]

        # Determine severity
        if value >= thresholds['sev1_threshold']:
            severity = IncidentSeverity.SEV1_CRITICAL
            threshold = thresholds['sev1_threshold']
        elif value >= thresholds['sev2_threshold']:
            severity = IncidentSeverity.SEV2_HIGH
            threshold = thresholds['sev2_threshold']
        elif value >= thresholds['sev3_threshold']:
            severity = IncidentSeverity.SEV3_MEDIUM
            threshold = thresholds['sev3_threshold']
        else:
            return None  # Below threshold

        return IncidentSignal(
            signal_type=metric_name,
            value=value,
            threshold=threshold,
            severity=severity,
            timestamp=datetime.now(),
            metadata={'metric_name': metric_name}
        )

    def _rule_matches(self, rule: Dict, signals: List[IncidentSignal]) -> bool:
        """Check if a detection rule matches current signals."""
        rule_signals = set(rule['signals'])
        triggered_signals = {s.signal_type for s in signals}

        matched_signals = rule_signals & triggered_signals

        if rule['logic'] == 'AND':
            return matched_signals == rule_signals
        elif rule['logic'] == 'OR':
            return len(matched_signals) > 0
        elif rule['logic'] == 'MAJORITY':
            return len(matched_signals) >= len(rule_signals) / 2
        else:
            return False

    def _create_incident(self, rule: Dict, signals: List[IncidentSignal]) -> Incident:
        """Create incident from matched rule."""
        # Determine overall severity (highest severity wins)
        max_severity = max(signals, key=lambda s: self._severity_to_int(s.severity)).severity

        # Generate incident ID
        incident_id = f"INC-{datetime.now().strftime('%Y%m%d%H%M%S')}"

        # Calculate priority score (0-100)
        priority_score = self._calculate_priority_score(signals, max_severity)

        # Estimate user impact
        user_impact = self._estimate_user_impact(signals)

        # Identify affected components
        affected_components = self._identify_affected_components(signals)

        return Incident(
            incident_id=incident_id,
            title=f"{rule['name'].replace('_', ' ').title()} Detected",
            severity=max_severity,
            signals=signals,
            detected_at=datetime.now(),
            priority_score=priority_score,
            affected_components=affected_components,
            estimated_user_impact=user_impact,
            runbook_id=rule.get('runbook_id')
        )

    def _create_incident_from_signal(self, signal: IncidentSignal) -> Incident:
        """Create incident from individual signal."""
        incident_id = f"INC-{datetime.now().strftime('%Y%m%d%H%M%S')}"

        return Incident(
            incident_id=incident_id,
            title=f"{signal.signal_type.replace('_', ' ').title()} Threshold Exceeded",
            severity=signal.severity,
            signals=[signal],
            detected_at=datetime.now(),
            priority_score=self._calculate_priority_score([signal], signal.severity),
            affected_components=[signal.signal_type],
            estimated_user_impact=self._estimate_user_impact([signal]),
            runbook_id=None
        )

    def _severity_to_int(self, severity: IncidentSeverity) -> int:
        """Convert severity to integer for comparison."""
        return {
            IncidentSeverity.SEV1_CRITICAL: 4,
            IncidentSeverity.SEV2_HIGH: 3,
            IncidentSeverity.SEV3_MEDIUM: 2,
            IncidentSeverity.SEV4_LOW: 1
        }[severity]

    def _calculate_priority_score(
        self,
        signals: List[IncidentSignal],
        severity: IncidentSeverity
    ) -> float:
        """
        Calculate priority score (0-100).

        Higher score = higher priority for response.
        """
        base_score = self._severity_to_int(severity) * 20

        # Adjust for number of signals
        signal_multiplier = min(len(signals) / 5.0, 1.0) * 20

        # Adjust for signal deviation from threshold
        max_deviation = max(
            (s.value - s.threshold) / s.threshold for s in signals
        )
        deviation_score = min(max_deviation * 20, 20)

        # Check for business impact signals
        business_impact_boost = 20 if any(
            'revenue' in s.signal_type or 'conversion' in s.signal_type
            for s in signals
        ) else 0

        total_score = base_score + signal_multiplier + deviation_score + business_impact_boost

        return min(total_score, 100)

    def _estimate_user_impact(self, signals: List[IncidentSignal]) -> str:
        """Estimate user impact based on signals."""
        signal_types = {s.signal_type for s in signals}

        if 'error_rate' in signal_types or 'model_latency_p99' in signal_types:
            return "HIGH - Users experiencing errors or delays"
        elif 'model_accuracy_drop' in signal_types:
            return "MEDIUM - Model quality degraded"
        elif 'drift_score' in signal_types:
            return "LOW - Data distribution shifted"
        else:
            return "UNKNOWN - Assess user impact"

    def _identify_affected_components(self, signals: List[IncidentSignal]) -> List[str]:
        """Identify affected system components."""
        components = set()

        for signal in signals:
            if 'model' in signal.signal_type:
                components.add('ML Model')
            if 'feature' in signal.signal_type or 'drift' in signal.signal_type:
                components.add('Feature Pipeline')
            if 'memory' in signal.signal_type or 'cpu' in signal.signal_type:
                components.add('Infrastructure')
            if 'conversion' in signal.signal_type or 'revenue' in signal.signal_type:
                components.add('Business Metrics')

        return list(components)
\end{lstlisting}

\subsubsection{Runbook Automation}

Runbook automation provides guided remediation procedures with automatic action execution, rollback capabilities, and escalation workflows.

\begin{lstlisting}[style=python, caption={Runbook automation with guided remediation}]
from typing import List, Dict, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import asyncio

class ActionType(Enum):
    """Types of runbook actions."""
    DIAGNOSTIC = "diagnostic"  # Gather information
    REMEDIATION = "remediation"  # Fix the problem
    ROLLBACK = "rollback"  # Revert changes
    NOTIFICATION = "notification"  # Alert stakeholders
    ESCALATION = "escalation"  # Escalate to human
    VALIDATION = "validation"  # Verify fix worked

class ActionStatus(Enum):
    """Status of runbook action execution."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCEEDED = "succeeded"
    FAILED = "failed"
    SKIPPED = "skipped"
    REQUIRES_APPROVAL = "requires_approval"

@dataclass
class RunbookAction:
    """Single action in a runbook."""
    action_id: str
    action_type: ActionType
    description: str
    command: Optional[Callable] = None  # Function to execute
    parameters: Dict[str, Any] = field(default_factory=dict)
    timeout_seconds: int = 300
    requires_approval: bool = False
    rollback_action: Optional['RunbookAction'] = None
    success_criteria: Optional[Callable] = None  # Validation function
    on_failure: str = "stop"  # stop, continue, escalate

@dataclass
class RunbookExecutionResult:
    """Result of executing a runbook action."""
    action_id: str
    status: ActionStatus
    started_at: datetime
    completed_at: Optional[datetime]
    output: Any
    error_message: Optional[str] = None
    rollback_executed: bool = False

@dataclass
class Runbook:
    """Automated remediation runbook."""
    runbook_id: str
    title: str
    description: str
    applicable_signals: List[str]  # Which signals this runbook addresses
    actions: List[RunbookAction]
    estimated_duration_minutes: int
    success_rate: float = 0.0  # Historical success rate
    requires_human_approval: bool = False

class RunbookExecutor:
    """
    Automated runbook execution for ML incident remediation.

    Features:
    - Guided step-by-step remediation procedures
    - Automatic action execution with timeout handling
    - Rollback capabilities for failed actions
    - Human approval gates for risky operations
    - Parallel action execution where safe
    - Escalation paths when automation fails
    - Execution history and success tracking
    """

    def __init__(self):
        self.runbooks: Dict[str, Runbook] = {}
        self.execution_history: List[Dict] = []
        self._initialize_runbooks()

    def _initialize_runbooks(self):
        """Initialize predefined runbooks for common ML incidents."""

        # Runbook: Model Degradation
        self.runbooks['RB001_MODEL_DEGRADATION'] = Runbook(
            runbook_id='RB001_MODEL_DEGRADATION',
            title='Model Performance Degradation',
            description='Remediate model accuracy/performance drop',
            applicable_signals=['model_accuracy_drop', 'error_rate'],
            actions=[
                RunbookAction(
                    action_id='diag_check_recent_predictions',
                    action_type=ActionType.DIAGNOSTIC,
                    description='Analyze recent prediction patterns',
                    command=self._check_recent_predictions,
                    timeout_seconds=60
                ),
                RunbookAction(
                    action_id='diag_check_input_distribution',
                    action_type=ActionType.DIAGNOSTIC,
                    description='Check for input data drift',
                    command=self._check_data_drift,
                    timeout_seconds=120
                ),
                RunbookAction(
                    action_id='remediate_rollback_model',
                    action_type=ActionType.ROLLBACK,
                    description='Rollback to previous model version',
                    command=self._rollback_model_version,
                    parameters={'rollback_versions': 1},
                    requires_approval=True,  # Risky operation
                    success_criteria=self._verify_model_health,
                    timeout_seconds=180
                ),
                RunbookAction(
                    action_id='validate_rollback_success',
                    action_type=ActionType.VALIDATION,
                    description='Verify model performance restored',
                    command=self._validate_model_metrics,
                    timeout_seconds=300
                ),
                RunbookAction(
                    action_id='notify_resolution',
                    action_type=ActionType.NOTIFICATION,
                    description='Notify stakeholders of resolution',
                    command=self._notify_stakeholders,
                    parameters={'message': 'Model rolled back successfully'}
                )
            ],
            estimated_duration_minutes=8,
            requires_human_approval=True
        )

        # Runbook: Data Quality Issues
        self.runbooks['RB002_DATA_QUALITY'] = Runbook(
            runbook_id='RB002_DATA_QUALITY',
            title='Data Quality Degradation',
            description='Remediate data quality issues',
            applicable_signals=['feature_null_rate', 'drift_score'],
            actions=[
                RunbookAction(
                    action_id='diag_identify_problematic_features',
                    action_type=ActionType.DIAGNOSTIC,
                    description='Identify features with quality issues',
                    command=self._identify_bad_features,
                    timeout_seconds=90
                ),
                RunbookAction(
                    action_id='remediate_enable_fallback_pipeline',
                    action_type=ActionType.REMEDIATION,
                    description='Switch to backup feature pipeline',
                    command=self._switch_feature_pipeline,
                    parameters={'pipeline': 'backup'},
                    rollback_action=RunbookAction(
                        action_id='rollback_restore_primary_pipeline',
                        action_type=ActionType.ROLLBACK,
                        description='Restore primary pipeline',
                        command=self._switch_feature_pipeline,
                        parameters={'pipeline': 'primary'}
                    )
                ),
                RunbookAction(
                    action_id='validate_feature_quality',
                    action_type=ActionType.VALIDATION,
                    description='Verify feature quality improved',
                    command=self._validate_feature_quality,
                    timeout_seconds=180
                )
            ],
            estimated_duration_minutes=5
        )

        # Runbook: Infrastructure Overload
        self.runbooks['RB003_INFRASTRUCTURE'] = Runbook(
            runbook_id='RB003_INFRASTRUCTURE',
            title='Infrastructure Resource Exhaustion',
            description='Remediate resource exhaustion (memory, CPU)',
            applicable_signals=['memory_usage', 'cpu_usage', 'model_latency_p99'],
            actions=[
                RunbookAction(
                    action_id='diag_identify_resource_hog',
                    action_type=ActionType.DIAGNOSTIC,
                    description='Identify resource-intensive processes',
                    command=self._identify_resource_usage,
                    timeout_seconds=60
                ),
                RunbookAction(
                    action_id='remediate_scale_up',
                    action_type=ActionType.REMEDIATION,
                    description='Scale up infrastructure (add replicas)',
                    command=self._scale_infrastructure,
                    parameters={'action': 'scale_up', 'replicas': 2},
                    timeout_seconds=300,
                    rollback_action=RunbookAction(
                        action_id='rollback_scale_down',
                        action_type=ActionType.ROLLBACK,
                        description='Scale back to original capacity',
                        command=self._scale_infrastructure,
                        parameters={'action': 'scale_down'}
                    )
                ),
                RunbookAction(
                    action_id='remediate_clear_caches',
                    action_type=ActionType.REMEDIATION,
                    description='Clear feature/prediction caches',
                    command=self._clear_caches,
                    timeout_seconds=30
                ),
                RunbookAction(
                    action_id='validate_resource_usage',
                    action_type=ActionType.VALIDATION,
                    description='Verify resource usage normalized',
                    command=self._validate_resource_levels,
                    timeout_seconds=180
                )
            ],
            estimated_duration_minutes=10
        )

    async def execute_runbook(self, runbook_id: str, incident: 'Incident',
                             auto_approve: bool = False) -> Dict:
        """
        Execute a runbook for an incident.

        Args:
            runbook_id: ID of runbook to execute
            incident: The incident being remediated
            auto_approve: Automatically approve actions requiring approval

        Returns:
            Execution results with action outcomes
        """
        if runbook_id not in self.runbooks:
            raise ValueError(f"Unknown runbook: {runbook_id}")

        runbook = self.runbooks[runbook_id]

        execution_id = f"exec_{runbook_id}_{datetime.now().isoformat()}"
        execution_start = datetime.now()

        print(f"\n{'='*70}")
        print(f"EXECUTING RUNBOOK: {runbook.title}")
        print(f"Runbook ID: {runbook_id}")
        print(f"Execution ID: {execution_id}")
        print(f"Incident: {incident.incident_id}")
        print(f"Estimated Duration: {runbook.estimated_duration_minutes} minutes")
        print(f"{'='*70}\n")

        action_results = []
        actions_completed = 0
        actions_failed = 0

        for i, action in enumerate(runbook.actions, 1):
            print(f"\n[Step {i}/{len(runbook.actions)}] {action.description}")
            print(f"Action Type: {action.action_type.value}")

            # Check if approval required
            if action.requires_approval and not auto_approve:
                approval = await self._request_human_approval(action, incident)
                if not approval:
                    print("  Status: SKIPPED (approval denied)")
                    action_results.append(RunbookExecutionResult(
                        action_id=action.action_id,
                        status=ActionStatus.SKIPPED,
                        started_at=datetime.now(),
                        completed_at=datetime.now(),
                        output=None,
                        error_message="Approval denied"
                    ))
                    continue

            # Execute action
            result = await self._execute_action(action, incident)
            action_results.append(result)

            if result.status == ActionStatus.SUCCEEDED:
                actions_completed += 1
                print(f"  Status: SUCCESS")
                if result.output:
                    print(f"  Output: {result.output}")

                # Validate if success criteria defined
                if action.success_criteria:
                    validation_result = await action.success_criteria()
                    if not validation_result:
                        print(f"  Warning: Success criteria not met")
                        result.status = ActionStatus.FAILED
                        actions_failed += 1

                        # Attempt rollback if available
                        if action.rollback_action:
                            print(f"  Executing rollback: {action.rollback_action.description}")
                            rollback_result = await self._execute_action(
                                action.rollback_action, incident
                            )
                            result.rollback_executed = True

            elif result.status == ActionStatus.FAILED:
                actions_failed += 1
                print(f"  Status: FAILED")
                print(f"  Error: {result.error_message}")

                # Handle failure
                if action.on_failure == "stop":
                    print(f"\n  Stopping runbook execution due to failure")
                    break
                elif action.on_failure == "escalate":
                    print(f"\n  Escalating to human intervention")
                    await self._escalate_to_human(incident, action, result)
                    break
                # If "continue", proceed to next action

        # Calculate execution summary
        execution_end = datetime.now()
        duration_seconds = (execution_end - execution_start).total_seconds()

        success = actions_failed == 0 and actions_completed > 0

        execution_summary = {
            'execution_id': execution_id,
            'runbook_id': runbook_id,
            'incident_id': incident.incident_id,
            'started_at': execution_start,
            'completed_at': execution_end,
            'duration_seconds': duration_seconds,
            'total_actions': len(runbook.actions),
            'actions_completed': actions_completed,
            'actions_failed': actions_failed,
            'success': success,
            'action_results': action_results
        }

        # Store execution history
        self.execution_history.append(execution_summary)

        # Update runbook success rate
        self._update_runbook_success_rate(runbook_id, success)

        print(f"\n{'='*70}")
        print(f"RUNBOOK EXECUTION {'COMPLETED' if success else 'FAILED'}")
        print(f"Duration: {duration_seconds:.1f} seconds")
        print(f"Actions Completed: {actions_completed}/{len(runbook.actions)}")
        print(f"{'='*70}\n")

        return execution_summary

    async def _execute_action(self, action: RunbookAction,
                             incident: 'Incident') -> RunbookExecutionResult:
        """Execute a single runbook action."""
        started_at = datetime.now()

        try:
            # Execute action command with timeout
            if action.command:
                output = await asyncio.wait_for(
                    action.command(**action.parameters),
                    timeout=action.timeout_seconds
                )
            else:
                output = f"No command defined for {action.action_id}"

            completed_at = datetime.now()

            return RunbookExecutionResult(
                action_id=action.action_id,
                status=ActionStatus.SUCCEEDED,
                started_at=started_at,
                completed_at=completed_at,
                output=output
            )

        except asyncio.TimeoutError:
            return RunbookExecutionResult(
                action_id=action.action_id,
                status=ActionStatus.FAILED,
                started_at=started_at,
                completed_at=datetime.now(),
                output=None,
                error_message=f"Action timed out after {action.timeout_seconds}s"
            )

        except Exception as e:
            return RunbookExecutionResult(
                action_id=action.action_id,
                status=ActionStatus.FAILED,
                started_at=started_at,
                completed_at=datetime.now(),
                output=None,
                error_message=str(e)
            )

    async def _request_human_approval(self, action: RunbookAction,
                                     incident: 'Incident') -> bool:
        """Request human approval for risky action."""
        print(f"\n  APPROVAL REQUIRED")
        print(f"  Action: {action.description}")
        print(f"  This is a potentially risky operation that requires approval")
        print(f"  Incident: {incident.incident_id} ({incident.severity})")

        # In production, this would integrate with Slack, PagerDuty, etc.
        # For now, return True to proceed (in real system, wait for human response)
        return True

    async def _escalate_to_human(self, incident: 'Incident',
                                failed_action: RunbookAction,
                                result: RunbookExecutionResult):
        """Escalate to human when automation fails."""
        print(f"\n  ESCALATING TO HUMAN")
        print(f"  Runbook automation unable to resolve incident")
        print(f"  Failed Action: {failed_action.description}")
        print(f"  Error: {result.error_message}")
        print(f"  Please investigate manually")

        # Would notify oncall engineer via PagerDuty

    def _update_runbook_success_rate(self, runbook_id: str, success: bool):
        """Update historical success rate for runbook."""
        runbook = self.runbooks[runbook_id]

        # Calculate success rate from history
        runbook_executions = [e for e in self.execution_history
                             if e['runbook_id'] == runbook_id]

        if runbook_executions:
            successes = sum(1 for e in runbook_executions if e['success'])
            runbook.success_rate = successes / len(runbook_executions)

    # Dummy action implementations (would integrate with actual systems)

    async def _check_recent_predictions(self) -> Dict:
        """Analyze recent model predictions."""
        return {'status': 'analyzed', 'patterns_found': ['high_error_rate_on_segment_A']}

    async def _check_data_drift(self) -> Dict:
        """Check for data drift."""
        return {'drift_detected': True, 'affected_features': ['feature_1', 'feature_5']}

    async def _rollback_model_version(self, rollback_versions: int = 1) -> Dict:
        """Rollback model to previous version."""
        return {'status': 'rolled_back', 'new_version': 'v2.3.0', 'old_version': 'v2.4.0'}

    async def _verify_model_health(self) -> bool:
        """Verify model is healthy after remediation."""
        return True

    async def _validate_model_metrics(self) -> Dict:
        """Validate model metrics are within acceptable range."""
        return {'accuracy': 0.92, 'within_threshold': True}

    async def _notify_stakeholders(self, message: str) -> Dict:
        """Notify stakeholders of action taken."""
        return {'notified': ['oncall', 'team_lead'], 'message': message}

    async def _identify_bad_features(self) -> List[str]:
        """Identify features with quality issues."""
        return ['feature_7', 'feature_12']

    async def _switch_feature_pipeline(self, pipeline: str) -> Dict:
        """Switch to different feature pipeline."""
        return {'status': 'switched', 'active_pipeline': pipeline}

    async def _validate_feature_quality(self) -> Dict:
        """Validate feature quality metrics."""
        return {'null_rate': 0.02, 'within_threshold': True}

    async def _identify_resource_usage(self) -> Dict:
        """Identify resource usage by component."""
        return {
            'model_inference': {'cpu': '45%', 'memory': '2.1GB'},
            'feature_pipeline': {'cpu': '30%', 'memory': '1.5GB'}
        }

    async def _scale_infrastructure(self, action: str, replicas: int = 1) -> Dict:
        """Scale infrastructure up or down."""
        return {'status': 'scaled', 'action': action, 'current_replicas': replicas + 1}

    async def _clear_caches(self) -> Dict:
        """Clear caches to free memory."""
        return {'status': 'cleared', 'memory_freed_mb': 512}

    async def _validate_resource_levels(self) -> Dict:
        """Validate resource usage is within limits."""
        return {'cpu_usage': 0.55, 'memory_usage': 0.62, 'within_limits': True}
\end{lstlisting}

\subsubsection{Post-Incident Analysis}

Post-incident analysis provides automated root cause identification, contributing factor analysis, and prevention measures.

\begin{lstlisting}[style=python, caption={Post-incident analysis with root cause identification}]
from typing import List, Dict, Optional, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import json

@dataclass
class TimelineEvent:
    """Single event in incident timeline."""
    timestamp: datetime
    event_type: str  # metric_change, alert_triggered, action_taken, etc.
    source: str  # monitoring_system, runbook, human, etc.
    description: str
    metadata: Dict = field(default_factory=dict)

@dataclass
class ContributingFactor:
    """Factor that contributed to the incident."""
    factor_type: str  # code_change, data_quality, infrastructure, etc.
    description: str
    confidence: float  # 0.0 to 1.0
    evidence: List[str]
    preventive_measures: List[str]

@dataclass
class IncidentAnalysis:
    """Complete post-incident analysis."""
    incident_id: str
    analysis_id: str
    analyzed_at: datetime
    root_cause: Optional[ContributingFactor]
    contributing_factors: List[ContributingFactor]
    timeline: List[TimelineEvent]
    impact_summary: Dict
    lessons_learned: List[str]
    action_items: List[Dict]
    similar_incidents: List[str]

class PostIncidentAnalyzer:
    """
    Automated post-incident analysis for ML systems.

    Features:
    - Timeline reconstruction from logs and metrics
    - Root cause identification using correlation analysis
    - Contributing factor analysis
    - Impact quantification (users affected, revenue, etc.)
    - Lessons learned extraction
    - Prevention measure recommendations
    - Similar incident pattern detection
    """

    def __init__(self,
                 metrics_client,
                 logs_client,
                 incident_history: List[Dict]):
        self.metrics_client = metrics_client
        self.logs_client = logs_client
        self.incident_history = incident_history
        self.analysis_history: List[IncidentAnalysis] = []

    async def analyze_incident(self, incident: 'Incident',
                               execution_result: Dict) -> IncidentAnalysis:
        """
        Perform comprehensive post-incident analysis.

        Args:
            incident: The incident object
            execution_result: Results from runbook execution

        Returns:
            Complete incident analysis with root cause and recommendations
        """
        analysis_id = f"analysis_{incident.incident_id}_{datetime.now().isoformat()}"

        # Reconstruct timeline
        timeline = await self._reconstruct_timeline(incident)

        # Identify root cause
        root_cause = await self._identify_root_cause(incident, timeline)

        # Find contributing factors
        contributing_factors = await self._analyze_contributing_factors(
            incident, timeline, root_cause
        )

        # Quantify impact
        impact_summary = await self._quantify_impact(incident, timeline)

        # Extract lessons learned
        lessons_learned = self._extract_lessons_learned(
            incident, root_cause, contributing_factors
        )

        # Generate action items
        action_items = self._generate_action_items(
            root_cause, contributing_factors
        )

        # Find similar historical incidents
        similar_incidents = self._find_similar_incidents(incident)

        analysis = IncidentAnalysis(
            incident_id=incident.incident_id,
            analysis_id=analysis_id,
            analyzed_at=datetime.now(),
            root_cause=root_cause,
            contributing_factors=contributing_factors,
            timeline=timeline,
            impact_summary=impact_summary,
            lessons_learned=lessons_learned,
            action_items=action_items,
            similar_incidents=similar_incidents
        )

        self.analysis_history.append(analysis)

        return analysis

    async def _reconstruct_timeline(self, incident: 'Incident') -> List[TimelineEvent]:
        """Reconstruct incident timeline from logs, metrics, and actions."""
        timeline = []

        # Add incident detection event
        timeline.append(TimelineEvent(
            timestamp=incident.detected_at,
            event_type='incident_detected',
            source='monitoring_system',
            description=f"Incident detected: {incident.title}",
            metadata={'severity': incident.severity, 'signals': len(incident.signals)}
        ))

        # Query metrics for significant changes around incident time
        window_start = incident.detected_at - timedelta(hours=2)
        window_end = incident.detected_at + timedelta(hours=1)

        # Get metric changes
        metric_changes = await self._get_metric_changes(window_start, window_end)
        for change in metric_changes:
            timeline.append(TimelineEvent(
                timestamp=change['timestamp'],
                event_type='metric_change',
                source='metrics_system',
                description=f"{change['metric_name']} changed by {change['delta']}%",
                metadata=change
            ))

        # Get relevant log events
        log_events = await self._get_significant_log_events(window_start, window_end)
        for log_event in log_events:
            timeline.append(TimelineEvent(
                timestamp=log_event['timestamp'],
                event_type='log_event',
                source='logging_system',
                description=log_event['message'],
                metadata=log_event
            ))

        # Add deployment events (often root cause)
        deployments = await self._get_recent_deployments(window_start, window_end)
        for deployment in deployments:
            timeline.append(TimelineEvent(
                timestamp=deployment['deployed_at'],
                event_type='deployment',
                source='ci_cd_system',
                description=f"Deployed {deployment['component']} version {deployment['version']}",
                metadata=deployment
            ))

        # Sort timeline chronologically
        timeline.sort(key=lambda e: e.timestamp)

        return timeline

    async def _identify_root_cause(self, incident: 'Incident',
                                   timeline: List[TimelineEvent]) -> Optional[ContributingFactor]:
        """
        Identify root cause using correlation analysis and heuristics.
        """
        # Check for recent deployments (common root cause)
        recent_deployments = [e for e in timeline
                            if e.event_type == 'deployment'
                            and e.timestamp < incident.detected_at
                            and (incident.detected_at - e.timestamp) < timedelta(hours=1)]

        if recent_deployments:
            latest_deployment = recent_deployments[-1]
            return ContributingFactor(
                factor_type='code_change',
                description=f"Recent deployment of {latest_deployment.metadata.get('component')} "
                           f"version {latest_deployment.metadata.get('version')}",
                confidence=0.85,
                evidence=[
                    f"Deployment occurred {(incident.detected_at - latest_deployment.timestamp).total_seconds() / 60:.0f} minutes before incident",
                    f"Incident signals: {', '.join(s.signal_type for s in incident.signals)}"
                ],
                preventive_measures=[
                    "Implement canary deployments with automated rollback",
                    "Add pre-deployment validation tests",
                    "Enhance monitoring during deployment windows"
                ]
            )

        # Check for data quality issues
        data_quality_signals = [s for s in incident.signals
                               if 'drift' in s.signal_type or 'null' in s.signal_type]

        if data_quality_signals:
            return ContributingFactor(
                factor_type='data_quality',
                description="Data quality degradation detected",
                confidence=0.75,
                evidence=[f"{s.signal_type}: {s.value}" for s in data_quality_signals],
                preventive_measures=[
                    "Implement upstream data validation",
                    "Add data quality monitoring at ingestion",
                    "Set up data SLAs with data providers"
                ]
            )

        # Check for infrastructure issues
        infra_signals = [s for s in incident.signals
                        if any(x in s.signal_type for x in ['memory', 'cpu', 'latency'])]

        if infra_signals:
            return ContributingFactor(
                factor_type='infrastructure',
                description="Infrastructure resource exhaustion",
                confidence=0.70,
                evidence=[f"{s.signal_type}: {s.value}" for s in infra_signals],
                preventive_measures=[
                    "Implement auto-scaling based on load",
                    "Add resource reservation and limits",
                    "Optimize resource-intensive operations"
                ]
            )

        # Default to unknown if no clear root cause
        return ContributingFactor(
            factor_type='unknown',
            description="Root cause not automatically determined",
            confidence=0.30,
            evidence=["Requires manual investigation"],
            preventive_measures=["Conduct detailed manual root cause analysis"]
        )

    async def _analyze_contributing_factors(self,
                                           incident: 'Incident',
                                           timeline: List[TimelineEvent],
                                           root_cause: ContributingFactor) -> List[ContributingFactor]:
        """Identify factors that contributed to incident severity or duration."""
        factors = []

        # Check if monitoring detected it quickly enough
        first_signal_time = min(s.timestamp for s in incident.signals)
        detection_delay = (incident.detected_at - first_signal_time).total_seconds()

        if detection_delay > 300:  # > 5 minutes
            factors.append(ContributingFactor(
                factor_type='monitoring_gap',
                description=f"Detection delayed by {detection_delay / 60:.1f} minutes",
                confidence=0.90,
                evidence=[f"First signal at {first_signal_time}, detected at {incident.detected_at}"],
                preventive_measures=[
                    "Lower alert thresholds for critical metrics",
                    "Add redundant detection methods"
                ]
            ))

        # Check if runbook execution was slow
        if hasattr(incident, 'runbook_execution_time'):
            if incident.runbook_execution_time > 600:  # > 10 minutes
                factors.append(ContributingFactor(
                    factor_type='slow_remediation',
                    description="Runbook execution took longer than expected",
                    confidence=0.80,
                    evidence=[f"Execution time: {incident.runbook_execution_time}s"],
                    preventive_measures=[
                        "Optimize runbook action performance",
                        "Enable parallel action execution"
                    ]
                ))

        # Check if incident occurred during known maintenance or high-traffic period
        hour_of_day = incident.detected_at.hour
        if 9 <= hour_of_day <= 17:  # Business hours
            factors.append(ContributingFactor(
                factor_type='high_impact_timing',
                description="Incident occurred during business hours (higher user impact)",
                confidence=0.95,
                evidence=[f"Detected at {incident.detected_at.strftime('%H:%M')}"],
                preventive_measures=[
                    "Schedule deployments outside business hours",
                    "Increase canary deployment duration during peak hours"
                ]
            ))

        return factors

    async def _quantify_impact(self, incident: 'Incident',
                              timeline: List[TimelineEvent]) -> Dict:
        """Quantify incident impact in business terms."""
        duration_minutes = (incident.resolved_at - incident.detected_at).total_seconds() / 60 \
                          if incident.resolved_at else None

        # Estimate affected users (would query from actual systems)
        affected_users = self._estimate_affected_users(incident, duration_minutes)

        # Estimate revenue impact
        revenue_impact = self._estimate_revenue_impact(incident, duration_minutes, affected_users)

        return {
            'duration_minutes': duration_minutes,
            'affected_users_estimated': affected_users,
            'revenue_impact_usd_estimated': revenue_impact,
            'sev_level': incident.severity,
            'services_affected': incident.affected_components,
            'mttr_minutes': duration_minutes  # Mean Time To Resolution
        }

    def _estimate_affected_users(self, incident: 'Incident', duration_minutes: float) -> int:
        """Estimate number of users affected."""
        # Simplified estimation - would use actual traffic data
        if incident.severity == 'SEV1':
            return 100000  # All users
        elif incident.severity == 'SEV2':
            return 50000  # Half of users
        elif incident.severity == 'SEV3':
            return 10000  # Subset of users
        else:
            return 1000  # Minimal impact

    def _estimate_revenue_impact(self, incident: 'Incident',
                                duration_minutes: float, affected_users: int) -> float:
        """Estimate revenue impact in USD."""
        # Simplified estimation
        avg_revenue_per_user_per_minute = 0.10  # $0.10/min/user
        return affected_users * duration_minutes * avg_revenue_per_user_per_minute

    def _extract_lessons_learned(self, incident: 'Incident',
                                root_cause: ContributingFactor,
                                contributing_factors: List[ContributingFactor]) -> List[str]:
        """Extract lessons learned from incident."""
        lessons = []

        if root_cause.factor_type == 'code_change':
            lessons.append("Deployment changes can cause immediate production impact")
            lessons.append("Faster automated rollback would reduce MTTR")

        if any(f.factor_type == 'monitoring_gap' for f in contributing_factors):
            lessons.append("Earlier detection would have reduced impact")
            lessons.append("Current alerting thresholds may be too conservative")

        if incident.severity in ['SEV1', 'SEV2']:
            lessons.append(f"{incident.severity} incidents require immediate oncall response")
            lessons.append("High-severity incidents justify investment in prevention")

        return lessons

    def _generate_action_items(self, root_cause: ContributingFactor,
                              contributing_factors: List[ContributingFactor]) -> List[Dict]:
        """Generate actionable follow-up items."""
        action_items = []

        # Aggregate all preventive measures
        all_measures = root_cause.preventive_measures.copy()
        for factor in contributing_factors:
            all_measures.extend(factor.preventive_measures)

        # Deduplicate and prioritize
        unique_measures = list(dict.fromkeys(all_measures))

        for i, measure in enumerate(unique_measures[:5], 1):  # Top 5
            action_items.append({
                'action_id': f"ACTION_{i}",
                'description': measure,
                'priority': 'high' if i <= 2 else 'medium',
                'owner': 'TBD',
                'due_date': (datetime.now() + timedelta(days=14)).isoformat(),
                'status': 'open'
            })

        return action_items

    def _find_similar_incidents(self, incident: 'Incident') -> List[str]:
        """Find similar historical incidents for pattern detection."""
        similar = []

        current_signal_types = {s.signal_type for s in incident.signals}

        for historical in self.incident_history:
            if historical.get('incident_id') == incident.incident_id:
                continue  # Skip self

            historical_signals = {s['signal_type'] for s in historical.get('signals', [])}

            # Calculate Jaccard similarity
            intersection = current_signal_types & historical_signals
            union = current_signal_types | historical_signals

            if union:
                similarity = len(intersection) / len(union)
                if similarity > 0.5:  # > 50% similar
                    similar.append(historical['incident_id'])

        return similar[:5]  # Top 5 most similar

    async def _get_metric_changes(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Get significant metric changes in time window."""
        # Would query actual metrics system
        return []

    async def _get_significant_log_events(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Get significant log events (errors, warnings) in time window."""
        # Would query actual logging system
        return []

    async def _get_recent_deployments(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """Get deployments in time window."""
        # Would query CI/CD system
        return []

    def generate_incident_report(self, analysis: IncidentAnalysis) -> str:
        """Generate human-readable incident report."""
        report = f"""
{'='*70}
POST-INCIDENT ANALYSIS REPORT
{'='*70}

Incident ID: {analysis.incident_id}
Analysis Date: {analysis.analyzed_at.strftime('%Y-%m-%d %H:%M:%S')}

ROOT CAUSE:
-----------
Type: {analysis.root_cause.factor_type if analysis.root_cause else 'Unknown'}
Description: {analysis.root_cause.description if analysis.root_cause else 'Not determined'}
Confidence: {analysis.root_cause.confidence * 100 if analysis.root_cause else 0:.0f}%

IMPACT SUMMARY:
---------------
Duration: {analysis.impact_summary.get('duration_minutes', 0):.1f} minutes
Affected Users: {analysis.impact_summary.get('affected_users_estimated', 0):,}
Estimated Revenue Impact: ${analysis.impact_summary.get('revenue_impact_usd_estimated', 0):,.2f}
Severity: {analysis.impact_summary.get('sev_level', 'Unknown')}

CONTRIBUTING FACTORS:
---------------------
"""
        for i, factor in enumerate(analysis.contributing_factors, 1):
            report += f"{i}. {factor.description} (confidence: {factor.confidence * 100:.0f}%)\n"

        report += "\nLESSONS LEARNED:\n----------------\n"
        for lesson in analysis.lessons_learned:
            report += f"- {lesson}\n"

        report += "\nACTION ITEMS:\n-------------\n"
        for action in analysis.action_items:
            report += f"[{action['priority'].upper()}] {action['description']} (Due: {action['due_date'][:10]})\n"

        report += "\nSIMILAR INCIDENTS:\n------------------\n"
        if analysis.similar_incidents:
            for incident_id in analysis.similar_incidents:
                report += f"- {incident_id}\n"
        else:
            report += "None found\n"

        report += f"\n{'='*70}\n"

        return report
\end{lstlisting}

\subsubsection{Incident Communication}

Incident communication provides stakeholder notification, status updates, and communication workflow management.

\begin{lstlisting}[style=python, caption={Incident communication with multi-channel notifications}]
from typing import List, Dict, Optional, Set
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
import asyncio

class CommunicationChannel(Enum):
    """Communication channels for incident notifications."""
    EMAIL = "email"
    SLACK = "slack"
    PAGERDUTY = "pagerduty"
    SMS = "sms"
    JIRA = "jira"
    STATUSPAGE = "statuspage"

class StakeholderRole(Enum):
    """Roles for incident stakeholders."""
    ONCALL_ENGINEER = "oncall_engineer"
    ML_TEAM_LEAD = "ml_team_lead"
    ENGINEERING_MANAGER = "engineering_manager"
    PRODUCT_MANAGER = "product_manager"
    EXECUTIVE = "executive"
    CUSTOMER_SUPPORT = "customer_support"

@dataclass
class Stakeholder:
    """Individual stakeholder to notify."""
    name: str
    role: StakeholderRole
    channels: List[CommunicationChannel]
    contact_info: Dict[str, str]  # channel -> contact (e.g., 'email' -> 'user@company.com')
    notify_for_severities: List[str]  # SEV1, SEV2, etc.

@dataclass
class CommunicationTemplate:
    """Template for incident communications."""
    template_id: str
    name: str
    subject_template: str
    body_template: str
    channels: List[CommunicationChannel]
    trigger_conditions: List[str]  # When to send

class IncidentCommunicator:
    """
    Automated incident communication management.

    Features:
    - Multi-channel stakeholder notifications
    - Severity-based escalation paths
    - Automated status updates
    - Communication templates
    - Notification tracking and deduplication
    - Status page integration
    - War room coordination
    """

    def __init__(self):
        self.stakeholders: List[Stakeholder] = []
        self.templates: Dict[str, CommunicationTemplate] = {}
        self.notification_history: List[Dict] = []
        self.active_war_rooms: Dict[str, Dict] = {}

        self._initialize_stakeholders()
        self._initialize_templates()

    def _initialize_stakeholders(self):
        """Initialize stakeholder registry."""
        self.stakeholders = [
            Stakeholder(
                name="ML Oncall Engineer",
                role=StakeholderRole.ONCALL_ENGINEER,
                channels=[CommunicationChannel.PAGERDUTY, CommunicationChannel.SLACK],
                contact_info={
                    'pagerduty': 'ml-oncall-schedule',
                    'slack': '@ml-oncall'
                },
                notify_for_severities=['SEV1', 'SEV2', 'SEV3', 'SEV4']
            ),
            Stakeholder(
                name="ML Team Lead",
                role=StakeholderRole.ML_TEAM_LEAD,
                channels=[CommunicationChannel.SLACK, CommunicationChannel.EMAIL],
                contact_info={
                    'slack': '@ml-lead',
                    'email': 'ml-lead@company.com'
                },
                notify_for_severities=['SEV1', 'SEV2', 'SEV3']
            ),
            Stakeholder(
                name="Engineering Manager",
                role=StakeholderRole.ENGINEERING_MANAGER,
                channels=[CommunicationChannel.SLACK, CommunicationChannel.EMAIL],
                contact_info={
                    'slack': '@eng-manager',
                    'email': 'eng-manager@company.com'
                },
                notify_for_severities=['SEV1', 'SEV2']
            ),
            Stakeholder(
                name="Product Manager",
                role=StakeholderRole.PRODUCT_MANAGER,
                channels=[CommunicationChannel.SLACK, CommunicationChannel.EMAIL],
                contact_info={
                    'slack': '@product-manager',
                    'email': 'pm@company.com'
                },
                notify_for_severities=['SEV1']
            ),
            Stakeholder(
                name="Executive",
                role=StakeholderRole.EXECUTIVE,
                channels=[CommunicationChannel.EMAIL],
                contact_info={
                    'email': 'exec@company.com'
                },
                notify_for_severities=['SEV1']
            ),
            Stakeholder(
                name="Customer Support Team",
                role=StakeholderRole.CUSTOMER_SUPPORT,
                channels=[CommunicationChannel.SLACK],
                contact_info={
                    'slack': '#customer-support'
                },
                notify_for_severities=['SEV1', 'SEV2']
            )
        ]

    def _initialize_templates(self):
        """Initialize communication templates."""
        self.templates['incident_detected'] = CommunicationTemplate(
            template_id='incident_detected',
            name='Incident Detection Alert',
            subject_template='{severity} Incident Detected: {title}',
            body_template="""
{severity} ML System Incident Detected

Incident ID: {incident_id}
Title: {title}
Severity: {severity}
Priority: {priority}
Detected At: {detected_at}

Description:
{description}

Affected Components:
{affected_components}

Signals Detected:
{signals}

User Impact:
{user_impact}

Assigned Runbook: {runbook_id}

Please acknowledge and begin incident response procedures.
            """,
            channels=[CommunicationChannel.PAGERDUTY, CommunicationChannel.SLACK],
            trigger_conditions=['incident_created']
        )

        self.templates['status_update'] = CommunicationTemplate(
            template_id='status_update',
            name='Incident Status Update',
            subject_template='Update: {title} ({severity})',
            body_template="""
Incident Status Update

Incident ID: {incident_id}
Status: {status}
Time Elapsed: {duration_minutes} minutes

Update:
{update_message}

Actions Completed:
{completed_actions}

Next Steps:
{next_steps}

Current Impact:
{current_impact}
            """,
            channels=[CommunicationChannel.SLACK, CommunicationChannel.EMAIL],
            trigger_conditions=['status_changed', 'every_30_minutes']
        )

        self.templates['incident_resolved'] = CommunicationTemplate(
            template_id='incident_resolved',
            name='Incident Resolved',
            subject_template='RESOLVED: {title}',
            body_template="""
Incident Resolved

Incident ID: {incident_id}
Title: {title}
Severity: {severity}
Resolved At: {resolved_at}
Total Duration: {duration_minutes} minutes

Resolution Summary:
{resolution_summary}

Root Cause:
{root_cause}

Actions Taken:
{actions_taken}

Post-Incident Analysis:
{post_incident_link}

Thank you for your attention to this incident.
            """,
            channels=[CommunicationChannel.SLACK, CommunicationChannel.EMAIL,
                     CommunicationChannel.STATUSPAGE],
            trigger_conditions=['incident_resolved']
        )

    async def notify_incident_detected(self, incident: 'Incident') -> Dict:
        """Send initial incident detection notifications."""
        # Determine which stakeholders to notify based on severity
        stakeholders_to_notify = [
            s for s in self.stakeholders
            if incident.severity in s.notify_for_severities
        ]

        # Prepare notification content
        template = self.templates['incident_detected']
        content = self._render_template(template, {
            'incident_id': incident.incident_id,
            'title': incident.title,
            'severity': incident.severity,
            'priority': incident.priority,
            'detected_at': incident.detected_at.strftime('%Y-%m-%d %H:%M:%S'),
            'description': incident.description,
            'affected_components': ', '.join(incident.affected_components),
            'signals': '\n'.join([f"- {s.signal_type}: {s.value}" for s in incident.signals]),
            'user_impact': incident.user_impact,
            'runbook_id': getattr(incident, 'assigned_runbook', 'None')
        })

        # Send notifications via each stakeholder's preferred channels
        notification_tasks = []
        for stakeholder in stakeholders_to_notify:
            for channel in stakeholder.channels:
                if channel in template.channels:
                    task = self._send_notification(
                        channel=channel,
                        recipient=stakeholder.contact_info.get(channel.value),
                        subject=content['subject'],
                        body=content['body'],
                        incident_id=incident.incident_id
                    )
                    notification_tasks.append(task)

        # Send all notifications in parallel
        results = await asyncio.gather(*notification_tasks, return_exceptions=True)

        # Track notifications
        notification_record = {
            'incident_id': incident.incident_id,
            'notification_type': 'incident_detected',
            'sent_at': datetime.now(),
            'stakeholders_notified': len(stakeholders_to_notify),
            'channels_used': list(set([c.value for s in stakeholders_to_notify for c in s.channels])),
            'success_count': sum(1 for r in results if not isinstance(r, Exception))
        }
        self.notification_history.append(notification_record)

        return notification_record

    async def send_status_update(self, incident: 'Incident',
                                update_message: str,
                                completed_actions: List[str],
                                next_steps: List[str]) -> Dict:
        """Send incident status update to stakeholders."""
        duration_minutes = (datetime.now() - incident.detected_at).total_seconds() / 60

        template = self.templates['status_update']
        content = self._render_template(template, {
            'incident_id': incident.incident_id,
            'title': incident.title,
            'severity': incident.severity,
            'status': incident.status,
            'duration_minutes': f"{duration_minutes:.0f}",
            'update_message': update_message,
            'completed_actions': '\n'.join([f"- {action}" for action in completed_actions]),
            'next_steps': '\n'.join([f"- {step}" for step in next_steps]),
            'current_impact': incident.user_impact
        })

        # Notify stakeholders via Slack and Email
        stakeholders_to_notify = [
            s for s in self.stakeholders
            if incident.severity in s.notify_for_severities
        ]

        notification_tasks = []
        for stakeholder in stakeholders_to_notify:
            for channel in [CommunicationChannel.SLACK, CommunicationChannel.EMAIL]:
                if channel in stakeholder.channels:
                    task = self._send_notification(
                        channel=channel,
                        recipient=stakeholder.contact_info.get(channel.value),
                        subject=content['subject'],
                        body=content['body'],
                        incident_id=incident.incident_id
                    )
                    notification_tasks.append(task)

        results = await asyncio.gather(*notification_tasks, return_exceptions=True)

        return {
            'sent_at': datetime.now(),
            'stakeholders_notified': len(stakeholders_to_notify),
            'success_count': sum(1 for r in results if not isinstance(r, Exception))
        }

    async def notify_incident_resolved(self, incident: 'Incident',
                                      analysis: 'IncidentAnalysis') -> Dict:
        """Send incident resolution notification."""
        duration_minutes = (incident.resolved_at - incident.detected_at).total_seconds() / 60

        template = self.templates['incident_resolved']
        content = self._render_template(template, {
            'incident_id': incident.incident_id,
            'title': incident.title,
            'severity': incident.severity,
            'resolved_at': incident.resolved_at.strftime('%Y-%m-%d %H:%M:%S'),
            'duration_minutes': f"{duration_minutes:.0f}",
            'resolution_summary': getattr(incident, 'resolution_summary', 'Incident resolved'),
            'root_cause': analysis.root_cause.description if analysis.root_cause else 'Under investigation',
            'actions_taken': '\n'.join([f"- {action['description']}"
                                       for action in getattr(incident, 'actions_taken', [])]),
            'post_incident_link': f"https://incidents.company.com/analysis/{analysis.analysis_id}"
        })

        # Notify all stakeholders who were originally notified
        stakeholders_to_notify = [
            s for s in self.stakeholders
            if incident.severity in s.notify_for_severities
        ]

        notification_tasks = []
        for stakeholder in stakeholders_to_notify:
            for channel in stakeholder.channels:
                task = self._send_notification(
                    channel=channel,
                    recipient=stakeholder.contact_info.get(channel.value),
                    subject=content['subject'],
                    body=content['body'],
                    incident_id=incident.incident_id
                )
                notification_tasks.append(task)

        # Update status page
        await self._update_status_page(
            status='operational',
            message=f"Incident {incident.incident_id} resolved. All systems operational."
        )

        results = await asyncio.gather(*notification_tasks, return_exceptions=True)

        return {
            'sent_at': datetime.now(),
            'stakeholders_notified': len(stakeholders_to_notify),
            'success_count': sum(1 for r in results if not isinstance(r, Exception))
        }

    def _render_template(self, template: CommunicationTemplate,
                        context: Dict) -> Dict[str, str]:
        """Render communication template with context."""
        subject = template.subject_template.format(**context)
        body = template.body_template.format(**context)
        return {'subject': subject, 'body': body}

    async def _send_notification(self, channel: CommunicationChannel,
                                recipient: str, subject: str, body: str,
                                incident_id: str) -> Dict:
        """Send notification via specific channel."""
        # Implementation would integrate with actual systems
        if channel == CommunicationChannel.SLACK:
            return await self._send_slack_message(recipient, subject, body)
        elif channel == CommunicationChannel.EMAIL:
            return await self._send_email(recipient, subject, body)
        elif channel == CommunicationChannel.PAGERDUTY:
            return await self._create_pagerduty_incident(recipient, subject, body, incident_id)
        elif channel == CommunicationChannel.SMS:
            return await self._send_sms(recipient, f"{subject}\n\n{body}")
        else:
            return {'status': 'not_implemented', 'channel': channel.value}

    async def _send_slack_message(self, channel: str, subject: str, body: str) -> Dict:
        """Send Slack message."""
        # Would use Slack API
        return {'status': 'sent', 'channel': 'slack', 'recipient': channel}

    async def _send_email(self, recipient: str, subject: str, body: str) -> Dict:
        """Send email."""
        # Would use email service (SendGrid, SES, etc.)
        return {'status': 'sent', 'channel': 'email', 'recipient': recipient}

    async def _create_pagerduty_incident(self, service: str, subject: str,
                                        body: str, incident_id: str) -> Dict:
        """Create PagerDuty incident."""
        # Would use PagerDuty API
        return {'status': 'sent', 'channel': 'pagerduty', 'incident_key': incident_id}

    async def _send_sms(self, phone_number: str, message: str) -> Dict:
        """Send SMS."""
        # Would use Twilio or similar
        return {'status': 'sent', 'channel': 'sms', 'recipient': phone_number}

    async def _update_status_page(self, status: str, message: str) -> Dict:
        """Update public status page."""
        # Would integrate with Statuspage.io or similar
        return {'status': 'updated', 'page_status': status}

    async def create_war_room(self, incident: 'Incident') -> Dict:
        """Create virtual war room for incident coordination."""
        war_room_id = f"war_room_{incident.incident_id}"

        war_room = {
            'war_room_id': war_room_id,
            'incident_id': incident.incident_id,
            'created_at': datetime.now(),
            'slack_channel': f"#incident-{incident.incident_id}",
            'video_call_link': f"https://meet.company.com/{war_room_id}",
            'shared_doc': f"https://docs.company.com/incident/{incident.incident_id}",
            'participants': []
        }

        self.active_war_rooms[incident.incident_id] = war_room

        # Create Slack channel and send invites
        await self._create_slack_channel(war_room['slack_channel'], incident)

        # Notify stakeholders with war room info
        stakeholders = [s for s in self.stakeholders
                       if incident.severity in s.notify_for_severities]

        for stakeholder in stakeholders:
            if CommunicationChannel.SLACK in stakeholder.channels:
                await self._send_slack_message(
                    stakeholder.contact_info['slack'],
                    f"War Room Created: {incident.title}",
                    f"""
War room created for incident {incident.incident_id}

Slack Channel: {war_room['slack_channel']}
Video Call: {war_room['video_call_link']}
Shared Doc: {war_room['shared_doc']}

Please join to coordinate incident response.
                    """
                )

        return war_room

    async def _create_slack_channel(self, channel_name: str, incident: 'Incident'):
        """Create Slack channel for incident."""
        # Would use Slack API to create channel and set topic
        pass
\end{lstlisting}

\subsection{Site Reliability Engineering for ML Systems}

Site Reliability Engineering (SRE) principles adapted for machine learning systems provide a framework for balancing innovation with system stability, defining clear service level agreements, and managing operational toil.

\subsubsection{Service Level Indicators and Objectives}

Service Level Indicators (SLIs) are quantifiable measures of service quality, while Service Level Objectives (SLOs) define target values for acceptable service levels.

\begin{lstlisting}[style=python, caption={SLIs and SLOs for ML systems}]
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import numpy as np

class SLIType(Enum):
    """Types of Service Level Indicators for ML systems."""
    AVAILABILITY = "availability"  # System uptime
    LATENCY = "latency"  # Response time
    ACCURACY = "accuracy"  # Model performance
    THROUGHPUT = "throughput"  # Requests per second
    ERROR_RATE = "error_rate"  # Percentage of failed requests
    DATA_FRESHNESS = "data_freshness"  # Age of training data
    PREDICTION_CONFIDENCE = "prediction_confidence"  # Model certainty

@dataclass
class SLI:
    """Service Level Indicator definition."""
    name: str
    sli_type: SLIType
    description: str
    measurement_window: timedelta  # How often to measure
    calculation_method: str  # How to calculate the SLI
    current_value: Optional[float] = None
    target_value: float = 0.0  # Target from SLO
    measurement_history: List[Dict] = field(default_factory=list)

@dataclass
class SLO:
    """Service Level Objective definition."""
    name: str
    sli: SLI
    target_percentage: float  # e.g., 99.9 means 99.9% of requests meet target
    measurement_period: timedelta  # e.g., 30 days
    error_budget: float = field(init=False)  # Calculated from target_percentage

    def __post_init__(self):
        """Calculate error budget."""
        self.error_budget = 100.0 - self.target_percentage

@dataclass
class ErrorBudget:
    """Error budget tracking."""
    slo_name: str
    total_budget: float  # Total allowable error (%)
    consumed_budget: float  # Error consumed so far (%)
    remaining_budget: float  # Error budget remaining (%)
    burn_rate: float  # Rate of budget consumption per day
    projected_depletion_date: Optional[datetime] = None

class SREMonitor:
    """
    SRE monitoring for ML systems.

    Features:
    - SLI/SLO definition and tracking
    - Error budget management
    - Burn rate alerting
    - SLO violation detection
    - Historical SLO compliance reporting
    """

    def __init__(self):
        self.slos: Dict[str, SLO] = {}
        self.error_budgets: Dict[str, ErrorBudget] = {}
        self._initialize_ml_slos()

    def _initialize_ml_slos(self):
        """Initialize standard SLOs for ML systems."""

        # SLO 1: Model Availability
        availability_sli = SLI(
            name="model_availability",
            sli_type=SLIType.AVAILABILITY,
            description="Percentage of time the model API is available",
            measurement_window=timedelta(minutes=1),
            calculation_method="(successful_health_checks / total_health_checks) * 100",
            target_value=99.9
        )
        self.slos['availability'] = SLO(
            name='Model Availability SLO',
            sli=availability_sli,
            target_percentage=99.9,  # 99.9% uptime
            measurement_period=timedelta(days=30)
        )

        # SLO 2: Prediction Latency
        latency_sli = SLI(
            name="prediction_latency_p99",
            sli_type=SLIType.LATENCY,
            description="99th percentile prediction latency",
            measurement_window=timedelta(minutes=5),
            calculation_method="np.percentile(latencies, 99)",
            target_value=100.0  # 100ms target
        )
        self.slos['latency'] = SLO(
            name='Prediction Latency SLO',
            sli=latency_sli,
            target_percentage=95.0,  # 95% of requests under 100ms
            measurement_period=timedelta(days=7)
        )

        # SLO 3: Model Accuracy
        accuracy_sli = SLI(
            name="model_accuracy",
            sli_type=SLIType.ACCURACY,
            description="Model prediction accuracy (when ground truth available)",
            measurement_window=timedelta(hours=1),
            calculation_method="(correct_predictions / total_predictions) * 100",
            target_value=92.0  # 92% accuracy target
        )
        self.slos['accuracy'] = SLO(
            name='Model Accuracy SLO',
            sli=accuracy_sli,
            target_percentage=99.0,  # Accuracy above 92% for 99% of measurements
            measurement_period=timedelta(days=30)
        )

        # SLO 4: Error Rate
        error_rate_sli = SLI(
            name="prediction_error_rate",
            sli_type=SLIType.ERROR_RATE,
            description="Percentage of prediction requests that fail",
            measurement_window=timedelta(minutes=1),
            calculation_method="(failed_requests / total_requests) * 100",
            target_value=0.1  # 0.1% max error rate
        )
        self.slos['error_rate'] = SLO(
            name='Error Rate SLO',
            sli=error_rate_sli,
            target_percentage=99.9,  # Error rate below 0.1% for 99.9% of time
            measurement_period=timedelta(days=30)
        )

        # SLO 5: Data Freshness
        freshness_sli = SLI(
            name="training_data_freshness",
            sli_type=SLIType.DATA_FRESHNESS,
            description="Age of data used to train current model (in hours)",
            measurement_window=timedelta(hours=6),
            calculation_method="(now - last_training_data_timestamp).total_seconds() / 3600",
            target_value=168.0  # 7 days max age
        )
        self.slos['data_freshness'] = SLO(
            name='Data Freshness SLO',
            sli=freshness_sli,
            target_percentage=95.0,  # Data less than 7 days old 95% of time
            measurement_period=timedelta(days=30)
        )

        # Initialize error budgets
        for slo_name, slo in self.slos.items():
            self.error_budgets[slo_name] = ErrorBudget(
                slo_name=slo.name,
                total_budget=slo.error_budget,
                consumed_budget=0.0,
                remaining_budget=slo.error_budget,
                burn_rate=0.0
            )

    def measure_sli(self, slo_name: str, measurements: List[float]) -> Dict:
        """
        Measure SLI value for a given SLO.

        Args:
            slo_name: Name of the SLO
            measurements: List of measurement values

        Returns:
            Measurement result with SLI value and SLO compliance
        """
        if slo_name not in self.slos:
            raise ValueError(f"Unknown SLO: {slo_name}")

        slo = self.slos[slo_name]
        sli = slo.sli

        # Calculate SLI value based on type
        if sli.sli_type == SLIType.LATENCY:
            sli_value = np.percentile(measurements, 99)
        elif sli.sli_type == SLIType.AVAILABILITY:
            sli_value = (sum(measurements) / len(measurements)) * 100
        elif sli.sli_type == SLIType.ACCURACY:
            sli_value = (sum(measurements) / len(measurements)) * 100
        elif sli.sli_type == SLIType.ERROR_RATE:
            sli_value = (sum(measurements) / len(measurements)) * 100
        elif sli.sli_type == SLIType.DATA_FRESHNESS:
            sli_value = max(measurements)  # Worst case freshness
        else:
            sli_value = np.mean(measurements)

        # Update current value
        sli.current_value = sli_value

        # Check if measurement meets SLO
        meets_slo = self._check_slo_compliance(sli, sli_value)

        # Record measurement
        measurement_record = {
            'timestamp': datetime.now(),
            'sli_value': sli_value,
            'target_value': sli.target_value,
            'meets_slo': meets_slo,
            'num_measurements': len(measurements)
        }
        sli.measurement_history.append(measurement_record)

        # Update error budget
        self._update_error_budget(slo_name, meets_slo)

        return measurement_record

    def _check_slo_compliance(self, sli: SLI, sli_value: float) -> bool:
        """Check if SLI measurement meets target."""
        if sli.sli_type in [SLIType.LATENCY, SLIType.ERROR_RATE, SLIType.DATA_FRESHNESS]:
            # For these metrics, lower is better
            return sli_value <= sli.target_value
        else:
            # For availability, accuracy, higher is better
            return sli_value >= sli.target_value

    def _update_error_budget(self, slo_name: str, meets_slo: bool):
        """Update error budget based on SLI measurement."""
        error_budget = self.error_budgets[slo_name]
        slo = self.slos[slo_name]

        # Calculate compliance over measurement period
        recent_measurements = [
            m for m in slo.sli.measurement_history
            if datetime.now() - m['timestamp'] <= slo.measurement_period
        ]

        if not recent_measurements:
            return

        # Calculate percentage of measurements meeting SLO
        meeting_slo = sum(1 for m in recent_measurements if m['meets_slo'])
        compliance_percentage = (meeting_slo / len(recent_measurements)) * 100

        # Consumed budget = target - actual compliance
        error_budget.consumed_budget = max(0, slo.target_percentage - compliance_percentage)
        error_budget.remaining_budget = error_budget.total_budget - error_budget.consumed_budget

        # Calculate burn rate (budget consumed per day)
        if len(recent_measurements) > 1:
            measurement_duration = (recent_measurements[-1]['timestamp'] -
                                   recent_measurements[0]['timestamp']).total_seconds() / 86400  # days
            if measurement_duration > 0:
                error_budget.burn_rate = error_budget.consumed_budget / measurement_duration

                # Project depletion date if burning budget
                if error_budget.burn_rate > 0 and error_budget.remaining_budget > 0:
                    days_until_depletion = error_budget.remaining_budget / error_budget.burn_rate
                    error_budget.projected_depletion_date = datetime.now() + timedelta(
                        days=days_until_depletion
                    )

    def get_error_budget_status(self, slo_name: str) -> ErrorBudget:
        """Get current error budget status."""
        return self.error_budgets[slo_name]

    def check_burn_rate_alert(self, slo_name: str, threshold_multiplier: float = 2.0) -> Dict:
        """
        Check if error budget is burning too fast.

        Args:
            slo_name: Name of SLO to check
            threshold_multiplier: Alert if burn rate exceeds expected rate by this factor

        Returns:
            Alert information if burn rate is too high
        """
        error_budget = self.error_budgets[slo_name]
        slo = self.slos[slo_name]

        # Expected burn rate: consume entire budget evenly over measurement period
        measurement_period_days = slo.measurement_period.total_seconds() / 86400
        expected_burn_rate = error_budget.total_budget / measurement_period_days

        # Check if actual burn rate exceeds threshold
        if error_budget.burn_rate > expected_burn_rate * threshold_multiplier:
            return {
                'alert': True,
                'slo_name': slo_name,
                'current_burn_rate': error_budget.burn_rate,
                'expected_burn_rate': expected_burn_rate,
                'multiplier': error_budget.burn_rate / expected_burn_rate,
                'remaining_budget': error_budget.remaining_budget,
                'projected_depletion_date': error_budget.projected_depletion_date,
                'recommended_action': 'Slow down deployments or implement fixes to reduce error rate'
            }

        return {'alert': False}

    def generate_slo_report(self) -> str:
        """Generate comprehensive SLO compliance report."""
        report = f"""
{'='*70}
SLO COMPLIANCE REPORT
{'='*70}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

"""
        for slo_name, slo in self.slos.items():
            error_budget = self.error_budgets[slo_name]

            # Calculate compliance
            recent_measurements = [
                m for m in slo.sli.measurement_history
                if datetime.now() - m['timestamp'] <= slo.measurement_period
            ]

            if recent_measurements:
                meeting_slo = sum(1 for m in recent_measurements if m['meets_slo'])
                compliance = (meeting_slo / len(recent_measurements)) * 100
            else:
                compliance = 0.0

            report += f"""
{slo.name}
{'-'*len(slo.name)}
SLI: {slo.sli.name} ({slo.sli.sli_type.value})
Target: {slo.sli.target_value} ({slo.target_percentage}% of time)
Current Value: {slo.sli.current_value if slo.sli.current_value else 'N/A'}
Compliance: {compliance:.2f}%
Status: {'✓ MEETING SLO' if compliance >= slo.target_percentage else '✗ VIOLATING SLO'}

Error Budget:
  Total: {error_budget.total_budget:.2f}%
  Consumed: {error_budget.consumed_budget:.2f}%
  Remaining: {error_budget.remaining_budget:.2f}%
  Burn Rate: {error_budget.burn_rate:.4f}%/day
  {"Projected Depletion: " + error_budget.projected_depletion_date.strftime('%Y-%m-%d') if error_budget.projected_depletion_date else "Budget Safe"}

"""

        report += f"{'='*70}\n"
        return report

    def should_block_deployment(self, slo_name: str,
                               budget_threshold: float = 10.0) -> Dict:
        """
        Determine if deployment should be blocked due to error budget.

        Args:
            slo_name: SLO to check
            budget_threshold: Minimum remaining budget (%) to allow deployment

        Returns:
            Decision dict with block recommendation and reasoning
        """
        error_budget = self.error_budgets[slo_name]

        if error_budget.remaining_budget < budget_threshold:
            return {
                'block_deployment': True,
                'reason': f"Error budget critically low: {error_budget.remaining_budget:.2f}% remaining",
                'recommendation': 'Focus on stability improvements before deploying new features',
                'required_budget': budget_threshold,
                'current_budget': error_budget.remaining_budget
            }

        return {
            'block_deployment': False,
            'remaining_budget': error_budget.remaining_budget
        }
\end{lstlisting}

\subsubsection{Blameless Post-Mortems}

Blameless post-mortems focus on systemic issues rather than individual mistakes, fostering a culture of learning and continuous improvement.

\begin{lstlisting}[style=python, caption={Blameless post-mortem template}]
from typing import List, Dict
from dataclasses import dataclass
from datetime import datetime

@dataclass
class PostMortem:
    """Blameless post-mortem structure."""
    incident_id: str
    title: str
    date: datetime
    participants: List[str]
    summary: str
    impact: Dict  # Users affected, revenue impact, duration
    timeline: List[Dict]  # Chronological event timeline
    root_cause: str
    contributing_factors: List[str]
    what_went_well: List[str]  # Things that worked during response
    what_went_wrong: List[str]  # Things that didn't work
    where_we_got_lucky: List[str]  # Near-misses or fortuitous circumstances
    lessons_learned: List[str]
    action_items: List[Dict]  # With owners and due dates

def generate_postmortem_template(incident_analysis: 'IncidentAnalysis') -> PostMortem:
    """
    Generate blameless post-mortem from incident analysis.

    Principles:
    - Focus on systems and processes, not people
    - Assume good intentions
    - Celebrate what went well
    - Learn from near-misses
    - Create actionable improvements
    """
    return PostMortem(
        incident_id=incident_analysis.incident_id,
        title=f"Post-Mortem: {incident_analysis.incident_id}",
        date=incident_analysis.analyzed_at,
        participants=[],  # To be filled
        summary="",  # High-level summary
        impact=incident_analysis.impact_summary,
        timeline=incident_analysis.timeline,
        root_cause=incident_analysis.root_cause.description if incident_analysis.root_cause else "TBD",
        contributing_factors=[f.description for f in incident_analysis.contributing_factors],
        what_went_well=[
            "Monitoring detected the issue automatically",
            "Runbook automation reduced MTTR",
            "Team coordinated effectively in war room"
        ],
        what_went_wrong=[
            "Detection was slower than ideal",
            "Manual intervention required for rollback approval"
        ],
        where_we_got_lucky=[
            "Incident occurred outside peak traffic hours",
            "Similar incident had been practiced in game day exercise"
        ],
        lessons_learned=incident_analysis.lessons_learned,
        action_items=incident_analysis.action_items
    )

# Example post-mortem facilitation guidelines
POSTMORTEM_GUIDELINES = """
BLAMELESS POST-MORTEM FACILITATION GUIDELINES

1. SET THE TONE
   - Emphasize learning, not blame
   - "What can we learn?" vs "Who caused this?"
   - Psychological safety is paramount

2. FACILITATE DISCUSSION
   - Let participants speak without interruption
   - Redirect blame toward systemic issues
   - Example: "The engineer made a mistake" → "The deployment process lacks validation"

3. FOCUS ON SYSTEMS
   - Why did the system allow this to happen?
   - What process improvements would prevent recurrence?
   - How can we make the right thing the easy thing?

4. CELEBRATE SUCCESSES
   - What went well during incident response?
   - Who demonstrated excellent judgment?
   - Which systems/processes helped recovery?

5. CAPTURE NEAR-MISSES
   - Where did we get lucky?
   - What almost went wrong but didn't?
   - These are learning opportunities

6. CREATE ACTIONABLE ITEMS
   - Specific, measurable improvements
   - Assigned owners and due dates
   - Track completion in follow-up meetings

7. SHARE WIDELY
   - Publish post-mortem to engineering team
   - Knowledge sharing prevents repeat incidents
   - Normalize talking about failures
"""
\end{lstlisting}

\subsubsection{Toil Reduction}

Toil is repetitive, manual operational work that can be automated. Reducing toil frees engineers for high-value work.

\begin{lstlisting}[style=python, caption={Toil identification and reduction framework}]
from typing import List, Dict
from dataclasses import dataclass
from enum import Enum

class ToilType(Enum):
    """Categories of operational toil."""
    MANUAL_INTERVENTION = "manual_intervention"  # Requires human action
    REPETITIVE_TASK = "repetitive_task"  # Same task performed regularly
    INTERRUPT_DRIVEN = "interrupt_driven"  # Responding to alerts/pages
    LOW_VALUE = "low_value"  # Doesn't provide lasting value
    SCALES_LINEARLY = "scales_linearly"  # Effort grows with service size

@dataclass
class ToilTask:
    """Single source of operational toil."""
    name: str
    toil_types: List[ToilType]
    frequency_per_week: int
    time_per_occurrence_minutes: int
    total_time_per_week: float = 0.0  # Calculated
    assignees: List[str] = None
    automation_difficulty: str = "medium"  # low, medium, high
    automation_roi_score: float = 0.0  # Calculated
    proposed_solution: str = ""

    def __post_init__(self):
        """Calculate derived fields."""
        self.total_time_per_week = self.frequency_per_week * self.time_per_occurrence_minutes / 60.0

class ToilTracker:
    """
    Track and prioritize toil reduction efforts.

    Principles:
    - Engineers should spend <50% time on toil
    - Automate high-frequency, low-complexity tasks first
    - Measure toil reduction over time
    """

    def __init__(self):
        self.toil_inventory: List[ToilTask] = []
        self._initialize_common_ml_toil()

    def _initialize_common_ml_toil(self):
        """Identify common sources of ML operational toil."""

        self.toil_inventory = [
            ToilTask(
                name="Manual model deployment",
                toil_types=[ToilType.MANUAL_INTERVENTION, ToilType.REPETITIVE_TASK],
                frequency_per_week=3,
                time_per_occurrence_minutes=45,
                automation_difficulty="medium",
                proposed_solution="Implement CI/CD pipeline with automated testing and deployment"
            ),
            ToilTask(
                name="Responding to false positive alerts",
                toil_types=[ToilType.INTERRUPT_DRIVEN, ToilType.LOW_VALUE],
                frequency_per_week=15,
                time_per_occurrence_minutes=10,
                automation_difficulty="low",
                proposed_solution="Tune alert thresholds and add alert context/runbooks"
            ),
            ToilTask(
                name="Manual data quality investigation",
                toil_types=[ToilType.MANUAL_INTERVENTION, ToilType.REPETITIVE_TASK],
                frequency_per_week=5,
                time_per_occurrence_minutes=30,
                automation_difficulty="medium",
                proposed_solution="Build automated data quality dashboard with drill-down capabilities"
            ),
            ToilTask(
                name="Manually triggering model retraining",
                toil_types=[ToilType.MANUAL_INTERVENTION, ToilType.REPETITIVE_TASK],
                frequency_per_week=2,
                time_per_occurrence_minutes=60,
                automation_difficulty="low",
                proposed_solution="Implement automated retraining triggers based on performance/drift"
            ),
            ToilTask(
                name="Recreating crashed inference servers",
                toil_types=[ToilType.INTERRUPT_DRIVEN, ToilType.MANUAL_INTERVENTION],
                frequency_per_week=4,
                time_per_occurrence_minutes=20,
                automation_difficulty="low",
                proposed_solution="Add auto-restart policies and health-check based auto-healing"
            ),
            ToilTask(
                name="Manually provisioning infrastructure for experiments",
                toil_types=[ToilType.MANUAL_INTERVENTION, ToilType.SCALES_LINEARLY],
                frequency_per_week=6,
                time_per_occurrence_minutes=25,
                automation_difficulty="medium",
                proposed_solution="Create self-service infrastructure provisioning portal"
            ),
            ToilTask(
                name="Investigating customer reports of bad predictions",
                toil_types=[ToilType.INTERRUPT_DRIVEN, ToilType.MANUAL_INTERVENTION],
                frequency_per_week=8,
                time_per_occurrence_minutes=40,
                automation_difficulty="medium",
                proposed_solution="Build prediction explainability dashboard with request tracing"
            )
        ]

        # Calculate ROI scores
        self._calculate_automation_roi()

    def _calculate_automation_roi(self):
        """
        Calculate automation ROI score for each toil task.

        ROI = (time_saved_per_week / automation_difficulty_score)
        Higher score = better automation candidate
        """
        difficulty_scores = {'low': 1.0, 'medium': 2.0, 'high': 4.0}

        for task in self.toil_inventory:
            difficulty_score = difficulty_scores.get(task.automation_difficulty, 2.0)
            task.automation_roi_score = task.total_time_per_week / difficulty_score

    def get_prioritized_toil_backlog(self) -> List[ToilTask]:
        """Get toil tasks prioritized by automation ROI."""
        return sorted(self.toil_inventory,
                     key=lambda t: t.automation_roi_score,
                     reverse=True)

    def calculate_total_toil(self) -> Dict:
        """Calculate team's total toil burden."""
        total_hours_per_week = sum(task.total_time_per_week
                                   for task in self.toil_inventory)

        # Assuming 40-hour work week
        toil_percentage = (total_hours_per_week / 40.0) * 100

        return {
            'total_hours_per_week': total_hours_per_week,
            'toil_percentage': toil_percentage,
            'num_toil_tasks': len(self.toil_inventory),
            'status': 'healthy' if toil_percentage < 50 else 'unhealthy',
            'recommendation': 'Focus on toil reduction' if toil_percentage > 50
                            else 'Toil levels acceptable'
        }

    def generate_toil_reduction_plan(self, target_reduction_hours: float = 10.0) -> List[ToilTask]:
        """
        Generate plan to reduce toil by target hours per week.

        Args:
            target_reduction_hours: How many hours/week to eliminate

        Returns:
            List of tasks to automate (in priority order)
        """
        prioritized_tasks = self.get_prioritized_toil_backlog()

        reduction_plan = []
        hours_reduced = 0.0

        for task in prioritized_tasks:
            if hours_reduced >= target_reduction_hours:
                break

            reduction_plan.append(task)
            hours_reduced += task.total_time_per_week

        return reduction_plan

    def generate_toil_report(self) -> str:
        """Generate comprehensive toil analysis report."""
        total_stats = self.calculate_total_toil()
        prioritized_tasks = self.get_prioritized_toil_backlog()

        report = f"""
{'='*70}
TOIL REDUCTION ANALYSIS
{'='*70}

SUMMARY:
--------
Total Toil: {total_stats['total_hours_per_week']:.1f} hours/week
Toil Percentage: {total_stats['toil_percentage']:.1f}%
Status: {total_stats['status'].upper()}
Number of Toil Tasks: {total_stats['num_toil_tasks']}

SRE GUIDELINE: Engineers should spend <50% of time on toil

TOP AUTOMATION OPPORTUNITIES (by ROI):
---------------------------------------
"""
        for i, task in enumerate(prioritized_tasks[:5], 1):
            report += f"""
{i}. {task.name}
   Frequency: {task.frequency_per_week}x/week
   Time per occurrence: {task.time_per_occurrence_minutes} minutes
   Total time: {task.total_time_per_week:.1f} hours/week
   Automation difficulty: {task.automation_difficulty}
   ROI Score: {task.automation_roi_score:.2f}
   Proposed solution: {task.proposed_solution}
"""

        reduction_plan = self.generate_toil_reduction_plan(target_reduction_hours=10.0)
        total_reduction = sum(t.total_time_per_week for t in reduction_plan)

        report += f"""
RECOMMENDED ACTION PLAN:
------------------------
Automate the following {len(reduction_plan)} tasks to reduce toil by {total_reduction:.1f} hours/week:
"""
        for task in reduction_plan:
            report += f"  - {task.name}\n"

        report += f"\n{'='*70}\n"
        return report
\end{lstlisting}

\section{Progressive Exercises}

The following exercises build progressively from foundational monitoring to advanced enterprise capabilities. Each exercise should be completed in sequence to develop comprehensive monitoring expertise.

\subsection{Exercise 1: Basic Monitoring Stack}

\textbf{Objective}: Establish foundational monitoring infrastructure using industry-standard tools.

\textbf{Task}: Deploy Prometheus for metrics collection and Grafana for visualization. Configure model serving endpoints to expose prediction latency, request counts, and error rates. Create basic dashboards showing real-time performance metrics. Implement simple threshold-based alerts for critical failures. Validate end-to-end metric flow from model to dashboard.

\textbf{Deliverables}: Running Prometheus instance, Grafana dashboard with 5+ metrics, documented alert rules.

\textbf{Success Criteria}: Metrics update within 30 seconds, alerts fire within 2 minutes of threshold breach.

\subsection{Exercise 2: Statistical Drift Detection}

\textbf{Objective}: Implement robust drift detection using multiple statistical methods for early warning.

\textbf{Task}: Build drift monitoring comparing production data against training baselines using Kolmogorov-Smirnov test, Population Stability Index, and Jensen-Shannon divergence. Calculate drift scores for all features hourly. Implement consensus mechanism flagging drift when multiple methods agree. Generate automated drift reports with feature importance ranking and visualization.

\textbf{Deliverables}: Drift detection pipeline, automated reports, alert integration.

\textbf{Success Criteria}: Detect synthetic drift within one hour, <1\% false positive rate.

\subsection{Exercise 3: Intelligent Alerting System}

\textbf{Objective}: Reduce alert fatigue through intelligent noise reduction and contextual enrichment.

\textbf{Task}: Implement alert aggregation grouping similar alerts within time windows. Add exponential backoff for recurring issues. Enrich alerts with diagnostic context: recent deployments, traffic patterns, correlated metric changes. Implement dynamic thresholds adapting to historical patterns. Add alert routing based on severity and component. Track alert actionability metrics.

\textbf{Deliverables}: Smart alerting system, alert analytics dashboard, runbook integration.

\textbf{Success Criteria}: Reduce alert volume by 60\% while maintaining incident detection rate.

\subsection{Exercise 4: Role-Specific Dashboards}

\textbf{Objective}: Design customized monitoring views for different stakeholder needs and technical expertise.

\textbf{Task}: Create executive dashboard showing business KPIs, SLO compliance, and incident trends. Build ML engineer dashboard with model performance, drift scores, and feature statistics. Design SRE dashboard with infrastructure metrics, error budgets, and service health. Implement data scientist dashboard with experiment comparisons and model evaluation metrics. Add personalization and bookmark capabilities.

\textbf{Deliverables}: Four role-specific dashboards, user guide documentation.

\textbf{Success Criteria}: Stakeholders can answer key questions within 30 seconds.

\subsection{Exercise 5: Business Impact Monitoring}

\textbf{Objective}: Correlate technical metrics with business outcomes for ROI visibility and prioritization.

\textbf{Task}: Implement tracking for business metrics: revenue attributed to ML, conversion rates, customer satisfaction scores. Build correlation analysis linking model performance degradation to business impact. Create impact estimation models predicting revenue effects of incidents. Design business-focused alerts for significant impact events. Generate weekly business impact reports.

\textbf{Deliverables}: Business metrics pipeline, correlation dashboard, impact reporting system.

\textbf{Success Criteria}: Quantify business impact of incidents within 15 minutes.

\subsection{Exercise 6: Anomaly Detection System}

\textbf{Objective}: Detect subtle performance issues using multiple machine learning anomaly detection algorithms.

\textbf{Task}: Implement Isolation Forest for multivariate anomaly detection across correlated metrics. Add LSTM autoencoder for time-series pattern anomalies. Use statistical process control for trend detection. Implement ensemble voting combining multiple methods. Configure automatic investigation triggers for high-confidence anomalies. Track anomaly detection precision and recall.

\textbf{Deliverables}: Multi-algorithm anomaly detector, evaluation framework, alert integration.

\textbf{Success Criteria}: Detect anomalies 30 minutes before threshold breaches, <5\% false positive rate.

\subsection{Exercise 7: Incident Response Automation}

\textbf{Objective}: Automate incident detection and response with guided remediation reducing manual intervention.

\textbf{Task}: Implement automated incident creation from monitoring alerts with severity classification. Build runbook executor for common remediation steps: model rollback, traffic shifting, cache clearing. Add approval gates for high-risk actions. Create war room automation: Slack channel creation, stakeholder notification, video call links. Implement post-incident analysis with timeline reconstruction and root cause identification.

\textbf{Deliverables}: Incident management system, runbook library, automation metrics dashboard.

\textbf{Success Criteria}: Automate 70\% of incident response actions, reduce MTTR by 50\%.

\subsection{Exercise 8: Cost Monitoring and Optimization}

\textbf{Objective}: Track and optimize ML infrastructure costs with automated recommendations and budget alerts.

\textbf{Task}: Implement cost tracking for model training, serving, and storage across cloud resources. Build cost attribution by model, team, and experiment. Detect cost anomalies and inefficiencies: idle resources, oversized instances, unnecessary data retention. Generate optimization recommendations with estimated savings. Implement budget alerts and cost forecasting. Create cost efficiency metrics: cost per prediction, training ROI.

\textbf{Deliverables}: Cost monitoring dashboard, optimization recommender, budget tracking system.

\textbf{Success Criteria}: Identify 20\% cost reduction opportunities, prevent budget overruns.

\subsection{Exercise 9: Fairness and Bias Monitoring}

\textbf{Objective}: Continuously monitor model fairness across demographic groups ensuring ethical AI deployment.

\textbf{Task}: Implement fairness metrics tracking: demographic parity, equalized odds, calibration across protected attributes. Monitor prediction distributions by group detecting bias drift. Calculate disparate impact ratios flagging violations. Build fairness dashboard with drill-down by demographic segment. Implement bias alerts for regulatory compliance. Generate fairness audit reports.

\textbf{Deliverables}: Fairness monitoring pipeline, compliance dashboard, audit trail system.

\textbf{Success Criteria}: Detect fairness violations within 24 hours, maintain audit compliance.

\subsection{Exercise 10: Predictive Monitoring System}

\textbf{Objective}: Predict future failures and performance degradation enabling proactive intervention before impact.

\textbf{Task}: Train failure prediction models using historical incident data and leading indicators. Implement performance decay forecasting predicting when retraining needed. Build capacity planning predictions for infrastructure scaling. Create early warning system for upcoming issues with confidence scores. Implement What-If analysis for scenario planning. Track prediction accuracy improving models over time.

\textbf{Deliverables}: Predictive monitoring models, early warning dashboard, accuracy tracking system.

\textbf{Success Criteria}: Predict failures 2 hours in advance with 80\% accuracy.

\subsection{Exercise 11: Distributed Tracing Infrastructure}

\textbf{Objective}: Implement end-to-end request tracing for ML inference pipelines identifying latency bottlenecks.

\textbf{Task}: Deploy OpenTelemetry instrumentation across model serving stack. Add trace context propagation through microservices. Instrument feature pipelines, model execution, and post-processing stages. Integrate with Jaeger for trace visualization. Build latency analysis identifying slowest spans. Create dependency mapping showing service relationships. Implement trace-based debugging for production issues.

\textbf{Deliverables}: Distributed tracing system, latency analysis dashboard, dependency maps.

\textbf{Success Criteria}: Trace 100\% of requests, identify bottlenecks reducing p99 latency by 30\%.

\subsection{Exercise 12: Automated Root Cause Analysis}

\textbf{Objective}: Automate root cause identification using correlation analysis and causal inference reducing investigation time.

\textbf{Task}: Implement metric correlation analysis identifying related degradations during incidents. Build change correlation linking deployments, config changes, and incidents. Use causal graphs modeling metric relationships. Implement automated hypothesis generation and testing. Create root cause confidence scoring. Generate automated RCA reports with evidence and recommendations.

\textbf{Deliverables}: Root cause analysis engine, correlation dashboard, automated RCA reports.

\textbf{Success Criteria}: Identify correct root cause in top 3 hypotheses 90\% of time.

\subsection{Exercise 13: Continuous Monitoring Improvement}

\textbf{Objective}: Build meta-monitoring system continuously evaluating and improving monitoring effectiveness.

\textbf{Task}: Implement monitoring coverage analysis identifying gaps in instrumentation. Track monitoring effectiveness metrics: detection latency, false positive rate, incident coverage. Build alert quality scoring based on actionability and noise. Implement automatic threshold tuning using historical data. Create monitoring ROI analysis quantifying value versus cost. Generate quarterly monitoring improvement recommendations.

\textbf{Deliverables}: Meta-monitoring dashboard, coverage analysis tool, improvement recommendation system.

\textbf{Success Criteria}: Achieve 95\% monitoring coverage, reduce false positives by 40\% annually.

\section{Monitoring Maturity Assessment}

Organizations should assess their monitoring maturity to identify improvement opportunities and prioritize investments. The following framework evaluates capabilities across five maturity levels.

\subsection{Maturity Levels}

\subsubsection{Level 1: Initial (Ad-hoc Monitoring)}

\textbf{Characteristics}:
\begin{itemize}
    \item Basic infrastructure monitoring only (CPU, memory)
    \item Manual log inspection for troubleshooting
    \item No model performance tracking in production
    \item Reactive incident response
    \item Monitoring decisions made case-by-case
\end{itemize}

\textbf{Capabilities}:
\begin{itemize}
    \item System uptime tracking
    \item Basic error logging
    \item Manual performance checks
\end{itemize}

\textbf{Gaps}:
\begin{itemize}
    \item No data quality monitoring
    \item Missing drift detection
    \item No automated alerting
    \item Limited observability
\end{itemize}

\textbf{Next Steps}: Implement basic metrics collection, deploy Prometheus/Grafana, add model performance tracking.

\subsubsection{Level 2: Repeatable (Basic Monitoring)}

\textbf{Characteristics}:
\begin{itemize}
    \item Standardized metrics collection across models
    \item Basic dashboards for key metrics
    \item Threshold-based alerting
    \item Model performance tracking (accuracy, latency)
    \item Manual drift investigation
\end{itemize}

\textbf{Capabilities}:
\begin{itemize}
    \item Prometheus metrics collection
    \item Grafana dashboards
    \item Basic alerting rules
    \item Performance baselines
\end{itemize}

\textbf{Gaps}:
\begin{itemize}
    \item High alert noise
    \item Manual incident response
    \item No business impact tracking
    \item Limited root cause analysis
\end{itemize}

\textbf{Next Steps}: Add statistical drift detection, implement alert deduplication, begin tracking business metrics.

\subsubsection{Level 3: Defined (Proactive Monitoring)}

\textbf{Characteristics}:
\begin{itemize}
    \item Automated drift detection with multiple methods
    \item Intelligent alerting with noise reduction
    \item SLIs and SLOs defined for critical services
    \item Business impact monitoring
    \item Runbook automation for common incidents
    \item Post-incident analysis process
\end{itemize}

\textbf{Capabilities}:
\begin{itemize}
    \item Multi-method drift detection
    \item Alert aggregation and routing
    \item Error budget tracking
    \item Automated remediation
    \item Incident management system
\end{itemize}

\textbf{Gaps}:
\begin{itemize}
    \item Limited predictive capabilities
    \item Manual capacity planning
    \item Inconsistent fairness monitoring
    \item Reactive cost management
\end{itemize}

\textbf{Next Steps}: Implement anomaly detection, add distributed tracing, deploy fairness monitoring, build cost tracking.

\subsubsection{Level 4: Managed (Optimized Monitoring)}

\textbf{Characteristics}:
\begin{itemize}
    \item ML-powered anomaly detection
    \item Predictive monitoring with failure forecasting
    \item Comprehensive distributed tracing
    \item Automated root cause analysis
    \item Fairness and bias monitoring
    \item Cost optimization recommendations
    \item Continuous monitoring improvement
\end{itemize}

\textbf{Capabilities}:
\begin{itemize}
    \item Anomaly detection ensemble
    \item Predictive alerting
    \item End-to-end tracing
    \item Automated RCA
    \item Fairness dashboards
    \item Cost analytics
    \item Meta-monitoring
\end{itemize}

\textbf{Gaps}:
\begin{itemize}
    \item Limited cross-team integration
    \item Manual monitoring tuning
    \item Siloed observability data
    \item Reactive improvement process
\end{itemize}

\textbf{Next Steps}: Implement unified observability platform, add self-tuning monitors, deploy AIOps capabilities.

\subsubsection{Level 5: Optimizing (Intelligent Monitoring)}

\textbf{Characteristics}:
\begin{itemize}
    \item Unified observability across all systems
    \item Self-tuning monitoring with automatic optimization
    \item AIOps with intelligent incident correlation
    \item Proactive issue prevention
    \item Full automation of incident response
    \item Continuous learning from incidents
    \item Monitoring as code with version control
\end{itemize}

\textbf{Capabilities}:
\begin{itemize}
    \item Unified data lake
    \item Self-optimizing thresholds
    \item Intelligent incident grouping
    \item Preventive recommendations
    \item Full runbook automation
    \item Knowledge graph of incidents
    \item GitOps for monitoring
\end{itemize}

\textbf{Characteristics of Excellence}:
\begin{itemize}
    \item 95\%+ monitoring coverage
    \item <5\% false positive rate
    \item <5 minute incident detection
    \item 80\%+ automated resolution
    \item Zero production surprises
\end{itemize}

\subsection{Assessment Questionnaire}

Rate your organization on each capability (0 = None, 1 = Basic, 2 = Intermediate, 3 = Advanced):

\textbf{Infrastructure Monitoring}:
\begin{itemize}
    \item [ ] System metrics collection (CPU, memory, disk)
    \item [ ] Network monitoring
    \item [ ] Container/Kubernetes monitoring
    \item [ ] Cloud resource monitoring
\end{itemize}

\textbf{Model Performance Monitoring}:
\begin{itemize}
    \item [ ] Prediction accuracy tracking
    \item [ ] Latency and throughput monitoring
    \item [ ] Error rate tracking
    \item [ ] Confidence score analysis
\end{itemize}

\textbf{Data Quality Monitoring}:
\begin{itemize}
    \item [ ] Feature drift detection
    \item [ ] Data validation rules
    \item [ ] Schema monitoring
    \item [ ] Completeness tracking
\end{itemize}

\textbf{Alerting and Incident Management}:
\begin{itemize}
    \item [ ] Intelligent alert routing
    \item [ ] Alert deduplication
    \item [ ] Automated incident creation
    \item [ ] Runbook automation
\end{itemize}

\textbf{Business Impact Monitoring}:
\begin{itemize}
    \item [ ] Business KPI tracking
    \item [ ] Revenue impact analysis
    \item [ ] Customer satisfaction metrics
    \item [ ] Correlation with technical metrics
\end{itemize}

\textbf{Observability}:
\begin{itemize}
    \item [ ] Distributed tracing
    \item [ ] Structured logging
    \item [ ] Service dependency mapping
    \item [ ] Request flow visualization
\end{itemize}

\textbf{Advanced Capabilities}:
\begin{itemize}
    \item [ ] Anomaly detection
    \item [ ] Predictive monitoring
    \item [ ] Automated root cause analysis
    \item [ ] Fairness monitoring
\end{itemize}

\textbf{Scoring}:
\begin{itemize}
    \item 0-20: Level 1 (Initial)
    \item 21-40: Level 2 (Repeatable)
    \item 41-60: Level 3 (Defined)
    \item 61-80: Level 4 (Managed)
    \item 81-84: Level 5 (Optimizing)
\end{itemize}

\section{Troubleshooting Guide}

Common monitoring issues and their resolutions for production ML systems.

\subsection{Issue 1: High False Positive Alert Rate}

\textbf{Symptoms}:
\begin{itemize}
    \item Receiving 50+ alerts per day
    \item Most alerts resolve without intervention
    \item Team ignoring alerts (alert fatigue)
    \item Important issues buried in noise
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Static thresholds not accounting for patterns (daily/weekly cycles)
    \item Alerting on transient spikes
    \item Too many low-severity alerts
    \item Duplicate alerts from correlated metrics
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Implement dynamic thresholds based on historical percentiles
    \item Add minimum duration requirements (e.g., alert only if sustained for 5+ minutes)
    \item Increase thresholds for non-critical alerts
    \item Add alert deduplication and grouping
    \item Implement alert scoring prioritizing actionable alerts
    \item Use anomaly detection instead of static thresholds
\end{itemize}

\textbf{Implementation}:
\begin{lstlisting}[style=python]
# Example: Dynamic threshold based on historical data
threshold = np.percentile(historical_values, 95) * 1.2  # 20% above p95
alert_if_above_for = timedelta(minutes=5)  # Sustained deviation
\end{lstlisting}

\subsection{Issue 2: Missing Data Quality Issues}

\textbf{Symptoms}:
\begin{itemize}
    \item Model performance degrading without alerts
    \item Discovering data issues through customer complaints
    \item Inconsistent feature distributions
    \item Unexpected null values in production
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item No automated data validation
    \item Missing drift detection
    \item Insufficient feature monitoring
    \item Upstream data changes not communicated
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Implement comprehensive data validation rules
    \item Add drift detection for all features
    \item Monitor feature statistics (mean, std, null rate)
    \item Set up schema validation
    \item Alert on distribution shifts
    \item Track data freshness
\end{itemize}

\textbf{Implementation}:
\begin{lstlisting}[style=python]
# Example: Comprehensive feature validation
validation_rules = {
    'feature_name': {
        'null_threshold': 0.05,  # Alert if >5% nulls
        'min_value': 0,
        'max_value': 100,
        'expected_mean_range': (45, 55),
        'drift_threshold': 0.1  # PSI threshold
    }
}
\end{lstlisting}

\subsection{Issue 3: Slow Incident Detection}

\textbf{Symptoms}:
\begin{itemize}
    \item Issues impacting users for 30+ minutes before detection
    \item Discovering incidents through user reports
    \item Delayed alerts
    \item Missing early warning signals
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Long monitoring intervals (e.g., 5-minute aggregation)
    \item Conservative thresholds
    \item Monitoring only lagging indicators
    \item Alert delivery delays
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Reduce monitoring intervals to 10-30 seconds
    \item Add leading indicators (request queue depth, error spike rate)
    \item Implement real-time stream processing
    \item Add health check monitoring
    \item Use multi-level alerting (warning, critical)
    \item Ensure alert delivery paths are fast (direct API calls vs email)
\end{itemize}

\textbf{Implementation}:
\begin{lstlisting}[style=python]
# Example: Multi-level alerting
if error_rate > 0.10:
    severity = 'CRITICAL'  # Immediate page
    notification_channels = ['pagerduty', 'slack']
elif error_rate > 0.05:
    severity = 'WARNING'  # Slack notification
    notification_channels = ['slack']
\end{lstlisting}

\subsection{Issue 4: Dashboard Overload}

\textbf{Symptoms}:
\begin{itemize}
    \item 50+ metrics per dashboard
    \item Difficulty finding relevant information
    \item Stakeholders not using dashboards
    \item Multiple overlapping dashboards
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Trying to show everything on one dashboard
    \item No clear dashboard purpose
    \item Not designed for specific personas
    \item Metrics without context
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Create role-specific dashboards (exec, engineer, SRE)
    \item Follow 5-7-5 rule: 5 dashboards, 7 panels per dashboard, 5 metrics per panel
    \item Add contextual annotations (deployments, incidents)
    \item Implement drill-down hierarchy (summary → details)
    \item Use traffic light indicators for quick status
    \item Remove unused metrics regularly
\end{itemize}

\textbf{Best Practices}:
\begin{itemize}
    \item Executive dashboard: Business KPIs, SLO status, incident summary
    \item ML Engineer dashboard: Model performance, drift, feature statistics
    \item SRE dashboard: Infrastructure, latency, error rate, saturation
\end{itemize}

\subsection{Issue 5: Inability to Correlate Metrics}

\textbf{Symptoms}:
\begin{itemize}
    \item Cannot identify related degradations
    \item Difficult to find root cause
    \item Metrics siloed across tools
    \item No unified view of system state
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Metrics in separate systems
    \item Inconsistent timestamps
    \item Missing trace context
    \item No common identifiers
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Centralize metrics in unified observability platform
    \item Implement correlation IDs across services
    \item Add distributed tracing
    \item Standardize metric labels (model\_name, version, environment)
    \item Build correlation analysis dashboards
    \item Use metric relationships for automated RCA
\end{itemize}

\textbf{Implementation}:
\begin{lstlisting}[style=python]
# Example: Standardized metric labels
metrics.labels(
    model_name='fraud_detector',
    model_version='v2.3',
    environment='production',
    region='us-east-1'
).set(value)
\end{lstlisting}

\subsection{Issue 6: Monitoring System Overhead}

\textbf{Symptoms}:
\begin{itemize}
    \item Monitoring consuming 10\%+ of infrastructure costs
    \item High cardinality causing storage issues
    \item Query timeouts on monitoring system
    \item Metrics ingestion lag
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Excessive metric granularity
    \item High-cardinality labels (user IDs, request IDs)
    \item Retaining all historical data
    \item Inefficient queries
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Implement metric aggregation and sampling
    \item Use fixed-cardinality labels only
    \item Set appropriate retention policies (7 days detailed, 90 days aggregated)
    \item Use recording rules for expensive queries
    \item Implement metric filtering at collection
    \item Consider separate systems for high vs low cardinality metrics
\end{itemize}

\textbf{Best Practices}:
\begin{itemize}
    \item Limit label cardinality to <1000 unique values
    \item Use sampling for high-volume metrics (1\% of requests)
    \item Aggregate detailed metrics after 24 hours
    \item Archive historical data to object storage
\end{itemize}

\subsection{Issue 7: Lack of Monitoring Coverage}

\textbf{Symptoms}:
\begin{itemize}
    \item Blind spots in system
    \item Incidents in unmonitored components
    \item New services deployed without monitoring
    \item No monitoring standards
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item No monitoring checklist for new services
    \item Monitoring not part of deployment process
    \item Lack of monitoring ownership
    \item No coverage analysis
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
    \item Create monitoring requirements for all services
    \item Implement monitoring-as-code with templates
    \item Add monitoring coverage to CI/CD checks
    \item Conduct quarterly coverage audits
    \item Assign monitoring owners for each component
    \item Build monitoring coverage dashboard
\end{itemize}

\textbf{Required Monitoring Coverage}:
\begin{itemize}
    \item All model endpoints: latency, error rate, throughput
    \item All data pipelines: data quality, freshness, completeness
    \item All dependencies: external APIs, databases, caches
    \item All critical paths: authentication, authorization, payment
\end{itemize}

\section{Chapter Summary}

\subsection{Key Takeaways}

\textbf{Comprehensive Monitoring Strategy}:
\begin{itemize}
    \item \textbf{Monitor the Full Stack}: Track model performance, data quality, infrastructure metrics, and business impact holistically
    \item \textbf{Detect Before Impact}: Use drift detection, anomaly detection, and predictive monitoring to catch issues before users are affected
    \item \textbf{Automate Response}: Implement automated remediation, incident management, and runbook execution to reduce MTTR
    \item \textbf{Learn Continuously}: Conduct blameless post-mortems, track monitoring effectiveness, and iterate on improvements
\end{itemize}

\textbf{Production ML Monitoring Principles}:
\begin{itemize}
    \item \textbf{Assume Degradation}: ML models decay over time; build systems expecting and detecting performance deterioration
    \item \textbf{Use Multiple Methods}: Combine statistical tests, business rules, and ML-based anomaly detection for comprehensive coverage
    \item \textbf{Reduce Alert Noise}: Implement intelligent alerting with aggregation, deduplication, and dynamic thresholds preventing alert fatigue
    \item \textbf{Track Business Impact}: Connect technical metrics to business outcomes demonstrating ML system value and prioritizing fixes
    \item \textbf{Define Clear SLOs}: Establish measurable service level objectives with error budgets balancing innovation and stability
\end{itemize}

\textbf{Observability Best Practices}:
\begin{itemize}
    \item \textbf{Distributed Tracing}: Implement end-to-end request tracing for debugging latency issues and understanding dependencies
    \item \textbf{Structured Logging}: Use JSON logs with correlation IDs enabling efficient search and debugging
    \item \textbf{Metric Standardization}: Apply consistent labeling across metrics for correlation and automated analysis
    \item \textbf{Health Checks}: Implement deep health validation beyond simple liveness probes
\end{itemize}

\textbf{Incident Management Essentials}:
\begin{itemize}
    \item \textbf{Automated Detection}: Use multi-signal analysis and severity classification for rapid incident identification
    \item \textbf{Guided Remediation}: Provide runbook automation with rollback capabilities and escalation paths
    \item \textbf{Blameless Culture}: Focus post-mortems on systems and processes, not individuals, fostering learning
    \item \textbf{Continuous Improvement}: Track incident patterns, implement preventive measures, and reduce toil systematically
\end{itemize}

\textbf{SRE for ML Systems}:
\begin{itemize}
    \item \textbf{Error Budgets}: Use error budgets to balance feature velocity with system stability
    \item \textbf{Toil Reduction}: Identify and automate repetitive operational work freeing engineers for high-value tasks
    \item \textbf{Measure Everything}: Track monitoring effectiveness: coverage, false positive rate, detection latency, MTTR
\end{itemize}

\subsection{Integration with Other Chapters}

Monitoring connects to and depends on capabilities from other handbook chapters:

\textbf{Chapter 3: Data Management \& Versioning}:
\begin{itemize}
    \item Monitor data quality using versioned schemas
    \item Track data lineage for root cause analysis
    \item Detect drift against training data baselines
    \item Reference: Section 3.4 on data quality validation
\end{itemize}

\textbf{Chapter 4: Experiment Tracking}:
\begin{itemize}
    \item Compare production metrics to experiment results
    \item Track model versions deployed to production
    \item Correlate experiments with performance changes
    \item Reference: Section 4.3 on experiment-production alignment
\end{itemize}

\textbf{Chapter 6: Model Development}:
\begin{itemize}
    \item Monitor evaluation metrics defined during development
    \item Track fairness metrics from model cards
    \item Use confidence thresholds from development
    \item Reference: Section 6.5 on model evaluation
\end{itemize}

\textbf{Chapter 7: Statistical Rigor}:
\begin{itemize}
    \item Apply statistical tests for drift detection
    \item Use confidence intervals for alert thresholds
    \item Implement hypothesis testing for anomaly detection
    \item Reference: Section 7.2 on statistical testing
\end{itemize}

\textbf{Chapter 8: Model Deployment}:
\begin{itemize}
    \item Monitor deployment health and rollback triggers
    \item Track canary metrics for gradual rollouts
    \item Correlate deployments with incidents
    \item Reference: Section 8.4 on deployment strategies
\end{itemize}

\textbf{Chapter 10: A/B Testing}:
\begin{itemize}
    \item Monitor experiment metrics in real-time
    \item Detect guardrail violations
    \item Track sample ratio mismatch
    \item Reference: Section 10.3 on experiment monitoring
\end{itemize}

\textbf{Chapter 12: MLOps Automation}:
\begin{itemize}
    \item Integrate monitoring with CI/CD pipelines
    \item Automate retraining based on monitoring signals
    \item Implement monitoring-as-code
    \item Reference: Section 12.2 on automation workflows
\end{itemize}

\textbf{Chapter 13: Ethics \& Governance}:
\begin{itemize}
    \item Monitor fairness and bias continuously
    \item Track model decisions for audit trails
    \item Alert on governance violations
    \item Reference: Section 13.3 on fairness monitoring
\end{itemize}

\subsection{Recommended Tools and Resources}

\textbf{Metrics and Visualization}:
\begin{itemize}
    \item \textbf{Prometheus}: Open-source metrics collection and alerting
    \item \textbf{Grafana}: Visualization and dashboarding platform
    \item \textbf{Datadog}: Commercial unified observability platform
    \item \textbf{New Relic}: APM and infrastructure monitoring
\end{itemize}

\textbf{Distributed Tracing}:
\begin{itemize}
    \item \textbf{Jaeger}: Open-source distributed tracing
    \item \textbf{Zipkin}: Distributed tracing system
    \item \textbf{OpenTelemetry}: Vendor-neutral observability framework
\end{itemize}

\textbf{Logging}:
\begin{itemize}
    \item \textbf{ELK Stack}: Elasticsearch, Logstash, Kibana for log aggregation
    \item \textbf{Splunk}: Commercial log analysis platform
    \item \textbf{Loki}: Prometheus-inspired log aggregation
\end{itemize}

\textbf{ML-Specific Monitoring}:
\begin{itemize}
    \item \textbf{Evidently AI}: Open-source ML monitoring and drift detection
    \item \textbf{WhyLabs}: Data and ML monitoring platform
    \item \textbf{Arize AI}: ML observability platform
    \item \textbf{Fiddler AI}: Model monitoring and explainability
\end{itemize}

\textbf{Incident Management}:
\begin{itemize}
    \item \textbf{PagerDuty}: Incident response platform
    \item \textbf{Opsgenie}: Alert and on-call management
    \item \textbf{VictorOps}: Incident management and collaboration
\end{itemize}

\textbf{Essential Reading}:
\begin{itemize}
    \item \textit{Site Reliability Engineering} by Beyer et al. (Google SRE book)
    \item \textit{The Site Reliability Workbook} by Beyer et al.
    \item \textit{Observability Engineering} by Majors et al.
    \item \textit{Monitoring Machine Learning Models in Production} (Aporia blog series)
    \item \textit{MLOps: Continuous Delivery for ML} by Gift \& Deza
\end{itemize}

\textbf{Online Resources}:
\begin{itemize}
    \item Google SRE Blog: \url{https://sre.google/}
    \item Prometheus Documentation: \url{https://prometheus.io/docs/}
    \item OpenTelemetry Documentation: \url{https://opentelemetry.io/docs/}
    \item MLOps Community: \url{https://mlops.community/}
\end{itemize}

\subsection{Implementation Roadmap}

\textbf{Month 1-2: Foundation}:
\begin{enumerate}
    \item Deploy Prometheus and Grafana
    \item Instrument models with basic metrics (latency, error rate)
    \item Create initial dashboards
    \item Set up basic threshold alerts
    \item Implement structured logging
\end{enumerate}

\textbf{Month 3-4: Data Quality}:
\begin{enumerate}
    \item Implement drift detection for all features
    \item Add data validation rules
    \item Build data quality dashboard
    \item Set up drift alerts
    \item Track feature statistics
\end{enumerate}

\textbf{Month 5-6: Advanced Monitoring}:
\begin{enumerate}
    \item Deploy distributed tracing
    \item Implement anomaly detection
    \item Add business impact tracking
    \item Create role-specific dashboards
    \item Reduce alert noise by 50\%
\end{enumerate}

\textbf{Month 7-9: Incident Management}:
\begin{enumerate}
    \item Build incident management system
    \item Implement runbook automation
    \item Add automated root cause analysis
    \item Establish blameless post-mortem process
    \item Track incident metrics
\end{enumerate}

\textbf{Month 10-12: SRE Maturity}:
\begin{enumerate}
    \item Define SLIs and SLOs
    \item Implement error budget tracking
    \item Add predictive monitoring
    \item Deploy fairness monitoring
    \item Build cost optimization system
    \item Achieve Level 4 maturity
\end{enumerate}

Production ML monitoring is not a one-time implementation but a continuous journey. Organizations should regularly assess maturity, identify gaps, and invest in capabilities that provide the highest ROI for their specific context. The goal is transforming monitoring from a reactive necessity into a proactive competitive advantage that enables teams to deploy ML systems confidently at scale.
