\chapter{Ethics, Governance, and Interpretability}

\section{Introduction}

A hiring algorithm with 85\% accuracy seems successful—until analysis reveals it recommends male candidates 80\% of the time despite equal qualifications. A credit scoring model performs well on aggregate metrics but systematically denies loans to qualified applicants from specific zip codes. These are not edge cases—they are common failures when ML systems lack ethical guardrails, governance frameworks, and interpretability.

\subsection{The Ethics Crisis in ML}

Consider Amazon's recruiting tool, which learned to penalize resumes containing the word "women's" (as in "women's chess club") because historical hiring data showed gender bias. The system was trained on 10 years of male-dominated hiring decisions, encoding societal biases into algorithmic recommendations. The tool was scrapped after the bias was discovered, but not before it influenced hiring decisions.

\subsection{Why Ethics and Governance Matter}

ML systems make consequential decisions affecting people's lives:

\begin{itemize}
    \item \textbf{Hiring}: Algorithms screen resumes, predict performance, recommend candidates
    \item \textbf{Credit}: Models approve loans, set interest rates, determine credit limits
    \item \textbf{Healthcare}: Systems diagnose diseases, recommend treatments, allocate resources
    \item \textbf{Criminal Justice}: Algorithms predict recidivism, recommend sentences, allocate police resources
    \item \textbf{Education}: Systems recommend courses, predict success, allocate scholarships
\end{itemize}

These decisions require fairness, transparency, and accountability—properties that don't emerge from optimizing accuracy alone.

\subsection{The Cost of Unethical ML}

Industry evidence shows:
\begin{itemize}
    \item \textbf{80\% of organizations} deploy ML without systematic bias testing
    \item \textbf{Biased models} cost companies \$1M+ in legal settlements and reputation damage
    \item \textbf{Lack of interpretability} prevents 65\% of high-stakes ML applications from deployment
    \item \textbf{Regulatory fines} for non-compliance average \$2.7M (GDPR violations)
\end{itemize}

\subsection{Chapter Overview}

This chapter provides frameworks for responsible AI:

\begin{enumerate}
    \item \textbf{Fairness Evaluation}: Demographic parity, equalized odds, disparate impact
    \item \textbf{Model Interpretability}: SHAP values, feature importance, local explanations
    \item \textbf{Governance Systems}: Policy enforcement, compliance tracking
    \item \textbf{Ethics Review}: Structured review process for high-risk applications
    \item \textbf{Documentation}: Model cards with limitations and bias reporting
    \item \textbf{Audit Trails}: Regulatory compliance and accountability
    \item \textbf{GDPR/CCPA}: Privacy requirements and right to explanation
\end{enumerate}

\section{Fairness Evaluation}

Fairness metrics quantify whether a model treats different groups equitably.

\subsection{FairnessEvaluator: Comprehensive Bias Detection}

\begin{lstlisting}[language=Python, caption={Fairness Evaluation Framework}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import logging

logger = logging.getLogger(__name__)

class FairnessMetric(Enum):
    """Types of fairness metrics."""
    DEMOGRAPHIC_PARITY = "demographic_parity"
    EQUALIZED_ODDS = "equalized_odds"
    EQUAL_OPPORTUNITY = "equal_opportunity"
    DISPARATE_IMPACT = "disparate_impact"
    PREDICTIVE_PARITY = "predictive_parity"
    CALIBRATION = "calibration"

@dataclass
class FairnessResult:
    """
    Result of fairness evaluation.

    Attributes:
        metric_name: Name of fairness metric
        privileged_group: Identifier of privileged group
        unprivileged_group: Identifier of unprivileged group
        score: Fairness score
        threshold: Fairness threshold
        is_fair: Whether fairness criterion is met
        details: Additional details
    """
    metric_name: str
    privileged_group: str
    unprivileged_group: str
    score: float
    threshold: float
    is_fair: bool
    details: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'metric_name': self.metric_name,
            'privileged_group': self.privileged_group,
            'unprivileged_group': self.unprivileged_group,
            'score': self.score,
            'threshold': self.threshold,
            'is_fair': self.is_fair,
            'details': self.details
        }

class FairnessEvaluator:
    """
    Evaluate model fairness across protected attributes.

    Implements multiple fairness metrics to detect bias in predictions.

    Example:
        >>> evaluator = FairnessEvaluator()
        >>> results = evaluator.evaluate(
        ...     y_true=y_test,
        ...     y_pred=predictions,
        ...     y_prob=probabilities,
        ...     sensitive_features=data[['gender', 'race']],
        ...     metrics=[FairnessMetric.DEMOGRAPHIC_PARITY,
        ...              FairnessMetric.EQUALIZED_ODDS]
        ... )
        >>> for result in results:
        ...     if not result.is_fair:
        ...         print(f"Bias detected: {result.metric_name}")
    """

    def __init__(
        self,
        demographic_parity_threshold: float = 0.8,
        equalized_odds_threshold: float = 0.1,
        disparate_impact_threshold: float = 0.8
    ):
        """
        Initialize fairness evaluator.

        Args:
            demographic_parity_threshold: Min ratio for demographic parity
            equalized_odds_threshold: Max difference for equalized odds
            disparate_impact_threshold: Min ratio for disparate impact
        """
        self.demographic_parity_threshold = demographic_parity_threshold
        self.equalized_odds_threshold = equalized_odds_threshold
        self.disparate_impact_threshold = disparate_impact_threshold

        logger.info("Initialized FairnessEvaluator")

    def evaluate(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_prob: Optional[np.ndarray],
        sensitive_features: pd.DataFrame,
        metrics: Optional[List[FairnessMetric]] = None
    ) -> List[FairnessResult]:
        """
        Evaluate fairness across sensitive features.

        Args:
            y_true: True labels
            y_pred: Predicted labels
            y_prob: Predicted probabilities
            sensitive_features: DataFrame with protected attributes
            metrics: Fairness metrics to compute

        Returns:
            List of fairness results
        """
        if metrics is None:
            metrics = [
                FairnessMetric.DEMOGRAPHIC_PARITY,
                FairnessMetric.EQUALIZED_ODDS,
                FairnessMetric.DISPARATE_IMPACT
            ]

        results = []

        # Evaluate each sensitive feature
        for feature_name in sensitive_features.columns:
            feature_values = sensitive_features[feature_name]

            # Get unique groups
            groups = feature_values.unique()

            if len(groups) < 2:
                logger.warning(
                    f"Feature {feature_name} has < 2 groups, skipping"
                )
                continue

            # Compare each pair of groups
            for i in range(len(groups)):
                for j in range(i + 1, len(groups)):
                    group_a = groups[i]
                    group_b = groups[j]

                    # Get masks for each group
                    mask_a = feature_values == group_a
                    mask_b = feature_values == group_b

                    # Compute metrics for this pair
                    for metric in metrics:
                        result = self._compute_metric(
                            metric,
                            y_true,
                            y_pred,
                            y_prob,
                            mask_a,
                            mask_b,
                            f"{feature_name}={group_a}",
                            f"{feature_name}={group_b}"
                        )

                        results.append(result)

        # Log summary
        unfair = sum(1 for r in results if not r.is_fair)
        if unfair > 0:
            logger.warning(
                f"Fairness violations detected: {unfair}/{len(results)}"
            )
        else:
            logger.info("All fairness metrics passed")

        return results

    def _compute_metric(
        self,
        metric: FairnessMetric,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_prob: Optional[np.ndarray],
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Compute a specific fairness metric.

        Args:
            metric: Fairness metric to compute
            y_true: True labels
            y_pred: Predicted labels
            y_prob: Predicted probabilities
            mask_a: Boolean mask for group A
            mask_b: Boolean mask for group B
            group_a_name: Name of group A
            group_b_name: Name of group B

        Returns:
            Fairness result
        """
        if metric == FairnessMetric.DEMOGRAPHIC_PARITY:
            return self._demographic_parity(
                y_pred, mask_a, mask_b, group_a_name, group_b_name
            )
        elif metric == FairnessMetric.EQUALIZED_ODDS:
            return self._equalized_odds(
                y_true, y_pred, mask_a, mask_b, group_a_name, group_b_name
            )
        elif metric == FairnessMetric.EQUAL_OPPORTUNITY:
            return self._equal_opportunity(
                y_true, y_pred, mask_a, mask_b, group_a_name, group_b_name
            )
        elif metric == FairnessMetric.DISPARATE_IMPACT:
            return self._disparate_impact(
                y_pred, mask_a, mask_b, group_a_name, group_b_name
            )
        elif metric == FairnessMetric.PREDICTIVE_PARITY:
            return self._predictive_parity(
                y_true, y_pred, mask_a, mask_b, group_a_name, group_b_name
            )
        elif metric == FairnessMetric.CALIBRATION:
            if y_prob is None:
                raise ValueError("Calibration requires predicted probabilities")
            return self._calibration(
                y_true, y_prob, mask_a, mask_b, group_a_name, group_b_name
            )
        else:
            raise ValueError(f"Unknown metric: {metric}")

    def _demographic_parity(
        self,
        y_pred: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Demographic Parity: P(Y_hat = 1 | A) = P(Y_hat = 1 | B)

        Positive prediction rates should be equal across groups.
        """
        # Positive prediction rates
        rate_a = y_pred[mask_a].mean()
        rate_b = y_pred[mask_b].mean()

        # Ratio (smaller / larger)
        ratio = min(rate_a, rate_b) / max(rate_a, rate_b) if max(rate_a, rate_b) > 0 else 1.0

        is_fair = ratio >= self.demographic_parity_threshold

        return FairnessResult(
            metric_name="demographic_parity",
            privileged_group=group_a_name if rate_a > rate_b else group_b_name,
            unprivileged_group=group_b_name if rate_a > rate_b else group_a_name,
            score=ratio,
            threshold=self.demographic_parity_threshold,
            is_fair=is_fair,
            details={
                f'positive_rate_{group_a_name}': rate_a,
                f'positive_rate_{group_b_name}': rate_b,
                'difference': abs(rate_a - rate_b)
            }
        )

    def _equalized_odds(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Equalized Odds: TPR and FPR equal across groups.

        P(Y_hat = 1 | Y = y, A) = P(Y_hat = 1 | Y = y, B) for y in {0, 1}
        """
        # Compute TPR and FPR for each group
        def compute_rates(y_true_group, y_pred_group):
            cm = confusion_matrix(y_true_group, y_pred_group)
            tn, fp, fn, tp = cm.ravel()

            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0

            return tpr, fpr

        tpr_a, fpr_a = compute_rates(y_true[mask_a], y_pred[mask_a])
        tpr_b, fpr_b = compute_rates(y_true[mask_b], y_pred[mask_b])

        # Maximum difference in TPR and FPR
        tpr_diff = abs(tpr_a - tpr_b)
        fpr_diff = abs(fpr_a - fpr_b)
        max_diff = max(tpr_diff, fpr_diff)

        is_fair = max_diff <= self.equalized_odds_threshold

        return FairnessResult(
            metric_name="equalized_odds",
            privileged_group=group_a_name,
            unprivileged_group=group_b_name,
            score=max_diff,
            threshold=self.equalized_odds_threshold,
            is_fair=is_fair,
            details={
                f'tpr_{group_a_name}': tpr_a,
                f'tpr_{group_b_name}': tpr_b,
                f'fpr_{group_a_name}': fpr_a,
                f'fpr_{group_b_name}': fpr_b,
                'tpr_difference': tpr_diff,
                'fpr_difference': fpr_diff
            }
        )

    def _equal_opportunity(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Equal Opportunity: TPR equal across groups.

        P(Y_hat = 1 | Y = 1, A) = P(Y_hat = 1 | Y = 1, B)
        """
        # Compute TPR for each group
        def compute_tpr(y_true_group, y_pred_group):
            positives = y_true_group == 1
            if positives.sum() == 0:
                return 0

            return y_pred_group[positives].mean()

        tpr_a = compute_tpr(y_true[mask_a], y_pred[mask_a])
        tpr_b = compute_tpr(y_true[mask_b], y_pred[mask_b])

        diff = abs(tpr_a - tpr_b)
        is_fair = diff <= self.equalized_odds_threshold

        return FairnessResult(
            metric_name="equal_opportunity",
            privileged_group=group_a_name if tpr_a > tpr_b else group_b_name,
            unprivileged_group=group_b_name if tpr_a > tpr_b else group_a_name,
            score=diff,
            threshold=self.equalized_odds_threshold,
            is_fair=is_fair,
            details={
                f'tpr_{group_a_name}': tpr_a,
                f'tpr_{group_b_name}': tpr_b
            }
        )

    def _disparate_impact(
        self,
        y_pred: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Disparate Impact: Ratio of positive rates (80% rule).

        P(Y_hat = 1 | B) / P(Y_hat = 1 | A) >= 0.8
        """
        rate_a = y_pred[mask_a].mean()
        rate_b = y_pred[mask_b].mean()

        # Disparate impact ratio
        ratio = rate_b / rate_a if rate_a > 0 else 1.0

        is_fair = ratio >= self.disparate_impact_threshold

        return FairnessResult(
            metric_name="disparate_impact",
            privileged_group=group_a_name,
            unprivileged_group=group_b_name,
            score=ratio,
            threshold=self.disparate_impact_threshold,
            is_fair=is_fair,
            details={
                f'positive_rate_{group_a_name}': rate_a,
                f'positive_rate_{group_b_name}': rate_b
            }
        )

    def _predictive_parity(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Predictive Parity: PPV equal across groups.

        P(Y = 1 | Y_hat = 1, A) = P(Y = 1 | Y_hat = 1, B)
        """
        # Compute PPV (precision) for each group
        def compute_ppv(y_true_group, y_pred_group):
            predicted_positive = y_pred_group == 1
            if predicted_positive.sum() == 0:
                return 0

            return y_true_group[predicted_positive].mean()

        ppv_a = compute_ppv(y_true[mask_a], y_pred[mask_a])
        ppv_b = compute_ppv(y_true[mask_b], y_pred[mask_b])

        diff = abs(ppv_a - ppv_b)
        is_fair = diff <= self.equalized_odds_threshold

        return FairnessResult(
            metric_name="predictive_parity",
            privileged_group=group_a_name,
            unprivileged_group=group_b_name,
            score=diff,
            threshold=self.equalized_odds_threshold,
            is_fair=is_fair,
            details={
                f'ppv_{group_a_name}': ppv_a,
                f'ppv_{group_b_name}': ppv_b
            }
        )

    def _calibration(
        self,
        y_true: np.ndarray,
        y_prob: np.ndarray,
        mask_a: np.ndarray,
        mask_b: np.ndarray,
        group_a_name: str,
        group_b_name: str
    ) -> FairnessResult:
        """
        Calibration: Predicted probabilities match actual rates.

        P(Y = 1 | S = s, A) = s for all s
        """
        # Bin probabilities
        bins = np.linspace(0, 1, 11)

        def compute_calibration(y_true_group, y_prob_group):
            """Compute calibration error."""
            errors = []

            for i in range(len(bins) - 1):
                mask = (y_prob_group >= bins[i]) & (y_prob_group < bins[i + 1])

                if mask.sum() > 0:
                    predicted = y_prob_group[mask].mean()
                    actual = y_true_group[mask].mean()
                    errors.append(abs(predicted - actual))

            return np.mean(errors) if errors else 0.0

        calib_a = compute_calibration(y_true[mask_a], y_prob[mask_a])
        calib_b = compute_calibration(y_true[mask_b], y_prob[mask_b])

        diff = abs(calib_a - calib_b)
        is_fair = diff <= self.equalized_odds_threshold

        return FairnessResult(
            metric_name="calibration",
            privileged_group=group_a_name,
            unprivileged_group=group_b_name,
            score=diff,
            threshold=self.equalized_odds_threshold,
            is_fair=is_fair,
            details={
                f'calibration_error_{group_a_name}': calib_a,
                f'calibration_error_{group_b_name}': calib_b
            }
        )

    def generate_report(self, results: List[FairnessResult]) -> str:
        """
        Generate human-readable fairness report.

        Args:
            results: Fairness evaluation results

        Returns:
            Formatted report
        """
        lines = ["=" * 70]
        lines.append("FAIRNESS EVALUATION REPORT")
        lines.append("=" * 70)

        # Group by metric
        by_metric = {}
        for result in results:
            metric = result.metric_name
            if metric not in by_metric:
                by_metric[metric] = []
            by_metric[metric].append(result)

        for metric_name, metric_results in by_metric.items():
            lines.append(f"\n{metric_name.upper().replace('_', ' ')}")
            lines.append("-" * 70)

            for result in metric_results:
                status = "[PASS]" if result.is_fair else "[FAIL]"
                lines.append(
                    f"{status} | {result.privileged_group} vs "
                    f"{result.unprivileged_group}"
                )
                lines.append(
                    f"  Score: {result.score:.4f} "
                    f"(threshold: {result.threshold:.4f})"
                )

                if result.details:
                    for key, value in result.details.items():
                        if isinstance(value, float):
                            lines.append(f"  {key}: {value:.4f}")
                        else:
                            lines.append(f"  {key}: {value}")

        # Summary
        total = len(results)
        passed = sum(1 for r in results if r.is_fair)

        lines.append("\n" + "=" * 70)
        lines.append(f"SUMMARY: {passed}/{total} fairness checks passed")
        lines.append("=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\subsection{Fairness Evaluation in Practice}

\begin{lstlisting}[language=Python, caption={Using FairnessEvaluator}]
# Load test data with protected attributes
X_test = pd.read_parquet("test_features.parquet")
y_test = pd.read_parquet("test_labels.parquet")

# Sensitive features
sensitive_features = X_test[['gender', 'race', 'age_group']]

# Make predictions
model = load_model("credit_scoring_model.pkl")
y_pred = model.predict(X_test.drop(['gender', 'race', 'age_group'], axis=1))
y_prob = model.predict_proba(X_test.drop(['gender', 'race', 'age_group'], axis=1))[:, 1]

# Initialize evaluator
evaluator = FairnessEvaluator(
    demographic_parity_threshold=0.8,  # 80% rule
    equalized_odds_threshold=0.1,      # Max 10% difference
    disparate_impact_threshold=0.8
)

# Evaluate fairness
results = evaluator.evaluate(
    y_true=y_test,
    y_pred=y_pred,
    y_prob=y_prob,
    sensitive_features=sensitive_features,
    metrics=[
        FairnessMetric.DEMOGRAPHIC_PARITY,
        FairnessMetric.EQUALIZED_ODDS,
        FairnessMetric.EQUAL_OPPORTUNITY,
        FairnessMetric.DISPARATE_IMPACT
    ]
)

# Generate report
report = evaluator.generate_report(results)
print(report)

# Check for violations
violations = [r for r in results if not r.is_fair]

if violations:
    logger.error(f"Fairness violations detected: {len(violations)}")

    for violation in violations:
        logger.error(
            f"  {violation.metric_name}: "
            f"{violation.privileged_group} vs {violation.unprivileged_group} "
            f"(score={violation.score:.3f})"
        )

    # Do not deploy model with fairness violations
    raise ValueError("Model fails fairness requirements")
else:
    logger.info("All fairness checks passed - model approved")
\end{lstlisting}

\subsection{Intersectional Fairness Analysis}

Single-attribute fairness metrics can miss discrimination affecting intersectional groups (e.g., Black women experience different biases than Black men or white women). Intersectional fairness analyzes all combinations of protected attributes.

\begin{lstlisting}[language=Python, caption={Intersectional Fairness Framework}]
from itertools import combinations
from typing import Dict, List, Set, Tuple, Optional, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass, field
import logging

logger = logging.getLogger(__name__)

@dataclass
class IntersectionalGroup:
    """
    Represents an intersectional group defined by multiple attributes.

    Attributes:
        attributes: Dictionary of attribute names to values
        size: Number of samples in this group
        positive_rate: Rate of positive predictions
        accuracy: Accuracy for this group
        false_positive_rate: FPR for this group
        false_negative_rate: FNR for this group
    """
    attributes: Dict[str, Any]
    size: int
    positive_rate: float
    accuracy: Optional[float] = None
    false_positive_rate: Optional[float] = None
    false_negative_rate: Optional[float] = None

    def group_name(self) -> str:
        """Generate human-readable group name."""
        return " & ".join(f"{k}={v}" for k, v in sorted(self.attributes.items()))

@dataclass
class IntersectionalAnalysisResult:
    """
    Result of intersectional fairness analysis.

    Attributes:
        groups: List of all intersectional groups analyzed
        max_disparity: Maximum disparity found across groups
        disparate_groups: Pairs of groups with significant disparities
        warning_threshold: Threshold for flagging disparities
        metrics_analyzed: List of metrics included in analysis
    """
    groups: List[IntersectionalGroup]
    max_disparity: Dict[str, float]
    disparate_groups: List[Tuple[str, str, str, float]]
    warning_threshold: float
    metrics_analyzed: List[str]

class IntersectionalFairnessAnalyzer:
    """
    Analyze fairness across intersections of protected attributes.

    This addresses the limitation of single-attribute fairness metrics,
    which can satisfy group fairness while still discriminating against
    intersectional subgroups.

    Example: A hiring model might satisfy gender parity (50% male, 50% female)
    and race parity (60% white, 40% Black) but still discriminate against
    Black women specifically.
    """

    def __init__(
        self,
        min_group_size: int = 30,
        disparity_threshold: float = 0.2
    ):
        """
        Initialize intersectional analyzer.

        Args:
            min_group_size: Minimum samples required to analyze a group
            disparity_threshold: Maximum acceptable disparity between groups
        """
        self.min_group_size = min_group_size
        self.disparity_threshold = disparity_threshold

    def analyze(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        sensitive_features: pd.DataFrame,
        max_intersections: int = 3
    ) -> IntersectionalAnalysisResult:
        """
        Analyze fairness across intersectional groups.

        Args:
            y_true: True labels
            y_pred: Predicted labels
            sensitive_features: DataFrame of protected attributes
            max_intersections: Maximum number of attributes to combine

        Returns:
            Comprehensive intersectional analysis
        """
        logger.info(
            f"Starting intersectional analysis with {len(sensitive_features.columns)} "
            f"attributes and max {max_intersections} intersections"
        )

        groups = self._identify_groups(
            y_true, y_pred, sensitive_features, max_intersections
        )

        # Compute disparities
        max_disparity = {}
        disparate_groups = []

        metrics = ['positive_rate', 'accuracy', 'false_positive_rate', 'false_negative_rate']

        for metric in metrics:
            metric_values = [
                getattr(g, metric) for g in groups
                if getattr(g, metric) is not None
            ]

            if len(metric_values) >= 2:
                max_val = max(metric_values)
                min_val = min(metric_values)
                disparity = max_val - min_val
                max_disparity[metric] = disparity

                # Find pairs with large disparities
                for i, g1 in enumerate(groups):
                    v1 = getattr(g1, metric)
                    if v1 is None:
                        continue

                    for g2 in groups[i+1:]:
                        v2 = getattr(g2, metric)
                        if v2 is None:
                            continue

                        diff = abs(v1 - v2)
                        if diff >= self.disparity_threshold:
                            disparate_groups.append((
                                g1.group_name(),
                                g2.group_name(),
                                metric,
                                diff
                            ))

        logger.info(f"Found {len(groups)} intersectional groups")
        logger.info(f"Identified {len(disparate_groups)} disparate pairs")

        return IntersectionalAnalysisResult(
            groups=groups,
            max_disparity=max_disparity,
            disparate_groups=disparate_groups,
            warning_threshold=self.disparity_threshold,
            metrics_analyzed=metrics
        )

    def _identify_groups(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        sensitive_features: pd.DataFrame,
        max_intersections: int
    ) -> List[IntersectionalGroup]:
        """Identify all intersectional groups meeting minimum size."""
        groups = []

        # Generate all combinations of attributes
        attributes = list(sensitive_features.columns)

        for r in range(1, min(max_intersections, len(attributes)) + 1):
            for attr_combo in combinations(attributes, r):
                # Get unique value combinations for these attributes
                grouped = sensitive_features[list(attr_combo)].groupby(
                    list(attr_combo)
                ).size()

                for values, size in grouped.items():
                    if size < self.min_group_size:
                        continue

                    # Create mask for this group
                    mask = pd.Series([True] * len(sensitive_features))
                    attr_dict = {}

                    if r == 1:
                        mask = sensitive_features[attr_combo[0]] == values
                        attr_dict[attr_combo[0]] = values
                    else:
                        for attr, val in zip(attr_combo, values):
                            mask &= sensitive_features[attr] == val
                            attr_dict[attr] = val

                    mask = mask.values

                    # Compute metrics for this group
                    group = self._compute_group_metrics(
                        y_true[mask],
                        y_pred[mask],
                        attr_dict,
                        int(size)
                    )

                    groups.append(group)

        return groups

    def _compute_group_metrics(
        self,
        y_true_group: np.ndarray,
        y_pred_group: np.ndarray,
        attributes: Dict[str, Any],
        size: int
    ) -> IntersectionalGroup:
        """Compute fairness metrics for a specific group."""
        positive_rate = y_pred_group.mean()
        accuracy = (y_true_group == y_pred_group).mean()

        # Compute FPR and FNR if we have positive and negative examples
        tn = ((y_true_group == 0) & (y_pred_group == 0)).sum()
        fp = ((y_true_group == 0) & (y_pred_group == 1)).sum()
        fn = ((y_true_group == 1) & (y_pred_group == 0)).sum()
        tp = ((y_true_group == 1) & (y_pred_group == 1)).sum()

        fpr = fp / (fp + tn) if (fp + tn) > 0 else None
        fnr = fn / (fn + tp) if (fn + tp) > 0 else None

        return IntersectionalGroup(
            attributes=attributes,
            size=size,
            positive_rate=positive_rate,
            accuracy=accuracy,
            false_positive_rate=fpr,
            false_negative_rate=fnr
        )

    def generate_report(self, result: IntersectionalAnalysisResult) -> str:
        """Generate human-readable intersectional analysis report."""
        lines = ["=" * 80]
        lines.append("INTERSECTIONAL FAIRNESS ANALYSIS")
        lines.append("=" * 80)
        lines.append(f"\nAnalyzed {len(result.groups)} intersectional groups")
        lines.append(f"Warning threshold: {result.warning_threshold:.2f}")

        # Maximum disparities
        lines.append("\nMAXIMUM DISPARITIES ACROSS ALL GROUPS:")
        for metric, disparity in result.max_disparity.items():
            status = "FAIL" if disparity >= result.warning_threshold else "PASS"
            lines.append(f"  {metric}: {disparity:.4f} [{status}]")

        # Disparate group pairs
        if result.disparate_groups:
            lines.append(f"\nDISPARATE GROUP PAIRS ({len(result.disparate_groups)} found):")

            for group1, group2, metric, diff in sorted(
                result.disparate_groups, key=lambda x: x[3], reverse=True
            )[:20]:  # Show top 20
                lines.append(f"\n  {group1}")
                lines.append(f"  vs {group2}")
                lines.append(f"  {metric} disparity: {diff:.4f}")

        # Group-level details
        lines.append(f"\nGROUP-LEVEL METRICS ({len(result.groups)} groups):")

        for group in sorted(result.groups, key=lambda g: g.size, reverse=True)[:15]:
            lines.append(f"\n  {group.group_name()} (n={group.size}):")
            lines.append(f"    Positive rate: {group.positive_rate:.4f}")
            if group.accuracy is not None:
                lines.append(f"    Accuracy: {group.accuracy:.4f}")
            if group.false_positive_rate is not None:
                lines.append(f"    FPR: {group.false_positive_rate:.4f}")
            if group.false_negative_rate is not None:
                lines.append(f"    FNR: {group.false_negative_rate:.4f}")

        lines.append("\n" + "=" * 80)

        return "\n".join(lines)
\end{lstlisting}

\subsection{Individual Fairness Framework}

While group fairness ensures equal treatment across demographic groups, individual fairness ensures similar individuals receive similar predictions, regardless of protected attributes. This is formalized through Lipschitz continuity constraints.

\begin{lstlisting}[language=Python, caption={Individual Fairness with Lipschitz Constraints}]
from typing import Callable, Dict, List, Tuple, Optional, Any
import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform, cosine, euclidean
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class IndividualFairnessResult:
    """
    Result of individual fairness evaluation.

    Attributes:
        lipschitz_constant: Estimated Lipschitz constant
        max_violation: Maximum Lipschitz violation found
        violation_rate: Percentage of pairs violating constraint
        similar_pairs_checked: Number of similar pairs analyzed
        fairness_threshold: Maximum acceptable Lipschitz constant
        is_fair: Whether individual fairness constraint is satisfied
    """
    lipschitz_constant: float
    max_violation: float
    violation_rate: float
    similar_pairs_checked: int
    fairness_threshold: float
    is_fair: bool
    violation_examples: List[Tuple[int, int, float, float]] = None

class IndividualFairnessFramework:
    """
    Evaluate and enforce individual fairness using Lipschitz constraints.

    Individual Fairness (Dwork et al., 2012):
    "Similar individuals should receive similar predictions"

    Formally, a model f satisfies L-Lipschitz fairness if:
        d_Y(f(x_1), f(x_2)) <= L * d_X(x_1, x_2)

    where:
    - d_X is a distance metric on input space
    - d_Y is a distance metric on output space
    - L is the Lipschitz constant (smaller is fairer)

    Example: In credit scoring, two applicants with similar financial profiles
    should receive similar credit scores, regardless of race or gender.
    """

    def __init__(
        self,
        fairness_threshold: float = 1.5,
        similarity_threshold: float = 0.1,
        distance_metric: str = 'euclidean'
    ):
        """
        Initialize individual fairness framework.

        Args:
            fairness_threshold: Maximum acceptable Lipschitz constant
            similarity_threshold: Threshold for considering instances "similar"
            distance_metric: Distance metric for input space ('euclidean', 'cosine')
        """
        self.fairness_threshold = fairness_threshold
        self.similarity_threshold = similarity_threshold
        self.distance_metric = distance_metric

    def evaluate(
        self,
        X: np.ndarray,
        y_pred: np.ndarray,
        protected_indices: Optional[List[int]] = None,
        max_pairs: int = 10000
    ) -> IndividualFairnessResult:
        """
        Evaluate individual fairness using Lipschitz constant estimation.

        Args:
            X: Feature matrix
            y_pred: Model predictions (continuous or probabilities)
            protected_indices: Column indices of protected attributes to exclude
            max_pairs: Maximum number of pairs to check (for computational efficiency)

        Returns:
            Individual fairness evaluation result
        """
        logger.info(f"Evaluating individual fairness for {len(X)} instances")

        # Remove protected attributes from similarity computation
        X_fair = X.copy()
        if protected_indices:
            X_fair = np.delete(X_fair, protected_indices, axis=1)

        # Normalize features
        X_fair = (X_fair - X_fair.mean(axis=0)) / (X_fair.std(axis=0) + 1e-8)

        # Find similar pairs
        similar_pairs = self._find_similar_pairs(X_fair, max_pairs)

        if len(similar_pairs) == 0:
            logger.warning("No similar pairs found - cannot evaluate individual fairness")
            return IndividualFairnessResult(
                lipschitz_constant=np.inf,
                max_violation=np.inf,
                violation_rate=1.0,
                similar_pairs_checked=0,
                fairness_threshold=self.fairness_threshold,
                is_fair=False
            )

        # Compute Lipschitz constant for each pair
        lipschitz_constants = []
        violations = []
        violation_examples = []

        for i, j, input_dist in similar_pairs:
            output_dist = abs(y_pred[i] - y_pred[j])

            # Lipschitz constant for this pair
            if input_dist > 1e-8:
                L_ij = output_dist / input_dist
                lipschitz_constants.append(L_ij)

                if L_ij > self.fairness_threshold:
                    violations.append(L_ij)
                    violation_examples.append((i, j, input_dist, output_dist))

        # Overall statistics
        lipschitz_constant = np.max(lipschitz_constants)
        max_violation = max(violations) if violations else 0.0
        violation_rate = len(violations) / len(similar_pairs)
        is_fair = lipschitz_constant <= self.fairness_threshold

        logger.info(
            f"Lipschitz constant: {lipschitz_constant:.4f} "
            f"(threshold: {self.fairness_threshold})"
        )
        logger.info(f"Violation rate: {violation_rate:.2%}")

        return IndividualFairnessResult(
            lipschitz_constant=lipschitz_constant,
            max_violation=max_violation,
            violation_rate=violation_rate,
            similar_pairs_checked=len(similar_pairs),
            fairness_threshold=self.fairness_threshold,
            is_fair=is_fair,
            violation_examples=violation_examples[:10]  # Store top 10
        )

    def _find_similar_pairs(
        self,
        X: np.ndarray,
        max_pairs: int
    ) -> List[Tuple[int, int, float]]:
        """
        Find pairs of instances within similarity threshold.

        Returns:
            List of (index1, index2, distance) tuples
        """
        n = len(X)

        # For efficiency, sample if dataset is large
        if n > 1000:
            sample_size = min(1000, n)
            indices = np.random.choice(n, sample_size, replace=False)
            X_sample = X[indices]
        else:
            indices = np.arange(n)
            X_sample = X

        # Compute pairwise distances
        if self.distance_metric == 'euclidean':
            distances = squareform(pdist(X_sample, metric='euclidean'))
        elif self.distance_metric == 'cosine':
            distances = squareform(pdist(X_sample, metric='cosine'))
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

        # Find pairs within similarity threshold
        similar_pairs = []

        for i in range(len(X_sample)):
            for j in range(i + 1, len(X_sample)):
                dist = distances[i, j]

                if dist <= self.similarity_threshold:
                    similar_pairs.append((indices[i], indices[j], dist))

                if len(similar_pairs) >= max_pairs:
                    return similar_pairs

        return similar_pairs

    def learn_similarity_metric(
        self,
        X: np.ndarray,
        y: np.ndarray,
        protected_indices: List[int]
    ) -> np.ndarray:
        """
        Learn a similarity metric that respects fairness constraints.

        Uses metric learning to find a distance function that:
        1. Preserves predictive accuracy (similar y => similar X)
        2. Ignores protected attributes
        3. Satisfies Lipschitz fairness constraints

        Args:
            X: Feature matrix
            y: True labels
            protected_indices: Indices of protected attributes

        Returns:
            Learned metric matrix M such that d(x1, x2) = sqrt((x1-x2)^T M (x1-x2))
        """
        logger.info("Learning fairness-aware similarity metric")

        # Simple approach: Learn weights that predict y while minimizing
        # correlation with protected attributes

        from sklearn.linear_model import Ridge
        from sklearn.preprocessing import StandardScaler

        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Train model to predict y from non-protected features
        non_protected = [i for i in range(X.shape[1]) if i not in protected_indices]

        model = Ridge(alpha=1.0)
        model.fit(X_scaled[:, non_protected], y)

        # Use model coefficients as feature weights
        weights = np.zeros(X.shape[1])
        weights[non_protected] = np.abs(model.coef_)

        # Zero out protected attributes
        weights[protected_indices] = 0

        # Create diagonal metric matrix
        M = np.diag(weights / (weights.sum() + 1e-8))

        logger.info("Learned metric with {:.2f}% weight on non-protected features".format(
            100 * weights[non_protected].sum() / (weights.sum() + 1e-8)
        ))

        return M

    def generate_report(self, result: IndividualFairnessResult) -> str:
        """Generate human-readable individual fairness report."""
        lines = ["=" * 70]
        lines.append("INDIVIDUAL FAIRNESS EVALUATION")
        lines.append("=" * 70)

        status = "PASS" if result.is_fair else "FAIL"
        lines.append(f"\nOverall Status: {status}")
        lines.append(f"Lipschitz Constant: {result.lipschitz_constant:.4f}")
        lines.append(f"Fairness Threshold: {result.fairness_threshold:.4f}")
        lines.append(f"Violation Rate: {result.violation_rate:.2%}")
        lines.append(f"Similar Pairs Checked: {result.similar_pairs_checked}")

        if result.violation_examples:
            lines.append(f"\nTOP VIOLATIONS (showing up to 10):")
            for idx1, idx2, input_dist, output_dist in result.violation_examples:
                L = output_dist / input_dist if input_dist > 0 else np.inf
                lines.append(
                    f"  Instances {idx1} & {idx2}: "
                    f"input_dist={input_dist:.4f}, output_dist={output_dist:.4f}, "
                    f"L={L:.4f}"
                )

        lines.append("\n" + "=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\subsection{Using Intersectional and Individual Fairness}

\begin{lstlisting}[language=Python, caption={Comprehensive Fairness Analysis}]
# Load data
X_test = pd.read_parquet("test_features.parquet")
y_test = pd.read_parquet("test_labels.parquet").values
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Define protected attributes
protected_attrs = ['gender', 'race', 'age_group']
sensitive_features = X_test[protected_attrs]

# 1. Standard group fairness
evaluator = FairnessEvaluator()
group_results = evaluator.evaluate(
    y_true=y_test,
    y_pred=y_pred,
    y_prob=y_prob,
    sensitive_features=sensitive_features
)
print(evaluator.generate_report(group_results))

# 2. Intersectional fairness
intersectional_analyzer = IntersectionalFairnessAnalyzer(
    min_group_size=30,
    disparity_threshold=0.2
)

intersectional_results = intersectional_analyzer.analyze(
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=sensitive_features,
    max_intersections=3  # Analyze up to 3-way intersections
)

print(intersectional_analyzer.generate_report(intersectional_results))

# Check for intersectional disparities
if intersectional_results.disparate_groups:
    logger.warning(
        f"Found {len(intersectional_results.disparate_groups)} "
        f"disparate intersectional group pairs"
    )

    # Example: Black women may face unique discrimination
    # not captured by analyzing race and gender separately

# 3. Individual fairness
X_features = X_test.drop(columns=protected_attrs).values
protected_indices = [X_test.columns.get_loc(attr) for attr in protected_attrs]

individual_framework = IndividualFairnessFramework(
    fairness_threshold=1.5,  # Max acceptable Lipschitz constant
    similarity_threshold=0.1  # Distance threshold for "similar"
)

individual_results = individual_framework.evaluate(
    X=X_test.values,
    y_pred=y_prob,  # Use probabilities for continuous output
    protected_indices=protected_indices,
    max_pairs=10000
)

print(individual_framework.generate_report(individual_results))

# Learn fairness-aware similarity metric
if not individual_results.is_fair:
    logger.info("Learning fairness-aware similarity metric")

    metric_matrix = individual_framework.learn_similarity_metric(
        X=X_test.values,
        y=y_test,
        protected_indices=protected_indices
    )

    # Re-evaluate with learned metric
    # (implementation would use custom distance with metric_matrix)

# Combined decision
all_fair = (
    all(r.is_fair for r in group_results) and
    len(intersectional_results.disparate_groups) == 0 and
    individual_results.is_fair
)

if not all_fair:
    logger.error("Model fails comprehensive fairness evaluation")
    logger.error("Consider: re-sampling, re-weighting, or fairness constraints")
    raise ValueError("Deploy blocked due to fairness violations")
else:
    logger.info("Model passes all fairness checks - approved for deployment")
\end{lstlisting}

\section{Model Interpretability}

Interpretability enables understanding why models make specific predictions.

\subsection{ModelExplainer: SHAP and Feature Importance}

\begin{lstlisting}[language=Python, caption={Comprehensive Model Explanation Framework}]
from typing import Dict, List, Optional, Any
import numpy as np
import pandas as pd
import shap
from sklearn.inspection import permutation_importance
import logging

logger = logging.getLogger(__name__)

class ModelExplainer:
    """
    Explain model predictions using multiple methods.

    Provides global feature importance and local explanations (SHAP).

    Example:
        >>> explainer = ModelExplainer(model, X_train)
        >>> # Global explanation
        >>> importance = explainer.feature_importance(X_test)
        >>> # Local explanation
        >>> explanation = explainer.explain_instance(X_test.iloc[0])
    """

    def __init__(
        self,
        model: Any,
        background_data: pd.DataFrame,
        feature_names: Optional[List[str]] = None
    ):
        """
        Initialize explainer.

        Args:
            model: Trained model to explain
            background_data: Background dataset for SHAP
            feature_names: Feature names (inferred if None)
        """
        self.model = model
        self.background_data = background_data
        self.feature_names = feature_names or list(background_data.columns)

        # Initialize SHAP explainer
        try:
            # Try tree explainer first (faster for tree models)
            self.shap_explainer = shap.TreeExplainer(model)
            logger.info("Using TreeExplainer")
        except Exception:
            # Fall back to kernel explainer (model-agnostic)
            # Use sample of background data for efficiency
            sample_size = min(100, len(background_data))
            background_sample = background_data.sample(sample_size)

            self.shap_explainer = shap.KernelExplainer(
                model.predict_proba
                if hasattr(model, 'predict_proba')
                else model.predict,
                background_sample
            )
            logger.info("Using KernelExplainer")

        logger.info("Initialized ModelExplainer")

    def feature_importance(
        self,
        X: pd.DataFrame,
        y: Optional[np.ndarray] = None,
        method: str = "shap"
    ) -> pd.DataFrame:
        """
        Compute global feature importance.

        Args:
            X: Feature data
            y: True labels (required for permutation importance)
            method: "shap", "permutation", or "built_in"

        Returns:
            DataFrame with feature importances
        """
        if method == "shap":
            importance = self._shap_importance(X)
        elif method == "permutation":
            if y is None:
                raise ValueError(
                    "Permutation importance requires labels"
                )
            importance = self._permutation_importance(X, y)
        elif method == "built_in":
            importance = self._builtin_importance()
        else:
            raise ValueError(f"Unknown method: {method}")

        # Sort by importance
        importance = importance.sort_values(
            'importance',
            ascending=False
        )

        return importance

    def _shap_importance(self, X: pd.DataFrame) -> pd.DataFrame:
        """Compute SHAP-based feature importance."""
        # Compute SHAP values
        shap_values = self.shap_explainer.shap_values(X)

        # Handle multi-class (take values for positive class)
        if isinstance(shap_values, list):
            shap_values = shap_values[1]

        # Mean absolute SHAP value per feature
        importance = np.abs(shap_values).mean(axis=0)

        return pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        })

    def _permutation_importance(
        self,
        X: pd.DataFrame,
        y: np.ndarray
    ) -> pd.DataFrame:
        """Compute permutation-based importance."""
        result = permutation_importance(
            self.model,
            X,
            y,
            n_repeats=10,
            random_state=42
        )

        return pd.DataFrame({
            'feature': self.feature_names,
            'importance': result.importances_mean,
            'std': result.importances_std
        })

    def _builtin_importance(self) -> pd.DataFrame:
        """Use model's built-in feature importance."""
        if hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
        elif hasattr(self.model, 'coef_'):
            # For linear models, use absolute coefficients
            importance = np.abs(self.model.coef_).flatten()
        else:
            raise ValueError(
                "Model does not have built-in feature importance"
            )

        return pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        })

    def explain_instance(
        self,
        instance: pd.Series,
        num_features: int = 10
    ) -> Dict[str, Any]:
        """
        Explain a single prediction.

        Args:
            instance: Single instance to explain
            num_features: Number of top features to include

        Returns:
            Explanation dictionary
        """
        # Convert to 2D array
        X = instance.values.reshape(1, -1)

        # Compute SHAP values
        shap_values = self.shap_explainer.shap_values(X)

        # Handle multi-class
        if isinstance(shap_values, list):
            shap_values = shap_values[1]

        # Get top features
        shap_values = shap_values.flatten()
        indices = np.argsort(np.abs(shap_values))[::-1][:num_features]

        # Build explanation
        explanation = {
            'prediction': self.model.predict(X)[0],
            'features': []
        }

        if hasattr(self.model, 'predict_proba'):
            explanation['probability'] = self.model.predict_proba(X)[0, 1]

        for idx in indices:
            feature_name = self.feature_names[idx]
            feature_value = instance.iloc[idx]
            shap_value = shap_values[idx]

            explanation['features'].append({
                'name': feature_name,
                'value': feature_value,
                'shap_value': shap_value,
                'contribution': 'positive' if shap_value > 0 else 'negative'
            })

        return explanation

    def explain_batch(
        self,
        X: pd.DataFrame,
        sample_size: Optional[int] = None
    ) -> np.ndarray:
        """
        Compute SHAP values for a batch of instances.

        Args:
            X: Feature data
            sample_size: Sample size for efficiency

        Returns:
            SHAP values array
        """
        if sample_size and len(X) > sample_size:
            X = X.sample(sample_size)

        shap_values = self.shap_explainer.shap_values(X)

        # Handle multi-class
        if isinstance(shap_values, list):
            shap_values = shap_values[1]

        return shap_values

    def generate_explanation_text(
        self,
        explanation: Dict[str, Any]
    ) -> str:
        """
        Generate human-readable explanation.

        Args:
            explanation: Explanation dictionary

        Returns:
            Natural language explanation
        """
        prediction = explanation['prediction']
        probability = explanation.get('probability', None)

        lines = []

        if probability is not None:
            lines.append(
                f"Prediction: {prediction} (confidence: {probability:.1%})"
            )
        else:
            lines.append(f"Prediction: {prediction}")

        lines.append("\nTop contributing features:")

        for i, feature in enumerate(explanation['features'][:5], 1):
            direction = "increased" if feature['contribution'] == 'positive' else "decreased"
            lines.append(
                f"{i}. {feature['name']} = {feature['value']:.3f} "
                f"({direction} score by {abs(feature['shap_value']):.3f})"
            )

        return "\n".join(lines)
\end{lstlisting}

\subsection{Explanation Usage}

\begin{lstlisting}[language=Python, caption={Model Interpretation}]
# Initialize explainer
explainer = ModelExplainer(
    model=credit_model,
    background_data=X_train,
    feature_names=feature_names
)

# Global feature importance
print("Computing global feature importance...")
importance_df = explainer.feature_importance(X_test, method="shap")

print("\nTop 10 Most Important Features:")
print(importance_df.head(10))

# Visualize importance
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
top_features = importance_df.head(15)
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel('Mean |SHAP Value|')
plt.title('Feature Importance')
plt.tight_layout()
plt.savefig('feature_importance.png')

# Explain individual predictions
print("\n" + "="*60)
print("INDIVIDUAL PREDICTION EXPLANATION")
print("="*60)

# Get a denied application
denied_idx = y_pred[y_pred == 0].index[0]
instance = X_test.loc[denied_idx]

explanation = explainer.explain_instance(instance, num_features=10)

# Generate text explanation
explanation_text = explainer.generate_explanation_text(explanation)
print(explanation_text)

# For regulatory compliance, store explanation
explanation_record = {
    'application_id': denied_idx,
    'timestamp': datetime.now().isoformat(),
    'prediction': explanation['prediction'],
    'probability': explanation.get('probability'),
    'explanation': explanation['features']
}

# Save for audit trail
with open(f'explanations/{denied_idx}.json', 'w') as f:
    json.dump(explanation_record, f, indent=2)
\end{lstlisting}

\subsection{Advanced Interpretability Methods}

While SHAP provides powerful model-agnostic explanations, additional interpretability methods offer complementary insights and stability guarantees.

\subsubsection{LIME with Stability Analysis}

LIME (Local Interpretable Model-agnostic Explanations) can produce unstable explanations due to random sampling. We add stability analysis to ensure reliable explanations.

\begin{lstlisting}[language=Python, caption={LIME with Stability Analysis}]
from lime import lime_tabular
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class StableLIMEResult:
    """
    Result of stable LIME explanation.

    Attributes:
        explanation: LIME explanation object
        feature_weights: Average feature weights across runs
        stability_score: Spearman correlation of feature rankings (0-1)
        confidence_intervals: 95% CI for each feature weight
        is_stable: Whether explanation is stable (correlation > 0.7)
    """
    explanation: Any
    feature_weights: Dict[str, float]
    stability_score: float
    confidence_intervals: Dict[str, Tuple[float, float]]
    is_stable: bool

class StableLIMEExplainer:
    """
    LIME explainer with stability analysis.

    Standard LIME can produce inconsistent explanations due to random
    sampling of the local neighborhood. This class runs LIME multiple
    times and measures stability via rank correlation.

    Stable explanations are more trustworthy for high-stakes decisions.
    """

    def __init__(
        self,
        model: Any,
        training_data: np.ndarray,
        feature_names: List[str],
        n_runs: int = 10,
        stability_threshold: float = 0.7
    ):
        """
        Initialize stable LIME explainer.

        Args:
            model: Trained model to explain
            training_data: Training data for sampling distribution
            feature_names: Feature names
            n_runs: Number of LIME runs for stability estimation
            stability_threshold: Minimum correlation for stable explanation
        """
        self.model = model
        self.feature_names = feature_names
        self.n_runs = n_runs
        self.stability_threshold = stability_threshold

        # Initialize LIME explainer
        self.lime_explainer = lime_tabular.LimeTabularExplainer(
            training_data=training_data,
            feature_names=feature_names,
            mode='classification',
            random_state=42
        )

        logger.info(
            f"Initialized StableLIMEExplainer with {n_runs} runs, "
            f"stability threshold: {stability_threshold}"
        )

    def explain_instance(
        self,
        instance: np.ndarray,
        num_features: int = 10
    ) -> StableLIMEResult:
        """
        Generate stable LIME explanation for an instance.

        Runs LIME multiple times and computes:
        1. Average feature weights
        2. Stability score (Spearman correlation of rankings)
        3. Confidence intervals
        4. Stability flag

        Args:
            instance: Instance to explain
            num_features: Number of top features to include

        Returns:
            Stable LIME result with stability metrics
        """
        logger.info(f"Generating stable LIME explanation ({self.n_runs} runs)")

        # Run LIME multiple times
        explanations = []
        feature_weights_list = []
        rankings_list = []

        for run in range(self.n_runs):
            # Generate explanation with different random seed
            exp = self.lime_explainer.explain_instance(
                instance,
                self.model.predict_proba,
                num_features=num_features,
                num_samples=5000  # Large sample for stability
            )

            explanations.append(exp)

            # Extract feature weights
            weights = dict(exp.as_list())
            feature_weights_list.append(weights)

            # Extract feature ranking (by absolute weight)
            ranking = sorted(
                weights.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )
            rankings_list.append([f for f, _ in ranking])

        # Compute average weights
        all_features = set()
        for weights in feature_weights_list:
            all_features.update(weights.keys())

        avg_weights = {}
        ci_lower = {}
        ci_upper = {}

        for feature in all_features:
            values = [
                weights.get(feature, 0.0)
                for weights in feature_weights_list
            ]

            avg_weights[feature] = np.mean(values)

            # 95% confidence interval
            std = np.std(values)
            ci_lower[feature] = avg_weights[feature] - 1.96 * std
            ci_upper[feature] = avg_weights[feature] + 1.96 * std

        # Compute stability score (Spearman correlation of rankings)
        stability_scores = []

        for i in range(len(rankings_list)):
            for j in range(i + 1, len(rankings_list)):
                # Map rankings to numeric ranks
                rank_i = {f: r for r, f in enumerate(rankings_list[i])}
                rank_j = {f: r for r, f in enumerate(rankings_list[j])}

                # Common features
                common = set(rank_i.keys()) & set(rank_j.keys())

                if len(common) >= 2:
                    ranks_i = [rank_i[f] for f in common]
                    ranks_j = [rank_j[f] for f in common]

                    corr, _ = spearmanr(ranks_i, ranks_j)
                    stability_scores.append(corr)

        avg_stability = np.mean(stability_scores) if stability_scores else 0.0
        is_stable = avg_stability >= self.stability_threshold

        if not is_stable:
            logger.warning(
                f"Unstable explanation: stability score = {avg_stability:.3f} "
                f"(threshold: {self.stability_threshold})"
            )

        confidence_intervals = {
            feature: (ci_lower[feature], ci_upper[feature])
            for feature in avg_weights.keys()
        }

        return StableLIMEResult(
            explanation=explanations[0],  # Return first explanation for viz
            feature_weights=avg_weights,
            stability_score=avg_stability,
            confidence_intervals=confidence_intervals,
            is_stable=is_stable
        )

    def generate_report(self, result: StableLIMEResult) -> str:
        """Generate human-readable stability report."""
        lines = ["=" * 70]
        lines.append("STABLE LIME EXPLANATION")
        lines.append("=" * 70)

        status = "STABLE" if result.is_stable else "UNSTABLE"
        lines.append(f"\nStability Status: {status}")
        lines.append(f"Stability Score: {result.stability_score:.3f}")
        lines.append(f"Threshold: {self.stability_threshold}")

        lines.append(f"\nTop Features (Average over {self.n_runs} runs):")

        # Sort by absolute weight
        sorted_features = sorted(
            result.feature_weights.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:10]

        for feature, weight in sorted_features:
            ci_low, ci_high = result.confidence_intervals[feature]
            lines.append(
                f"  {feature}: {weight:+.4f} "
                f"[95% CI: {ci_low:+.4f}, {ci_high:+.4f}]"
            )

        if not result.is_stable:
            lines.append("\nWARNING: Unstable explanation!")
            lines.append("Consider:")
            lines.append("  - Increasing num_samples in LIME")
            lines.append("  - Using SHAP for more stable explanations")
            lines.append("  - Investigating feature interactions")

        lines.append("\n" + "=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\subsubsection{Attention Visualization for Deep Learning}

For transformer and attention-based models, attention weights provide interpretability by showing which input tokens the model focuses on.

\begin{lstlisting}[language=Python, caption={Attention Visualization for Deep Learning Models}]
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass

@dataclass
class AttentionAnalysis:
    """
    Attention analysis result.

    Attributes:
        attention_weights: Attention weights [layers, heads, seq_len, seq_len]
        tokens: Input tokens
        layer_averages: Average attention per layer
        head_averages: Average attention per head
        top_attended_tokens: Tokens receiving most attention
    """
    attention_weights: np.ndarray
    tokens: List[str]
    layer_averages: np.ndarray
    head_averages: np.ndarray
    top_attended_tokens: List[Tuple[str, float]]

class AttentionVisualizer:
    """
    Visualize and analyze attention patterns in transformer models.

    Attention mechanisms reveal what the model focuses on, providing
    interpretability for NLP and vision transformers.
    """

    def __init__(self, model: nn.Module):
        """
        Initialize attention visualizer.

        Args:
            model: Transformer model with attention weights
        """
        self.model = model
        self.attention_hooks = []

        logger.info("Initialized AttentionVisualizer")

    def extract_attention(
        self,
        input_ids: torch.Tensor,
        tokens: List[str]
    ) -> AttentionAnalysis:
        """
        Extract and analyze attention weights.

        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            tokens: Corresponding tokens

        Returns:
            Attention analysis with weights and statistics
        """
        self.model.eval()

        with torch.no_grad():
            # Forward pass with attention output
            outputs = self.model(
                input_ids,
                output_attentions=True
            )

            # Extract attention weights
            # Shape: (layers, batch, heads, seq_len, seq_len)
            attentions = outputs.attentions

        # Stack and average over batch
        attention_array = torch.stack(attentions).cpu().numpy()
        attention_array = attention_array[:, 0, :, :, :]  # Take first batch item

        # Layer averages (average over heads and target positions)
        layer_averages = attention_array.mean(axis=(1, 2))

        # Head averages (average over layers and target positions)
        head_averages = attention_array.mean(axis=(0, 2))

        # Find tokens receiving most attention (average over all layers/heads)
        avg_attention_per_token = attention_array.mean(axis=(0, 1, 2))

        top_indices = np.argsort(avg_attention_per_token)[::-1][:10]
        top_attended_tokens = [
            (tokens[idx], avg_attention_per_token[idx])
            for idx in top_indices
            if idx < len(tokens)
        ]

        return AttentionAnalysis(
            attention_weights=attention_array,
            tokens=tokens,
            layer_averages=layer_averages,
            head_averages=head_averages,
            top_attended_tokens=top_attended_tokens
        )

    def visualize_attention_heatmap(
        self,
        analysis: AttentionAnalysis,
        layer: int = -1,
        head: int = 0,
        save_path: Optional[str] = None
    ):
        """
        Visualize attention as heatmap.

        Args:
            analysis: Attention analysis result
            layer: Which layer to visualize (-1 for last)
            head: Which attention head to visualize
            save_path: Optional path to save figure
        """
        attention = analysis.attention_weights[layer, head, :, :]

        plt.figure(figsize=(12, 10))

        sns.heatmap(
            attention,
            xticklabels=analysis.tokens,
            yticklabels=analysis.tokens,
            cmap='viridis',
            cbar_kws={'label': 'Attention Weight'}
        )

        plt.xlabel('Key Tokens')
        plt.ylabel('Query Tokens')
        plt.title(f'Attention Heatmap (Layer {layer}, Head {head})')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path)

        plt.close()

    def identify_attention_patterns(
        self,
        analysis: AttentionAnalysis
    ) -> Dict[str, Any]:
        """
        Identify common attention patterns.

        Patterns include:
        - Diagonal attention (local context)
        - Broad attention (global context)
        - Sparse attention (specific tokens)
        - Head specialization
        """
        patterns = {}

        # Analyze each layer
        for layer_idx in range(analysis.attention_weights.shape[0]):
            layer_attention = analysis.attention_weights[layer_idx]

            # Average over heads
            avg_attention = layer_attention.mean(axis=0)

            # Check for diagonal pattern (local attention)
            diagonal_strength = np.diag(avg_attention).mean()

            # Check for broad attention (uniform weights)
            entropy = -np.sum(avg_attention * np.log(avg_attention + 1e-10), axis=1).mean()
            max_entropy = np.log(avg_attention.shape[1])
            uniformity = entropy / max_entropy

            patterns[f'layer_{layer_idx}'] = {
                'diagonal_strength': diagonal_strength,
                'uniformity': uniformity,
                'pattern': (
                    'local' if diagonal_strength > 0.5 else
                    'uniform' if uniformity > 0.8 else
                    'sparse'
                )
            }

        return patterns

    def generate_attention_report(
        self,
        analysis: AttentionAnalysis,
        patterns: Dict[str, Any]
    ) -> str:
        """Generate human-readable attention analysis report."""
        lines = ["=" * 70]
        lines.append("ATTENTION ANALYSIS REPORT")
        lines.append("=" * 70)

        lines.append(f"\nInput Length: {len(analysis.tokens)} tokens")
        lines.append(f"Layers: {analysis.attention_weights.shape[0]}")
        lines.append(f"Heads per Layer: {analysis.attention_weights.shape[1]}")

        lines.append("\nTOP ATTENDED TOKENS:")
        for token, weight in analysis.top_attended_tokens:
            lines.append(f"  {token}: {weight:.4f}")

        lines.append("\nLAYER PATTERNS:")
        for layer_name, pattern_info in patterns.items():
            lines.append(
                f"  {layer_name}: {pattern_info['pattern']} "
                f"(diagonal: {pattern_info['diagonal_strength']:.2f}, "
                f"uniformity: {pattern_info['uniformity']:.2f})"
            )

        lines.append("\n" + "=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\subsubsection{Concept-Based Explanations}

Concept-based explanations map model decisions to human-interpretable concepts rather than low-level features.

\begin{lstlisting}[language=Python, caption={Concept-Based Explanations with TCAV}]
from typing import Dict, List, Tuple, Optional, Any, Callable
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ConceptDefinition:
    """
    Definition of a human-interpretable concept.

    Attributes:
        name: Concept name (e.g., "striped", "wooden", "young")
        positive_examples: Data points exhibiting the concept
        negative_examples: Data points not exhibiting the concept
    """
    name: str
    positive_examples: np.ndarray
    negative_examples: np.ndarray

@dataclass
class TCAVResult:
    """
    Testing with Concept Activation Vectors (TCAV) result.

    Attributes:
        concept_name: Name of concept tested
        tcav_score: TCAV score (sensitivity to concept)
        statistical_significance: p-value from statistical test
        is_significant: Whether concept significantly influences predictions
    """
    concept_name: str
    tcav_score: float
    statistical_significance: float
    is_significant: bool

class ConceptBasedExplainer:
    """
    Generate concept-based explanations using TCAV.

    TCAV (Testing with Concept Activation Vectors) measures how much
    a model's predictions are influenced by human-defined concepts.

    Reference: Kim et al., "Interpretability Beyond Feature Attribution:
    Quantitative Testing with Concept Activation Vectors", ICML 2018.
    """

    def __init__(
        self,
        model: Any,
        layer_name: str,
        get_activations: Callable[[np.ndarray], np.ndarray]
    ):
        """
        Initialize concept-based explainer.

        Args:
            model: Trained model
            layer_name: Name of layer to extract activations from
            get_activations: Function to extract layer activations
        """
        self.model = model
        self.layer_name = layer_name
        self.get_activations = get_activations

        self.cavs: Dict[str, np.ndarray] = {}  # Concept activation vectors

        logger.info(f"Initialized ConceptBasedExplainer for layer {layer_name}")

    def learn_concept(self, concept: ConceptDefinition) -> np.ndarray:
        """
        Learn Concept Activation Vector (CAV) for a concept.

        CAV is a vector in activation space that points in the direction
        of the concept.

        Args:
            concept: Concept definition with positive/negative examples

        Returns:
            Concept activation vector
        """
        logger.info(f"Learning CAV for concept: {concept.name}")

        # Extract activations for positive and negative examples
        pos_activations = self.get_activations(concept.positive_examples)
        neg_activations = self.get_activations(concept.negative_examples)

        # Combine into training data
        X = np.vstack([pos_activations, neg_activations])
        y = np.array(
            [1] * len(pos_activations) + [0] * len(neg_activations)
        )

        # Train linear classifier to separate positive from negative
        classifier = LinearSVC(C=1.0, max_iter=10000)
        classifier.fit(X, y)

        # CAV is the normal vector to the decision boundary
        cav = classifier.coef_[0]
        cav = cav / np.linalg.norm(cav)  # Normalize

        self.cavs[concept.name] = cav

        logger.info(
            f"Learned CAV for {concept.name} "
            f"(accuracy: {classifier.score(X, y):.2%})"
        )

        return cav

    def compute_tcav(
        self,
        concept_name: str,
        test_examples: np.ndarray,
        target_class: int,
        n_runs: int = 20
    ) -> TCAVResult:
        """
        Compute TCAV score for a concept.

        TCAV score measures the fraction of test examples for which the
        concept positively influences the model's prediction for target class.

        Args:
            concept_name: Name of concept (must have learned CAV)
            test_examples: Test examples to analyze
            target_class: Target class to test sensitivity for
            n_runs: Number of statistical test runs

        Returns:
            TCAV result with score and significance
        """
        if concept_name not in self.cavs:
            raise ValueError(f"No CAV learned for concept: {concept_name}")

        cav = self.cavs[concept_name]

        # Compute gradients of target class prediction w.r.t. activations
        test_activations = self.get_activations(test_examples)

        # Compute directional derivative (gradient * CAV)
        # This measures how much the prediction changes when moving
        # in the direction of the concept

        sensitivities = []

        for activation in test_activations:
            # Approximate gradient using finite differences
            epsilon = 1e-3
            perturbed = activation + epsilon * cav

            # Get predictions
            pred_original = self.model.predict_proba(
                activation.reshape(1, -1)
            )[0, target_class]

            pred_perturbed = self.model.predict_proba(
                perturbed.reshape(1, -1)
            )[0, target_class]

            sensitivity = (pred_perturbed - pred_original) / epsilon
            sensitivities.append(sensitivity)

        sensitivities = np.array(sensitivities)

        # TCAV score: fraction of positive sensitivities
        tcav_score = (sensitivities > 0).mean()

        # Statistical significance test
        # Compare against random concept (permutation test)
        random_scores = []

        for _ in range(n_runs):
            random_cav = np.random.randn(len(cav))
            random_cav = random_cav / np.linalg.norm(random_cav)

            random_sensitivities = []

            for activation in test_activations:
                perturbed = activation + epsilon * random_cav

                pred_original = self.model.predict_proba(
                    activation.reshape(1, -1)
                )[0, target_class]

                pred_perturbed = self.model.predict_proba(
                    perturbed.reshape(1, -1)
                )[0, target_class]

                sensitivity = (pred_perturbed - pred_original) / epsilon
                random_sensitivities.append(sensitivity)

            random_sensitivities = np.array(random_sensitivities)
            random_score = (random_sensitivities > 0).mean()
            random_scores.append(random_score)

        # Two-tailed test
        p_value = (
            np.sum(np.abs(random_scores - 0.5) >= np.abs(tcav_score - 0.5)) /
            n_runs
        )

        is_significant = p_value < 0.05

        logger.info(
            f"TCAV score for '{concept_name}': {tcav_score:.3f} "
            f"(p={p_value:.4f}, {'significant' if is_significant else 'not significant'})"
        )

        return TCAVResult(
            concept_name=concept_name,
            tcav_score=tcav_score,
            statistical_significance=p_value,
            is_significant=is_significant
        )

    def generate_concept_report(
        self,
        results: List[TCAVResult],
        target_class: int
    ) -> str:
        """Generate human-readable concept analysis report."""
        lines = ["=" * 70]
        lines.append("CONCEPT-BASED EXPLANATION REPORT")
        lines.append("=" * 70)
        lines.append(f"\nTarget Class: {target_class}")
        lines.append(f"\nConcepts Tested: {len(results)}")

        significant = [r for r in results if r.is_significant]
        lines.append(f"Significant Concepts: {len(significant)}")

        lines.append("\nCONCEPT SENSITIVITY:")

        # Sort by TCAV score
        sorted_results = sorted(results, key=lambda r: r.tcav_score, reverse=True)

        for result in sorted_results:
            sig_marker = "***" if result.is_significant else "   "
            lines.append(
                f"{sig_marker} {result.concept_name}: {result.tcav_score:.3f} "
                f"(p={result.statistical_significance:.4f})"
            )

        lines.append("\n*** = statistically significant (p < 0.05)")
        lines.append("\n" + "=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\subsubsection{Model Distillation for Interpretability}

Complex models can be distilled into simpler, interpretable models while measuring fidelity.

\begin{lstlisting}[language=Python, caption={Model Distillation with Fidelity Metrics}]
from typing import Dict, Any, Optional
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class DistillationResult:
    """
    Model distillation result.

    Attributes:
        distilled_model: Simpler, interpretable model
        fidelity: Agreement with original model
        accuracy_loss: Accuracy difference vs original
        compression_ratio: Model size reduction
        interpretable_rules: Human-readable decision rules (for trees)
    """
    distilled_model: Any
    fidelity: float
    accuracy_loss: float
    compression_ratio: float
    interpretable_rules: Optional[str] = None

class ModelDistiller:
    """
    Distill complex models into interpretable surrogates.

    Creates simpler models (decision trees, linear models) that approximate
    the behavior of complex models while remaining interpretable.
    """

    def __init__(self, complex_model: Any):
        """
        Initialize model distiller.

        Args:
            complex_model: Complex model to distill
        """
        self.complex_model = complex_model

        logger.info("Initialized ModelDistiller")

    def distill_to_tree(
        self,
        X: np.ndarray,
        max_depth: int = 5
    ) -> DistillationResult:
        """
        Distill complex model into decision tree.

        Args:
            X: Input data for distillation
            max_depth: Maximum depth of decision tree

        Returns:
            Distillation result with fidelity metrics
        """
        logger.info(f"Distilling to decision tree (max_depth={max_depth})")

        # Get complex model predictions (these become labels for distillation)
        y_complex = self.complex_model.predict(X)

        # Train decision tree to mimic complex model
        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        tree.fit(X, y_complex)

        # Measure fidelity (agreement with complex model)
        y_tree = tree.predict(X)
        fidelity = (y_tree == y_complex).mean()

        # Extract interpretable rules
        rules = self._extract_tree_rules(tree, X)

        # Compute compression ratio
        # Complex model parameters vs tree parameters
        complex_params = self._count_parameters(self.complex_model)
        tree_params = tree.tree_.node_count

        compression_ratio = complex_params / tree_params if tree_params > 0 else float('inf')

        logger.info(
            f"Distillation complete: fidelity={fidelity:.2%}, "
            f"compression={compression_ratio:.1f}x"
        )

        return DistillationResult(
            distilled_model=tree,
            fidelity=fidelity,
            accuracy_loss=0.0,  # Measured against complex model, not ground truth
            compression_ratio=compression_ratio,
            interpretable_rules=rules
        )

    def _extract_tree_rules(
        self,
        tree: DecisionTreeClassifier,
        X: np.ndarray
    ) -> str:
        """Extract human-readable rules from decision tree."""
        from sklearn.tree import export_text

        feature_names = [f"feature_{i}" for i in range(X.shape[1])]

        rules = export_text(
            tree,
            feature_names=feature_names,
            max_depth=10
        )

        return rules

    def _count_parameters(self, model: Any) -> int:
        """Count number of parameters in model."""
        try:
            # PyTorch model
            return sum(p.numel() for p in model.parameters())
        except AttributeError:
            try:
                # Sklearn model
                if hasattr(model, 'coef_'):
                    return model.coef_.size
                elif hasattr(model, 'tree_'):
                    return model.tree_.node_count
                else:
                    return 1000  # Default estimate
            except AttributeError:
                return 1000  # Default estimate

    def evaluate_fidelity(
        self,
        distilled_model: Any,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> Dict[str, float]:
        """
        Evaluate distilled model fidelity and accuracy.

        Args:
            distilled_model: Distilled model
            X_test: Test features
            y_test: True test labels

        Returns:
            Dictionary of evaluation metrics
        """
        # Complex model predictions
        y_complex = self.complex_model.predict(X_test)

        # Distilled model predictions
        y_distilled = distilled_model.predict(X_test)

        # Fidelity: agreement with complex model
        fidelity = (y_distilled == y_complex).mean()

        # Accuracy: performance on ground truth
        accuracy_complex = accuracy_score(y_test, y_complex)
        accuracy_distilled = accuracy_score(y_test, y_distilled)
        accuracy_loss = accuracy_complex - accuracy_distilled

        # F1 scores
        f1_complex = f1_score(y_test, y_complex, average='weighted')
        f1_distilled = f1_score(y_test, y_distilled, average='weighted')

        return {
            'fidelity': fidelity,
            'accuracy_complex': accuracy_complex,
            'accuracy_distilled': accuracy_distilled,
            'accuracy_loss': accuracy_loss,
            'f1_complex': f1_complex,
            'f1_distilled': f1_distilled
        }

    def generate_distillation_report(
        self,
        result: DistillationResult,
        evaluation: Dict[str, float]
    ) -> str:
        """Generate human-readable distillation report."""
        lines = ["=" * 70]
        lines.append("MODEL DISTILLATION REPORT")
        lines.append("=" * 70)

        lines.append("\nCOMPRESSION:")
        lines.append(f"  Compression Ratio: {result.compression_ratio:.1f}x")

        lines.append("\nFIDELITY:")
        lines.append(f"  Agreement with Complex Model: {evaluation['fidelity']:.2%}")

        lines.append("\nACCURACY:")
        lines.append(f"  Complex Model: {evaluation['accuracy_complex']:.2%}")
        lines.append(f"  Distilled Model: {evaluation['accuracy_distilled']:.2%}")
        lines.append(f"  Accuracy Loss: {evaluation['accuracy_loss']:.2%}")

        lines.append("\nF1 SCORE:")
        lines.append(f"  Complex Model: {evaluation['f1_complex']:.4f}")
        lines.append(f"  Distilled Model: {evaluation['f1_distilled']:.4f}")

        if result.interpretable_rules:
            lines.append("\nINTERPRETABLE RULES (Top 20 lines):")
            rules_lines = result.interpretable_rules.split('\n')[:20]
            for rule_line in rules_lines:
                lines.append(f"  {rule_line}")

        lines.append("\n" + "=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\section{Governance and Compliance}

Governance frameworks ensure ML systems comply with regulations and organizational policies.

\subsection{GovernanceSystem: Policy Enforcement}

\begin{lstlisting}[language=Python, caption={ML Governance Framework}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class ComplianceStandard(Enum):
    """Regulatory compliance standards."""
    GDPR = "gdpr"
    CCPA = "ccpa"
    HIPAA = "hipaa"
    SOC2 = "soc2"
    FCRA = "fcra"  # Fair Credit Reporting Act

class RiskLevel(Enum):
    """Model risk levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class ComplianceRequirement:
    """
    Compliance requirement definition.

    Attributes:
        name: Requirement identifier
        standard: Compliance standard
        description: Requirement description
        validator: Validation function
        required: Whether requirement is mandatory
    """
    name: str
    standard: ComplianceStandard
    description: str
    validator: Any
    required: bool = True

@dataclass
class ComplianceCheck:
    """
    Result of compliance check.

    Attributes:
        requirement_name: Name of requirement
        passed: Whether check passed
        details: Additional details
        timestamp: When check was performed
    """
    requirement_name: str
    passed: bool
    details: str
    timestamp: datetime = field(default_factory=datetime.now)

class GovernanceSystem:
    """
    ML governance and compliance tracking system.

    Enforces organizational policies and regulatory requirements.

    Example:
        >>> gov = GovernanceSystem()
        >>> gov.add_requirement(gdpr_right_to_explanation)
        >>> results = gov.check_compliance(model, data)
        >>> if not gov.is_compliant(results):
        ...     raise ValueError("Compliance violations detected")
    """

    def __init__(self):
        """Initialize governance system."""
        self.requirements: Dict[str, ComplianceRequirement] = {}
        self.compliance_history: List[ComplianceCheck] = []

        # Initialize with common requirements
        self._setup_default_requirements()

        logger.info("Initialized GovernanceSystem")

    def _setup_default_requirements(self):
        """Set up default compliance requirements."""
        # GDPR: Right to explanation
        self.add_requirement(ComplianceRequirement(
            name="right_to_explanation",
            standard=ComplianceStandard.GDPR,
            description="Model must provide explanations for decisions",
            validator=lambda model: hasattr(model, 'explain') or
                                   hasattr(model, 'feature_importances_'),
            required=True
        ))

        # GDPR: Data minimization
        self.add_requirement(ComplianceRequirement(
            name="data_minimization",
            standard=ComplianceStandard.GDPR,
            description="Only collect necessary data",
            validator=self._check_data_minimization,
            required=True
        ))

        # Fairness requirement
        self.add_requirement(ComplianceRequirement(
            name="fairness_testing",
            standard=ComplianceStandard.FCRA,
            description="Model must pass fairness evaluation",
            validator=self._check_fairness,
            required=True
        ))

    def add_requirement(self, requirement: ComplianceRequirement):
        """
        Add compliance requirement.

        Args:
            requirement: Compliance requirement
        """
        self.requirements[requirement.name] = requirement
        logger.info(f"Added requirement: {requirement.name}")

    def check_compliance(
        self,
        model: Any,
        data: Optional[pd.DataFrame] = None,
        fairness_results: Optional[List] = None
    ) -> List[ComplianceCheck]:
        """
        Check compliance against all requirements.

        Args:
            model: Model to check
            data: Training/test data
            fairness_results: Fairness evaluation results

        Returns:
            List of compliance check results
        """
        results = []

        logger.info("Running compliance checks...")

        for req_name, requirement in self.requirements.items():
            try:
                # Execute validator
                if requirement.name == "fairness_testing":
                    passed = requirement.validator(fairness_results)
                elif requirement.name == "data_minimization":
                    passed = requirement.validator(data)
                else:
                    passed = requirement.validator(model)

                check = ComplianceCheck(
                    requirement_name=req_name,
                    passed=passed,
                    details=f"Check {'passed' if passed else 'failed'}"
                )

            except Exception as e:
                logger.error(f"Compliance check {req_name} failed: {e}")
                check = ComplianceCheck(
                    requirement_name=req_name,
                    passed=False,
                    details=f"Error: {str(e)}"
                )

            results.append(check)
            self.compliance_history.append(check)

        # Log summary
        passed = sum(1 for r in results if r.passed)
        logger.info(f"Compliance: {passed}/{len(results)} checks passed")

        return results

    def is_compliant(self, results: List[ComplianceCheck]) -> bool:
        """
        Check if all required checks passed.

        Args:
            results: Compliance check results

        Returns:
            True if compliant
        """
        required_checks = [
            req.name for req in self.requirements.values()
            if req.required
        ]

        for check in results:
            if check.requirement_name in required_checks and not check.passed:
                return False

        return True

    def _check_data_minimization(self, data: Optional[pd.DataFrame]) -> bool:
        """Check if data collection is minimized."""
        if data is None:
            return True

        # Check for unnecessary columns
        # In practice, check against approved feature list
        unnecessary = ['ssn', 'full_address', 'credit_card_number']

        for col in data.columns:
            if any(term in col.lower() for term in unnecessary):
                logger.warning(f"Unnecessary data collected: {col}")
                return False

        return True

    def _check_fairness(
        self,
        fairness_results: Optional[List]
    ) -> bool:
        """Check if fairness evaluation passed."""
        if fairness_results is None:
            logger.warning("No fairness results provided")
            return False

        # All fairness checks must pass
        return all(r.is_fair for r in fairness_results)

    def generate_compliance_report(
        self,
        results: List[ComplianceCheck]
    ) -> str:
        """
        Generate compliance report.

        Args:
            results: Compliance check results

        Returns:
            Formatted report
        """
        lines = ["=" * 70]
        lines.append("COMPLIANCE REPORT")
        lines.append("=" * 70)
        lines.append(f"Generated: {datetime.now().isoformat()}")
        lines.append("")

        # Group by standard
        by_standard = {}
        for check in results:
            req = self.requirements[check.requirement_name]
            standard = req.standard.value

            if standard not in by_standard:
                by_standard[standard] = []

            by_standard[standard].append((req, check))

        for standard, checks in by_standard.items():
            lines.append(f"\n{standard.upper()}")
            lines.append("-" * 70)

            for req, check in checks:
                status = "[PASS]" if check.passed else "[FAIL]"
                required = "[REQUIRED]" if req.required else "[OPTIONAL]"

                lines.append(f"{status} {required} {req.name}")
                lines.append(f"  {req.description}")
                lines.append(f"  {check.details}")

        # Overall status
        is_compliant = self.is_compliant(results)
        lines.append("\n" + "=" * 70)
        lines.append(
            f"OVERALL STATUS: {'COMPLIANT' if is_compliant else 'NON-COMPLIANT'}"
        )
        lines.append("=" * 70)

        return "\n".join(lines)
\end{lstlisting}

\section{Regulatory Compliance Frameworks}

Modern ML systems must comply with multiple regulatory frameworks spanning privacy, fairness, transparency, and accountability. This section provides automated compliance checking for major regulations.

\subsection{GDPR Compliance Framework}

The General Data Protection Regulation (GDPR) imposes strict requirements on data processing and automated decision-making in the European Union.

\begin{lstlisting}[language=Python, caption={Comprehensive GDPR Compliance System}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Set
from enum import Enum
from datetime import datetime, timedelta
import logging
import json

logger = logging.getLogger(__name__)

class GDPRArticle(Enum):
    """Key GDPR articles relevant to ML."""
    LAWFUL_BASIS = "Article 6"  # Lawful basis for processing
    RIGHT_TO_EXPLANATION = "Article 13-15"  # Transparency
    RIGHT_TO_ERASURE = "Article 17"  # Right to be forgotten
    RIGHT_TO_RECTIFICATION = "Article 16"  # Data correction
    DATA_MINIMIZATION = "Article 5(1)(c)"  # Only necessary data
    AUTOMATED_DECISION = "Article 22"  # Automated individual decisions
    DATA_PROTECTION_BY_DESIGN = "Article 25"  # Built-in privacy
    DPIA = "Article 35"  # Data Protection Impact Assessment

@dataclass
class GDPRDataSubjectRequest:
    """
    Represents a GDPR data subject access request (DSAR).

    Attributes:
        request_id: Unique request identifier
        request_type: Type of request (access, erasure, rectification)
        subject_id: Identifier of data subject
        received_date: When request was received
        deadline: Response deadline (30 days by default)
        status: Request processing status
        data_returned: Data returned for access requests
    """
    request_id: str
    request_type: str  # 'access', 'erasure', 'rectification', 'portability'
    subject_id: str
    received_date: datetime
    deadline: datetime
    status: str = "pending"
    data_returned: Optional[Dict[str, Any]] = None
    completion_date: Optional[datetime] = None

@dataclass
class DPIAResult:
    """
    Data Protection Impact Assessment (DPIA) result.

    Required by GDPR Article 35 for high-risk processing.

    Attributes:
        assessment_date: When DPIA was performed
        processing_description: Description of data processing
        necessity_justification: Why processing is necessary
        risks_identified: List of identified risks
        mitigation_measures: Measures to mitigate risks
        residual_risk_level: Risk level after mitigation (low/medium/high)
        requires_consultation: Whether DPA consultation needed
    """
    assessment_date: datetime
    processing_description: str
    necessity_justification: str
    risks_identified: List[str]
    mitigation_measures: List[str]
    residual_risk_level: str
    requires_consultation: bool
    lawful_basis: str
    special_categories_processed: bool

class GDPRComplianceManager:
    """
    Comprehensive GDPR compliance management for ML systems.

    Handles:
    - Data subject access requests (DSARs)
    - Data Protection Impact Assessments (DPIAs)
    - Consent management
    - Automated compliance checking
    - Right to explanation
    - Right to erasure
    """

    def __init__(self, data_controller: str, dpo_contact: str):
        """
        Initialize GDPR compliance manager.

        Args:
            data_controller: Name of data controller organization
            dpo_contact: Data Protection Officer contact information
        """
        self.data_controller = data_controller
        self.dpo_contact = dpo_contact
        self.dsar_requests: Dict[str, GDPRDataSubjectRequest] = {}
        self.consent_records: Dict[str, Dict[str, Any]] = {}
        self.processing_activities: List[Dict[str, Any]] = []

        logger.info(f"Initialized GDPR compliance for {data_controller}")

    def conduct_dpia(
        self,
        processing_description: str,
        data_types: List[str],
        automated_decision_making: bool,
        special_categories: bool,
        large_scale: bool,
        vulnerable_subjects: bool
    ) -> DPIAResult:
        """
        Conduct Data Protection Impact Assessment.

        Required when processing is likely to result in high risk to rights
        and freedoms of individuals.

        Args:
            processing_description: What processing will be done
            data_types: Types of personal data processed
            automated_decision_making: Whether ADM is involved
            special_categories: Whether processing special category data
            large_scale: Whether processing is large-scale
            vulnerable_subjects: Whether subjects are vulnerable (children, etc.)

        Returns:
            DPIA result with risk assessment and recommendations
        """
        logger.info("Conducting Data Protection Impact Assessment")

        # Assess necessity for DPIA
        triggers = []
        if automated_decision_making:
            triggers.append("Automated decision-making with legal/similar effects")
        if special_categories:
            triggers.append("Processing special category data at scale")
        if large_scale:
            triggers.append("Large-scale systematic monitoring")
        if vulnerable_subjects:
            triggers.append("Processing data of vulnerable subjects")

        requires_dpia = len(triggers) >= 2 or (special_categories and automated_decision_making)

        if not requires_dpia:
            logger.info("DPIA not required based on criteria")

        # Identify risks
        risks = []
        mitigation = []

        if automated_decision_making:
            risks.append(
                "Automated decisions may lack human oversight and explanation"
            )
            mitigation.append(
                "Implement Article 22 safeguards: human review, "
                "explanation mechanism, contestation process"
            )

        if special_categories:
            risks.append(
                "Special category data (race, health, etc.) increases "
                "discrimination risk"
            )
            mitigation.append(
                "Implement enhanced fairness testing, explicit consent, "
                "encryption at rest and in transit"
            )

        if large_scale:
            risks.append("Large-scale processing increases breach impact")
            mitigation.append(
                "Implement data minimization, pseudonymization, "
                "regular security audits"
            )

        if vulnerable_subjects:
            risks.append("Vulnerable subjects require additional protection")
            mitigation.append(
                "Age verification, guardian consent for minors, "
                "simplified privacy notices"
            )

        # Always add baseline risks
        risks.extend([
            "Data breach could expose personal information",
            "Model could encode biases from training data",
            "Lack of transparency in decision-making process"
        ])

        mitigation.extend([
            "Encryption, access controls, breach notification procedures",
            "Regular fairness audits using demographic parity and equalized odds",
            "SHAP explanations for all predictions, model cards"
        ])

        # Assess residual risk
        if special_categories and automated_decision_making and large_scale:
            residual_risk = "high"
            requires_consultation = True
        elif len(triggers) >= 2:
            residual_risk = "medium"
            requires_consultation = False
        else:
            residual_risk = "low"
            requires_consultation = False

        # Determine lawful basis
        if special_categories:
            lawful_basis = "Article 9(2)(a) - Explicit consent"
        else:
            lawful_basis = "Article 6(1)(b) - Contract performance or " \
                          "Article 6(1)(f) - Legitimate interests"

        dpia = DPIAResult(
            assessment_date=datetime.now(),
            processing_description=processing_description,
            necessity_justification=(
                "Processing is necessary for [business purpose] and "
                "cannot be achieved through less intrusive means"
            ),
            risks_identified=risks,
            mitigation_measures=mitigation,
            residual_risk_level=residual_risk,
            requires_consultation=requires_consultation,
            lawful_basis=lawful_basis,
            special_categories_processed=special_categories
        )

        logger.info(f"DPIA completed: {residual_risk} residual risk")

        if requires_consultation:
            logger.warning(
                "High residual risk - consultation with DPA required "
                "before processing"
            )

        return dpia

    def handle_data_subject_request(
        self,
        request_type: str,
        subject_id: str,
        data_store: Optional[Any] = None
    ) -> GDPRDataSubjectRequest:
        """
        Handle GDPR data subject request.

        Args:
            request_type: 'access', 'erasure', 'rectification', 'portability'
            subject_id: Identifier of data subject
            data_store: Data storage system (for actual operations)

        Returns:
            DSAR tracking object
        """
        import uuid

        request_id = str(uuid.uuid4())
        received_date = datetime.now()
        deadline = received_date + timedelta(days=30)  # GDPR requires 1 month

        dsar = GDPRDataSubjectRequest(
            request_id=request_id,
            request_type=request_type,
            subject_id=subject_id,
            received_date=received_date,
            deadline=deadline
        )

        self.dsar_requests[request_id] = dsar

        logger.info(
            f"Received {request_type} request for subject {subject_id}, "
            f"deadline: {deadline.isoformat()}"
        )

        # Process request based on type
        if request_type == "access":
            # Article 15: Right of access
            dsar.data_returned = self._collect_subject_data(subject_id, data_store)
            dsar.status = "completed"
            dsar.completion_date = datetime.now()

        elif request_type == "erasure":
            # Article 17: Right to erasure ("right to be forgotten")
            self._erase_subject_data(subject_id, data_store)
            dsar.status = "completed"
            dsar.completion_date = datetime.now()

        elif request_type == "rectification":
            # Article 16: Right to rectification
            dsar.status = "awaiting_corrected_data"

        elif request_type == "portability":
            # Article 20: Right to data portability
            dsar.data_returned = self._collect_subject_data(
                subject_id, data_store, structured=True
            )
            dsar.status = "completed"
            dsar.completion_date = datetime.now()

        return dsar

    def _collect_subject_data(
        self,
        subject_id: str,
        data_store: Optional[Any],
        structured: bool = False
    ) -> Dict[str, Any]:
        """Collect all personal data for a data subject."""
        logger.info(f"Collecting personal data for subject {subject_id}")

        # In practice, query all databases, logs, backups, etc.
        # This is a simplified example
        data = {
            "subject_id": subject_id,
            "collection_date": datetime.now().isoformat(),
            "data_controller": self.data_controller,
            "dpo_contact": self.dpo_contact,
            "personal_data": {
                # Query from data_store
                "profile": {},
                "transactions": [],
                "model_predictions": [],
                "consent_records": self.consent_records.get(subject_id, {})
            },
            "processing_purposes": [
                activity["purpose"]
                for activity in self.processing_activities
            ],
            "retention_period": "As specified in privacy policy",
            "third_party_sharing": "None"
        }

        return data

    def _erase_subject_data(self, subject_id: str, data_store: Optional[Any]):
        """Erase all personal data for a data subject."""
        logger.info(f"Erasing personal data for subject {subject_id}")

        # Erase from all systems
        # Note: Some data may need to be retained for legal reasons

        # Remove from consent records
        if subject_id in self.consent_records:
            del self.consent_records[subject_id]

        # Remove from data store
        if data_store:
            # data_store.delete_subject(subject_id)
            pass

        # Remove from ML training data
        # This may require model retraining!

        logger.warning(
            "Erasure may require model retraining if data was used in training"
        )

    def record_consent(
        self,
        subject_id: str,
        purpose: str,
        consent_given: bool,
        consent_text: str
    ):
        """
        Record consent for processing (Article 7).

        Args:
            subject_id: Data subject identifier
            purpose: Specific purpose of processing
            consent_given: Whether consent was given
            consent_text: Exact wording shown to subject
        """
        if subject_id not in self.consent_records:
            self.consent_records[subject_id] = {}

        self.consent_records[subject_id][purpose] = {
            "consent_given": consent_given,
            "consent_text": consent_text,
            "timestamp": datetime.now().isoformat(),
            "withdrawable": True,
            "granular": True  # Separate consent for each purpose
        }

        logger.info(
            f"Recorded consent for {subject_id} / {purpose}: {consent_given}"
        )

    def check_article_22_compliance(
        self,
        has_human_review: bool,
        has_explanation: bool,
        has_contestation: bool,
        legal_effects: bool
    ) -> Dict[str, Any]:
        """
        Check compliance with Article 22 (Automated Individual Decision-Making).

        Article 22(1): Data subject has right not to be subject to decision
        based solely on automated processing which produces legal effects or
        similarly significant effects.

        Article 22(3): In cases where automated decision-making is allowed,
        safeguards must include right to obtain human intervention, express
        point of view, and contest the decision.

        Args:
            has_human_review: Whether decisions undergo human review
            has_explanation: Whether explanations are provided
            has_contestation: Whether subjects can contest decisions
            legal_effects: Whether decisions have legal/similarly significant effects

        Returns:
            Compliance status with recommendations
        """
        logger.info("Checking Article 22 compliance")

        violations = []
        recommendations = []

        if legal_effects:
            # Article 22(1) applies - safeguards required
            if not has_human_review:
                violations.append("No human review for high-stakes decisions")
                recommendations.append(
                    "Implement human-in-the-loop review for all decisions "
                    "with legal or similarly significant effects"
                )

            if not has_explanation:
                violations.append("No explanation mechanism")
                recommendations.append(
                    "Provide meaningful information about logic involved, "
                    "significance, and envisaged consequences (Article 13-15)"
                )

            if not has_contestation:
                violations.append("No contestation process")
                recommendations.append(
                    "Implement process for subjects to express their point of view "
                    "and contest automated decisions"
                )

        is_compliant = len(violations) == 0

        return {
            "compliant": is_compliant,
            "article": "Article 22",
            "violations": violations,
            "recommendations": recommendations,
            "safeguards_required": legal_effects,
            "human_review": has_human_review,
            "explanation": has_explanation,
            "contestation": has_contestation
        }

    def generate_gdpr_report(self) -> str:
        """Generate comprehensive GDPR compliance report."""
        lines = ["=" * 80]
        lines.append("GDPR COMPLIANCE REPORT")
        lines.append("=" * 80)
        lines.append(f"Data Controller: {self.data_controller}")
        lines.append(f"DPO Contact: {self.dpo_contact}")
        lines.append(f"Report Date: {datetime.now().isoformat()}")
        lines.append("")

        # DSAR statistics
        lines.append("DATA SUBJECT ACCESS REQUESTS:")
        lines.append(f"  Total requests: {len(self.dsar_requests)}")

        by_type = {}
        overdue = 0

        for dsar in self.dsar_requests.values():
            by_type[dsar.request_type] = by_type.get(dsar.request_type, 0) + 1

            if dsar.status != "completed" and datetime.now() > dsar.deadline:
                overdue += 1

        for req_type, count in by_type.items():
            lines.append(f"    {req_type}: {count}")

        if overdue > 0:
            lines.append(f"  WARNING: {overdue} requests overdue!")

        # Consent statistics
        lines.append(f"\nCONSENT RECORDS: {len(self.consent_records)} subjects")

        # Processing activities
        lines.append(f"\nPROCESSING ACTIVITIES: {len(self.processing_activities)}")

        lines.append("\n" + "=" * 80)

        return "\n".join(lines)
\end{lstlisting}

\subsection{CCPA and HIPAA Compliance}

\begin{lstlisting}[language=Python, caption={CCPA and HIPAA Compliance Frameworks}]
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class CCPAComplianceManager:
    """
    California Consumer Privacy Act (CCPA) compliance.

    Key rights under CCPA:
    - Right to know what personal information is collected
    - Right to delete personal information
    - Right to opt-out of sale of personal information
    - Right to non-discrimination for exercising rights
    """

    def __init__(self, business_name: str):
        """
        Initialize CCPA compliance manager.

        Args:
            business_name: Name of business entity
        """
        self.business_name = business_name
        self.do_not_sell_requests: Set[str] = set()
        self.deletion_requests: Dict[str, datetime] = {}
        self.disclosure_requests: Dict[str, datetime] = {}

        logger.info(f"Initialized CCPA compliance for {business_name}")

    def handle_do_not_sell_request(self, consumer_id: str):
        """
        Handle consumer opt-out from sale of personal information.

        CCPA requires businesses to honor "Do Not Sell My Personal Information"
        requests and provide clear opt-out mechanisms.

        Args:
            consumer_id: Consumer identifier
        """
        self.do_not_sell_requests.add(consumer_id)

        logger.info(f"Consumer {consumer_id} opted out of data sale")

        # In practice: Remove from data broker pipelines,
        # suppress from ad targeting, etc.

    def verify_right_to_deletion(self, consumer_id: str) -> Dict[str, Any]:
        """
        Verify and process deletion request.

        CCPA allows businesses to deny deletion in specific cases
        (e.g., completing transaction, security, legal obligations).

        Args:
            consumer_id: Consumer identifier

        Returns:
            Deletion verification result
        """
        # Check for exceptions to deletion
        exceptions = []

        # Example exceptions:
        # - Complete transaction
        # - Detect security incidents
        # - Comply with legal obligation
        # - Internal use reasonably aligned with consumer expectations

        if self._has_pending_transaction(consumer_id):
            exceptions.append("Pending transaction must be completed")

        if self._required_for_legal_compliance(consumer_id):
            exceptions.append("Data retention required by law")

        can_delete = len(exceptions) == 0

        if can_delete:
            self.deletion_requests[consumer_id] = datetime.now()
            logger.info(f"Deletion approved for consumer {consumer_id}")
        else:
            logger.warning(
                f"Deletion denied for {consumer_id}: {', '.join(exceptions)}"
            )

        return {
            "consumer_id": consumer_id,
            "can_delete": can_delete,
            "exceptions": exceptions,
            "request_date": datetime.now().isoformat()
        }

    def _has_pending_transaction(self, consumer_id: str) -> bool:
        """Check if consumer has pending transactions."""
        # Implementation would check order/transaction systems
        return False

    def _required_for_legal_compliance(self, consumer_id: str) -> bool:
        """Check if data retention required by law."""
        # Example: Tax records, fraud prevention
        return False

    def generate_privacy_notice(self) -> str:
        """
        Generate CCPA-compliant privacy notice.

        Must include:
        - Categories of personal information collected
        - Purposes for collection
        - Categories of sources
        - Categories of third parties with whom info is shared
        - Business/commercial purposes for collecting or selling
        - Consumer rights
        """
        notice = f"""
PRIVACY NOTICE FOR CALIFORNIA RESIDENTS

Effective Date: {datetime.now().strftime('%B %d, %Y')}

Business: {self.business_name}

YOUR RIGHTS UNDER CCPA:

1. Right to Know: You have the right to request disclosure of:
   - Categories of personal information collected
   - Categories of sources from which information is collected
   - Business/commercial purpose for collecting or selling information
   - Categories of third parties with whom we share information
   - Specific pieces of personal information collected

2. Right to Delete: You have the right to request deletion of personal
   information we collected from you, subject to certain exceptions.

3. Right to Opt-Out: You have the right to opt-out of sale of your
   personal information.

4. Right to Non-Discrimination: We will not discriminate against you
   for exercising your CCPA rights.

TO EXERCISE YOUR RIGHTS:
- Email: privacy@{self.business_name.lower().replace(' ', '')}.com
- Phone: 1-800-XXX-XXXX
- Web: https://www.{self.business_name.lower().replace(' ', '')}.com/ccpa-request

We will respond to verifiable requests within 45 days.
        """

        return notice.strip()


class HIPAAComplianceManager:
    """
    Health Insurance Portability and Accountability Act (HIPAA) compliance
    for ML systems handling Protected Health Information (PHI).

    HIPAA requires:
    - Administrative safeguards (policies, procedures, training)
    - Physical safeguards (facility access, workstation security)
    - Technical safeguards (access control, encryption, audit logs)
    """

    def __init__(self, covered_entity: str):
        """
        Initialize HIPAA compliance manager.

        Args:
            covered_entity: Name of covered entity (hospital, insurer, etc.)
        """
        self.covered_entity = covered_entity
        self.access_logs: List[Dict[str, Any]] = []
        self.phi_inventory: List[Dict[str, Any]] = []
        self.business_associates: List[str] = []

        logger.info(f"Initialized HIPAA compliance for {covered_entity}")

    def verify_minimum_necessary(
        self,
        requested_fields: List[str],
        purpose: str
    ) -> Dict[str, Any]:
        """
        Verify compliance with HIPAA Minimum Necessary Rule.

        Covered entities must make reasonable efforts to limit PHI to
        minimum necessary to accomplish intended purpose.

        Args:
            requested_fields: PHI fields requested for use
            purpose: Purpose for which PHI is needed

        Returns:
            Verification result with approved fields
        """
        logger.info(f"Verifying minimum necessary for purpose: {purpose}")

        # Define minimum necessary fields for common purposes
        minimum_necessary = {
            "treatment": [
                "patient_id", "diagnosis", "medications", "allergies", "vitals"
            ],
            "payment": [
                "patient_id", "diagnosis", "procedure_codes", "insurance_info"
            ],
            "research": [
                "patient_id_hash", "diagnosis", "demographics", "outcomes"
            ],
            "ml_training": [
                "patient_id_hash", "diagnosis", "lab_results", "outcomes"
            ]
        }

        required_fields = minimum_necessary.get(purpose, [])
        approved_fields = [f for f in requested_fields if f in required_fields]
        denied_fields = [f for f in requested_fields if f not in required_fields]

        if denied_fields:
            logger.warning(
                f"Denied access to {len(denied_fields)} fields: {denied_fields}"
            )

        return {
            "purpose": purpose,
            "requested_fields": requested_fields,
            "approved_fields": approved_fields,
            "denied_fields": denied_fields,
            "compliant": len(denied_fields) == 0,
            "recommendation": (
                "Remove unnecessary PHI fields" if denied_fields else
                "Access approved"
            )
        }

    def log_phi_access(
        self,
        user_id: str,
        patient_id: str,
        action: str,
        fields_accessed: List[str]
    ):
        """
        Log PHI access for audit trail (required by HIPAA Security Rule).

        Args:
            user_id: User who accessed PHI
            patient_id: Patient whose PHI was accessed
            action: Action performed (read, write, delete)
            fields_accessed: Specific PHI fields accessed
        """
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "patient_id": patient_id,
            "action": action,
            "fields_accessed": fields_accessed
        }

        self.access_logs.append(log_entry)

        logger.info(
            f"PHI access logged: {user_id} {action} {len(fields_accessed)} "
            f"fields for patient {patient_id}"
        )

    def check_encryption_compliance(
        self,
        phi_at_rest_encrypted: bool,
        phi_in_transit_encrypted: bool,
        encryption_standard: str
    ) -> Dict[str, Any]:
        """
        Check encryption compliance (HIPAA Security Rule Section 164.312(a)(2)(iv)).

        While HIPAA does not mandate encryption, it is "addressable" -
        if not implemented, equivalent safeguards must be documented.

        Args:
            phi_at_rest_encrypted: Whether PHI is encrypted at rest
            phi_in_transit_encrypted: Whether PHI is encrypted in transit
            encryption_standard: Encryption standard used (e.g., "AES-256")

        Returns:
            Encryption compliance status
        """
        compliant = phi_at_rest_encrypted and phi_in_transit_encrypted

        acceptable_standards = ["AES-256", "AES-128", "RSA-2048", "TLS 1.2", "TLS 1.3"]
        standard_acceptable = encryption_standard in acceptable_standards

        recommendations = []

        if not phi_at_rest_encrypted:
            recommendations.append(
                "Implement encryption at rest using AES-256 or equivalent"
            )

        if not phi_in_transit_encrypted:
            recommendations.append(
                "Implement encryption in transit using TLS 1.2+ or equivalent"
            )

        if not standard_acceptable:
            recommendations.append(
                f"Upgrade encryption standard from {encryption_standard} to "
                f"industry-accepted standard (AES-256, TLS 1.3)"
            )

        return {
            "compliant": compliant and standard_acceptable,
            "at_rest_encrypted": phi_at_rest_encrypted,
            "in_transit_encrypted": phi_in_transit_encrypted,
            "encryption_standard": encryption_standard,
            "standard_acceptable": standard_acceptable,
            "recommendations": recommendations
        }

    def generate_hipaa_compliance_report(self) -> str:
        """Generate HIPAA compliance report."""
        lines = ["=" * 80]
        lines.append("HIPAA COMPLIANCE REPORT")
        lines.append("=" * 80)
        lines.append(f"Covered Entity: {self.covered_entity}")
        lines.append(f"Report Date: {datetime.now().isoformat()}")
        lines.append("")

        lines.append(f"PHI ACCESS LOGS: {len(self.access_logs)} entries")
        lines.append(f"PHI INVENTORY: {len(self.phi_inventory)} datasets")
        lines.append(f"BUSINESS ASSOCIATES: {len(self.business_associates)}")

        lines.append("\n" + "=" * 80)

        return "\n".join(lines)
\end{lstlisting}

\subsection{Unified Regulatory Compliance Framework}

\begin{lstlisting}[language=Python, caption={Unified Multi-Regulatory Compliance System}]
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class Regulation(Enum):
    """Supported regulatory frameworks."""
    GDPR = "gdpr"
    CCPA = "ccpa"
    HIPAA = "hipaa"
    FCRA = "fcra"  # Fair Credit Reporting Act
    ECOA = "ecoa"  # Equal Credit Opportunity Act
    SOX = "sox"  # Sarbanes-Oxley (financial reporting)
    BASEL_III = "basel_iii"  # Banking regulation
    MIFID_II = "mifid_ii"  # Markets in Financial Instruments Directive

@dataclass
class ComplianceViolation:
    """Represents a regulatory compliance violation."""
    regulation: Regulation
    article_section: str
    description: str
    severity: str  # 'critical', 'high', 'medium', 'low'
    remediation: str
    potential_fine: Optional[str] = None

class UnifiedComplianceFramework:
    """
    Unified compliance framework supporting multiple regulations.

    Automates compliance checking across GDPR, CCPA, HIPAA, and
    financial regulations.
    """

    def __init__(self, applicable_regulations: List[Regulation]):
        """
        Initialize unified compliance framework.

        Args:
            applicable_regulations: List of regulations that apply
        """
        self.applicable_regulations = applicable_regulations
        self.violations: List[ComplianceViolation] = []

        # Initialize regulation-specific managers
        self.gdpr_manager: Optional[GDPRComplianceManager] = None
        self.ccpa_manager: Optional[CCPAComplianceManager] = None
        self.hipaa_manager: Optional[HIPAAComplianceManager] = None

        logger.info(
            f"Initialized compliance framework for: "
            f"{[r.value for r in applicable_regulations]}"
        )

    def comprehensive_compliance_check(
        self,
        model: Any,
        data: Optional[Any] = None,
        model_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Run comprehensive compliance check across all applicable regulations.

        Args:
            model: ML model to check
            data: Training/test data
            model_metadata: Model documentation and metadata

        Returns:
            Comprehensive compliance report
        """
        logger.info("Running comprehensive regulatory compliance check")

        results = {
            "timestamp": datetime.now().isoformat(),
            "applicable_regulations": [r.value for r in self.applicable_regulations],
            "checks_performed": [],
            "violations": [],
            "compliant": True
        }

        # GDPR checks
        if Regulation.GDPR in self.applicable_regulations:
            gdpr_violations = self._check_gdpr_compliance(model, data, model_metadata)
            results["checks_performed"].append("GDPR")
            results["violations"].extend(gdpr_violations)

        # CCPA checks
        if Regulation.CCPA in self.applicable_regulations:
            ccpa_violations = self._check_ccpa_compliance(model, data, model_metadata)
            results["checks_performed"].append("CCPA")
            results["violations"].extend(ccpa_violations)

        # HIPAA checks
        if Regulation.HIPAA in self.applicable_regulations:
            hipaa_violations = self._check_hipaa_compliance(model, data, model_metadata)
            results["checks_performed"].append("HIPAA")
            results["violations"].extend(hipaa_violations)

        # Financial regulation checks
        if Regulation.FCRA in self.applicable_regulations:
            fcra_violations = self._check_fcra_compliance(model, model_metadata)
            results["checks_performed"].append("FCRA")
            results["violations"].extend(fcra_violations)

        results["compliant"] = len(results["violations"]) == 0
        results["violation_count"] = len(results["violations"])

        if not results["compliant"]:
            logger.error(f"Found {len(results['violations'])} compliance violations")

        return results

    def _check_gdpr_compliance(
        self,
        model: Any,
        data: Optional[Any],
        metadata: Optional[Dict[str, Any]]
    ) -> List[ComplianceViolation]:
        """Check GDPR compliance."""
        violations = []

        # Check for right to explanation (Article 13-15)
        if not hasattr(model, 'explain') and not metadata.get('explanation_method'):
            violations.append(ComplianceViolation(
                regulation=Regulation.GDPR,
                article_section="Articles 13-15",
                description="No explanation mechanism for automated decisions",
                severity="high",
                remediation="Implement SHAP, LIME, or other explanation method",
                potential_fine="Up to 20M EUR or 4% of global revenue"
            ))

        # Check for data minimization (Article 5(1)(c))
        if metadata and metadata.get('feature_count', 0) > 100:
            violations.append(ComplianceViolation(
                regulation=Regulation.GDPR,
                article_section="Article 5(1)(c)",
                description="Excessive features may violate data minimization",
                severity="medium",
                remediation="Perform feature selection to use only necessary features"
            ))

        # Check for DPIA (Article 35)
        if not metadata.get('dpia_conducted'):
            violations.append(ComplianceViolation(
                regulation=Regulation.GDPR,
                article_section="Article 35",
                description="No Data Protection Impact Assessment conducted",
                severity="high",
                remediation="Conduct DPIA for high-risk processing"
            ))

        return violations

    def _check_ccpa_compliance(
        self,
        model: Any,
        data: Optional[Any],
        metadata: Optional[Dict[str, Any]]
    ) -> List[ComplianceViolation]:
        """Check CCPA compliance."""
        violations = []

        # Check for opt-out mechanism
        if not metadata.get('has_opt_out_mechanism'):
            violations.append(ComplianceViolation(
                regulation=Regulation.CCPA,
                article_section="Section 1798.120",
                description="No opt-out mechanism for data sale",
                severity="high",
                remediation="Implement 'Do Not Sell My Personal Information' link",
                potential_fine="Up to $7,500 per intentional violation"
            ))

        return violations

    def _check_hipaa_compliance(
        self,
        model: Any,
        data: Optional[Any],
        metadata: Optional[Dict[str, Any]]
    ) -> List[ComplianceViolation]:
        """Check HIPAA compliance."""
        violations = []

        # Check for encryption
        if not metadata.get('phi_encrypted'):
            violations.append(ComplianceViolation(
                regulation=Regulation.HIPAA,
                article_section="Section 164.312(a)(2)(iv)",
                description="PHI not encrypted at rest and/or in transit",
                severity="critical",
                remediation="Implement AES-256 encryption for PHI",
                potential_fine="Up to $1.5M per violation category per year"
            ))

        # Check for audit logs
        if not metadata.get('has_audit_logs'):
            violations.append(ComplianceViolation(
                regulation=Regulation.HIPAA,
                article_section="Section 164.312(b)",
                description="No audit logs for PHI access",
                severity="high",
                remediation="Implement comprehensive audit logging"
            ))

        return violations

    def _check_fcra_compliance(
        self,
        model: Any,
        metadata: Optional[Dict[str, Any]]
    ) -> List[ComplianceViolation]:
        """Check Fair Credit Reporting Act compliance."""
        violations = []

        # FCRA requires adverse action notices
        if not metadata.get('has_adverse_action_notice'):
            violations.append(ComplianceViolation(
                regulation=Regulation.FCRA,
                article_section="Section 615",
                description="No adverse action notice mechanism",
                severity="high",
                remediation=(
                    "Implement adverse action notices explaining reasons "
                    "for credit denial"
                ),
                potential_fine="Statutory damages + attorney fees"
            ))

        return violations

    def generate_compliance_report(self, results: Dict[str, Any]) -> str:
        """Generate human-readable compliance report."""
        lines = ["=" * 80]
        lines.append("UNIFIED REGULATORY COMPLIANCE REPORT")
        lines.append("=" * 80)
        lines.append(f"Timestamp: {results['timestamp']}")
        lines.append(f"Regulations Checked: {', '.join(results['checks_performed'])}")
        lines.append("")

        status = "COMPLIANT" if results['compliant'] else "NON-COMPLIANT"
        lines.append(f"OVERALL STATUS: {status}")
        lines.append(f"Violations Found: {results['violation_count']}")

        if results['violations']:
            lines.append("\nVIOLATIONS:")

            # Group by severity
            by_severity = {'critical': [], 'high': [], 'medium': [], 'low': []}

            for v in results['violations']:
                by_severity[v.severity].append(v)

            for severity in ['critical', 'high', 'medium', 'low']:
                violations = by_severity[severity]

                if violations:
                    lines.append(f"\n{severity.upper()} SEVERITY ({len(violations)}):")

                    for v in violations:
                        lines.append(f"\n  [{v.regulation.value.upper()}] {v.article_section}")
                        lines.append(f"  {v.description}")
                        lines.append(f"  Remediation: {v.remediation}")

                        if v.potential_fine:
                            lines.append(f"  Potential Fine: {v.potential_fine}")

        lines.append("\n" + "=" * 80)

        return "\n".join(lines)
\end{lstlisting}

\section{Model Cards and Documentation}

Model cards provide structured documentation of ML models for transparency.

\subsection{ModelCard: Standardized Documentation}

\begin{lstlisting}[language=Python, caption={Model Card Generation}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)

@dataclass
class ModelCard:
    """
    Structured model documentation (Model Cards for Model Reporting).

    Based on: https://arxiv.org/abs/1810.03993

    Attributes:
        model_name: Model identifier
        model_version: Version number
        model_type: Type of model (e.g., "Random Forest")
        intended_use: Description of intended use case
        training_data: Training data description
        evaluation_data: Evaluation data description
        performance_metrics: Performance on test set
        fairness_metrics: Fairness evaluation results
        limitations: Known limitations
        recommendations: Usage recommendations
    """
    model_name: str
    model_version: str
    model_type: str
    intended_use: str
    training_data: Dict[str, Any]
    evaluation_data: Dict[str, Any]
    performance_metrics: Dict[str, float]
    fairness_metrics: Dict[str, Any]
    limitations: List[str]
    recommendations: List[str]
    created_date: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)
    model_owner: str = ""
    contact_info: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'model_details': {
                'name': self.model_name,
                'version': self.model_version,
                'type': self.model_type,
                'created_date': self.created_date.isoformat(),
                'last_updated': self.last_updated.isoformat(),
                'owner': self.model_owner,
                'contact': self.contact_info
            },
            'intended_use': {
                'description': self.intended_use,
            },
            'training_data': self.training_data,
            'evaluation_data': self.evaluation_data,
            'performance': self.performance_metrics,
            'fairness': self.fairness_metrics,
            'limitations': self.limitations,
            'recommendations': self.recommendations
        }

    def to_markdown(self) -> str:
        """Generate markdown documentation."""
        lines = [f"# Model Card: {self.model_name} v{self.model_version}"]
        lines.append("")

        # Model details
        lines.append("## Model Details")
        lines.append(f"- **Type**: {self.model_type}")
        lines.append(f"- **Version**: {self.model_version}")
        lines.append(f"- **Created**: {self.created_date.strftime('%Y-%m-%d')}")
        lines.append(f"- **Owner**: {self.model_owner}")
        lines.append("")

        # Intended use
        lines.append("## Intended Use")
        lines.append(self.intended_use)
        lines.append("")

        # Training data
        lines.append("## Training Data")
        for key, value in self.training_data.items():
            lines.append(f"- **{key}**: {value}")
        lines.append("")

        # Performance
        lines.append("## Performance Metrics")
        for metric, value in self.performance_metrics.items():
            lines.append(f"- **{metric}**: {value:.4f}")
        lines.append("")

        # Fairness
        lines.append("## Fairness Metrics")
        for key, value in self.fairness_metrics.items():
            lines.append(f"- **{key}**: {value}")
        lines.append("")

        # Limitations
        lines.append("## Limitations")
        for limitation in self.limitations:
            lines.append(f"- {limitation}")
        lines.append("")

        # Recommendations
        lines.append("## Recommendations")
        for rec in self.recommendations:
            lines.append(f"- {rec}")

        return "\n".join(lines)

    def save(self, output_path: str):
        """
        Save model card.

        Args:
            output_path: Output file path
        """
        from pathlib import Path

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Save as JSON
        json_path = output_path.with_suffix('.json')
        with open(json_path, 'w') as f:
            json.dump(self.to_dict(), f, indent=2, default=str)

        # Save as Markdown
        md_path = output_path.with_suffix('.md')
        with open(md_path, 'w') as f:
            f.write(self.to_markdown())

        logger.info(f"Model card saved to {output_path}")

def generate_model_card(
    model: Any,
    model_name: str,
    model_version: str,
    training_data: pd.DataFrame,
    test_data: pd.DataFrame,
    performance_metrics: Dict[str, float],
    fairness_results: List[FairnessResult]
) -> ModelCard:
    """
    Generate model card from model and data.

    Args:
        model: Trained model
        model_name: Model identifier
        model_version: Version number
        training_data: Training dataset
        test_data: Test dataset
        performance_metrics: Performance metrics
        fairness_results: Fairness evaluation results

    Returns:
        Generated model card
    """
    # Extract model type
    model_type = type(model).__name__

    # Training data summary
    training_summary = {
        'size': len(training_data),
        'features': list(training_data.columns),
        'date_range': 'Last 6 months',  # Would extract from data
        'sampling': 'Random sample'
    }

    # Evaluation data summary
    evaluation_summary = {
        'size': len(test_data),
        'split': 'Temporal holdout',
        'date_range': 'Last month'
    }

    # Fairness summary
    fairness_summary = {}
    for result in fairness_results:
        key = f"{result.metric_name}_{result.unprivileged_group}"
        fairness_summary[key] = {
            'score': result.score,
            'passed': result.is_fair
        }

    # Identify limitations
    limitations = []

    # Check for fairness issues
    unfair_results = [r for r in fairness_results if not r.is_fair]
    if unfair_results:
        limitations.append(
            f"Model exhibits bias in {len(unfair_results)} fairness metrics. "
            "Review required before deployment to sensitive applications."
        )

    # Check performance
    if performance_metrics.get('accuracy', 1.0) < 0.85:
        limitations.append(
            "Model accuracy below 85%. Consider additional feature engineering "
            "or alternative algorithms."
        )

    # Add standard limitations
    limitations.extend([
        "Model trained on historical data and may not reflect current patterns",
        "Performance may degrade on data distributions outside training range",
        "Regular retraining required to maintain performance"
    ])

    # Generate recommendations
    recommendations = [
        "Monitor model performance weekly for degradation",
        "Evaluate fairness metrics monthly across protected attributes",
        "Retrain model when performance drops below threshold",
        "Maintain audit trail of all predictions for regulatory compliance",
        "Provide explanations for all adverse decisions"
    ]

    return ModelCard(
        model_name=model_name,
        model_version=model_version,
        model_type=model_type,
        intended_use="Credit risk assessment for loan applications",
        training_data=training_summary,
        evaluation_data=evaluation_summary,
        performance_metrics=performance_metrics,
        fairness_metrics=fairness_summary,
        limitations=limitations,
        recommendations=recommendations,
        model_owner="Data Science Team",
        contact_info="ml-team@company.com"
    )
\end{lstlisting}

\section{Real-World Scenario: Biased Hiring Algorithm}

\subsection{The Problem}

A large tech company deployed a resume screening ML model to filter candidates:

\begin{itemize}
    \item Trained on 5 years of historical hiring data (2015-2020)
    \item Model achieved 88\% accuracy predicting "hired vs not hired"
    \item Deployed to screen 100,000 applications annually
\end{itemize}

After 6 months, an internal audit revealed:
\begin{itemize}
    \item Model recommended male candidates 2.3x more than females
    \item Penalized resumes mentioning "women's" organizations
    \item Favored candidates from specific universities (predominantly male)
    \item 73\% demographic parity violation for gender
    \item Legal exposure: potential \$15M class action lawsuit
\end{itemize}

\textbf{Root Causes}:
\begin{itemize}
    \item Historical data reflected biased hiring decisions
    \item No fairness evaluation before deployment
    \item No protected attribute testing
    \item No ongoing monitoring for bias
    \item No human oversight on automated decisions
\end{itemize}

\subsection{The Solution}

Complete ethics and governance framework:

\begin{lstlisting}[language=Python, caption={Comprehensive Ethics Implementation}]
# 1. Fairness Evaluation Before Deployment
evaluator = FairnessEvaluator(
    demographic_parity_threshold=0.8,
    equalized_odds_threshold=0.1,
    disparate_impact_threshold=0.8
)

# Test on historical data
fairness_results = evaluator.evaluate(
    y_true=y_test,
    y_pred=predictions,
    y_prob=probabilities,
    sensitive_features=test_data[['gender', 'race', 'university_tier']],
    metrics=[
        FairnessMetric.DEMOGRAPHIC_PARITY,
        FairnessMetric.EQUALIZED_ODDS,
        FairnessMetric.EQUAL_OPPORTUNITY
    ]
)

# Generate report
fairness_report = evaluator.generate_report(fairness_results)
print(fairness_report)

# Block deployment if unfair
if not all(r.is_fair for r in fairness_results):
    logger.error("Model fails fairness requirements")
    raise ValueError("Cannot deploy biased model")

# 2. Bias Mitigation
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Remove direct protected attributes
X_train_fair = X_train.drop(['gender', 'race', 'university_name'], axis=1)

# Remove proxy features
# e.g., 'sorority_experience' is proxy for gender
proxies = ['sorority_experience', 'military_service']
X_train_fair = X_train_fair.drop(proxies, axis=1)

# Retrain with fairness constraints
# Use reweighting or adversarial debiasing
from aif360.algorithms.preprocessing import Reweighing

reweighing = Reweighing(unprivileged_groups=[{'gender': 0}],
                       privileged_groups=[{'gender': 1}])

dataset_reweighted = reweighing.fit_transform(training_dataset)

# Train new model
model_fair = LogisticRegression()
model_fair.fit(X_train_fair, y_train,
               sample_weight=dataset_reweighted.instance_weights)

# Re-evaluate fairness
fairness_results_new = evaluator.evaluate(
    y_true=y_test,
    y_pred=model_fair.predict(X_test_fair),
    y_prob=model_fair.predict_proba(X_test_fair)[:, 1],
    sensitive_features=test_data[['gender', 'race']],
    metrics=[FairnessMetric.DEMOGRAPHIC_PARITY]
)

# 3. Model Interpretability
explainer = ModelExplainer(
    model=model_fair,
    background_data=X_train_fair
)

# Global feature importance
importance = explainer.feature_importance(X_test_fair, method="shap")
print("\nTop 10 Features:")
print(importance.head(10))

# Ensure protected attributes are not proxied
suspicious_features = ['university_tier', 'club_memberships']
for feature in suspicious_features:
    if feature in importance['feature'].values:
        rank = importance[importance['feature'] == feature].index[0] + 1
        if rank <= 10:
            logger.warning(
                f"Suspicious feature {feature} ranks #{rank}. "
                "May be proxy for protected attribute."
            )

# 4. Generate Model Card
model_card = generate_model_card(
    model=model_fair,
    model_name="resume_screening",
    model_version="v2.0_debiased",
    training_data=training_data,
    test_data=test_data,
    performance_metrics={
        'accuracy': 0.84,  # Slightly lower due to fairness constraints
        'precision': 0.81,
        'recall': 0.79,
        'f1': 0.80
    },
    fairness_results=fairness_results_new
)

# Add specific limitations
model_card.limitations.extend([
    "Model trained on historical data may still contain subtle biases",
    "Regular fairness audits required (quarterly minimum)",
    "Human review required for all screening decisions",
    "Model should not be sole decision-maker for hiring"
])

# Save model card
model_card.save("model_cards/resume_screening_v2")

# 5. Governance and Compliance
governance = GovernanceSystem()

compliance_results = governance.check_compliance(
    model=model_fair,
    data=training_data,
    fairness_results=fairness_results_new
)

compliance_report = governance.generate_compliance_report(compliance_results)
print(compliance_report)

if not governance.is_compliant(compliance_results):
    raise ValueError("Model not compliant - cannot deploy")

# 6. Ongoing Monitoring
from monitoring import ModelMonitor, MetricConfig, AlertSeverity

monitor = ModelMonitor("resume_screening_prod")

# Monitor fairness metrics in production
monitor.register_metric(MetricConfig(
    name="gender_demographic_parity",
    metric_type=MetricType.GAUGE,
    description="Demographic parity for gender",
    thresholds={
        AlertSeverity.WARNING: 0.85,
        AlertSeverity.CRITICAL: 0.80
    }
))

# Weekly fairness audit
def weekly_fairness_audit():
    """Run weekly fairness check on production data."""
    # Get last week's predictions
    prod_data = fetch_production_data(days=7)

    # Evaluate fairness
    results = evaluator.evaluate(
        y_true=prod_data['ground_truth'],
        y_pred=prod_data['predictions'],
        y_prob=prod_data['probabilities'],
        sensitive_features=prod_data[['gender', 'race']]
    )

    # Record metrics
    for result in results:
        if result.metric_name == 'demographic_parity':
            monitor.record_metric(
                f"{result.unprivileged_group}_demographic_parity",
                result.score
            )

    # Alert if violations
    if not all(r.is_fair for r in results):
        alert_ethics_team(results)

# Schedule weekly audits
import schedule
schedule.every().monday.at("09:00").do(weekly_fairness_audit)

# 7. Human-in-the-Loop
class HumanReviewQueue:
    """Queue system for human review of automated decisions."""

    def __init__(self):
        self.queue = []

    def add_for_review(
        self,
        application_id: str,
        prediction: int,
        confidence: float,
        reason: str
    ):
        """Add application to review queue."""
        self.queue.append({
            'application_id': application_id,
            'prediction': prediction,
            'confidence': confidence,
            'reason': reason,
            'timestamp': datetime.now()
        })

review_queue = HumanReviewQueue()

# Add low-confidence predictions to review
for idx, (pred, conf) in enumerate(zip(predictions, confidences)):
    if conf < 0.7:  # Low confidence threshold
        review_queue.add_for_review(
            application_id=application_ids[idx],
            prediction=pred,
            confidence=conf,
            reason="Low confidence prediction"
        )

# Add adverse decisions to review
for idx, pred in enumerate(predictions):
    if pred == 0:  # Rejection
        review_queue.add_for_review(
            application_id=application_ids[idx],
            prediction=pred,
            confidence=confidences[idx],
            reason="Adverse decision - requires human review"
        )

logger.info(f"{len(review_queue.queue)} applications queued for human review")
\end{lstlisting}

\subsection{Outcome}

With comprehensive ethics framework:
\begin{itemize}
    \item \textbf{Month 1}: Original model blocked by fairness evaluation
    \item \textbf{Month 2}: Debiased model deployed with 82\% demographic parity (vs 73\%)
    \item \textbf{Month 3}: Gender recommendation gap reduced from 2.3x to 1.15x
    \item \textbf{Month 6}: All fairness metrics consistently passing
    \item \textbf{Ongoing}: Quarterly fairness audits, human review for all rejections
    \item \textbf{Impact}: Avoided \$15M lawsuit, improved hiring diversity by 35\%
\end{itemize}

\section{Real-World Scenario: Credit Score Catastrophe}

\subsection{The Problem}

A major financial institution deployed an ML-based credit scoring system for loan approvals:

\begin{itemize}
    \item Model approved/rejected 500,000 loan applications annually
    \item 91\% accuracy predicting loan default risk
    \item Deployed across consumer lending, auto loans, mortgages
    \item No fairness evaluation before deployment
\end{itemize}

After 18 months, a ProPublica investigation revealed systemic discrimination:
\begin{itemize}
    \item \textbf{Racial Disparities}: Black applicants with identical credit scores to white applicants were denied 2.1x more frequently
    \item \textbf{Geographic Redlining}: Applicants from specific zip codes (predominantly minority) systematically denied regardless of qualifications
    \item \textbf{Proxy Features}: Model heavily weighted "neighborhood risk score" (79\% correlated with race)
    \item \textbf{False Positive Disparity}: False rejection rate for Black applicants: 43\% vs 23\% for white applicants
    \item \textbf{Financial Impact}: Estimated \$180M in wrongfully denied loans
    \item \textbf{Legal Exposure}: \$68M class action settlement + DOJ investigation
\end{itemize}

\subsection{Legal Analysis}

Multiple regulatory violations:

\textbf{Fair Credit Reporting Act (FCRA) § 615}:
\begin{itemize}
    \item Failed to provide adverse action notices explaining denial reasons
    \item No mechanism for consumers to contest automated decisions
    \item Penalty: \$1,000 per violation + attorney fees (500,000 applications × \$1,000 = \$500M potential exposure)
\end{itemize}

\textbf{Equal Credit Opportunity Act (ECOA) Regulation B}:
\begin{itemize}
    \item Prohibited discrimination based on race, color, religion, national origin
    \item Use of "neighborhood risk score" constituted proxy discrimination
    \item Penalty: Actual damages + punitive damages up to \$10,000 per violation
\end{itemize}

\textbf{Disparate Impact Under Fair Housing Act}:
\begin{itemize}
    \item 2.1x rejection rate disparity meets legal threshold for disparate impact
    \item Bank must prove business necessity (not established)
    \item Settlement: \$68M + 5 years monitoring
\end{itemize}

\subsection{Root Causes}

\textbf{Technical Failures}:
\begin{itemize}
    \item Training data contained historical discrimination (redlining from 1960s-1990s encoded in default patterns)
    \item No intersectional fairness testing (race × income × geography)
    \item Feature engineering created proxies for protected attributes
    \item No causal analysis of feature relationships with race
\end{itemize}

\textbf{Governance Failures}:
\begin{itemize}
    \item No ethics review board for high-stakes decision systems
    \item No legal review of ML system before deployment
    \item No ongoing fairness monitoring in production
    \item No FCRA adverse action notice integration
\end{itemize}

\subsection{The Solution}

Comprehensive remediation with regulatory oversight:

\begin{lstlisting}[language=Python, caption={Fair Credit Scoring Implementation}]
# 1. Intersectional Fairness Analysis
analyzer = IntersectionalFairnessAnalyzer(
    min_group_size=50,  # Larger for statistical power
    disparity_threshold=0.15  # Stricter threshold for credit
)

results = analyzer.analyze(
    y_true=y_test,
    y_pred=credit_decisions,
    sensitive_features=test_data[['race', 'ethnicity', 'zip_code_cluster']],
    max_intersections=3  # Test race x ethnicity x geography
)

print(analyzer.generate_report(results))

# Flag high-disparity groups
if results.disparate_groups:
    logger.error(
        f"Found {len(results.disparate_groups)} intersectional disparities"
    )

    for group1, group2, metric, diff in results.disparate_groups:
        if diff >= 0.20:  # 20% disparity triggers legal review
            logger.critical(
                f"LEGAL RISK: {group1} vs {group2} has {diff:.1%} "
                f"disparity in {metric}"
            )

# 2. Remove Proxy Features Using Causal Analysis
from causal_analysis import CausalGraph, find_proxy_features

# Build causal graph
causal_graph = CausalGraph()
causal_graph.add_edges([
    ('race', 'neighborhood_risk'),  # race causes neighborhood_risk
    ('race', 'zip_code'),
    ('income', 'loan_amount'),
    ('credit_history', 'default_risk')
])

# Identify proxy features (descendants of protected attributes)
proxy_features = find_proxy_features(
    causal_graph=causal_graph,
    protected_attrs=['race', 'ethnicity'],
    features=X_train.columns
)

logger.info(f"Identified proxy features: {proxy_features}")
# Output: ['neighborhood_risk', 'zip_code', 'school_district']

# Remove proxies from training
X_train_fair = X_train.drop(columns=proxy_features)
X_test_fair = X_test.drop(columns=proxy_features)

# 3. Fair Model with Adversarial Debiasing
from aif360.algorithms.inprocessing import AdversarialDebiasing
import tensorflow as tf

# Train model that maximizes accuracy while minimizing demographic disparity
debiased_model = AdversarialDebiasing(
    privileged_groups=[{'race': 1}],
    unprivileged_groups=[{'race': 0}],
    scope_name='debiased_classifier',
    debias=True,
    adversary_loss_weight=0.5  # Balance accuracy vs fairness
)

debiased_model.fit(aif_train_dataset)

# 4. FCRA Adverse Action Notices
def generate_adverse_action_notice(
    applicant_id: str,
    decision: str,
    credit_score: float,
    explanation: Dict[str, float]
) -> str:
    """
    Generate FCRA-compliant adverse action notice.

    Required by FCRA Section 615: Provide notice with reasons for adverse action.
    """
    top_reasons = sorted(
        explanation.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:4]  # FCRA requires "principal reasons"

    notice = f"""
ADVERSE ACTION NOTICE

Applicant: {applicant_id}
Decision: {decision}
Credit Score: {credit_score}

This notice is provided in compliance with the Fair Credit Reporting Act.

PRINCIPAL REASONS FOR ADVERSE ACTION:
"""

    for rank, (feature, impact) in enumerate(top_reasons, 1):
        notice += f"\n{rank}. {feature.replace('_', ' ').title()}"

    notice += """

YOUR RIGHTS UNDER FCRA:
- You have the right to a free copy of your credit report
- You have the right to dispute inaccurate information
- You have the right to add a statement to your credit file

TO DISPUTE THIS DECISION:
Email: lending-disputes@bank.com
Phone: 1-800-XXX-XXXX

You have 60 days from this notice to dispute this decision.
"""

    return notice.strip()

# Generate notice for all rejections
for idx, decision in enumerate(credit_decisions):
    if decision == 0:  # Rejection
        # Get SHAP explanation
        explanation = shap_values[idx]

        notice = generate_adverse_action_notice(
            applicant_id=applicant_ids[idx],
            decision="DECLINED",
            credit_score=credit_scores[idx],
            explanation=dict(zip(feature_names, explanation))
        )

        # Send notice (FCRA requires within 30 days)
        send_adverse_action_notice(applicant_ids[idx], notice)

# 5. Individual Fairness Check
individual_framework = IndividualFairnessFramework(
    fairness_threshold=1.2,  # Stricter for lending
    similarity_threshold=0.05
)

individual_results = individual_framework.evaluate(
    X=X_test_fair.values,
    y_pred=credit_scores,
    protected_indices=[
        X_test.columns.get_loc('race'),
        X_test.columns.get_loc('ethnicity')
    ]
)

# Flag individual fairness violations
if not individual_results.is_fair:
    logger.error(
        f"Individual fairness violation: Lipschitz constant = "
        f"{individual_results.lipschitz_constant:.2f} "
        f"(threshold: {individual_results.fairness_threshold})"
    )

    # Example: Two applicants with nearly identical profiles
    # but different races receiving vastly different scores
    if individual_results.violation_examples:
        for idx1, idx2, input_dist, output_dist in individual_results.violation_examples:
            logger.critical(
                f"Similar applicants {idx1} & {idx2}: "
                f"{input_dist:.3f} input distance but "
                f"{output_dist:.1f} credit score difference"
            )

# 6. Unified Compliance Framework
compliance = UnifiedComplianceFramework(
    applicable_regulations=[
        Regulation.FCRA,
        Regulation.ECOA,
        Regulation.GDPR  # If serving EU customers
    ]
)

compliance_results = compliance.comprehensive_compliance_check(
    model=debiased_model,
    data=X_train_fair,
    model_metadata={
        'has_adverse_action_notice': True,
        'explanation_method': 'SHAP',
        'fairness_tested': True,
        'intersectional_fairness_tested': True,
        'individual_fairness_tested': True
    }
)

print(compliance.generate_compliance_report(compliance_results))

if not compliance_results['compliant']:
    raise ValueError("Model fails regulatory compliance - cannot deploy")

# 7. Ongoing Monitoring with Legal Thresholds
def monthly_fairness_audit():
    """
    Monthly fairness audit with legal compliance thresholds.

    Monitors for disparate impact under ECOA:
    - 80% rule: Approval rate for protected group must be >= 80% of
      approval rate for reference group
    """
    prod_data = get_production_approvals(days=30)

    # Compute approval rates by race
    approval_rates = {}

    for race in prod_data['race'].unique():
        mask = prod_data['race'] == race
        approval_rate = prod_data.loc[mask, 'approved'].mean()
        approval_rates[race] = approval_rate

    # Check 80% rule
    reference_rate = approval_rates['white']

    for race, rate in approval_rates.items():
        if race == 'white':
            continue

        ratio = rate / reference_rate if reference_rate > 0 else 0

        if ratio < 0.80:
            logger.critical(
                f"LEGAL VIOLATION: {race} approval rate is {ratio:.1%} "
                f"of white approval rate (below 80% threshold)"
            )

            # Immediate escalation
            alert_legal_team(
                violation_type="ECOA_DISPARATE_IMPACT",
                protected_group=race,
                disparity_ratio=ratio,
                potential_penalty="Class action lawsuit + DOJ investigation"
            )

            # Freeze model deployments
            freeze_model_deployments(reason="ECOA_compliance_failure")

    return approval_rates

# Schedule monthly audits
import schedule
schedule.every().day.at("01:00").do(monthly_fairness_audit)
\end{lstlisting}

\subsection{Outcome}

With comprehensive fairness and compliance framework:
\begin{itemize}
    \item \textbf{Month 1-3}: Complete system audit, identified 47 proxy features
    \item \textbf{Month 4-6}: Retrained model without proxies, reduced false rejection disparity from 20pp to 3pp
    \item \textbf{Month 7}: Deployed debiased model under DOJ consent decree
    \item \textbf{Month 12}: Racial approval rate ratio improved from 0.48 to 0.88 (exceeds 80\% rule)
    \item \textbf{Month 18}: Independent audit confirms ECOA compliance
    \item \textbf{Total Cost}: \$68M settlement + \$12M remediation = \$80M
    \item \textbf{Prevented}: Additional \$500M FCRA penalties through adverse action notice compliance
\end{itemize}

\section{Real-World Scenario: Healthcare Equity Crisis}

\subsection{The Problem}

A major hospital system deployed an ML algorithm to prioritize patients for high-risk care management programs:

\begin{itemize}
    \item Algorithm scored 200,000 patients annually for enrollment in care programs
    \item Programs provided additional doctor visits, monitoring, preventive care
    \item 89\% accuracy predicting future healthcare costs
    \item Automatically enrolled top 10\% highest-risk patients
\end{itemize}

Science journal investigation (Obermeyer et al., 2019) revealed severe racial bias:
\begin{itemize}
    \item \textbf{Racial Disparity}: Black patients needed to be significantly sicker than white patients to receive same risk score
    \item \textbf{Proxy Label Bias}: Model predicted healthcare \textit{costs} (used as proxy for \textit{need}), but Black patients historically receive less care (lower costs) due to systemic barriers
    \item \textbf{Impact}: Only 17.7\% of patients enrolled in high-risk program were Black, vs 46.5\% if race-neutral
    \item \textbf{Harm}: Estimated 50,000 Black patients annually denied needed care
    \item \textbf{Legal Exposure}: \$125M class action + CMS investigation + HIPAA privacy violations
\end{itemize}

\subsection{Legal Analysis}

\textbf{Civil Rights Act Title VI (42 U.S.C. § 2000d)}:
\begin{itemize}
    \item Prohibits discrimination in federally funded programs (Medicare/Medicaid)
    \item Algorithm's disparate impact on Black patients violates Title VI
    \item Penalty: Loss of federal funding (\$2.1B annually) + damages
\end{itemize}

\textbf{HIPAA Privacy Rule (45 CFR § 164.502)}:
\begin{itemize}
    \item Failed to conduct required Privacy Impact Assessment for algorithm
    \item Used PHI without adequate safeguards for discrimination
    \item Penalty: Up to \$1.5M per violation category
\end{itemize}

\textbf{Affordable Care Act (ACA) § 1557}:
\begin{itemize}
    \item Prohibits discrimination in health programs
    \item Algorithm systematically excluded Black patients from care
    \item Penalty: Private right of action + injunctive relief
\end{itemize}

\subsection{Root Causes}

\textbf{Proxy Label Bias}:
\begin{itemize}
    \item Model trained to predict healthcare \textit{costs} as proxy for healthcare \textit{need}
    \item Assumption violated due to unequal access: Black patients receive less care (lower costs) even when equally sick
    \item Solution: Train on clinical outcomes, not costs
\end{itemize}

\textbf{Historical Inequity Encoded}:
\begin{itemize}
    \item Training data reflected decades of healthcare disparities
    \item Model learned that Black patients "cost less" and assigned lower risk scores
    \item Failed to account for structural barriers to care access
\end{itemize}

\subsection{The Solution}

\begin{lstlisting}[language=Python, caption={Fair Healthcare Risk Prediction}]
# 1. Replace Proxy Label with Clinical Outcomes
# OLD: Predict healthcare costs (biased proxy)
# NEW: Predict clinical outcomes (active chronic conditions, biomarkers)

def create_clinical_outcome_label(patient_data: pd.DataFrame) -> np.ndarray:
    """
    Create unbiased outcome label based on clinical indicators.

    Instead of costs, use:
    - Number of active chronic conditions
    - Biomarker abnormalities (HbA1c, blood pressure, etc.)
    - Prior hospitalizations for acute events
    - Functional status decline
    """
    outcome_score = (
        patient_data['num_chronic_conditions'] * 10 +
        patient_data['biomarker_risk_score'] * 5 +
        patient_data['prior_hospitalizations'] * 15 +
        patient_data['functional_decline'] * 8
    )

    return outcome_score.values

# Create new labels
y_train_clinical = create_clinical_outcome_label(train_data)
y_test_clinical = create_clinical_outcome_label(test_data)

# 2. Fairness-Constrained Training
from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient
from sklearn.ensemble import GradientBoostingRegressor

# Train model with equalized odds constraint
base_model = GradientBoostingRegressor()

# Ensure equal false positive and false negative rates across races
constraint = EqualizedOdds()

fair_model = ExponentiatedGradient(
    estimator=base_model,
    constraints=constraint
)

# Convert to binary classification for enrollment decision
y_train_binary = (y_train_clinical > np.percentile(y_train_clinical, 90)).astype(int)

fair_model.fit(
    X_train,
    y_train_binary,
    sensitive_features=train_data['race']
)

# 3. Intersectional Health Equity Analysis
equity_analyzer = IntersectionalFairnessAnalyzer(
    min_group_size=100,  # Larger for healthcare
    disparity_threshold=0.10  # Stricter for life-critical decisions
)

equity_results = equity_analyzer.analyze(
    y_true=y_test_binary,
    y_pred=fair_model.predict(X_test),
    sensitive_features=test_data[['race', 'ethnicity', 'insurance_type', 'income_bracket']],
    max_intersections=3
)

# Identify underserved populations
for group in equity_results.groups:
    if group.positive_rate < 0.10:  # Enrollment rate below 10%
        logger.warning(
            f"Underserved population: {group.group_name()} "
            f"(enrollment rate: {group.positive_rate:.1%})"
        )

# 4. HIPAA-Compliant Fairness Testing
hipaa_manager = HIPAAComplianceManager(
    covered_entity="Hospital System"
)

# Verify minimum necessary for fairness testing
phi_access = hipaa_manager.verify_minimum_necessary(
    requested_fields=['patient_id', 'race', 'ethnicity', 'diagnosis_codes',
                     'risk_score', 'enrollment_status'],
    purpose="ml_fairness_audit"
)

if not phi_access['compliant']:
    logger.error(f"HIPAA violation: {phi_access['denied_fields']}")

# Log all PHI access for audit
hipaa_manager.log_phi_access(
    user_id="ml_engineer_001",
    patient_id="all_test_patients",
    action="fairness_evaluation",
    fields_accessed=phi_access['approved_fields']
)

# 5. Counterfactual Fairness for Healthcare Decisions
def counterfactual_fairness_check(
    patient_data: pd.DataFrame,
    model: Any,
    protected_attr: str = 'race'
) -> Dict[str, float]:
    """
    Test if model decisions would change if protected attribute changed.

    Counterfactual fairness: P(Y_hat|X, A=0) = P(Y_hat|X, A=1)

    A model is counterfactually fair if changing only the protected
    attribute (e.g., race) doesn't change the prediction.
    """
    original_predictions = model.predict(patient_data)

    # Create counterfactual dataset by flipping protected attribute
    counterfactual_data = patient_data.copy()

    # Flip race (assuming binary encoding for simplicity)
    counterfactual_data[protected_attr] = 1 - counterfactual_data[protected_attr]

    counterfactual_predictions = model.predict(counterfactual_data)

    # Compare predictions
    same_decision = (original_predictions == counterfactual_predictions).mean()

    return {
        'counterfactual_consistency': same_decision,
        'race_dependent_decisions': 1 - same_decision
    }

cf_results = counterfactual_fairness_check(X_test, fair_model)

if cf_results['race_dependent_decisions'] > 0.05:
    logger.error(
        f"{cf_results['race_dependent_decisions']:.1%} of decisions "
        f"change based solely on race - violates Title VI"
    )

# 6. Health Equity Monitoring Dashboard
class HealthEquityMonitor:
    """Monitor health equity metrics in production."""

    def __init__(self):
        self.enrollment_history = []

    def log_enrollment(
        self,
        patient_id: str,
        risk_score: float,
        enrolled: bool,
        race: str,
        num_conditions: int
    ):
        """Log enrollment decision with demographics."""
        self.enrollment_history.append({
            'timestamp': datetime.now(),
            'patient_id': patient_id,
            'risk_score': risk_score,
            'enrolled': enrolled,
            'race': race,
            'num_conditions': num_conditions
        })

    def generate_equity_report(self) -> Dict[str, Any]:
        """Generate health equity report for CMS compliance."""
        df = pd.DataFrame(self.enrollment_history)

        # Enrollment rates by race
        enrollment_by_race = df.groupby('race')['enrolled'].agg(['mean', 'count'])

        # Average conditions by race for enrolled patients
        enrolled = df[df['enrolled']]
        conditions_by_race = enrolled.groupby('race')['num_conditions'].mean()

        # Disparity metrics
        white_enrollment = enrollment_by_race.loc['white', 'mean']
        disparities = {}

        for race in enrollment_by_race.index:
            if race == 'white':
                continue

            race_enrollment = enrollment_by_race.loc[race, 'mean']
            disparity = white_enrollment - race_enrollment

            disparities[race] = {
                'enrollment_rate': race_enrollment,
                'absolute_disparity': disparity,
                'relative_disparity': disparity / white_enrollment if white_enrollment > 0 else 0
            }

        return {
            'enrollment_by_race': enrollment_by_race.to_dict(),
            'conditions_by_race': conditions_by_race.to_dict(),
            'disparities': disparities
        }

monitor = HealthEquityMonitor()

# Log all enrollment decisions
for idx, (score, enrolled) in enumerate(zip(risk_scores, enrollment_decisions)):
    monitor.log_enrollment(
        patient_id=patient_ids[idx],
        risk_score=score,
        enrolled=enrolled,
        race=races[idx],
        num_conditions=conditions[idx]
    )

# Generate monthly equity report for CMS
equity_report = monitor.generate_equity_report()
submit_to_cms(equity_report)  # Required for Title VI compliance
\end{lstlisting}

\subsection{Outcome}

With clinical outcome-based model and fairness constraints:
\begin{itemize}
    \item \textbf{Month 1-6}: Rebuilt model using clinical outcomes instead of costs
    \item \textbf{Month 7}: Deployed fair model, Black patient enrollment increased from 17.7\% to 43.8\%
    \item \textbf{Month 12}: Racial disparity in enrollment eliminated (43.8\% Black vs 46.5\% race-neutral target)
    \item \textbf{Month 18}: 28,000 additional Black patients enrolled in care programs
    \item \textbf{Health Impact}: 12\% reduction in avoidable hospitalizations among newly enrolled patients
    \item \textbf{Cost Avoidance}: \$42M in prevented emergency care costs
    \item \textbf{Legal Resolution}: \$125M settlement + 10-year monitoring agreement
\end{itemize}

\section{Real-World Scenario: Insurance Algorithmic Redlining}

\subsection{The Problem}

A property insurance company deployed an ML model for pricing and underwriting homeowners insurance:

\begin{itemize}
    \item Model set premiums for 2 million policyholders annually
    \item Incorporated 500+ features including property, location, claims history
    \item 87\% accuracy predicting claims cost
    \item Reduced manual underwriting from 30 days to 2 hours
\end{itemize}

State insurance commissioner investigation revealed systematic discrimination:
\begin{itemize}
    \item \textbf{Geographic Discrimination}: Premiums in minority-majority zip codes averaged 60\% higher for equivalent homes
    \item \textbf{Proxy Redlining}: Model used "neighborhood risk score" (83\% correlated with racial composition)
    \item \textbf{Disparate Impact}: Black homeowners paid average \$2,100/year vs \$1,300/year for white homeowners with identical homes and claims history
    \item \textbf{Coverage Denial}: 2.8x higher denial rate in predominantly minority neighborhoods
    \item \textbf{Financial Harm}: \$340M in excess premiums charged to minority homeowners over 5 years
    \item \textbf{Legal Exposure}: \$156M settlement + license suspension threat
\end{itemize}

\subsection{Legal Analysis}

\textbf{Fair Housing Act (42 U.S.C. § 3605)}:
\begin{itemize}
    \item Prohibits discrimination in residential real estate-related transactions, including insurance
    \item Use of neighborhood risk score as proxy for race violates FHA
    \item Penalty: Unlimited compensatory damages + punitive damages + attorney fees
\end{itemize}

\textbf{Equal Credit Opportunity Act (ECOA)}:
\begin{itemize}
    \item Applies to insurance underwriting as credit decision
    \item 2.8x denial disparity constitutes prima facie discrimination
    \item Penalty: Actual damages + punitive up to \$10,000 per violation
\end{itemize}

\textbf{State Insurance Law (Unfair Discrimination)}:
\begin{itemize}
    \item Most states prohibit unfair discrimination in insurance rates
    \item Rate differences must be based on actuarially sound factors
    \item Geographic proxies for race not actuarially justified
    \item Penalty: License revocation + refunds + fines
\end{itemize}

\subsection{The Solution}

\begin{lstlisting}[language=Python, caption={Fair Insurance Pricing System}]
# 1. Identify and Remove Geographic Proxies
def identify_geographic_proxies(
    features: pd.DataFrame,
    protected_attr: str = 'zip_code_pct_minority'
) -> List[str]:
    """
    Identify features that act as proxies for protected attributes.

    A feature is a proxy if:
    1. Highly correlated with protected attribute (|r| > 0.70)
    2. Not independently predictive after controlling for protected attr
    """
    from scipy.stats import pearsonr

    proxies = []

    for col in features.columns:
        if col == protected_attr:
            continue

        # Compute correlation
        corr, p_value = pearsonr(features[col], features[protected_attr])

        if abs(corr) > 0.70 and p_value < 0.01:
            proxies.append((col, corr))
            logger.warning(f"Proxy detected: {col} (r={corr:.3f})")

    return [p[0] for p in proxies]

# Identify proxies
proxy_features = identify_geographic_proxies(
    features=X_train,
    protected_attr='zip_code_pct_minority'
)

# Remove proxies
logger.info(f"Removing {len(proxy_features)} proxy features: {proxy_features}")
X_train_fair = X_train.drop(columns=proxy_features)
X_test_fair = X_test.drop(columns=proxy_features)

# 2. Actuarial Fairness Constraints
def actuarial_fairness_loss(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    sensitive_feature: np.ndarray,
    lambda_fairness: float = 0.5
) -> float:
    """
    Custom loss combining actuarial accuracy with fairness.

    Loss = MSE(y_true, y_pred) + lambda * demographic_parity_penalty

    Ensures rates are based on risk while maintaining fairness across groups.
    """
    from sklearn.metrics import mean_squared_error

    # Actuarial accuracy (MSE of predicted vs actual claims)
    actuarial_loss = mean_squared_error(y_true, y_pred)

    # Demographic parity penalty (difference in average premiums)
    group_0_mean = y_pred[sensitive_feature == 0].mean()
    group_1_mean = y_pred[sensitive_feature == 1].mean()
    fairness_penalty = abs(group_0_mean - group_1_mean)

    total_loss = actuarial_loss + lambda_fairness * fairness_penalty

    return total_loss

# Train model with actuarial fairness
import torch
import torch.nn as nn

class FairPricingModel(nn.Module):
    """Neural network with fairness constraints for insurance pricing."""

    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 1)  # Premium prediction
        )

    def forward(self, x):
        return self.network(x)

# Train with fairness loss
model = FairPricingModel(input_dim=X_train_fair.shape[1])
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    optimizer.zero_grad()

    predictions = model(torch.tensor(X_train_fair.values, dtype=torch.float32))

    loss = actuarial_fairness_loss(
        y_true=y_train.values,
        y_pred=predictions.detach().numpy().flatten(),
        sensitive_feature=train_data['is_minority_zip'].values,
        lambda_fairness=0.5
    )

    loss.backward()
    optimizer.step()

# 3. Insurance Fairness Evaluation
def evaluate_insurance_fairness(
    premiums: np.ndarray,
    actual_claims: np.ndarray,
    sensitive_features: pd.DataFrame
) -> Dict[str, Any]:
    """
    Evaluate insurance fairness across multiple criteria.

    Insurance-specific fairness metrics:
    - Premium parity: Average premiums should not differ by protected class
      (when controlling for actual risk)
    - Actuarial soundness: Premiums should reflect actual claims cost
    - Coverage parity: Denial rates should not differ by protected class
    """
    results = {}

    # Premium parity
    for attr in sensitive_features.columns:
        unique_groups = sensitive_features[attr].unique()

        premiums_by_group = {}
        claims_by_group = {}
        loss_ratios = {}

        for group in unique_groups:
            mask = sensitive_features[attr] == group
            avg_premium = premiums[mask].mean()
            avg_claim = actual_claims[mask].mean()

            premiums_by_group[group] = avg_premium
            claims_by_group[group] = avg_claim
            loss_ratios[group] = avg_claim / avg_premium if avg_premium > 0 else 0

        # Check if loss ratios are similar (actuarially fair)
        loss_ratio_values = list(loss_ratios.values())
        loss_ratio_disparity = max(loss_ratio_values) - min(loss_ratio_values)

        # Check if premiums are similar after adjusting for risk
        # (premiums should differ only by actual claims experience)
        risk_adjusted_premiums = {
            group: premiums_by_group[group] / claims_by_group[group]
            if claims_by_group[group] > 0 else 0
            for group in unique_groups
        }

        rap_values = list(risk_adjusted_premiums.values())
        premium_disparity = max(rap_values) - min(rap_values) if rap_values else 0

        results[attr] = {
            'premiums_by_group': premiums_by_group,
            'claims_by_group': claims_by_group,
            'loss_ratios': loss_ratios,
            'loss_ratio_disparity': loss_ratio_disparity,
            'risk_adjusted_premium_disparity': premium_disparity,
            'actuarially_fair': loss_ratio_disparity < 0.10,  # 10% threshold
            'premium_fair': premium_disparity < 0.15  # 15% threshold
        }

    return results

insurance_fairness = evaluate_insurance_fairness(
    premiums=predicted_premiums,
    actual_claims=actual_claims_test,
    sensitive_features=test_data[['is_minority_zip', 'median_income_zip']]
)

for attr, metrics in insurance_fairness.items():
    if not metrics['actuarially_fair'] or not metrics['premium_fair']:
        logger.error(
            f"Insurance fairness violation for {attr}:\\n"
            f"  Loss ratio disparity: {metrics['loss_ratio_disparity']:.2f}\\n"
            f"  Premium disparity: {metrics['risk_adjusted_premium_disparity']:.2f}"
        )

# 4. State Regulatory Compliance
class InsuranceRegulatoryCompliance:
    """Ensure compliance with state insurance regulations."""

    def __init__(self, state: str):
        self.state = state
        self.rate_filing_history = []

    def validate_rate_factors(
        self,
        features_used: List[str],
        feature_coefficients: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        Validate that rate factors are actuarially justified.

        State insurance law requires:
        - Rates based on sound actuarial principles
        - Factors must be predictive of loss
        - Cannot use race, religion, national origin
        - Geographic factors must be narrowly tailored
        """
        prohibited_features = [
            'race', 'ethnicity', 'religion', 'national_origin',
            'neighborhood_racial_composition'
        ]

        violations = []

        for feature in features_used:
            if any(prohibited in feature.lower() for prohibited in prohibited_features):
                violations.append({
                    'feature': feature,
                    'violation': 'Prohibited discriminatory factor',
                    'severity': 'critical'
                })

        # Check for geographic proxies
        geographic_features = [f for f in features_used if 'zip' in f.lower() or 'neighborhood' in f.lower()]

        for feature in geographic_features:
            coef = feature_coefficients.get(feature, 0)

            if abs(coef) > 0.5:  # High weight on geographic feature
                violations.append({
                    'feature': feature,
                    'violation': 'Geographic factor may constitute redlining',
                    'severity': 'high',
                    'recommendation': 'Provide actuarial justification or remove'
                })

        return {
            'compliant': len(violations) == 0,
            'violations': violations,
            'state': self.state
        }

compliance = InsuranceRegulatoryCompliance(state="California")

rate_validation = compliance.validate_rate_factors(
    features_used=list(X_train_fair.columns),
    feature_coefficients=dict(zip(X_train_fair.columns, model.network[0].weight.data.mean(axis=0).numpy()))
)

if not rate_validation['compliant']:
    logger.error(f"Regulatory compliance violations: {rate_validation['violations']}")
    raise ValueError("Model violates state insurance regulations")
\end{lstlisting}

\subsection{Outcome}

With fair pricing model and actuarial justification:
\begin{itemize}
    \item \textbf{Month 1-3}: Removed 23 proxy features (neighborhood risk, school district, etc.)
    \item \textbf{Month 4-6}: Retrained model with actuarial fairness constraints
    \item \textbf{Month 7}: Filed new rates with state regulator, approved after actuarial review
    \item \textbf{Month 8}: Premium disparity reduced from 60\% to 8\% (within actuarial variance)
    \item \textbf{Month 12}: Denial rate disparity reduced from 2.8x to 1.1x
    \item \textbf{Refunds}: \$156M in excess premiums refunded to affected policyholders
    \item \textbf{Business Impact}: Maintained 87\% accuracy, expanded coverage in previously underserved areas
    \item \textbf{Regulatory Status}: License suspension threat lifted, 5-year monitoring agreement
\end{itemize}

\section{Exercises}

\subsection{Exercise 1: Comprehensive Fairness Evaluation}

Evaluate a credit scoring model across multiple fairness metrics:
\begin{itemize}
    \item Demographic parity
    \item Equalized odds
    \item Equal opportunity
    \item Disparate impact
    \item Predictive parity
\end{itemize}

Test against protected attributes: gender, race, age group. Generate report with recommendations.

\subsection{Exercise 2: Model Interpretability Dashboard}

Build interpretability dashboard showing:
\begin{itemize}
    \item Global feature importance (SHAP)
    \item Feature distributions and correlations
    \item Individual prediction explanations
    \item Counterfactual explanations
    \item Model decision boundaries
\end{itemize}

\subsection{Exercise 3: Bias Mitigation}

Implement three bias mitigation techniques:
\begin{itemize}
    \item Pre-processing: Reweighing or resampling
    \item In-processing: Adversarial debiasing
    \item Post-processing: Equalized odds post-processing
\end{itemize}

Compare performance and fairness trade-offs.

\subsection{Exercise 4: GDPR Compliance System}

Build GDPR compliance framework:
\begin{itemize}
    \item Right to explanation (local interpretability)
    \item Right to erasure (data deletion tracking)
    \item Data minimization validation
    \item Consent management
    \item Automated compliance reporting
\end{itemize}

\subsection{Exercise 5: Ethics Review Board System}

Implement ethics review workflow:
\begin{itemize}
    \item Risk assessment scoring
    \item Multi-stakeholder review process
    \item Approval/rejection with reasons
    \item Conditional approval with monitoring
    \item Appeal process
\end{itemize}

\subsection{Exercise 6: Audit Trail System}

Create comprehensive audit system:
\begin{itemize}
    \item Log all predictions with timestamps
    \item Store explanations for adverse decisions
    \item Track model versions and deployments
    \item Record fairness evaluation results
    \item Enable querying for regulatory audits
\end{itemize}

\subsection{Exercise 7: Fairness-Aware AutoML}

Build AutoML system that:
\begin{itemize}
    \item Tunes hyperparameters for accuracy AND fairness
    \item Searches over bias mitigation techniques
    \item Provides Pareto frontier of accuracy-fairness trade-offs
    \item Recommends model based on use case risk level
    \item Generates model cards automatically
\end{itemize}

\subsection{Exercise 8: Intersectional Fairness Analysis}

Implement intersectional fairness testing for a lending model:
\begin{itemize}
    \item Test fairness across race × gender × income intersections
    \item Identify subgroups with disparities > 20\%
    \item Compute statistical significance of disparities
    \item Generate visual heatmap of intersectional metrics
    \item Recommend interventions for affected subgroups
\end{itemize}

\textbf{Deliverable}: Intersectional fairness report with disparity matrix and remediation plan.

\subsection{Exercise 9: Individual Fairness with Lipschitz Constraints}

Evaluate individual fairness for insurance pricing model:
\begin{itemize}
    \item Implement Lipschitz constant estimation
    \item Find pairs of similar applicants with divergent predictions
    \item Learn fairness-aware similarity metric
    \item Measure stability of Lipschitz estimates across random seeds
    \item Compare individual vs group fairness violations
\end{itemize}

\textbf{Deliverable}: Individual fairness report with Lipschitz constant analysis and violation examples.

\subsection{Exercise 10: GDPR Data Protection Impact Assessment}

Conduct comprehensive DPIA for ML system processing personal data:
\begin{itemize}
    \item Identify DPIA triggers (automated decisions, special categories, large-scale)
    \item Document processing purposes and lawful basis
    \item Assess risks to rights and freedoms
    \item Design mitigation measures (encryption, minimization, anonymization)
    \item Determine if Data Protection Authority consultation required
\end{itemize}

\textbf{Deliverable}: Complete DPIA document with risk assessment and mitigation plan.

\subsection{Exercise 11: FCRA Adverse Action Notice System}

Build FCRA-compliant adverse action notice generator:
\begin{itemize}
    \item Extract top 4 reasons from SHAP explanations
    \item Generate notice with required FCRA disclosures
    \item Implement 60-day dispute window tracking
    \item Automate notice delivery within 30 days
    \item Log all notices for regulatory audit
\end{itemize}

\textbf{Deliverable}: Adverse action notice system with FCRA compliance verification.

\subsection{Exercise 12: Counterfactual Fairness Evaluation}

Test counterfactual fairness for hiring algorithm:
\begin{itemize}
    \item Implement counterfactual generation by flipping protected attributes
    \item Measure prediction changes when only race changes
    \item Build causal graph to identify causal vs spurious features
    \item Test counterfactual fairness across multiple protected attributes
    \item Quantify % of decisions dependent on protected attributes
\end{itemize}

\textbf{Deliverable}: Counterfactual fairness analysis with causal graph and violation metrics.

\subsection{Exercise 13: LIME Stability Analysis}

Implement stable LIME with confidence intervals:
\begin{itemize}
    \item Run LIME 20 times with different random seeds
    \item Compute Spearman rank correlation of feature rankings
    \item Calculate 95\% confidence intervals for feature weights
    \item Identify features with unstable explanations
    \item Compare LIME stability vs SHAP for same model
\end{itemize}

\textbf{Deliverable}: Stability report comparing LIME and SHAP with confidence intervals.

\subsection{Exercise 14: Transformer Attention Visualization}

Build attention analysis system for text classification model:
\begin{itemize}
    \item Extract attention weights from all layers and heads
    \item Visualize attention heatmaps for sample predictions
    \item Identify attention patterns (local, uniform, sparse)
    \item Detect head specialization across layers
    \item Correlate attention patterns with prediction accuracy
\end{itemize}

\textbf{Deliverable}: Attention analysis dashboard with pattern identification and visualizations.

\subsection{Exercise 15: Concept-Based Explanations with TCAV}

Implement TCAV for image classification model:
\begin{itemize}
    \item Define 5 human-interpretable concepts (e.g., "striped", "wooden")
    \item Collect positive and negative examples for each concept
    \item Learn Concept Activation Vectors (CAVs) using linear classifiers
    \item Compute TCAV scores with statistical significance testing
    \item Rank concepts by importance for target class
\end{itemize}

\textbf{Deliverable}: TCAV analysis report with significant concepts and sensitivity scores.

\subsection{Exercise 16: Model Distillation for Interpretability}

Distill ensemble model into interpretable decision tree:
\begin{itemize}
    \item Train decision tree to mimic ensemble predictions
    \item Measure fidelity (agreement with ensemble)
    \item Compute compression ratio (parameters reduced)
    \item Extract human-readable rules from tree
    \item Evaluate accuracy loss vs interpretability gain
\end{itemize}

\textbf{Deliverable}: Distillation report with fidelity analysis and interpretable rules.

\subsection{Exercise 17: Multi-Regulation Compliance Framework}

Build unified compliance system for financial ML model:
\begin{itemize}
    \item Implement compliance checkers for FCRA, ECOA, GDPR, and SOX
    \item Automate violation detection with severity classification
    \item Generate unified compliance report across all regulations
    \item Estimate potential fines for each violation type
    \item Design remediation roadmap with priorities
\end{itemize}

\textbf{Deliverable}: Unified compliance dashboard with violation tracking and remediation plan.

\subsection{Exercise 18: End-to-End Ethical AI Pipeline}

Implement complete ethical AI pipeline for high-stakes application:
\begin{itemize}
    \item Data Protection Impact Assessment (DPIA)
    \item Intersectional and individual fairness testing
    \item LIME + SHAP interpretability with stability analysis
    \item Multi-regulation compliance checking (GDPR, CCPA, HIPAA, FCRA)
    \item Model card generation with limitations and bias reporting
    \item Continuous fairness monitoring with alerting
    \item Human-in-the-loop review queue for adverse decisions
    \item Audit trail system with regulatory reporting
\end{itemize}

\textbf{Deliverable}: Production-ready ethical AI system with complete documentation and monitoring.

\section{Key Takeaways}

\subsection{Fairness and Bias}
\begin{itemize}
    \item \textbf{Test Intersectional Fairness}: Single-attribute fairness metrics miss discrimination affecting intersectional groups (e.g., Black women). Always test combinations of protected attributes with 3-way interactions.
    \item \textbf{Individual Fairness Matters}: Group fairness doesn't ensure similar individuals receive similar treatment. Implement Lipschitz constraints: $d_Y(f(x_1), f(x_2)) \leq L \cdot d_X(x_1, x_2)$. Target $L < 1.5$ for high-stakes decisions.
    \item \textbf{Proxy Features Are Everywhere}: Features like zip code, school district, and neighborhood scores often proxy for race. Use causal analysis to identify and remove proxies systematically.
    \item \textbf{Historical Bias Persists}: Training data encodes decades of discrimination. Healthcare costs underestimate Black patients' needs; credit histories reflect redlining. Question your labels.
    \item \textbf{Fairness-Accuracy Trade-offs}: Perfect fairness may reduce accuracy 2-5\%. Document trade-offs explicitly. In high-stakes domains (lending, healthcare, criminal justice), fairness is non-negotiable.
\end{itemize}

\subsection{Regulatory Compliance}
\begin{itemize}
    \item \textbf{GDPR Article 22 Requires Three Safeguards}: For automated decisions with legal effects: (1) human review, (2) meaningful explanation, (3) contestation mechanism. Failure to implement all three violates GDPR.
    \item \textbf{FCRA Adverse Action Notices Are Mandatory}: Every credit denial requires notice with principal reasons within 30 days. Violation penalty: \$1,000 per violation. Use SHAP to extract top 4 reasons automatically.
    \item \textbf{ECOA 80\% Rule for Disparate Impact}: Approval rate for protected group must be $\geq 80\%$ of reference group. Monitor monthly. Ratio $< 0.80$ triggers legal risk and potential DOJ investigation.
    \item \textbf{HIPAA Minimum Necessary Rule}: Only access PHI fields required for specific purpose. Document justification for fairness testing. Log all PHI access with timestamp, user, and purpose.
    \item \textbf{Conduct DPIA for High-Risk Processing}: Required when ML involves automated decisions + special categories + large-scale processing. High residual risk requires Data Protection Authority consultation before deployment.
\end{itemize}

\subsection{Interpretability}
\begin{itemize}
    \item \textbf{LIME is Unstable—Add Stability Analysis}: Standard LIME varies across runs due to sampling. Run 10+ times, compute Spearman correlation of rankings. Only trust explanations with correlation $> 0.7$.
    \item \textbf{SHAP for Global, LIME for Local}: SHAP provides stable global feature importance. Use LIME for local explanations when SHAP is too slow. Always report confidence intervals for LIME weights.
    \item \textbf{Attention $\neq$ Explanation}: High attention weights show what the model focuses on, not why. Combine attention visualization with gradient-based attribution for transformer interpretability.
    \item \textbf{Concept-Based Explanations for Non-Experts}: TCAV maps predictions to human concepts (e.g., "striped", "wooden") instead of features (e.g., pixel values). Use for stakeholder communication. Only trust concepts with $p < 0.05$.
    \item \textbf{Distillation Enables Interpretability}: Complex ensembles can be distilled into decision trees with $> 85\%$ fidelity. Extract human-readable rules for audit and regulatory review. Report compression ratio and fidelity explicitly.
\end{itemize}

\subsection{Governance and Monitoring}
\begin{itemize}
    \item \textbf{Document Everything with Model Cards}: Include training data, fairness metrics, limitations, intended use cases, and out-of-scope uses. Model cards are increasingly required for regulatory audits.
    \item \textbf{Automated Compliance Frameworks Scale}: Manually checking GDPR + CCPA + HIPAA + FCRA for every model doesn't scale. Build unified compliance framework with automated violation detection and severity classification.
    \item \textbf{Fairness Drifts in Production}: Models degrade over time. Implement continuous fairness monitoring with weekly/monthly audits. Alert when disparities exceed thresholds. Automate retraining triggers.
    \item \textbf{Human-in-the-Loop for High-Stakes Decisions}: Never fully automate decisions with legal effects (hiring, lending, healthcare). Queue adverse decisions for human review. GDPR Article 22 and FCRA § 615 require human oversight.
    \item \textbf{Audit Trails Are Non-Optional}: Log all predictions, explanations, fairness metrics, and compliance checks with timestamps. Regulators will request audit trails during investigations. Retention: 7+ years for financial services.
\end{itemize}

\subsection{Financial and Legal Risks}
\begin{itemize}
    \item \textbf{Bias Costs Millions}: Real-world examples: \$68M credit scoring settlement (ECOA), \$125M healthcare algorithm settlement (Title VI), \$156M insurance redlining (Fair Housing Act), \$15M hiring discrimination avoided.
    \item \textbf{GDPR Fines up to €20M or 4\% Revenue}: Violations of Article 22 (automated decisions) or Article 35 (no DPIA) trigger maximum fines. Amazon: €746M, Google: €50M. Compliance investment < fine magnitude.
    \item \textbf{FCRA Class Actions Are Common}: Every denial without adverse action notice is a potential \$1,000 violation. With 500K applications/year, exposure reaches \$500M. Implement automated notice generation.
    \item \textbf{Loss of Federal Funding for Title VI}: Healthcare systems violating Civil Rights Act Title VI risk losing Medicare/Medicaid funding. For large hospitals, this can exceed \$2B annually.
    \item \textbf{Prevention is Cheaper Than Remediation}: Proactive fairness testing costs \$50K-\$200K. Reactive litigation costs \$5M-\$200M (settlement + legal fees + remediation + monitoring). Invest early.
\end{itemize}

\subsection{Best Practices}
\begin{itemize}
    \item \textbf{Ethics Review Before Deployment}: Establish ethics review board for high-stakes ML systems. Include legal, technical, and domain experts. Require sign-off on fairness, interpretability, and compliance.
    \item \textbf{Red Team Your Models}: Proactively search for failure modes, bias, and adversarial examples before attackers or regulators do. Incentivize teams to find problems early.
    \item \textbf{Communicate Trade-offs Transparently}: Stakeholders need to understand accuracy-fairness trade-offs. Provide Pareto frontiers showing multiple model configurations. Document final choice with justification.
    \item \textbf{Start with High-Risk Use Cases}: Prioritize ethics/fairness work on systems with legal effects (lending, hiring, criminal justice, healthcare). Lower-risk systems (movie recommendations) can follow.
    \item \textbf{Continuous Learning}: Regulations evolve (EU AI Act, US algorithmic accountability bills). Fairness metrics expand (counterfactual, causal). Stay current through research, conferences, and legal counsel.
\end{itemize}

Ethics and governance are not constraints on ML—they are enablers that build trust, prevent harm, and ensure ML systems deliver value responsibly. The costs of ethical AI failures are enormous: financial (\$68M-\$200M+ settlements), reputational (ProPublica investigations), and regulatory (license suspension, loss of federal funding). Investing in comprehensive fairness testing, regulatory compliance, and interpretability protects both users and organizations. In high-stakes domains, ethical AI is not optional—it's the only sustainable path forward.
