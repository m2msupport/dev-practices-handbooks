\chapter{A/B Testing and Experimentation for ML}

\section{Introduction}

A/B testing validates whether a new ML model genuinely improves outcomes or merely optimizes for training metrics. A recommendation model with 92\% offline accuracy might decrease user engagement by 15\% in production. A fraud detection model with higher AUC might generate more false positives, damaging customer experience. The only way to measure real-world impact is rigorous experimentation.

However, traditional A/B testing—while essential—leaves value on the table. Fixed-allocation experiments assign 50\% of traffic to an inferior model throughout the entire test duration. If the new model improves conversion from 10\% to 12\%, half your users still experience the worse experience for weeks. This is where \textit{multi-armed bandit algorithms} provide a powerful alternative: they learn which variant performs best while automatically shifting traffic toward better-performing options, minimizing regret during the learning process.

\subsection{The A/B Testing Imperative}

Consider a ranking model that achieves 5\% higher NDCG in offline evaluation. The team deploys it to all users, celebrating the improvement. Two weeks later, revenue drops 8\% because the new model reduces product diversity, leading to browse abandonment. Proper A/B testing would have detected this before full deployment.

\subsection{Why ML A/B Testing is Different}

Traditional software A/B testing compares two static implementations. ML A/B testing introduces unique challenges:

\begin{itemize}
    \item \textbf{Model Uncertainty}: Predictions vary by confidence, requiring variance-aware analysis
    \item \textbf{Continuous Learning}: Models may update during experiments, affecting validity
    \item \textbf{Feature Dependencies}: Network effects cause user interactions to violate independence
    \item \textbf{Delayed Outcomes}: Labels arrive days/weeks after predictions (e.g., loan defaults)
    \item \textbf{Multiple Metrics}: Success requires balancing accuracy, latency, user satisfaction
    \item \textbf{Heterogeneous Effects}: Models perform differently across user segments
\end{itemize}

\subsection{The Exploration-Exploitation Dilemma}

At the heart of online experimentation lies a fundamental trade-off:

\begin{itemize}
    \item \textbf{Exploitation}: Serve the variant that appears best based on current data, maximizing short-term reward
    \item \textbf{Exploration}: Test variants with uncertain performance to gather information, potentially improving long-term outcomes
\end{itemize}

Traditional A/B testing uses pure exploration—fixed allocation regardless of observed performance—until the experiment concludes. This maximizes statistical validity but wastes traffic on inferior variants. Multi-armed bandit algorithms dynamically balance exploration and exploitation, reducing \textit{regret}: the opportunity cost of not always choosing the best arm.

\textbf{Regret} is formally defined as:

\begin{equation}
R_T = \sum_{t=1}^{T} \mu^* - \mu_{a_t}
\end{equation}

where $\mu^*$ is the mean reward of the optimal arm, $a_t$ is the arm chosen at time $t$, and $\mu_{a_t}$ is its mean reward. The goal of bandit algorithms is to minimize cumulative regret over time, achieving \textit{sublinear regret}: $R_T = o(T)$, meaning the average regret per trial vanishes as $T \to \infty$.

\subsection{The Cost of Poor Experimentation}

Industry evidence shows:
\begin{itemize}
    \item \textbf{70\% of A/B tests} are stopped before statistical significance
    \item \textbf{False positives} from poor design cost \$200K+ in wasted development
    \item \textbf{Sample size errors} extend tests by 2-3x, delaying launches
    \item \textbf{Network effects} cause 30\% of conclusions to reverse when accounted for
    \item \textbf{Fixed allocation} in A/B tests wastes 20-40\% of traffic on inferior variants
\end{itemize}

\subsection{Chapter Overview}

This chapter provides production-grade experimentation frameworks:

\begin{enumerate}
    \item \textbf{Experimental Design}: Randomization, stratification, and balance validation
    \item \textbf{Power Analysis}: Sample size calculation for desired sensitivity
    \item \textbf{Multi-Armed Bandits}: Continuous optimization with Thompson sampling and UCB
    \item \textbf{A/A Testing}: Validation of infrastructure and bias detection
    \item \textbf{Network Effects}: Cluster randomization and interference modeling
    \item \textbf{Statistical Analysis}: Proper testing with multiple comparison corrections
    \item \textbf{Sequential Testing}: Early stopping with controlled error rates
\end{enumerate}

\section{Experimental Design}

Rigorous experimental design ensures valid causal inference from A/B tests.

\subsection{ExperimentDesign: Randomization and Stratification}

\begin{lstlisting}[language=Python, caption={Comprehensive Experiment Design System}]
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable, Tuple
from enum import Enum
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import StandardScaler
import logging
import hashlib

logger = logging.getLogger(__name__)

class RandomizationMethod(Enum):
    """Methods for treatment assignment."""
    SIMPLE = "simple"  # Simple random assignment
    STRATIFIED = "stratified"  # Stratified by covariates
    BLOCKED = "blocked"  # Block randomization
    CLUSTER = "cluster"  # Cluster-level randomization
    COVARIATE_ADAPTIVE = "covariate_adaptive"  # Minimize imbalance

@dataclass
class TreatmentArm:
    """
    Experimental treatment arm.

    Attributes:
        name: Arm identifier
        allocation: Proportion of traffic (0-1)
        model_config: Configuration for this arm
        description: Human-readable description
    """
    name: str
    allocation: float
    model_config: Dict[str, Any]
    description: str = ""

    def __post_init__(self):
        """Validate allocation."""
        if not 0 <= self.allocation <= 1:
            raise ValueError(
                f"Allocation must be in [0, 1], got {self.allocation}"
            )

@dataclass
class ExperimentConfig:
    """
    Complete experiment configuration.

    Attributes:
        name: Experiment identifier
        arms: List of treatment arms
        randomization_method: Method for assignment
        stratification_vars: Variables for stratification
        cluster_var: Variable for cluster randomization
        min_sample_size: Minimum samples per arm
        max_duration_days: Maximum experiment duration
        metrics: Primary and secondary metrics to track
    """
    name: str
    arms: List[TreatmentArm]
    randomization_method: RandomizationMethod
    stratification_vars: Optional[List[str]] = None
    cluster_var: Optional[str] = None
    min_sample_size: int = 1000
    max_duration_days: int = 30
    metrics: Dict[str, str] = field(default_factory=dict)

    def __post_init__(self):
        """Validate configuration."""
        # Check allocations sum to 1
        total_allocation = sum(arm.allocation for arm in self.arms)
        if not np.isclose(total_allocation, 1.0):
            raise ValueError(
                f"Allocations must sum to 1.0, got {total_allocation}"
            )

        # Validate stratification requirements
        if (self.randomization_method == RandomizationMethod.STRATIFIED
            and not self.stratification_vars):
            raise ValueError(
                "Stratified randomization requires stratification_vars"
            )

        # Validate cluster requirements
        if (self.randomization_method == RandomizationMethod.CLUSTER
            and not self.cluster_var):
            raise ValueError(
                "Cluster randomization requires cluster_var"
            )

class ExperimentDesign:
    """
    Experimental design with proper randomization.

    Supports multiple randomization strategies with balance validation.

    Example:
        >>> design = ExperimentDesign(config)
        >>> assignments = design.assign_treatments(users_df)
        >>> balance_report = design.validate_balance(users_df, assignments)
    """

    def __init__(self, config: ExperimentConfig, seed: Optional[int] = None):
        """
        Initialize experiment design.

        Args:
            config: Experiment configuration
            seed: Random seed for reproducibility
        """
        self.config = config
        self.seed = seed or 42
        self.rng = np.random.RandomState(self.seed)

        # Track assignments
        self.assignments: Dict[str, str] = {}

        # Balance tracking
        self.balance_metrics: Dict[str, float] = {}

        logger.info(
            f"Initialized experiment: {config.name} "
            f"with {len(config.arms)} arms"
        )

    def assign_treatments(
        self,
        units: pd.DataFrame,
        unit_id_col: str = "user_id"
    ) -> pd.Series:
        """
        Assign treatments to experimental units.

        Args:
            units: DataFrame with experimental units
            unit_id_col: Column containing unit identifiers

        Returns:
            Series mapping unit_id to treatment arm
        """
        if self.config.randomization_method == RandomizationMethod.SIMPLE:
            assignments = self._simple_randomization(units, unit_id_col)
        elif self.config.randomization_method == RandomizationMethod.STRATIFIED:
            assignments = self._stratified_randomization(units, unit_id_col)
        elif self.config.randomization_method == RandomizationMethod.BLOCKED:
            assignments = self._blocked_randomization(units, unit_id_col)
        elif self.config.randomization_method == RandomizationMethod.CLUSTER:
            assignments = self._cluster_randomization(units, unit_id_col)
        else:  # COVARIATE_ADAPTIVE
            assignments = self._covariate_adaptive_randomization(
                units, unit_id_col
            )

        # Store assignments
        self.assignments.update(assignments.to_dict())

        logger.info(
            f"Assigned {len(assignments)} units to treatments. "
            f"Distribution: {assignments.value_counts().to_dict()}"
        )

        return assignments

    def _simple_randomization(
        self,
        units: pd.DataFrame,
        unit_id_col: str
    ) -> pd.Series:
        """
        Simple random assignment.

        Uses deterministic hashing for consistency across calls.
        """
        def assign_unit(unit_id: str) -> str:
            # Deterministic hash-based assignment
            hash_value = int(
                hashlib.md5(
                    f"{self.config.name}_{unit_id}".encode()
                ).hexdigest(),
                16
            )
            # Map to [0, 1]
            uniform = (hash_value % 1000000) / 1000000

            # Assign to arm based on allocation
            cumulative = 0.0
            for arm in self.config.arms:
                cumulative += arm.allocation
                if uniform < cumulative:
                    return arm.name

            # Fallback to last arm
            return self.config.arms[-1].name

        return units[unit_id_col].apply(assign_unit)

    def _stratified_randomization(
        self,
        units: pd.DataFrame,
        unit_id_col: str
    ) -> pd.Series:
        """
        Stratified randomization by covariates.

        Ensures balance within strata.
        """
        assignments = pd.Series(index=units.index, dtype=str)

        # Create strata
        strata_cols = self.config.stratification_vars
        strata = units.groupby(strata_cols)

        for stratum_key, stratum_df in strata:
            # Randomize within stratum
            stratum_assignments = self._simple_randomization(
                stratum_df,
                unit_id_col
            )
            assignments.loc[stratum_df.index] = stratum_assignments

        return assignments

    def _blocked_randomization(
        self,
        units: pd.DataFrame,
        unit_id_col: str
    ) -> pd.Series:
        """
        Block randomization for temporal balance.

        Divides units into blocks and balances within each.
        """
        block_size = 100  # Units per block
        n_arms = len(self.config.arms)

        assignments = []

        for start_idx in range(0, len(units), block_size):
            end_idx = min(start_idx + block_size, len(units))
            block = units.iloc[start_idx:end_idx]

            # Create balanced block
            block_assignments = []
            for arm in self.config.arms:
                n_in_arm = int(len(block) * arm.allocation)
                block_assignments.extend([arm.name] * n_in_arm)

            # Fill remaining with random arms
            while len(block_assignments) < len(block):
                arm = self.rng.choice(
                    self.config.arms,
                    p=[a.allocation for a in self.config.arms]
                )
                block_assignments.append(arm.name)

            # Shuffle block
            self.rng.shuffle(block_assignments)

            assignments.extend(block_assignments[:len(block)])

        return pd.Series(assignments, index=units.index)

    def _cluster_randomization(
        self,
        units: pd.DataFrame,
        unit_id_col: str
    ) -> pd.Series:
        """
        Cluster-level randomization.

        All units in a cluster get same treatment.
        """
        cluster_col = self.config.cluster_var

        # Get unique clusters
        clusters = units[cluster_col].unique()

        # Assign clusters to treatments
        cluster_assignments = {}
        for cluster in clusters:
            # Deterministic hash-based assignment
            hash_value = int(
                hashlib.md5(
                    f"{self.config.name}_{cluster}".encode()
                ).hexdigest(),
                16
            )
            uniform = (hash_value % 1000000) / 1000000

            cumulative = 0.0
            for arm in self.config.arms:
                cumulative += arm.allocation
                if uniform < cumulative:
                    cluster_assignments[cluster] = arm.name
                    break
            else:
                cluster_assignments[cluster] = self.config.arms[-1].name

        # Map units to cluster assignments
        return units[cluster_col].map(cluster_assignments)

    def _covariate_adaptive_randomization(
        self,
        units: pd.DataFrame,
        unit_id_col: str
    ) -> pd.Series:
        """
        Covariate-adaptive randomization (minimization).

        Assigns treatments to minimize imbalance in covariates.
        """
        assignments = pd.Series(index=units.index, dtype=str)

        # Standardize covariates
        covariate_cols = self.config.stratification_vars or []
        if not covariate_cols:
            # Fall back to simple randomization
            return self._simple_randomization(units, unit_id_col)

        scaler = StandardScaler()
        covariates_scaled = scaler.fit_transform(
            units[covariate_cols].fillna(0)
        )

        # Track arm statistics
        arm_stats = {
            arm.name: {
                'n': 0,
                'covariate_sums': np.zeros(len(covariate_cols))
            }
            for arm in self.config.arms
        }

        # Assign each unit
        for idx, (row_idx, row) in enumerate(units.iterrows()):
            unit_covariates = covariates_scaled[idx]

            # Compute imbalance for each arm
            imbalances = {}
            for arm in self.config.arms:
                # Compute imbalance if assigned to this arm
                new_n = arm_stats[arm.name]['n'] + 1
                new_sums = (
                    arm_stats[arm.name]['covariate_sums'] + unit_covariates
                )
                new_means = new_sums / new_n

                # Compare with other arms
                max_diff = 0.0
                for other_arm in self.config.arms:
                    if other_arm.name == arm.name:
                        continue

                    if arm_stats[other_arm.name]['n'] > 0:
                        other_means = (
                            arm_stats[other_arm.name]['covariate_sums']
                            / arm_stats[other_arm.name]['n']
                        )
                        diff = np.abs(new_means - other_means).sum()
                        max_diff = max(max_diff, diff)

                imbalances[arm.name] = max_diff

            # Choose arm with minimum imbalance
            # With some randomness (80% minimize, 20% random)
            if self.rng.random() < 0.8:
                assigned_arm = min(imbalances, key=imbalances.get)
            else:
                assigned_arm = self.rng.choice(
                    [arm.name for arm in self.config.arms],
                    p=[arm.allocation for arm in self.config.arms]
                )

            assignments.loc[row_idx] = assigned_arm

            # Update statistics
            arm_stats[assigned_arm]['n'] += 1
            arm_stats[assigned_arm]['covariate_sums'] += unit_covariates

        return assignments

    def validate_balance(
        self,
        units: pd.DataFrame,
        assignments: pd.Series,
        covariates: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Validate balance across treatment arms.

        Args:
            units: DataFrame with unit characteristics
            assignments: Treatment assignments
            covariates: List of covariates to check

        Returns:
            Dictionary with balance metrics
        """
        # Merge assignments with units
        data = units.copy()
        data['treatment'] = assignments

        # Use stratification vars if covariates not specified
        if covariates is None:
            covariates = self.config.stratification_vars or []

        if not covariates:
            logger.warning("No covariates specified for balance check")
            return {}

        balance_results = {}

        for covariate in covariates:
            if covariate not in data.columns:
                logger.warning(f"Covariate {covariate} not found")
                continue

            # Test balance
            arm_groups = data.groupby('treatment')[covariate]

            # Check if continuous or categorical
            if pd.api.types.is_numeric_dtype(data[covariate]):
                # Continuous: use ANOVA
                groups = [
                    group.dropna()
                    for name, group in arm_groups
                ]

                if len(groups) >= 2 and all(len(g) > 0 for g in groups):
                    f_stat, p_value = stats.f_oneway(*groups)

                    balance_results[covariate] = {
                        'type': 'continuous',
                        'means': arm_groups.mean().to_dict(),
                        'stds': arm_groups.std().to_dict(),
                        'f_statistic': f_stat,
                        'p_value': p_value,
                        'balanced': p_value > 0.05
                    }
            else:
                # Categorical: use chi-square
                contingency = pd.crosstab(
                    data['treatment'],
                    data[covariate]
                )

                chi2, p_value, dof, expected = stats.chi2_contingency(
                    contingency
                )

                balance_results[covariate] = {
                    'type': 'categorical',
                    'distributions': contingency.to_dict(),
                    'chi2_statistic': chi2,
                    'p_value': p_value,
                    'balanced': p_value > 0.05
                }

        # Overall balance score
        p_values = [
            result['p_value']
            for result in balance_results.values()
            if 'p_value' in result
        ]

        if p_values:
            # Minimum p-value indicates worst imbalance
            balance_results['overall'] = {
                'min_p_value': min(p_values),
                'all_balanced': all(
                    result.get('balanced', True)
                    for result in balance_results.values()
                ),
                'n_covariates': len(p_values)
            }

        self.balance_metrics = balance_results

        return balance_results

    def get_assignment(self, unit_id: str) -> Optional[str]:
        """
        Get treatment assignment for a unit.

        Args:
            unit_id: Unit identifier

        Returns:
            Treatment arm name or None if not assigned
        """
        return self.assignments.get(unit_id)
\end{lstlisting}

\subsection{Balance Validation in Practice}

\begin{lstlisting}[language=Python, caption={Validating Experimental Balance}]
# Define experiment
config = ExperimentConfig(
    name="model_v2_test",
    arms=[
        TreatmentArm(
            name="control",
            allocation=0.5,
            model_config={"model_version": "v1"},
            description="Current production model"
        ),
        TreatmentArm(
            name="treatment",
            allocation=0.5,
            model_config={"model_version": "v2"},
            description="New model with additional features"
        )
    ],
    randomization_method=RandomizationMethod.STRATIFIED,
    stratification_vars=["country", "user_segment"],
    min_sample_size=10000,
    metrics={
        "primary": "conversion_rate",
        "secondary": "revenue_per_user"
    }
)

# Create design
design = ExperimentDesign(config, seed=42)

# Assign treatments
assignments = design.assign_treatments(users_df, unit_id_col="user_id")

# Validate balance
balance_report = design.validate_balance(
    users_df,
    assignments,
    covariates=["age", "tenure_days", "country", "user_segment"]
)

# Check balance
print("Balance Validation Results:")
for covariate, result in balance_report.items():
    if covariate == "overall":
        continue

    print(f"\n{covariate}:")
    print(f"  Type: {result['type']}")
    print(f"  P-value: {result['p_value']:.4f}")
    print(f"  Balanced: {result['balanced']}")

    if result['type'] == 'continuous':
        print(f"  Means by arm: {result['means']}")

if 'overall' in balance_report:
    overall = balance_report['overall']
    print(f"\nOverall Balance:")
    print(f"  All balanced: {overall['all_balanced']}")
    print(f"  Minimum p-value: {overall['min_p_value']:.4f}")

# Flag for imbalance
if not balance_report.get('overall', {}).get('all_balanced', True):
    logger.warning("Imbalance detected - consider re-randomization")
\end{lstlisting}

\section{Statistical Power Analysis}

Power analysis determines required sample size for detecting meaningful effects.

\subsection{StatisticalPowerAnalyzer: Sample Size Calculation}

\begin{lstlisting}[language=Python, caption={Comprehensive Power Analysis}]
from typing import Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
from scipy import stats
import logging

logger = logging.getLogger(__name__)

class MetricType(Enum):
    """Types of metrics for power analysis."""
    CONTINUOUS = "continuous"  # Mean-based metrics
    PROPORTION = "proportion"  # Conversion rates
    COUNT = "count"  # Event counts
    TIME_TO_EVENT = "time_to_event"  # Survival analysis

@dataclass
class PowerAnalysisResult:
    """
    Result of power analysis.

    Attributes:
        sample_size_per_arm: Required samples per treatment arm
        total_sample_size: Total samples needed
        power: Statistical power achieved
        alpha: Significance level
        effect_size: Detectable effect size
        metric_type: Type of metric
        assumptions: Assumptions used in calculation
    """
    sample_size_per_arm: int
    total_sample_size: int
    power: float
    alpha: float
    effect_size: float
    metric_type: MetricType
    assumptions: Dict[str, Any]

class StatisticalPowerAnalyzer:
    """
    Calculate statistical power and required sample sizes.

    Supports multiple metric types and accounts for practical
    constraints like allocation ratios and expected uplift.

    Example:
        >>> analyzer = StatisticalPowerAnalyzer(
        ...     alpha=0.05,
        ...     power=0.80
        ... )
        >>> result = analyzer.calculate_sample_size(
        ...     metric_type=MetricType.PROPORTION,
        ...     baseline_value=0.10,
        ...     mde=0.01  # 1pp absolute increase
        ... )
        >>> print(f"Need {result.sample_size_per_arm} per arm")
    """

    def __init__(
        self,
        alpha: float = 0.05,
        power: float = 0.80,
        n_arms: int = 2,
        allocation_ratio: Optional[List[float]] = None
    ):
        """
        Initialize power analyzer.

        Args:
            alpha: Significance level (Type I error rate)
            power: Desired statistical power (1 - Type II error)
            n_arms: Number of treatment arms
            allocation_ratio: Traffic allocation per arm
        """
        self.alpha = alpha
        self.power = power
        self.n_arms = n_arms
        self.allocation_ratio = allocation_ratio or [1/n_arms] * n_arms

        if not np.isclose(sum(self.allocation_ratio), 1.0):
            raise ValueError("Allocation ratios must sum to 1.0")

        logger.info(
            f"Initialized PowerAnalyzer: "
            f"alpha={alpha}, power={power}, arms={n_arms}"
        )

    def calculate_sample_size(
        self,
        metric_type: MetricType,
        baseline_value: float,
        mde: float,
        baseline_variance: Optional[float] = None,
        alternative: str = "two-sided"
    ) -> PowerAnalysisResult:
        """
        Calculate required sample size.

        Args:
            metric_type: Type of metric
            baseline_value: Baseline metric value (control arm)
            mde: Minimum detectable effect (absolute)
            baseline_variance: Variance of metric (for continuous)
            alternative: "two-sided" or "one-sided"

        Returns:
            Power analysis result with sample size
        """
        if metric_type == MetricType.CONTINUOUS:
            result = self._power_continuous(
                baseline_value,
                mde,
                baseline_variance,
                alternative
            )
        elif metric_type == MetricType.PROPORTION:
            result = self._power_proportion(
                baseline_value,
                mde,
                alternative
            )
        elif metric_type == MetricType.COUNT:
            result = self._power_count(
                baseline_value,
                mde,
                alternative
            )
        else:  # TIME_TO_EVENT
            result = self._power_survival(
                baseline_value,
                mde,
                alternative
            )

        logger.info(
            f"Power analysis complete: "
            f"{result.sample_size_per_arm} per arm, "
            f"{result.total_sample_size} total"
        )

        return result

    def _power_continuous(
        self,
        baseline_mean: float,
        mde: float,
        baseline_std: Optional[float],
        alternative: str
    ) -> PowerAnalysisResult:
        """
        Power analysis for continuous metrics (t-test).

        Uses formula: n = 2 * (z_alpha + z_beta)^2 * sigma^2 / delta^2
        """
        if baseline_std is None:
            # Assume coefficient of variation = 1
            baseline_std = abs(baseline_mean)

        # Z-scores for alpha and power
        z_alpha = stats.norm.ppf(
            1 - self.alpha / (2 if alternative == "two-sided" else 1)
        )
        z_beta = stats.norm.ppf(self.power)

        # Effect size (Cohen's d)
        effect_size = mde / baseline_std

        # Sample size per arm
        n_per_arm = 2 * ((z_alpha + z_beta) / effect_size) ** 2

        # Adjust for allocation ratio
        # For unequal allocation: n1 = n * r / (1+r), n2 = n / (1+r)
        # where r = n1/n2
        if len(self.allocation_ratio) == 2:
            ratio = self.allocation_ratio[1] / self.allocation_ratio[0]
            n_control = n_per_arm * ratio / (1 + ratio)
            n_treatment = n_per_arm / (1 + ratio)
            n_per_arm = max(n_control, n_treatment)

        n_per_arm = int(np.ceil(n_per_arm))
        total_n = n_per_arm * self.n_arms

        return PowerAnalysisResult(
            sample_size_per_arm=n_per_arm,
            total_sample_size=total_n,
            power=self.power,
            alpha=self.alpha,
            effect_size=effect_size,
            metric_type=MetricType.CONTINUOUS,
            assumptions={
                'baseline_mean': baseline_mean,
                'baseline_std': baseline_std,
                'mde': mde,
                'alternative': alternative
            }
        )

    def _power_proportion(
        self,
        baseline_rate: float,
        mde: float,
        alternative: str
    ) -> PowerAnalysisResult:
        """
        Power analysis for proportion metrics (conversion rates).

        Uses pooled proportion for variance estimation.
        """
        # Treatment rate
        treatment_rate = baseline_rate + mde

        # Pooled proportion
        pooled_p = (baseline_rate + treatment_rate) / 2

        # Pooled standard deviation
        pooled_std = np.sqrt(2 * pooled_p * (1 - pooled_p))

        # Z-scores
        z_alpha = stats.norm.ppf(
            1 - self.alpha / (2 if alternative == "two-sided" else 1)
        )
        z_beta = stats.norm.ppf(self.power)

        # Sample size per arm
        n_per_arm = ((z_alpha + z_beta) * pooled_std / mde) ** 2
        n_per_arm = int(np.ceil(n_per_arm))

        total_n = n_per_arm * self.n_arms

        # Effect size (h - Cohen's h for proportions)
        effect_size = 2 * (
            np.arcsin(np.sqrt(treatment_rate))
            - np.arcsin(np.sqrt(baseline_rate))
        )

        return PowerAnalysisResult(
            sample_size_per_arm=n_per_arm,
            total_sample_size=total_n,
            power=self.power,
            alpha=self.alpha,
            effect_size=effect_size,
            metric_type=MetricType.PROPORTION,
            assumptions={
                'baseline_rate': baseline_rate,
                'treatment_rate': treatment_rate,
                'mde': mde,
                'alternative': alternative
            }
        )

    def _power_count(
        self,
        baseline_rate: float,
        mde: float,
        alternative: str
    ) -> PowerAnalysisResult:
        """
        Power analysis for count metrics (Poisson).

        Assumes Poisson distribution for event counts.
        """
        treatment_rate = baseline_rate + mde

        # For Poisson: variance = mean
        pooled_var = (baseline_rate + treatment_rate) / 2

        # Z-scores
        z_alpha = stats.norm.ppf(
            1 - self.alpha / (2 if alternative == "two-sided" else 1)
        )
        z_beta = stats.norm.ppf(self.power)

        # Sample size per arm
        n_per_arm = 2 * ((z_alpha + z_beta) ** 2) * pooled_var / (mde ** 2)
        n_per_arm = int(np.ceil(n_per_arm))

        total_n = n_per_arm * self.n_arms

        # Effect size
        effect_size = mde / np.sqrt(pooled_var)

        return PowerAnalysisResult(
            sample_size_per_arm=n_per_arm,
            total_sample_size=total_n,
            power=self.power,
            alpha=self.alpha,
            effect_size=effect_size,
            metric_type=MetricType.COUNT,
            assumptions={
                'baseline_rate': baseline_rate,
                'treatment_rate': treatment_rate,
                'mde': mde,
                'alternative': alternative
            }
        )

    def _power_survival(
        self,
        baseline_hazard: float,
        hazard_ratio: float,
        alternative: str
    ) -> PowerAnalysisResult:
        """
        Power analysis for time-to-event metrics.

        Uses log-rank test assumptions.
        """
        # Log hazard ratio
        log_hr = np.log(hazard_ratio)

        # Z-scores
        z_alpha = stats.norm.ppf(
            1 - self.alpha / (2 if alternative == "two-sided" else 1)
        )
        z_beta = stats.norm.ppf(self.power)

        # Number of events needed
        n_events = 4 * ((z_alpha + z_beta) / log_hr) ** 2

        # Convert to sample size (assumes ~70% event rate)
        event_rate = 0.7
        n_per_arm = int(np.ceil(n_events / (2 * event_rate)))

        total_n = n_per_arm * self.n_arms

        return PowerAnalysisResult(
            sample_size_per_arm=n_per_arm,
            total_sample_size=total_n,
            power=self.power,
            alpha=self.alpha,
            effect_size=abs(log_hr),
            metric_type=MetricType.TIME_TO_EVENT,
            assumptions={
                'baseline_hazard': baseline_hazard,
                'hazard_ratio': hazard_ratio,
                'assumed_event_rate': event_rate,
                'alternative': alternative
            }
        )

    def sensitivity_analysis(
        self,
        metric_type: MetricType,
        baseline_value: float,
        mde_range: List[float],
        baseline_variance: Optional[float] = None
    ) -> pd.DataFrame:
        """
        Sensitivity analysis across different effect sizes.

        Args:
            metric_type: Type of metric
            baseline_value: Baseline metric value
            mde_range: Range of MDEs to test
            baseline_variance: Variance (for continuous)

        Returns:
            DataFrame with sample sizes for each MDE
        """
        results = []

        for mde in mde_range:
            result = self.calculate_sample_size(
                metric_type=metric_type,
                baseline_value=baseline_value,
                mde=mde,
                baseline_variance=baseline_variance
            )

            results.append({
                'mde': mde,
                'relative_lift': mde / baseline_value,
                'sample_size_per_arm': result.sample_size_per_arm,
                'total_sample_size': result.total_sample_size,
                'effect_size': result.effect_size
            })

        return pd.DataFrame(results)
\end{lstlisting}

\subsection{Sample Size Calculation in Practice}

\begin{lstlisting}[language=Python, caption={Practical Power Analysis}]
# Initialize analyzer
analyzer = StatisticalPowerAnalyzer(
    alpha=0.05,  # 5% significance level
    power=0.80,  # 80% power
    n_arms=2
)

# Example 1: Conversion rate improvement
conversion_result = analyzer.calculate_sample_size(
    metric_type=MetricType.PROPORTION,
    baseline_value=0.10,  # 10% baseline conversion
    mde=0.01  # Want to detect 1pp increase (10% -> 11%)
)

print(f"Conversion Rate Test:")
print(f"  Baseline: 10%")
print(f"  MDE: 1pp (10% relative lift)")
print(f"  Sample size per arm: {conversion_result.sample_size_per_arm:,}")
print(f"  Total sample size: {conversion_result.total_sample_size:,}")

# Example 2: Revenue per user (continuous)
revenue_result = analyzer.calculate_sample_size(
    metric_type=MetricType.CONTINUOUS,
    baseline_value=50.0,  # $50 baseline
    mde=2.5,  # Want to detect $2.5 increase (5% lift)
    baseline_variance=625.0  # std = $25
)

print(f"\nRevenue per User Test:")
print(f"  Baseline: $50")
print(f"  MDE: $2.5 (5% lift)")
print(f"  Sample size per arm: {revenue_result.sample_size_per_arm:,}")

# Example 3: Sensitivity analysis
print("\nSensitivity Analysis for Conversion Rate:")
mde_range = [0.005, 0.01, 0.015, 0.02]  # 0.5pp to 2pp
sensitivity_df = analyzer.sensitivity_analysis(
    metric_type=MetricType.PROPORTION,
    baseline_value=0.10,
    mde_range=mde_range
)

print(sensitivity_df.to_string(index=False))

# Example 4: Multiple metrics with Bonferroni correction
# Testing 3 metrics, adjust alpha
n_metrics = 3
bonferroni_alpha = 0.05 / n_metrics

analyzer_bonferroni = StatisticalPowerAnalyzer(
    alpha=bonferroni_alpha,
    power=0.80
)

adjusted_result = analyzer_bonferroni.calculate_sample_size(
    metric_type=MetricType.PROPORTION,
    baseline_value=0.10,
    mde=0.01
)

print(f"\nWith Bonferroni correction for {n_metrics} metrics:")
print(f"  Adjusted alpha: {bonferroni_alpha:.4f}")
print(f"  Sample size per arm: {adjusted_result.sample_size_per_arm:,}")
print(f"  Increase vs. single metric: "
      f"{(adjusted_result.sample_size_per_arm / conversion_result.sample_size_per_arm - 1):.1%}")
\end{lstlisting}

\section{Multi-Armed Bandits}

Multi-armed bandits provide an elegant solution to the exploration-exploitation trade-off, dynamically allocating traffic to maximize cumulative reward while learning which variant performs best. Unlike traditional A/B testing with fixed allocation, bandits adapt in real-time based on observed performance.

\subsection{Real-World Scenario: The Exploration vs Exploitation Dilemma in Product Recommendations}

\subsubsection{The Challenge}

A major e-commerce platform operates a recommendation system suggesting products on the homepage. The data science team has developed three competing algorithms:

\begin{itemize}
    \item \textbf{Algorithm A (Baseline)}: Collaborative filtering with known 8.5\% click-through rate
    \item \textbf{Algorithm B}: Matrix factorization with uncertain performance (limited A/B test data)
    \item \textbf{Algorithm C}: Deep neural network, completely new, no production data
\end{itemize}

\textbf{The Business Problem:} The platform serves 10 million recommendations daily. Each 0.1\% improvement in CTR generates \$50,000 monthly revenue. However, testing with traditional A/B testing poses challenges:

\begin{enumerate}
    \item \textbf{Opportunity Cost}: With 33\% traffic to each variant, if Algorithm C actually achieves 10\% CTR (1.5pp improvement), running a 4-week A/B test costs \$200,000 in lost revenue from traffic wasted on inferior variants
    \item \textbf{Statistical Certainty vs Speed}: Waiting for 95\% confidence requires large samples, delaying launch and prolonging inferior experience for users
    \item \textbf{Unknown Best}: Without prior data on B and C, the team doesn't know if the test will even find a winner
\end{enumerate}

\subsubsection{The Multi-Armed Bandit Solution}

Instead of fixed allocation, the team implements Thompson Sampling, which:

\begin{enumerate}
    \item Starts with equal uncertainty about all three algorithms
    \item Gradually shifts traffic toward better-performing algorithms as data accumulates
    \item Maintains exploration of promising but uncertain options
    \item Minimizes regret by reducing traffic to clearly inferior variants
\end{enumerate}

\textbf{Week 1 Results:}
\begin{itemize}
    \item Algorithm A: 8.5\% CTR (15,000 impressions)
    \item Algorithm B: 7.2\% CTR (8,000 impressions) — clearly worse, traffic reduced
    \item Algorithm C: 9.8\% CTR (12,000 impressions) — promising, traffic increasing
\end{itemize}

\textbf{Week 4 Final Allocation:}
\begin{itemize}
    \item Algorithm A: 25\% traffic (baseline, safe fallback)
    \item Algorithm B: 5\% traffic (exploration only, clearly underperforming)
    \item Algorithm C: 70\% traffic (best performer, dominant allocation)
\end{itemize}

\textbf{Outcome:} By Week 4, the system confidently identifies Algorithm C as superior (10.1\% CTR, 95\% credible interval: [9.8\%, 10.4\%]). Compared to traditional A/B testing:

\begin{itemize}
    \item \textbf{Traditional A/B (4 weeks, 33\% each)}: Lost revenue from sub-optimal allocation: \$133,000
    \item \textbf{Thompson Sampling (4 weeks, adaptive)}: Lost revenue: \$48,000
    \item \textbf{Regret Reduction}: 64\% lower opportunity cost
    \item \textbf{Same Statistical Rigor}: 95\% confidence that C beats A
\end{itemize}

This scenario illustrates bandits' power: \textit{learning while earning}. Rather than sacrificing revenue for statistical certainty, bandits minimize regret by allocating traffic proportionally to each variant's probability of being optimal.

\subsection{Mathematical Foundation of Multi-Armed Bandits}

\subsubsection{Problem Formulation}

The multi-armed bandit problem models sequential decision-making under uncertainty. At each time step $t = 1, 2, \ldots, T$:

\begin{enumerate}
    \item The agent selects an arm $a_t \in \{1, 2, \ldots, K\}$
    \item The environment reveals reward $r_t \sim p(r | a_t)$ from an unknown distribution
    \item The agent updates beliefs about arm reward distributions
\end{enumerate}

Each arm $a$ has unknown true mean reward $\mu_a$. The optimal arm is $a^* = \arg\max_a \mu_a$ with mean reward $\mu^*$. The \textbf{instantaneous regret} at time $t$ is:

\begin{equation}
\ell_t = \mu^* - \mu_{a_t}
\end{equation}

The \textbf{cumulative regret} after $T$ rounds is:

\begin{equation}
R_T = \sum_{t=1}^{T} \ell_t = T\mu^* - \sum_{t=1}^{T} \mu_{a_t}
\end{equation}

An effective bandit algorithm achieves \textbf{sublinear regret}: $R_T = o(T)$, ensuring that $\lim_{T \to \infty} R_T / T = 0$. This means the algorithm eventually identifies and exploits the best arm.

\subsubsection{Theoretical Lower Bound}

Lai and Robbins (1985) proved that for any consistent algorithm (one that eventually identifies the optimal arm), the expected regret satisfies:

\begin{equation}
\liminf_{T \to \infty} \frac{R_T}{\log T} \geq \sum_{a: \mu_a < \mu^*} \frac{\mu^* - \mu_a}{D(\mu_a || \mu^*)}
\end{equation}

where $D(\mu_a || \mu^*)$ is the Kullback-Leibler divergence between reward distributions. This establishes a fundamental lower bound: regret must grow at least logarithmically with time for any algorithm that reliably finds the best arm.

\subsection{Thompson Sampling: Bayesian Approach}

Thompson Sampling maintains a posterior distribution over each arm's reward parameter, selecting arms by sampling from these posteriors. For Bernoulli rewards (success/failure), this uses the Beta-Bernoulli conjugate prior.

\subsubsection{Beta-Bernoulli Model}

For each arm $a$:
\begin{itemize}
    \item \textbf{Prior}: $\theta_a \sim \text{Beta}(\alpha_a, \beta_a)$ where $\theta_a$ is the success probability
    \item \textbf{Likelihood}: After observing $s_a$ successes and $f_a$ failures, $p(\text{data} | \theta_a) = \theta_a^{s_a} (1 - \theta_a)^{f_a}$
    \item \textbf{Posterior}: $\theta_a | \text{data} \sim \text{Beta}(\alpha_a + s_a, \beta_a + f_a)$
\end{itemize}

Starting with uniform prior $\text{Beta}(1, 1)$, after observing data:

\begin{equation}
p(\theta_a | s_a, f_a) = \text{Beta}(\alpha_a', \beta_a') \quad \text{where} \quad \alpha_a' = 1 + s_a, \quad \beta_a' = 1 + f_a
\end{equation}

The posterior mean is:

\begin{equation}
\mathbb{E}[\theta_a | \text{data}] = \frac{\alpha_a'}{\alpha_a' + \beta_a'} = \frac{1 + s_a}{2 + s_a + f_a}
\end{equation}

\subsubsection{Thompson Sampling Algorithm}

At each round $t$:

\begin{enumerate}
    \item For each arm $a$, sample $\tilde{\theta}_a \sim \text{Beta}(\alpha_a', \beta_a')$
    \item Select arm $a_t = \arg\max_a \tilde{\theta}_a$
    \item Observe reward $r_t$ (success or failure)
    \item Update posterior: if success, $\alpha_{a_t}' \leftarrow \alpha_{a_t}' + 1$; if failure, $\beta_{a_t}' \leftarrow \beta_{a_t}' + 1$
\end{enumerate}

This approach has elegant theoretical properties: Agrawal and Goyal (2012) proved Thompson Sampling achieves $O(\log T)$ regret, matching the theoretical lower bound.

\subsection{Upper Confidence Bound (UCB): Optimism Under Uncertainty}

UCB uses the principle of \textit{optimism in the face of uncertainty}: select the arm with the highest plausible mean reward, accounting for uncertainty.

\subsubsection{UCB1 Algorithm}

For each arm $a$, after $n_a$ pulls with empirical mean $\hat{\mu}_a$, the UCB is:

\begin{equation}
\text{UCB}_a(t) = \hat{\mu}_a + \sqrt{\frac{2 \log t}{n_a}}
\end{equation}

where $t$ is the total number of rounds so far. The algorithm selects:

\begin{equation}
a_t = \arg\max_a \text{UCB}_a(t)
\end{equation}

\subsubsection{Intuition}

The UCB formula balances:
\begin{itemize}
    \item \textbf{Exploitation term} $\hat{\mu}_a$: Prefer arms with high observed reward
    \item \textbf{Exploration bonus} $\sqrt{2 \log t / n_a}$: Prefer arms with high uncertainty (low $n_a$)
\end{itemize}

As $n_a$ increases, the exploration bonus shrinks, and the algorithm exploits more. The $\log t$ term ensures that even arms pulled many times are occasionally re-explored if enough total time has passed.

\subsubsection{Theoretical Guarantee}

Auer et al. (2002) proved that UCB1 achieves regret bound:

\begin{equation}
R_T \leq 8 \sum_{a: \mu_a < \mu^*} \frac{\log T}{\Delta_a} + \left(1 + \frac{\pi^2}{3}\right) \sum_{a: \mu_a < \mu^*} \Delta_a
\end{equation}

where $\Delta_a = \mu^* - \mu_a$ is the gap between the optimal arm and arm $a$. This establishes $O(\log T)$ regret.

\subsection{Comparison: A/B Testing vs Bandits}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Property} & \textbf{Traditional A/B Test} & \textbf{Multi-Armed Bandit} \\
\midrule
Allocation & Fixed (e.g., 50/50) & Adaptive (shifts to best) \\
Regret & Linear $O(T)$ & Sublinear $O(\log T)$ \\
Statistical Analysis & Single final comparison & Continuous Bayesian updates \\
Stopping Rule & Pre-defined sample size & Can run indefinitely \\
Certainty & High (controlled Type I error) & Probabilistic (credible intervals) \\
Use Case & Definitive model comparisons & Real-time optimization \\
Traffic Cost & High (50\% to inferior variant) & Low (minimizes inferior traffic) \\
\bottomrule
\end{tabular}
\caption{Comparison of A/B testing and multi-armed bandit approaches}
\end{table}

\textbf{When to Use A/B Testing:}
\begin{itemize}
    \item Regulatory or compliance requirements demand fixed pre-specified tests
    \item Strong stakeholder preference for traditional statistical rigor
    \item Testing involves irreversible decisions (e.g., UI redesign launch)
    \item Need to test many secondary and guardrail metrics simultaneously
\end{itemize}

\textbf{When to Use Bandits:}
\begin{itemize}
    \item High traffic volume enables fast learning
    \item Cost of showing inferior variant is high (e.g., revenue, user retention)
    \item Continuous optimization needed (e.g., content recommendation, ad targeting)
    \item Many variants to test (3+ arms), making fixed allocation expensive
\end{itemize}

\subsection{MultiArmedBandit: Thompson Sampling and UCB Implementation}

\begin{lstlisting}[language=Python, caption={Multi-Armed Bandit Implementation}]
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
import numpy as np
from scipy import stats
import logging

logger = logging.getLogger(__name__)

class BanditAlgorithm(Enum):
    """Multi-armed bandit algorithms."""
    EPSILON_GREEDY = "epsilon_greedy"
    UCB = "upper_confidence_bound"
    THOMPSON_SAMPLING = "thompson_sampling"
    EXP3 = "exp3"  # For adversarial settings

@dataclass
class ArmStatistics:
    """
    Statistics for a bandit arm.

    Attributes:
        name: Arm identifier
        n_pulls: Number of times arm was pulled
        n_successes: Number of successful outcomes
        total_reward: Cumulative reward
        alpha: Beta distribution alpha (successes + 1)
        beta: Beta distribution beta (failures + 1)
    """
    name: str
    n_pulls: int = 0
    n_successes: int = 0
    total_reward: float = 0.0

    @property
    def alpha(self) -> float:
        """Beta distribution alpha parameter."""
        return self.n_successes + 1

    @property
    def beta(self) -> float:
        """Beta distribution beta parameter."""
        return (self.n_pulls - self.n_successes) + 1

    @property
    def mean_reward(self) -> float:
        """Empirical mean reward."""
        return self.total_reward / self.n_pulls if self.n_pulls > 0 else 0.0

    @property
    def success_rate(self) -> float:
        """Empirical success rate."""
        return self.n_successes / self.n_pulls if self.n_pulls > 0 else 0.0

class MultiArmedBandit(ABC):
    """
    Abstract base for multi-armed bandit algorithms.

    Subclasses implement specific exploration strategies.
    """

    def __init__(self, arm_names: List[str], seed: Optional[int] = None):
        """
        Initialize bandit.

        Args:
            arm_names: Names of arms to choose from
            seed: Random seed
        """
        self.arm_names = arm_names
        self.arms = {
            name: ArmStatistics(name=name)
            for name in arm_names
        }
        self.rng = np.random.RandomState(seed)

        self.total_pulls = 0
        self.regret_history: List[float] = []

    @abstractmethod
    def select_arm(self) -> str:
        """
        Select an arm to pull.

        Returns:
            Name of selected arm
        """
        pass

    def update(self, arm_name: str, reward: float, success: bool = True):
        """
        Update arm statistics after observation.

        Args:
            arm_name: Name of pulled arm
            reward: Observed reward
            success: Whether outcome was success (for binary rewards)
        """
        arm = self.arms[arm_name]
        arm.n_pulls += 1
        arm.total_reward += reward

        if success:
            arm.n_successes += 1

        self.total_pulls += 1

    def get_statistics(self) -> Dict[str, Dict[str, float]]:
        """
        Get current statistics for all arms.

        Returns:
            Dictionary mapping arm names to statistics
        """
        return {
            name: {
                'n_pulls': arm.n_pulls,
                'success_rate': arm.success_rate,
                'mean_reward': arm.mean_reward,
                'total_reward': arm.total_reward
            }
            for name, arm in self.arms.items()
        }

class ThompsonSampling(MultiArmedBandit):
    """
    Thompson Sampling for Bernoulli bandits.

    Uses Beta-Bernoulli conjugate prior for Bayesian inference.

    Example:
        >>> bandit = ThompsonSampling(["model_a", "model_b", "model_c"])
        >>> arm = bandit.select_arm()
        >>> bandit.update(arm, reward=1.0, success=True)
    """

    def select_arm(self) -> str:
        """
        Select arm by sampling from posterior distributions.

        Each arm's posterior is Beta(alpha, beta).
        """
        samples = {}

        for name, arm in self.arms.items():
            # Sample from Beta posterior
            sample = self.rng.beta(arm.alpha, arm.beta)
            samples[name] = sample

        # Choose arm with highest sample
        selected_arm = max(samples, key=samples.get)

        logger.debug(
            f"Thompson Sampling: selected {selected_arm}, "
            f"samples={samples}"
        )

        return selected_arm

    def get_posterior_probabilities(self) -> Dict[str, Tuple[float, float]]:
        """
        Get posterior mean and std for each arm.

        Returns:
            Dictionary mapping arm names to (mean, std)
        """
        posteriors = {}

        for name, arm in self.arms.items():
            # Beta distribution mean and variance
            alpha, beta = arm.alpha, arm.beta
            mean = alpha / (alpha + beta)
            variance = (alpha * beta) / ((alpha + beta) ** 2 * (alpha + beta + 1))
            std = np.sqrt(variance)

            posteriors[name] = (mean, std)

        return posteriors

class UCB(MultiArmedBandit):
    """
    Upper Confidence Bound algorithm.

    Selects arm with highest upper confidence bound:
    UCB_i = mean_i + sqrt(2 * log(t) / n_i)

    Example:
        >>> bandit = UCB(["model_a", "model_b"], confidence=2.0)
        >>> arm = bandit.select_arm()
    """

    def __init__(
        self,
        arm_names: List[str],
        confidence: float = 2.0,
        seed: Optional[int] = None
    ):
        """
        Initialize UCB.

        Args:
            arm_names: Names of arms
            confidence: Confidence parameter (higher = more exploration)
            seed: Random seed
        """
        super().__init__(arm_names, seed)
        self.confidence = confidence

    def select_arm(self) -> str:
        """
        Select arm with highest UCB.

        For arms never pulled, UCB = infinity (pull first).
        """
        ucb_values = {}

        for name, arm in self.arms.items():
            if arm.n_pulls == 0:
                # Pull unpulled arms first
                ucb_values[name] = float('inf')
            else:
                # UCB formula
                exploitation = arm.mean_reward
                exploration = self.confidence * np.sqrt(
                    2 * np.log(self.total_pulls) / arm.n_pulls
                )
                ucb_values[name] = exploitation + exploration

        selected_arm = max(ucb_values, key=ucb_values.get)

        logger.debug(
            f"UCB: selected {selected_arm}, UCBs={ucb_values}"
        )

        return selected_arm

class EpsilonGreedy(MultiArmedBandit):
    """
    Epsilon-Greedy algorithm.

    Explores randomly with probability epsilon, exploits otherwise.

    Example:
        >>> bandit = EpsilonGreedy(["model_a", "model_b"], epsilon=0.1)
    """

    def __init__(
        self,
        arm_names: List[str],
        epsilon: float = 0.1,
        decay: bool = False,
        seed: Optional[int] = None
    ):
        """
        Initialize epsilon-greedy.

        Args:
            arm_names: Names of arms
            epsilon: Exploration probability
            decay: Whether to decay epsilon over time
            seed: Random seed
        """
        super().__init__(arm_names, seed)
        self.epsilon = epsilon
        self.decay = decay
        self.initial_epsilon = epsilon

    def select_arm(self) -> str:
        """
        Select arm using epsilon-greedy strategy.
        """
        # Decay epsilon if enabled
        if self.decay and self.total_pulls > 0:
            self.epsilon = self.initial_epsilon / (1 + self.total_pulls / 1000)

        # Explore with probability epsilon
        if self.rng.random() < self.epsilon:
            # Random exploration
            selected_arm = self.rng.choice(self.arm_names)
            logger.debug(f"Epsilon-Greedy: exploring {selected_arm}")
        else:
            # Greedy exploitation
            # Choose arm with highest mean reward
            if self.total_pulls == 0:
                # No data yet, choose randomly
                selected_arm = self.rng.choice(self.arm_names)
            else:
                mean_rewards = {
                    name: arm.mean_reward
                    for name, arm in self.arms.items()
                }
                selected_arm = max(mean_rewards, key=mean_rewards.get)

            logger.debug(f"Epsilon-Greedy: exploiting {selected_arm}")

        return selected_arm

class BanditExperiment:
    """
    Run bandit experiment with performance tracking.

    Example:
        >>> bandit = ThompsonSampling(["model_a", "model_b"])
        >>> experiment = BanditExperiment(bandit, true_rewards={"model_a": 0.10, "model_b": 0.12})
        >>> experiment.run(n_iterations=1000)
        >>> print(experiment.get_summary())
    """

    def __init__(
        self,
        bandit: MultiArmedBandit,
        true_rewards: Dict[str, float],
        reward_noise: float = 0.0
    ):
        """
        Initialize experiment.

        Args:
            bandit: Bandit algorithm to test
            true_rewards: True mean rewards for each arm
            reward_noise: Gaussian noise std for rewards
        """
        self.bandit = bandit
        self.true_rewards = true_rewards
        self.reward_noise = reward_noise

        # Best arm
        self.best_arm = max(true_rewards, key=true_rewards.get)
        self.best_reward = true_rewards[self.best_arm]

        # Tracking
        self.cumulative_reward = 0.0
        self.cumulative_regret = 0.0
        self.arm_selection_history: List[str] = []

    def run(self, n_iterations: int):
        """
        Run experiment for n iterations.

        Args:
            n_iterations: Number of iterations
        """
        for i in range(n_iterations):
            # Select arm
            arm = self.bandit.select_arm()
            self.arm_selection_history.append(arm)

            # Observe reward (with noise)
            true_reward = self.true_rewards[arm]
            observed_reward = true_reward + self.bandit.rng.normal(
                0, self.reward_noise
            )

            # Clamp to [0, 1] for conversion rates
            observed_reward = np.clip(observed_reward, 0, 1)

            # Update bandit
            success = observed_reward > 0.5  # Binary outcome
            self.bandit.update(arm, observed_reward, success)

            # Track performance
            self.cumulative_reward += observed_reward

            # Regret = reward of best arm - reward of chosen arm
            regret = self.best_reward - true_reward
            self.cumulative_regret += regret

            if (i + 1) % 100 == 0:
                logger.info(
                    f"Iteration {i+1}: "
                    f"Cumulative regret={self.cumulative_regret:.2f}, "
                    f"Best arm selection rate="
                    f"{self.arm_selection_history.count(self.best_arm) / (i+1):.1%}"
                )

    def get_summary(self) -> Dict[str, Any]:
        """
        Get experiment summary.

        Returns:
            Summary statistics
        """
        n_iterations = len(self.arm_selection_history)

        # Selection rates
        selection_rates = {}
        for arm in self.bandit.arm_names:
            count = self.arm_selection_history.count(arm)
            selection_rates[arm] = count / n_iterations

        # Bandit statistics
        bandit_stats = self.bandit.get_statistics()

        return {
            'n_iterations': n_iterations,
            'cumulative_reward': self.cumulative_reward,
            'cumulative_regret': self.cumulative_regret,
            'average_reward': self.cumulative_reward / n_iterations,
            'average_regret': self.cumulative_regret / n_iterations,
            'best_arm': self.best_arm,
            'best_arm_selection_rate': selection_rates[self.best_arm],
            'selection_rates': selection_rates,
            'arm_statistics': bandit_stats
        }
\end{lstlisting}

\subsection{Bandit Comparison}

\begin{lstlisting}[language=Python, caption={Comparing Bandit Algorithms}]
# True conversion rates for three models
true_rewards = {
    "model_a": 0.10,  # Baseline
    "model_b": 0.11,  # 10% improvement
    "model_c": 0.12   # 20% improvement (best)
}

# Test different algorithms
algorithms = [
    ("Thompson Sampling", ThompsonSampling(list(true_rewards.keys()), seed=42)),
    ("UCB", UCB(list(true_rewards.keys()), confidence=2.0, seed=42)),
    ("Epsilon-Greedy (0.1)", EpsilonGreedy(list(true_rewards.keys()), epsilon=0.1, seed=42)),
    ("Epsilon-Greedy (Decay)", EpsilonGreedy(list(true_rewards.keys()), epsilon=0.3, decay=True, seed=42))
]

results = []

for name, bandit in algorithms:
    experiment = BanditExperiment(
        bandit=bandit,
        true_rewards=true_rewards,
        reward_noise=0.1
    )

    experiment.run(n_iterations=2000)
    summary = experiment.get_summary()

    results.append({
        'algorithm': name,
        'cumulative_regret': summary['cumulative_regret'],
        'average_regret': summary['average_regret'],
        'best_arm_selection_rate': summary['best_arm_selection_rate'],
        'final_exploitation_rate': summary['selection_rates']['model_c']
    })

# Display results
results_df = pd.DataFrame(results)
print("Bandit Algorithm Comparison:")
print(results_df.to_string(index=False))

# Thompson Sampling typically has lowest regret for this scenario
\end{lstlisting}

\section{A/A Testing and Bias Detection}

A/A tests validate experimental infrastructure before running real tests.

\subsection{A/A Testing Implementation}

\begin{lstlisting}[language=Python, caption={A/A Testing for Infrastructure Validation}]
from typing import Dict, List, Optional
import numpy as np
import pandas as pd
from scipy import stats
import logging

logger = logging.getLogger(__name__)

class AATestValidator:
    """
    A/A testing for infrastructure validation.

    A/A tests assign users to identical treatments to validate
    that randomization and measurement systems work correctly.

    Example:
        >>> validator = AATestValidator(alpha=0.05)
        >>> result = validator.run_aa_test(control_data, treatment_data)
        >>> if result['valid']:
        ...     print("Infrastructure validated")
    """

    def __init__(self, alpha: float = 0.05, n_simulations: int = 1000):
        """
        Initialize A/A test validator.

        Args:
            alpha: Significance level
            n_simulations: Number of simulations for FPR estimation
        """
        self.alpha = alpha
        self.n_simulations = n_simulations

    def run_aa_test(
        self,
        control_data: pd.Series,
        treatment_data: pd.Series,
        metric_name: str = "metric"
    ) -> Dict[str, Any]:
        """
        Run A/A test comparing two identical treatments.

        Args:
            control_data: Data from "control" arm
            treatment_data: Data from "treatment" arm
            metric_name: Name of metric being tested

        Returns:
            Dictionary with validation results
        """
        # Test for difference (should find none)
        if pd.api.types.is_numeric_dtype(control_data):
            # Continuous metric: t-test
            statistic, p_value = stats.ttest_ind(
                control_data.dropna(),
                treatment_data.dropna()
            )
            test_type = "t-test"
        else:
            # Categorical metric: chi-square
            contingency = pd.crosstab(
                pd.Series(["control"] * len(control_data) + ["treatment"] * len(treatment_data)),
                pd.concat([control_data, treatment_data])
            )
            statistic, p_value, _, _ = stats.chi2_contingency(contingency)
            test_type = "chi-square"

        # Check if significant (bad for A/A test)
        is_significant = p_value < self.alpha

        # Compute effect size
        if pd.api.types.is_numeric_dtype(control_data):
            # Cohen's d
            pooled_std = np.sqrt(
                (control_data.var() + treatment_data.var()) / 2
            )
            effect_size = abs(
                control_data.mean() - treatment_data.mean()
            ) / pooled_std
        else:
            effect_size = None

        result = {
            'metric_name': metric_name,
            'test_type': test_type,
            'p_value': p_value,
            'statistic': statistic,
            'is_significant': is_significant,
            'effect_size': effect_size,
            'valid': not is_significant,
            'control_mean': control_data.mean() if pd.api.types.is_numeric_dtype(control_data) else None,
            'treatment_mean': treatment_data.mean() if pd.api.types.is_numeric_dtype(treatment_data) else None,
            'control_n': len(control_data),
            'treatment_n': len(treatment_data)
        }

        if is_significant:
            logger.warning(
                f"A/A test FAILED for {metric_name}: "
                f"p-value={p_value:.4f} < {self.alpha} "
                f"(found spurious difference)"
            )
        else:
            logger.info(
                f"A/A test PASSED for {metric_name}: "
                f"p-value={p_value:.4f} >= {self.alpha}"
            )

        return result

    def estimate_false_positive_rate(
        self,
        data: pd.Series
    ) -> Dict[str, float]:
        """
        Estimate false positive rate through simulation.

        Randomly splits data into two groups and tests for difference.
        Should find ~alpha% significant results.

        Args:
            data: Combined data to split

        Returns:
            Dictionary with FPR estimates
        """
        significant_count = 0
        p_values = []

        for _ in range(self.n_simulations):
            # Random split
            indices = np.random.permutation(len(data))
            mid = len(indices) // 2

            group_a = data.iloc[indices[:mid]]
            group_b = data.iloc[indices[mid:]]

            # Test
            if pd.api.types.is_numeric_dtype(data):
                _, p_value = stats.ttest_ind(group_a, group_b)
            else:
                contingency = pd.crosstab(
                    pd.Series(["a"] * len(group_a) + ["b"] * len(group_b)),
                    pd.concat([group_a, group_b])
                )
                _, p_value, _, _ = stats.chi2_contingency(contingency)

            p_values.append(p_value)

            if p_value < self.alpha:
                significant_count += 1

        observed_fpr = significant_count / self.n_simulations

        return {
            'observed_fpr': observed_fpr,
            'expected_fpr': self.alpha,
            'fpr_within_bounds': abs(observed_fpr - self.alpha) < 2 * np.sqrt(self.alpha * (1 - self.alpha) / self.n_simulations),
            'mean_p_value': np.mean(p_values),
            'p_value_uniformity': stats.kstest(p_values, 'uniform').pvalue
        }

class BiasDetector:
    """
    Detect bias in randomization and measurement.

    Checks for selection bias, measurement bias, and temporal bias.
    """

    def __init__(self):
        """Initialize bias detector."""
        pass

    def check_selection_bias(
        self,
        assignments: pd.Series,
        covariates: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        Check for selection bias in treatment assignment.

        Tests if covariates predict treatment assignment.

        Args:
            assignments: Treatment assignments
            covariates: Covariate data

        Returns:
            Bias detection results
        """
        from sklearn.linear_model import LogisticRegression
        from sklearn.model_selection import cross_val_score

        # Encode assignments as binary (control=0, treatment=1)
        unique_arms = assignments.unique()
        if len(unique_arms) != 2:
            logger.warning("Selection bias check requires 2 arms")
            return {}

        y = (assignments == unique_arms[1]).astype(int)

        # Fit logistic regression
        X = covariates.fillna(0)

        # One-hot encode categorical variables
        X_encoded = pd.get_dummies(X, drop_first=True)

        model = LogisticRegression(random_state=42, max_iter=1000)

        # Cross-validated AUC
        auc_scores = cross_val_score(
            model,
            X_encoded,
            y,
            cv=5,
            scoring='roc_auc'
        )

        mean_auc = auc_scores.mean()

        # AUC ~0.5 indicates no bias
        bias_detected = mean_auc > 0.55 or mean_auc < 0.45

        return {
            'bias_detected': bias_detected,
            'mean_auc': mean_auc,
            'auc_std': auc_scores.std(),
            'interpretation': (
                "No selection bias" if not bias_detected
                else "Covariates predict treatment assignment"
            )
        }

    def check_temporal_bias(
        self,
        data: pd.DataFrame,
        timestamp_col: str,
        treatment_col: str,
        metric_col: str
    ) -> Dict[str, Any]:
        """
        Check for temporal bias (time-varying effects).

        Args:
            data: Experiment data
            timestamp_col: Column with timestamps
            treatment_col: Column with treatment assignments
            metric_col: Column with metric values

        Returns:
            Temporal bias results
        """
        # Split into time windows
        data = data.sort_values(timestamp_col)
        n_windows = 5

        window_size = len(data) // n_windows
        window_effects = []

        for i in range(n_windows):
            start_idx = i * window_size
            end_idx = (i + 1) * window_size if i < n_windows - 1 else len(data)

            window_data = data.iloc[start_idx:end_idx]

            # Compute treatment effect in window
            control = window_data[
                window_data[treatment_col] == window_data[treatment_col].unique()[0]
            ][metric_col]

            treatment = window_data[
                window_data[treatment_col] == window_data[treatment_col].unique()[1]
            ][metric_col]

            if len(control) > 0 and len(treatment) > 0:
                effect = treatment.mean() - control.mean()
                window_effects.append(effect)

        # Test if effects vary across windows
        if len(window_effects) > 1:
            # High variance indicates temporal instability
            effect_std = np.std(window_effects)
            effect_mean = np.mean(window_effects)

            # Coefficient of variation
            cv = abs(effect_std / effect_mean) if effect_mean != 0 else float('inf')

            temporal_bias = cv > 0.5  # 50% variation

            return {
                'temporal_bias_detected': temporal_bias,
                'window_effects': window_effects,
                'effect_mean': effect_mean,
                'effect_std': effect_std,
                'coefficient_of_variation': cv
            }
        else:
            return {'error': 'Insufficient windows for analysis'}
\end{lstlisting}

\section{Bayesian A/B Testing}

Bayesian methods provide an alternative framework to frequentist hypothesis testing, offering intuitive probability statements about treatment effects, continuous learning from data, and natural incorporation of prior knowledge. Unlike p-values that answer "how likely is this data under the null hypothesis?", Bayesian posteriors answer "what is the probability that treatment B is better than A?"

\subsection{Why Bayesian Methods for A/B Testing?}

\textbf{Advantages over Frequentist Approaches:}
\begin{itemize}
    \item \textbf{Direct Probability Statements}: $P(\text{B better than A} | \text{data}) = 0.95$ is clearer than "p=0.03"
    \item \textbf{No Peeking Problem}: Continuous monitoring without inflating error rates
    \item \textbf{Prior Knowledge Integration}: Incorporate domain expertise, historical data, or business constraints
    \item \textbf{Natural Decision Framework}: Probability of superiority maps directly to risk tolerance
    \item \textbf{Small Sample Flexibility}: Useful when sample sizes are limited or expensive
    \item \textbf{Asymmetric Loss Functions}: Model business costs of false positives vs false negatives
\end{itemize}

\textbf{Trade-offs:}
\begin{itemize}
    \item Requires careful prior selection and sensitivity analysis
    \item Computational complexity for complex models
    \item Prior specification can be subjective (but sensitivity analysis addresses this)
    \item Less familiar to stakeholders trained in frequentist methods
\end{itemize}

\subsection{Real-World Scenario: The Prior Belief Challenge}

\textbf{The Setup:}

A fintech company tested a new credit risk model (Model B) against their production model (Model A). The ML team claimed Model B would reduce default rates by 20-30\% based on offline metrics. However, the risk management team was skeptical—previous "breakthrough" models had failed in production.

\textbf{Initial Frequentist Analysis:}
\begin{itemize}
    \item Sample: 5,000 loans per arm over 3 months
    \item Default rates: Model A = 4.2\%, Model B = 3.8\%
    \item Relative reduction: 9.5\%
    \item Frequentist test: p = 0.18 (not significant at $\alpha=0.05$)
    \item \textbf{Decision}: No clear signal, need 20K+ loans to reach power
\end{itemize}

\textbf{The Dilemma:}
\begin{itemize}
    \item Waiting 12+ months for 20K loans meant delaying potential \$2M/year in reduced losses
    \item But deploying on inconclusive evidence risked regulatory scrutiny and actual losses
    \item ML team insisted Model B was superior despite p=0.18
    \item Risk team wanted "statistical proof" before deployment
\end{itemize}

\textbf{Bayesian Analysis with Skeptical Prior:}

The team agreed on a Bayesian analysis with three prior scenarios:
\begin{enumerate}
    \item \textbf{Optimistic Prior} (ML team): Model B reduces defaults by 15-25\% (Beta prior centered at 20\% reduction)
    \item \textbf{Neutral Prior} (uninformed): Flat prior, no assumption about superiority
    \item \textbf{Skeptical Prior} (Risk team): Model B likely \textit{increases} defaults by 0-10\% based on past model failures
\end{enumerate}

\textbf{Results with 5,000 loans:}
\begin{center}
\begin{tabular}{lccc}
\hline
\textbf{Prior} & \textbf{P(B better than A)} & \textbf{Expected Reduction} & \textbf{95\% Credible Interval} \\
\hline
Optimistic & 0.89 & -11.2\% & [-22\%, +1\%] \\
Neutral & 0.82 & -9.8\% & [-21\%, +3\%] \\
Skeptical & 0.71 & -7.5\% & [-19\%, +5\%] \\
\hline
\end{tabular}
\end{center}

\textbf{The Decision:}

Even with the skeptical prior incorporating past failures, there was 71\% probability that Model B was superior. The team agreed on a staged deployment:
\begin{itemize}
    \item Deploy to 25\% of traffic (low-risk segment)
    \item Continuous Bayesian updating with each month's data
    \item Automatic rollback if P(B better than A) drops below 60\%
    \item Full deployment if probability exceeds 95\% or credible interval excludes harm
\end{itemize}

\textbf{Outcome After 6 Months:}
\begin{itemize}
    \item With 12,000 total loans, P(B better than A) reached 0.96 even under skeptical prior
    \item Observed reduction: 11\% (95\% CI: [6\%, 15\%])
    \item Full deployment proceeded, saving \$1.8M/year
    \item \textbf{Key insight}: Bayesian updating allowed early deployment with risk management, rather than waiting 12+ months for frequentist significance
\end{itemize}

\textbf{Cost Savings vs Frequentist Approach:}
\begin{itemize}
    \item Frequentist: Would have taken 18 months to reach 20K samples for p<0.05
    \item Bayesian: Made confident decision at 6 months with 12K samples
    \item \textbf{Benefit}: 12 months of early deployment = \$1.8M additional value
    \item Risk was managed through continuous monitoring and automatic rollback thresholds
\end{itemize}

\subsection{Mathematical Foundations of Bayesian A/B Testing}

\textbf{Bayes' Theorem for A/B Testing:}

The foundation is Bayes' theorem:
\begin{equation}
P(\theta | \text{data}) = \frac{P(\text{data} | \theta) \cdot P(\theta)}{P(\text{data})}
\end{equation}

Where:
\begin{itemize}
    \item $P(\theta | \text{data})$: Posterior distribution (what we want)
    \item $P(\text{data} | \theta)$: Likelihood (how probable is the data given parameters)
    \item $P(\theta)$: Prior distribution (our initial beliefs)
    \item $P(\text{data})$: Marginal likelihood (normalizing constant)
\end{itemize}

\textbf{Binary Metrics (Conversion Rates, CTR):}

For binary outcomes, we use the Beta-Binomial conjugate pair:
\begin{align}
\text{Prior: } & \theta_A, \theta_B \sim \text{Beta}(\alpha, \beta) \\
\text{Likelihood: } & X | \theta \sim \text{Binomial}(n, \theta) \\
\text{Posterior: } & \theta | X \sim \text{Beta}(\alpha + x, \beta + n - x)
\end{align}

Where $x$ is the number of successes out of $n$ trials.

\textbf{Example:} With uninformed prior Beta(1,1) and data showing 45 conversions out of 500 users:
\begin{equation}
\theta_A | \text{data} \sim \text{Beta}(1 + 45, 1 + 500 - 45) = \text{Beta}(46, 456)
\end{equation}

\textbf{Probability of Superiority:}

The key business question: What is $P(\theta_B > \theta_A | \text{data})$?

For Beta distributions, this can be computed analytically or via Monte Carlo:
\begin{equation}
P(\theta_B > \theta_A) = \int_0^1 \int_0^{\theta_B} f_A(\theta_A) f_B(\theta_B) \, d\theta_A \, d\theta_B
\end{equation}

Monte Carlo approximation (simpler):
\begin{equation}
P(\theta_B > \theta_A) \approx \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\theta_B^{(i)} > \theta_A^{(i)}]
\end{equation}

Where $\theta_A^{(i)}, \theta_B^{(i)}$ are samples from the posterior distributions.

\textbf{Credible Intervals:}

Unlike confidence intervals, credible intervals have direct probabilistic interpretation:
\begin{equation}
P(\theta \in [L, U] | \text{data}) = 0.95
\end{equation}

The 95\% credible interval $[L, U]$ can be computed from posterior quantiles.

\textbf{Continuous Metrics (Revenue, Time):}

For continuous metrics, use Normal-Normal conjugacy:
\begin{align}
\text{Prior: } & \mu \sim \mathcal{N}(\mu_0, \sigma_0^2) \\
\text{Likelihood: } & X | \mu \sim \mathcal{N}(\mu, \sigma^2) \\
\text{Posterior: } & \mu | X \sim \mathcal{N}(\mu_n, \sigma_n^2)
\end{align}

Where:
\begin{align}
\sigma_n^2 &= \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1} \\
\mu_n &= \sigma_n^2 \left(\frac{\mu_0}{\sigma_0^2} + \frac{n \bar{x}}{\sigma^2}\right)
\end{align}

The posterior is a precision-weighted average of prior and data.

\textbf{Expected Loss and Decision Theory:}

Beyond probability of superiority, we can compute expected loss:
\begin{equation}
\mathcal{L}(\text{choose B}) = \mathbb{E}[\max(0, \theta_A - \theta_B) | \text{data}]
\end{equation}

This quantifies the expected regret if we choose B but A is actually better. Make a decision only when expected loss is below a threshold.

\textbf{Prior Selection:}

Common prior choices:
\begin{itemize}
    \item \textbf{Uninformative}: Beta(1,1) for proportions, $\mathcal{N}(0, \infty)$ for means
    \item \textbf{Weakly Informative}: Beta(2,2) centers at 0.5 with some uncertainty
    \item \textbf{Informative}: Beta($\alpha$, $\beta$) where mode = historical conversion rate
    \item \textbf{Skeptical}: Prior centered at null effect or negative effect
\end{itemize}

\subsection{Bayesian A/B Test Implementation}

\begin{lstlisting}[language=Python, caption={Comprehensive Bayesian A/B Testing Framework}]
from typing import Dict, List, Optional, Tuple, Union
import numpy as np
import pandas as pd
from scipy import stats
from scipy.special import betaln, logsumexp
from dataclasses import dataclass
import matplotlib.pyplot as plt

@dataclass
class BayesianResult:
    """Results from Bayesian A/B test analysis."""
    prob_b_beats_a: float
    expected_lift: float
    credible_interval: Tuple[float, float]
    expected_loss_choose_b: float
    posterior_a: Dict[str, float]
    posterior_b: Dict[str, float]
    samples_a: Optional[np.ndarray] = None
    samples_b: Optional[np.ndarray] = None

class BayesianABTest:
    """
    Bayesian A/B testing with posterior probability calculations.

    Supports binary and continuous metrics with Beta-Binomial and
    Normal-Normal conjugate models.

    Example:
        >>> test = BayesianABTest(metric_type='binary')
        >>> test.set_prior('beta', alpha=1, beta=1)  # Uniform prior
        >>> result = test.analyze(
        ...     control_data={'successes': 45, 'trials': 500},
        ...     treatment_data={'successes': 58, 'trials': 500}
        ... )
        >>> print(f"P(B > A) = {result.prob_b_beats_a:.3f}")
    """

    def __init__(
        self,
        metric_type: str = 'binary',
        n_samples: int = 100000,
        random_state: int = 42
    ):
        """
        Initialize Bayesian A/B test.

        Args:
            metric_type: 'binary' for conversion rates, 'continuous' for revenue/time
            n_samples: Number of posterior samples for Monte Carlo estimation
            random_state: Random seed for reproducibility
        """
        self.metric_type = metric_type
        self.n_samples = n_samples
        self.random_state = random_state
        self.prior = None
        np.random.seed(random_state)

    def set_prior(
        self,
        distribution: str,
        **params
    ) -> None:
        """
        Set prior distribution.

        Args:
            distribution: 'beta' for binary, 'normal' for continuous
            **params: Distribution parameters
                For beta: alpha, beta
                For normal: mu, sigma
        """
        if distribution == 'beta' and self.metric_type == 'binary':
            self.prior = {
                'distribution': 'beta',
                'alpha': params.get('alpha', 1),
                'beta': params.get('beta', 1)
            }
        elif distribution == 'normal' and self.metric_type == 'continuous':
            self.prior = {
                'distribution': 'normal',
                'mu': params.get('mu', 0),
                'sigma': params.get('sigma', 1000)  # Weakly informative
            }
        else:
            raise ValueError(f"Invalid distribution {distribution} for metric type {self.metric_type}")

    def _compute_beta_posterior(
        self,
        successes: int,
        trials: int
    ) -> Dict[str, float]:
        """Compute Beta posterior from binomial data."""
        if self.prior is None:
            self.set_prior('beta', alpha=1, beta=1)

        alpha_post = self.prior['alpha'] + successes
        beta_post = self.prior['beta'] + (trials - successes)

        # Posterior statistics
        mean = alpha_post / (alpha_post + beta_post)
        var = (alpha_post * beta_post) / (
            (alpha_post + beta_post)**2 * (alpha_post + beta_post + 1)
        )
        mode = (alpha_post - 1) / (alpha_post + beta_post - 2) if alpha_post > 1 and beta_post > 1 else mean

        return {
            'alpha': alpha_post,
            'beta': beta_post,
            'mean': mean,
            'variance': var,
            'std': np.sqrt(var),
            'mode': mode
        }

    def _compute_normal_posterior(
        self,
        data: np.ndarray
    ) -> Dict[str, float]:
        """Compute Normal posterior from continuous data."""
        if self.prior is None:
            self.set_prior('normal', mu=0, sigma=1000)

        n = len(data)
        sample_mean = np.mean(data)
        sample_var = np.var(data, ddof=1)

        # Assume known variance for simplicity (can extend to Normal-Inverse-Gamma)
        # Posterior precision = prior precision + data precision
        prior_precision = 1 / self.prior['sigma']**2
        data_precision = n / sample_var

        post_precision = prior_precision + data_precision
        post_variance = 1 / post_precision
        post_std = np.sqrt(post_variance)

        # Posterior mean is precision-weighted average
        post_mean = post_variance * (
            prior_precision * self.prior['mu'] +
            data_precision * sample_mean
        )

        return {
            'mu': post_mean,
            'sigma': post_std,
            'variance': post_variance,
            'mean': post_mean,
            'std': post_std
        }

    def analyze(
        self,
        control_data: Union[Dict, np.ndarray, pd.Series],
        treatment_data: Union[Dict, np.ndarray, pd.Series],
        credible_level: float = 0.95
    ) -> BayesianResult:
        """
        Perform Bayesian analysis of A/B test.

        Args:
            control_data: For binary: {'successes': int, 'trials': int}
                         For continuous: array of observations
            treatment_data: Same format as control_data
            credible_level: Level for credible interval (default 0.95)

        Returns:
            BayesianResult with posterior statistics
        """
        if self.metric_type == 'binary':
            return self._analyze_binary(control_data, treatment_data, credible_level)
        else:
            return self._analyze_continuous(control_data, treatment_data, credible_level)

    def _analyze_binary(
        self,
        control_data: Dict,
        treatment_data: Dict,
        credible_level: float
    ) -> BayesianResult:
        """Analyze binary metric A/B test."""
        # Compute posteriors
        post_a = self._compute_beta_posterior(
            control_data['successes'],
            control_data['trials']
        )
        post_b = self._compute_beta_posterior(
            treatment_data['successes'],
            treatment_data['trials']
        )

        # Sample from posteriors
        samples_a = np.random.beta(post_a['alpha'], post_a['beta'], self.n_samples)
        samples_b = np.random.beta(post_b['alpha'], post_b['beta'], self.n_samples)

        # Probability B beats A
        prob_b_beats_a = np.mean(samples_b > samples_a)

        # Expected lift: E[(theta_B - theta_A) / theta_A]
        lift_samples = (samples_b - samples_a) / samples_a
        expected_lift = np.mean(lift_samples)

        # Credible interval for lift
        ci_lower = np.percentile(lift_samples, (1 - credible_level) / 2 * 100)
        ci_upper = np.percentile(lift_samples, (1 + credible_level) / 2 * 100)

        # Expected loss if we choose B
        # Loss = max(0, theta_A - theta_B) when B is chosen
        loss_samples = np.maximum(0, samples_a - samples_b)
        expected_loss = np.mean(loss_samples)

        return BayesianResult(
            prob_b_beats_a=prob_b_beats_a,
            expected_lift=expected_lift,
            credible_interval=(ci_lower, ci_upper),
            expected_loss_choose_b=expected_loss,
            posterior_a=post_a,
            posterior_b=post_b,
            samples_a=samples_a,
            samples_b=samples_b
        )

    def _analyze_continuous(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        credible_level: float
    ) -> BayesianResult:
        """Analyze continuous metric A/B test."""
        # Compute posteriors
        post_a = self._compute_normal_posterior(np.array(control_data))
        post_b = self._compute_normal_posterior(np.array(treatment_data))

        # Sample from posteriors
        samples_a = np.random.normal(post_a['mu'], post_a['sigma'], self.n_samples)
        samples_b = np.random.normal(post_b['mu'], post_b['sigma'], self.n_samples)

        # Probability B beats A
        prob_b_beats_a = np.mean(samples_b > samples_a)

        # Expected lift
        lift_samples = (samples_b - samples_a) / samples_a
        expected_lift = np.mean(lift_samples)

        # Credible interval
        ci_lower = np.percentile(lift_samples, (1 - credible_level) / 2 * 100)
        ci_upper = np.percentile(lift_samples, (1 + credible_level) / 2 * 100)

        # Expected loss
        loss_samples = np.maximum(0, samples_a - samples_b)
        expected_loss = np.mean(loss_samples)

        return BayesianResult(
            prob_b_beats_a=prob_b_beats_a,
            expected_lift=expected_lift,
            credible_interval=(ci_lower, ci_upper),
            expected_loss_choose_b=expected_loss,
            posterior_a=post_a,
            posterior_b=post_b,
            samples_a=samples_a,
            samples_b=samples_b
        )
\end{lstlisting}

\subsection{Posterior Analysis and Credible Intervals}

\begin{lstlisting}[language=Python, caption={Posterior Analysis with Visualization}]
class PosteriorAnalyzer:
    """
    Analyze and visualize Bayesian posterior distributions.

    Provides credible intervals, highest density intervals (HDI),
    probability of practical significance, and posterior visualization.

    Example:
        >>> analyzer = PosteriorAnalyzer()
        >>> analyzer.plot_posteriors(result.samples_a, result.samples_b)
        >>> hdi = analyzer.compute_hdi(result.samples_b - result.samples_a, 0.95)
    """

    @staticmethod
    def compute_credible_interval(
        samples: np.ndarray,
        credible_level: float = 0.95
    ) -> Tuple[float, float]:
        """
        Compute equal-tailed credible interval.

        Args:
            samples: Posterior samples
            credible_level: Credible level (e.g., 0.95 for 95%)

        Returns:
            (lower, upper) bounds of credible interval
        """
        alpha = 1 - credible_level
        lower = np.percentile(samples, alpha / 2 * 100)
        upper = np.percentile(samples, (1 - alpha / 2) * 100)
        return (lower, upper)

    @staticmethod
    def compute_hdi(
        samples: np.ndarray,
        credible_level: float = 0.95
    ) -> Tuple[float, float]:
        """
        Compute Highest Density Interval (HDI).

        HDI is the narrowest interval containing credible_level probability.
        Often preferred over equal-tailed intervals.

        Args:
            samples: Posterior samples
            credible_level: Credible level

        Returns:
            (lower, upper) bounds of HDI
        """
        sorted_samples = np.sort(samples)
        n = len(sorted_samples)
        interval_size = int(np.floor(credible_level * n))
        n_intervals = n - interval_size

        # Find narrowest interval
        interval_widths = sorted_samples[interval_size:] - sorted_samples[:n_intervals]
        min_idx = np.argmin(interval_widths)

        hdi_min = sorted_samples[min_idx]
        hdi_max = sorted_samples[min_idx + interval_size]

        return (hdi_min, hdi_max)

    @staticmethod
    def prob_practical_significance(
        samples_a: np.ndarray,
        samples_b: np.ndarray,
        rope: Tuple[float, float] = (-0.01, 0.01)
    ) -> Dict[str, float]:
        """
        Compute probability of practical significance using ROPE.

        ROPE (Region of Practical Equivalence) defines a range of effects
        considered practically negligible.

        Args:
            samples_a: Control posterior samples
            samples_b: Treatment posterior samples
            rope: (lower, upper) bounds of ROPE as proportion of baseline

        Returns:
            Dictionary with probabilities for different regions
        """
        # Compute lift samples
        lift_samples = (samples_b - samples_a) / samples_a

        rope_lower, rope_upper = rope

        prob_below_rope = np.mean(lift_samples < rope_lower)
        prob_in_rope = np.mean((lift_samples >= rope_lower) & (lift_samples <= rope_upper))
        prob_above_rope = np.mean(lift_samples > rope_upper)

        return {
            'prob_practically_worse': prob_below_rope,
            'prob_practically_equivalent': prob_in_rope,
            'prob_practically_better': prob_above_rope,
            'rope': rope
        }

    @staticmethod
    def plot_posteriors(
        samples_a: np.ndarray,
        samples_b: np.ndarray,
        labels: Tuple[str, str] = ('Control', 'Treatment'),
        save_path: Optional[str] = None
    ) -> None:
        """
        Plot posterior distributions for visual comparison.

        Args:
            samples_a: Control posterior samples
            samples_b: Treatment posterior samples
            labels: (control_label, treatment_label)
            save_path: Optional path to save figure
        """
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # Plot 1: Overlay of posteriors
        axes[0].hist(samples_a, bins=50, alpha=0.5, label=labels[0], density=True)
        axes[0].hist(samples_b, bins=50, alpha=0.5, label=labels[1], density=True)
        axes[0].axvline(np.mean(samples_a), color='blue', linestyle='--', label=f'{labels[0]} Mean')
        axes[0].axvline(np.mean(samples_b), color='orange', linestyle='--', label=f'{labels[1]} Mean')
        axes[0].set_xlabel('Conversion Rate')
        axes[0].set_ylabel('Density')
        axes[0].set_title('Posterior Distributions')
        axes[0].legend()
        axes[0].grid(alpha=0.3)

        # Plot 2: Distribution of lift
        lift_samples = (samples_b - samples_a) / samples_a
        axes[1].hist(lift_samples, bins=50, alpha=0.7, color='green', density=True)
        axes[1].axvline(0, color='red', linestyle='--', label='No Effect')
        axes[1].axvline(np.mean(lift_samples), color='black', linestyle='-',
                       label=f'Mean Lift: {np.mean(lift_samples):.2%}')

        # Add credible interval
        ci_lower, ci_upper = PosteriorAnalyzer.compute_credible_interval(lift_samples, 0.95)
        axes[1].axvspan(ci_lower, ci_upper, alpha=0.2, color='green',
                       label=f'95% CI: [{ci_lower:.2%}, {ci_upper:.2%}]')

        axes[1].set_xlabel('Relative Lift')
        axes[1].set_ylabel('Density')
        axes[1].set_title('Distribution of Lift (B vs A)')
        axes[1].legend()
        axes[1].grid(alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

    @staticmethod
    def generate_report(
        result: BayesianResult,
        control_label: str = 'A',
        treatment_label: str = 'B'
    ) -> str:
        """
        Generate human-readable report of Bayesian analysis.

        Args:
            result: BayesianResult object
            control_label: Label for control
            treatment_label: Label for treatment

        Returns:
            Formatted report string
        """
        report = f"""
Bayesian A/B Test Analysis Report
{'=' * 50}

Probability {treatment_label} beats {control_label}: {result.prob_b_beats_a:.1%}

Expected Lift: {result.expected_lift:.2%}
95% Credible Interval: [{result.credible_interval[0]:.2%}, {result.credible_interval[1]:.2%}]

Expected Loss (if choosing {treatment_label}): {result.expected_loss_choose_b:.4f}

Posterior Statistics:
{'=' * 50}
{control_label} (Control):
  Mean: {result.posterior_a['mean']:.4f}
  Std:  {result.posterior_a['std']:.4f}

{treatment_label} (Treatment):
  Mean: {result.posterior_b['mean']:.4f}
  Std:  {result.posterior_b['std']:.4f}

Decision Guidance:
{'=' * 50}
"""

        # Decision thresholds
        if result.prob_b_beats_a > 0.95 and result.expected_loss_choose_b < 0.01:
            decision = f"STRONG EVIDENCE for {treatment_label}. Deploy with confidence."
        elif result.prob_b_beats_a > 0.90:
            decision = f"MODERATE EVIDENCE for {treatment_label}. Consider deployment with monitoring."
        elif result.prob_b_beats_a < 0.10:
            decision = f"STRONG EVIDENCE for {control_label}. Do not deploy {treatment_label}."
        elif result.prob_b_beats_a < 0.20:
            decision = f"MODERATE EVIDENCE for {control_label}. Do not deploy {treatment_label}."
        else:
            decision = "INCONCLUSIVE. Continue collecting data or use expected loss threshold."

        report += decision

        return report
\end{lstlisting}

\subsection{Prior Selection and Sensitivity Analysis}

\begin{lstlisting}[language=Python, caption={Prior Selection with Sensitivity Analysis}]
class PriorSelector:
    """
    Tools for prior selection and sensitivity analysis.

    Helps choose appropriate priors and assess robustness of conclusions
    to prior specification.

    Example:
        >>> selector = PriorSelector()
        >>> priors = selector.create_prior_grid(
        ...     distribution='beta',
        ...     param_ranges={'alpha': [1, 2, 5], 'beta': [1, 2, 5]}
        ... )
        >>> sensitivity = selector.sensitivity_analysis(
        ...     control_data, treatment_data, priors
        ... )
    """

    @staticmethod
    def create_uninformative_prior(distribution: str = 'beta') -> Dict:
        """
        Create uninformative prior (Jeffreys prior).

        Args:
            distribution: 'beta' or 'normal'

        Returns:
            Prior specification dictionary
        """
        if distribution == 'beta':
            # Jeffreys prior for Bernoulli: Beta(0.5, 0.5)
            return {'distribution': 'beta', 'alpha': 0.5, 'beta': 0.5}
        elif distribution == 'normal':
            # Flat prior (very large variance)
            return {'distribution': 'normal', 'mu': 0, 'sigma': 10000}
        else:
            raise ValueError(f"Unknown distribution: {distribution}")

    @staticmethod
    def create_informative_prior(
        distribution: str,
        historical_data: Optional[Dict] = None,
        **params
    ) -> Dict:
        """
        Create informative prior from historical data or expert knowledge.

        Args:
            distribution: 'beta' or 'normal'
            historical_data: For beta: {'successes': int, 'trials': int}
                            For normal: {'mean': float, 'std': float, 'n': int}
            **params: Direct parameters if not using historical data

        Returns:
            Prior specification dictionary
        """
        if distribution == 'beta':
            if historical_data:
                # Use historical data to parameterize prior
                alpha = historical_data['successes'] + 1
                beta = historical_data['trials'] - historical_data['successes'] + 1
                return {'distribution': 'beta', 'alpha': alpha, 'beta': beta}
            else:
                return {'distribution': 'beta', **params}

        elif distribution == 'normal':
            if historical_data:
                # Historical mean and uncertainty
                mu = historical_data['mean']
                # Sigma reflects uncertainty: larger n = more certain prior
                sigma = historical_data['std'] / np.sqrt(historical_data['n'])
                return {'distribution': 'normal', 'mu': mu, 'sigma': sigma}
            else:
                return {'distribution': 'normal', **params}

        else:
            raise ValueError(f"Unknown distribution: {distribution}")

    @staticmethod
    def create_skeptical_prior(
        distribution: str,
        null_effect: bool = True,
        **params
    ) -> Dict:
        """
        Create skeptical prior centered at null or negative effect.

        Useful when testing "breakthrough" claims or new unproven methods.

        Args:
            distribution: 'beta' or 'normal'
            null_effect: If True, center at null; if False, center at negative effect
            **params: Additional parameters

        Returns:
            Prior specification dictionary
        """
        if distribution == 'beta':
            if null_effect:
                # Centered at 0.5 (no difference)
                return {'distribution': 'beta', 'alpha': 5, 'beta': 5}
            else:
                # Skewed toward worse performance
                return {'distribution': 'beta', 'alpha': 3, 'beta': 7}

        elif distribution == 'normal':
            if null_effect:
                # Centered at 0
                return {'distribution': 'normal', 'mu': 0, 'sigma': params.get('sigma', 10)}
            else:
                # Centered at negative effect
                return {'distribution': 'normal', 'mu': -0.05, 'sigma': params.get('sigma', 0.02)}

        else:
            raise ValueError(f"Unknown distribution: {distribution}")

    def create_prior_grid(
        self,
        distribution: str,
        param_ranges: Dict[str, List]
    ) -> List[Dict]:
        """
        Create grid of priors for sensitivity analysis.

        Args:
            distribution: 'beta' or 'normal'
            param_ranges: Dictionary mapping parameter names to lists of values
                         E.g., {'alpha': [1, 2, 5], 'beta': [1, 2, 5]}

        Returns:
            List of prior specifications
        """
        from itertools import product

        param_names = list(param_ranges.keys())
        param_values = list(param_ranges.values())

        priors = []
        for combo in product(*param_values):
            prior = {'distribution': distribution}
            for name, value in zip(param_names, combo):
                prior[name] = value
            priors.append(prior)

        return priors

    def sensitivity_analysis(
        self,
        control_data: Union[Dict, np.ndarray],
        treatment_data: Union[Dict, np.ndarray],
        priors: List[Dict],
        metric_type: str = 'binary'
    ) -> pd.DataFrame:
        """
        Perform sensitivity analysis across multiple priors.

        Args:
            control_data: Control arm data
            treatment_data: Treatment arm data
            priors: List of prior specifications
            metric_type: 'binary' or 'continuous'

        Returns:
            DataFrame with results for each prior
        """
        results = []

        for i, prior in enumerate(priors):
            test = BayesianABTest(metric_type=metric_type)
            test.set_prior(**prior)

            result = test.analyze(control_data, treatment_data)

            results.append({
                'prior_id': i,
                'prior': str(prior),
                'prob_b_beats_a': result.prob_b_beats_a,
                'expected_lift': result.expected_lift,
                'ci_lower': result.credible_interval[0],
                'ci_upper': result.credible_interval[1],
                'expected_loss': result.expected_loss_choose_b
            })

        df = pd.DataFrame(results)

        # Summary statistics
        print("Sensitivity Analysis Summary:")
        print(f"  Prob(B > A) range: [{df['prob_b_beats_a'].min():.3f}, {df['prob_b_beats_a'].max():.3f}]")
        print(f"  Expected lift range: [{df['expected_lift'].min():.2%}, {df['expected_lift'].max():.2%}]")
        print(f"  Prob(B > A) std dev: {df['prob_b_beats_a'].std():.3f}")

        # Check robustness
        if df['prob_b_beats_a'].std() < 0.05:
            print("\n  ROBUST: Conclusion is insensitive to prior choice")
        else:
            print("\n  CAUTION: Conclusion depends on prior specification")

        return df
\end{lstlisting}

\subsection{Bayesian Updating with Continuous Data Collection}

\begin{lstlisting}[language=Python, caption={Continuous Bayesian Updating}]
class BayesianUpdater:
    """
    Continuous Bayesian updating as data accumulates.

    Tracks posterior evolution over time, enabling continuous monitoring
    without the "peeking problem" of frequentist methods.

    Example:
        >>> updater = BayesianUpdater(metric_type='binary')
        >>> updater.set_initial_prior('beta', alpha=1, beta=1)
        >>>
        >>> # Day 1: 100 users per arm
        >>> updater.update(
        ...     control_data={'successes': 10, 'trials': 100},
        ...     treatment_data={'successes': 13, 'trials': 100}
        ... )
        >>>
        >>> # Day 2: 100 more users per arm (cumulative)
        >>> updater.update(
        ...     control_data={'successes': 21, 'trials': 200},
        ...     treatment_data={'successes': 28, 'trials': 200}
        ... )
        >>>
        >>> updater.plot_evolution()
    """

    def __init__(
        self,
        metric_type: str = 'binary',
        decision_threshold: float = 0.95,
        loss_threshold: float = 0.01
    ):
        """
        Initialize Bayesian updater.

        Args:
            metric_type: 'binary' or 'continuous'
            decision_threshold: Probability threshold for deployment decision
            loss_threshold: Expected loss threshold for deployment
        """
        self.metric_type = metric_type
        self.decision_threshold = decision_threshold
        self.loss_threshold = loss_threshold

        self.history = []
        self.initial_prior = None
        self.test = BayesianABTest(metric_type=metric_type)

    def set_initial_prior(self, distribution: str, **params) -> None:
        """Set initial prior before any data."""
        self.initial_prior = {'distribution': distribution, **params}
        self.test.set_prior(distribution, **params)

    def update(
        self,
        control_data: Union[Dict, np.ndarray],
        treatment_data: Union[Dict, np.ndarray],
        timestamp: Optional[str] = None
    ) -> BayesianResult:
        """
        Update posteriors with new data.

        Args:
            control_data: Cumulative control data
            treatment_data: Cumulative treatment data
            timestamp: Optional timestamp for tracking

        Returns:
            Current BayesianResult
        """
        result = self.test.analyze(control_data, treatment_data)

        # Record history
        self.history.append({
            'timestamp': timestamp or len(self.history),
            'prob_b_beats_a': result.prob_b_beats_a,
            'expected_lift': result.expected_lift,
            'ci_lower': result.credible_interval[0],
            'ci_upper': result.credible_interval[1],
            'expected_loss': result.expected_loss_choose_b,
            'decision': self._make_decision(result)
        })

        # Update prior to current posterior for next iteration
        # (This is implicit since we provide cumulative data)

        return result

    def _make_decision(self, result: BayesianResult) -> str:
        """Make deployment decision based on thresholds."""
        if (result.prob_b_beats_a >= self.decision_threshold and
            result.expected_loss_choose_b <= self.loss_threshold):
            return 'DEPLOY_B'
        elif (result.prob_b_beats_a <= (1 - self.decision_threshold) and
              result.expected_loss_choose_b >= self.loss_threshold):
            return 'KEEP_A'
        else:
            return 'CONTINUE'

    def get_current_decision(self) -> str:
        """Get the most recent decision."""
        if self.history:
            return self.history[-1]['decision']
        return 'NO_DATA'

    def plot_evolution(self, save_path: Optional[str] = None) -> None:
        """
        Plot evolution of posterior probability over time.

        Args:
            save_path: Optional path to save figure
        """
        if not self.history:
            print("No data to plot")
            return

        df = pd.DataFrame(self.history)

        fig, axes = plt.subplots(2, 1, figsize=(10, 8))

        # Plot 1: Probability B beats A over time
        axes[0].plot(df['timestamp'], df['prob_b_beats_a'],
                    marker='o', linewidth=2, label='P(B > A)')
        axes[0].axhline(self.decision_threshold, color='green',
                       linestyle='--', label=f'Deploy Threshold ({self.decision_threshold})')
        axes[0].axhline(1 - self.decision_threshold, color='red',
                       linestyle='--', label=f'Reject Threshold ({1-self.decision_threshold})')
        axes[0].axhline(0.5, color='gray', linestyle=':', alpha=0.5, label='Equiprobable')
        axes[0].fill_between(df['timestamp'],
                            1 - self.decision_threshold,
                            self.decision_threshold,
                            alpha=0.2, color='yellow', label='Inconclusive Region')
        axes[0].set_xlabel('Time / Sample Size')
        axes[0].set_ylabel('Probability')
        axes[0].set_title('Evolution of P(Treatment > Control)')
        axes[0].legend()
        axes[0].grid(alpha=0.3)
        axes[0].set_ylim([0, 1])

        # Plot 2: Expected lift with credible intervals
        axes[1].plot(df['timestamp'], df['expected_lift'],
                    marker='o', linewidth=2, color='blue', label='Expected Lift')
        axes[1].fill_between(df['timestamp'],
                            df['ci_lower'],
                            df['ci_upper'],
                            alpha=0.3, color='blue', label='95% Credible Interval')
        axes[1].axhline(0, color='red', linestyle='--', label='No Effect')
        axes[1].set_xlabel('Time / Sample Size')
        axes[1].set_ylabel('Relative Lift')
        axes[1].set_title('Evolution of Expected Lift with Uncertainty')
        axes[1].legend()
        axes[1].grid(alpha=0.3)

        # Format y-axis as percentage
        axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

    def get_summary(self) -> pd.DataFrame:
        """Get summary of update history."""
        return pd.DataFrame(self.history)
\end{lstlisting}

\subsection{Practical Example: Bayesian A/B Test in Production}

\begin{lstlisting}[language=Python, caption={Complete Bayesian A/B Test Workflow}]
# Scenario: E-commerce testing new checkout flow
# Historical conversion rate: 8% (from 10,000 past users)

# Step 1: Set up test with informative prior based on historical data
selector = PriorSelector()
prior = selector.create_informative_prior(
    distribution='beta',
    historical_data={'successes': 800, 'trials': 10000}
)

print(f"Informative prior: Beta({prior['alpha']}, {prior['beta']})")
# Output: Informative prior: Beta(801, 9201)
# This is centered at 8% with reasonable uncertainty

# Step 2: Initialize updater for continuous monitoring
updater = BayesianUpdater(
    metric_type='binary',
    decision_threshold=0.95,  # 95% probability to deploy
    loss_threshold=0.005      # Max 0.5pp absolute loss acceptable
)

updater.set_initial_prior('beta', alpha=prior['alpha'], beta=prior['beta'])

# Step 3: Simulate continuous data collection
# Day 1: 500 users per arm
result_day1 = updater.update(
    control_data={'successes': 42, 'trials': 500},  # 8.4%
    treatment_data={'successes': 48, 'trials': 500},  # 9.6%
    timestamp='Day 1'
)

print(f"\nDay 1 Results:")
print(f"  P(B > A) = {result_day1.prob_b_beats_a:.3f}")
print(f"  Expected lift = {result_day1.expected_lift:.2%}")
print(f"  Decision: {updater.get_current_decision()}")

# Day 3: 1,000 users per arm (cumulative)
result_day3 = updater.update(
    control_data={'successes': 83, 'trials': 1000},  # 8.3%
    treatment_data={'successes': 97, 'trials': 1000},  # 9.7%
    timestamp='Day 3'
)

print(f"\nDay 3 Results:")
print(f"  P(B > A) = {result_day3.prob_b_beats_a:.3f}")
print(f"  Expected lift = {result_day3.expected_lift:.2%}")
print(f"  Decision: {updater.get_current_decision()}")

# Day 7: 2,500 users per arm (cumulative)
result_day7 = updater.update(
    control_data={'successes': 205, 'trials': 2500},  # 8.2%
    treatment_data={'successes': 243, 'trials': 2500},  # 9.7%
    timestamp='Day 7'
)

print(f"\nDay 7 Results:")
print(f"  P(B > A) = {result_day7.prob_b_beats_a:.3f}")
print(f"  Expected lift = {result_day7.expected_lift:.2%}")
print(f"  Decision: {updater.get_current_decision()}")

# Step 4: Generate detailed report
analyzer = PosteriorAnalyzer()
report = analyzer.generate_report(result_day7, 'Old Checkout', 'New Checkout')
print(report)

# Step 5: Sensitivity analysis with different priors
print("\n" + "="*50)
print("Sensitivity Analysis")
print("="*50)

priors_to_test = [
    selector.create_uninformative_prior('beta'),
    selector.create_informative_prior('beta', historical_data={'successes': 800, 'trials': 10000}),
    selector.create_skeptical_prior('beta', null_effect=True)
]

for i, test_prior in enumerate(priors_to_test):
    test = BayesianABTest(metric_type='binary')
    test.set_prior(**test_prior)
    result = test.analyze(
        control_data={'successes': 205, 'trials': 2500},
        treatment_data={'successes': 243, 'trials': 2500}
    )
    print(f"\nPrior {i+1}: {test_prior}")
    print(f"  P(B > A) = {result.prob_b_beats_a:.3f}")
    print(f"  Expected lift = {result.expected_lift:.2%}")

# Step 6: Visualize posterior evolution
updater.plot_evolution(save_path='bayesian_evolution.png')

# Step 7: Plot final posteriors
analyzer.plot_posteriors(
    result_day7.samples_a,
    result_day7.samples_b,
    labels=('Old Checkout', 'New Checkout'),
    save_path='bayesian_posteriors.png'
)

# Step 8: Compute probability of practical significance
rope_analysis = analyzer.prob_practical_significance(
    result_day7.samples_a,
    result_day7.samples_b,
    rope=(-0.02, 0.02)  # Within +/-2% is considered equivalent
)

print("\n" + "="*50)
print("Practical Significance Analysis (ROPE: +/-2%)")
print("="*50)
print(f"Prob(Practically Better): {rope_analysis['prob_practically_better']:.3f}")
print(f"Prob(Practically Equivalent): {rope_analysis['prob_practically_equivalent']:.3f}")
print(f"Prob(Practically Worse): {rope_analysis['prob_practically_worse']:.3f}")
\end{lstlisting}

This Bayesian framework offers a powerful alternative to frequentist A/B testing, particularly when:
\begin{itemize}
    \item You have valuable prior information from historical data or domain expertise
    \item Sample sizes are limited or expensive to collect
    \item Stakeholders prefer direct probability statements over p-values
    \item Continuous monitoring is required without sequential testing complications
    \item Decision-making involves quantifying expected loss and risk
\end{itemize}

The sensitivity analysis demonstrates that conclusions are robust to prior specification when data is informative, addressing the main criticism of Bayesian methods. Combined with the continuous updating capability, this approach enables faster, more informed decisions while maintaining rigorous quantification of uncertainty.

\section{Sequential Testing and Early Stopping}

Sequential testing enables stopping experiments early while controlling error rates. Unlike fixed-horizon A/B tests that require waiting for a pre-determined sample size, sequential methods allow for interim analyses with statistically rigorous early stopping criteria. This reduces time-to-decision and minimizes exposure to inferior treatments while maintaining Type I and Type II error guarantees.

\subsection{The Sequential Testing Challenge}

Traditional A/B tests face a critical tension:
\begin{itemize}
    \item \textbf{Statistical Rigor}: Requires large samples and waiting until the end
    \item \textbf{Business Pressure}: Stakeholders want results quickly to make decisions
    \item \textbf{Peeking Problem}: Looking at results multiple times inflates Type I error rate
\end{itemize}

Without proper sequential methods, repeatedly checking p-values leads to false positives. If you check a null A/A test 10 times, the probability of finding p < 0.05 at least once is approximately 40\%, not 5\%!

\subsection{Real-World Scenario: The Impatient Product Manager}

\subsubsection{The Setup}

A fintech company is testing a new onboarding flow designed to increase conversion from signup to first deposit. The PM, Sarah, is under pressure from executives to either launch the new flow or allocate engineering resources elsewhere.

\textbf{Experiment Design:}
\begin{itemize}
    \item \textbf{Control}: Current onboarding (15\% conversion)
    \item \textbf{Treatment}: Simplified 3-step flow (unknown conversion)
    \item \textbf{Target}: Detect 2pp improvement (15\% → 17\%, 13\% relative lift)
    \item \textbf{Power Analysis}: Need 8,500 users per arm for 80\% power at $\alpha=0.05$
    \item \textbf{Timeline}: 6 weeks at current traffic levels
\end{itemize}

\subsubsection{Week 1: The First Peek}

After just 1 week with 1,500 users per arm, Sarah checks the results:
\begin{itemize}
    \item Control: 15.2\% conversion (228/1500)
    \item Treatment: 17.8\% conversion (267/1500)
    \item Difference: 2.6pp (p = 0.048)
\end{itemize}

\textbf{Sarah's Reaction:} "It's significant! p < 0.05! Let's ship it now and save 5 weeks!"

\textbf{The Data Scientist's Warning:} "This is a classic peeking problem. We're at 18\% of planned sample size. If we make decisions at this point, our effective Type I error rate is much higher than 5\%."

\subsubsection{Weeks 2-4: The Roller Coaster}

Sarah checks results every Monday despite warnings:

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Week} & \textbf{Control} & \textbf{Treatment} & \textbf{Diff} & \textbf{p-value} \\
\midrule
1 & 15.2\% & 17.8\% & +2.6pp & 0.048 \\
2 & 15.1\% & 16.2\% & +1.1pp & 0.156 \\
3 & 15.0\% & 15.8\% & +0.8pp & 0.342 \\
4 & 15.2\% & 16.1\% & +0.9pp & 0.287 \\
\bottomrule
\end{tabular}
\caption{Weekly results showing variance in early estimates}
\end{table}

Week 1's "significant" result was noise. By Week 3, Sarah is panicking: "Should we stop for futility? We're wasting traffic on a test that won't win!"

\subsubsection{The Solution: Group Sequential Design}

The data scientist implements a proper sequential testing framework:

\textbf{Group Sequential Design with 5 Planned Looks:}
\begin{enumerate}
    \item Week 2: 25\% information (2,125 per arm)
    \item Week 3: 50\% information (4,250 per arm)
    \item Week 4: 75\% information (6,375 per arm)
    \item Week 5: 90\% information (7,650 per arm)
    \item Week 6: 100\% information (8,500 per arm)
\end{enumerate}

\textbf{O'Brien-Fleming Boundaries:}
\begin{itemize}
    \item Look 1 (25\%): Reject if p < 0.0001 (efficacy) or futility boundary crossed
    \item Look 2 (50\%): Reject if p < 0.005
    \item Look 3 (75\%): Reject if p < 0.014
    \item Look 4 (90\%): Reject if p < 0.023
    \item Look 5 (100\%): Reject if p < 0.041
\end{itemize}

\subsubsection{Week 3: Early Efficacy Stop}

At the planned Week 3 interim analysis (50\% information, 4,250 per arm):
\begin{itemize}
    \item Control: 15.1\% (642/4250)
    \item Treatment: 17.6\% (748/4250)
    \item Difference: 2.5pp (95\% CI: [1.1pp, 3.9pp])
    \item p-value: 0.0008
\end{itemize}

\textbf{Decision:} p = 0.0008 < 0.005 (the O'Brien-Fleming boundary for Look 2)

The test crosses the efficacy boundary! The data scientist recommends stopping for efficacy with controlled Type I error. The new flow is launched 3 weeks early, saving:
\begin{itemize}
    \item \textbf{Time}: 3 weeks faster to market
    \item \textbf{Traffic}: 50\% reduction in sample size needed
    \item \textbf{Revenue}: 3 weeks of improved conversion = \$180,000 additional deposits
    \item \textbf{Statistical Rigor}: Maintained $\alpha = 0.05$ despite early stopping
\end{itemize}

\subsubsection{Key Lessons}

\begin{enumerate}
    \item \textbf{Plan Interim Analyses}: Pre-specify when you'll look and with what boundaries
    \item \textbf{Use Alpha Spending}: Properly allocate Type I error across looks
    \item \textbf{Don't Peek Ad-Hoc}: Unplanned peeking destroys error rate guarantees
    \item \textbf{Consider Futility}: Early stopping for lack of effect saves resources
    \item \textbf{Communicate Uncertainty}: Early results are noisier than final results
\end{enumerate}

\textbf{What if Sarah had shipped at Week 1?} The 2.6pp effect was inflated by early variance. Proper sequential testing revealed the true effect was 2.5pp, and the formal boundaries prevented a premature decision that would have been vindicated by luck but violated statistical principles.

\subsection{Mathematical Foundations of Sequential Testing}

\subsubsection{The Peeking Problem}

Let $H_0$ be the null hypothesis of no treatment effect. With a fixed $\alpha = 0.05$ test, if we peek at the data $k$ times and stop when p < 0.05, the overall Type I error rate is:

\begin{equation}
P(\text{Reject } H_0 | H_0 \text{ true}) \approx 1 - (1 - \alpha)^k
\end{equation}

For k = 10 peeks: $P \approx 1 - 0.95^{10} \approx 0.40$ (40\% false positive rate!)

\subsubsection{Sequential Probability Ratio Test (SPRT)}

The SPRT (Wald, 1945) is the optimal sequential test that minimizes expected sample size while controlling error rates. For testing $H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$, define the likelihood ratio:

\begin{equation}
\Lambda_n = \frac{L(\theta_1 | x_1, \ldots, x_n)}{L(\theta_0 | x_1, \ldots, x_n)} = \prod_{i=1}^{n} \frac{f(x_i | \theta_1)}{f(x_i | \theta_0)}
\end{equation}

The SPRT stopping rule uses boundaries A and B (where A < 1 < B):

\begin{equation}
\begin{cases}
\text{Reject } H_0 \text{ (accept } H_1\text{)} & \text{if } \Lambda_n \geq B \\
\text{Accept } H_0 \text{ (reject } H_1\text{)} & \text{if } \Lambda_n \leq A \\
\text{Continue sampling} & \text{if } A < \Lambda_n < B
\end{cases}
\end{equation}

The boundaries are set using desired error rates $\alpha$ (Type I) and $\beta$ (Type II):

\begin{equation}
A = \frac{\beta}{1 - \alpha}, \quad B = \frac{1 - \beta}{\alpha}
\end{equation}

For $\alpha = 0.05$ and $\beta = 0.20$: A = 0.211, B = 19.0

\subsubsection{Group Sequential Methods}

Group sequential designs (Pocock, 1977; O'Brien-Fleming, 1979) allow K planned interim analyses at information fractions $t_1, t_2, \ldots, t_K$ where $0 < t_1 < t_2 < \cdots < t_K = 1$.

\textbf{Information Fraction:} $t_k = n_k / n_{max}$ where $n_k$ is sample size at look k.

At each look k, we test:
\begin{equation}
Z_k = \frac{\bar{X}_{T,k} - \bar{X}_{C,k}}{\sigma \sqrt{2/n_k}}
\end{equation}

\textbf{Pocock Boundary:} Uses constant critical value c across all looks:
\begin{equation}
\text{Reject } H_0 \text{ if } |Z_k| \geq c_p \quad \text{for any } k
\end{equation}

where $c_p$ satisfies $P(|Z_k| \geq c_p \text{ for any } k | H_0) = \alpha$. For K=5, $\alpha=0.05$: $c_p \approx 2.41$ (vs 1.96 for fixed design).

\textbf{O'Brien-Fleming Boundary:} Uses conservative early stopping:
\begin{equation}
\text{Reject } H_0 \text{ if } |Z_k| \geq \frac{c_{OF}}{\sqrt{t_k}}
\end{equation}

For K=5, $\alpha=0.05$: $c_{OF} \approx 2.04$. At look 1 (t=0.25): critical value = $2.04/\sqrt{0.25} = 4.08$ (very conservative). At look 5 (t=1.0): critical value = 2.04 (close to fixed 1.96).

\subsubsection{Alpha Spending Functions}

Lan and DeMets (1983) generalized boundaries using alpha spending functions $\alpha(t)$ that specify cumulative Type I error spent by information time t:

\begin{equation}
\alpha(t): [0, 1] \to [0, \alpha], \quad \alpha(0) = 0, \quad \alpha(1) = \alpha
\end{equation}

At look k, reject if p-value < $\alpha(t_k) - \alpha(t_{k-1})$ (incremental alpha).

\textbf{O'Brien-Fleming Spending:}
\begin{equation}
\alpha_{OF}(t) = 2\left(1 - \Phi\left(\frac{z_{\alpha/2}}{\sqrt{t}}\right)\right)
\end{equation}

\textbf{Pocock-like Spending:}
\begin{equation}
\alpha_{Pocock}(t) = \alpha \log(1 + (e - 1)t)
\end{equation}

\textbf{Kim-DeMets Power Family:}
\begin{equation}
\alpha_\rho(t) = \alpha t^\rho, \quad \rho > 0
\end{equation}

where $\rho = 1$ is linear spending, $\rho = 3$ approximates O'Brien-Fleming.

\subsection{Comprehensive Sequential Testing Framework}

\begin{lstlisting}[language=Python, caption={Comprehensive Sequential Testing Framework}]
from typing import Optional, Dict, Any, Tuple, List, Callable
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from scipy import stats
import logging

logger = logging.getLogger(__name__)

class AlphaSpendingFunction(Enum):
    """Alpha spending function types."""
    OBRIEN_FLEMING = "obrien_fleming"
    POCOCK = "pocock"
    LINEAR = "linear"
    KIM_DEMETS_POWER_3 = "kim_demets_power_3"
    CUSTOM = "custom"

class StoppingBoundary(Enum):
    """Types of stopping boundaries."""
    EFFICACY = "efficacy"  # Stop when treatment shows benefit
    FUTILITY = "futility"  # Stop when unlikely to show benefit
    BOTH = "both"

@dataclass
class InterimAnalysis:
    """
    Result from an interim analysis.

    Attributes:
        look: Which interim analysis (1, 2, ...)
        information_fraction: Fraction of planned information (0-1)
        n_control: Sample size in control arm
        n_treatment: Sample size in treatment arm
        control_mean: Observed control mean
        treatment_mean: Observed treatment mean
        effect_size: Standardized effect size
        z_statistic: Z-test statistic
        p_value: Two-sided p-value
        efficacy_boundary: Critical value for efficacy
        futility_boundary: Critical value for futility (if applicable)
        decision: Stop for efficacy, futility, or continue
        cumulative_alpha_spent: Total alpha spent up to this look
    """
    look: int
    information_fraction: float
    n_control: int
    n_treatment: int
    control_mean: float
    treatment_mean: float
    effect_size: float
    z_statistic: float
    p_value: float
    efficacy_boundary: float
    futility_boundary: Optional[float]
    decision: str
    cumulative_alpha_spent: float
    conditional_power: Optional[float] = None

class SequentialTester:
    """
    Comprehensive sequential testing framework with efficacy and futility stopping.

    Implements group sequential designs with:
    - Multiple alpha spending functions (O'Brien-Fleming, Pocock, Kim-DeMets)
    - Futility boundaries for early stopping when treatment unlikely to win
    - Adaptive sample size re-estimation
    - Conditional power monitoring

    Example:
        >>> tester = SequentialTester(
        ...     alpha=0.05,
        ...     power=0.80,
        ...     n_looks=5,
        ...     spending_function=AlphaSpendingFunction.OBRIEN_FLEMING,
        ...     use_futility=True
        ... )
        >>> result = tester.analyze_interim(
        ...     control_data, treatment_data,
        ...     information_fraction=0.5
        ... )
        >>> if result.decision != 'continue':
        ...     print(f"Stop for {result.decision}")
    """

    def __init__(
        self,
        alpha: float = 0.05,
        power: float = 0.80,
        n_looks: int = 5,
        spending_function: AlphaSpendingFunction = AlphaSpendingFunction.OBRIEN_FLEMING,
        use_futility: bool = True,
        futility_threshold: float = 0.20,
        max_sample_size: Optional[int] = None,
        effect_size: Optional[float] = None
    ):
        """
        Initialize sequential tester.

        Args:
            alpha: Overall significance level (Type I error)
            power: Desired power (1 - Type II error)
            n_looks: Number of planned interim analyses
            spending_function: Alpha spending function type
            use_futility: Whether to implement futility stopping
            futility_threshold: Conditional power threshold for futility (default 20%)
            max_sample_size: Maximum planned sample size per arm
            effect_size: Expected effect size (Cohen's d)
        """
        self.alpha = alpha
        self.power = power
        self.n_looks = n_looks
        self.spending_function = spending_function
        self.use_futility = use_futility
        self.futility_threshold = futility_threshold
        self.max_sample_size = max_sample_size
        self.effect_size = effect_size

        # Planned information fractions (equally spaced by default)
        self.information_fractions = np.linspace(
            1/n_looks, 1.0, n_looks
        ).tolist()

        # Compute efficacy boundaries
        self.efficacy_boundaries = self._compute_efficacy_boundaries()

        # Compute futility boundaries if requested
        if use_futility:
            self.futility_boundaries = self._compute_futility_boundaries()
        else:
            self.futility_boundaries = [None] * n_looks

        # Track interim analyses
        self.current_look = 0
        self.interim_results: List[InterimAnalysis] = []
        self.stopped = False
        self.stop_reason = None

        logger.info(
            f"Initialized SequentialTester: "
            f"n_looks={n_looks}, spending={spending_function.value}, "
            f"futility={use_futility}"
        )

    def _compute_alpha_spending(self, information_fraction: float) -> float:
        """
        Compute cumulative alpha spent at given information fraction.

        Args:
            information_fraction: Fraction of planned information (0-1)

        Returns:
            Cumulative alpha spent
        """
        t = information_fraction
        z_alpha_2 = stats.norm.ppf(1 - self.alpha / 2)

        if self.spending_function == AlphaSpendingFunction.OBRIEN_FLEMING:
            # O'Brien-Fleming: conservative early, liberal late
            alpha_spent = 2 * (1 - stats.norm.cdf(z_alpha_2 / np.sqrt(t)))

        elif self.spending_function == AlphaSpendingFunction.POCOCK:
            # Pocock-like: more uniform spending
            alpha_spent = self.alpha * np.log(1 + (np.e - 1) * t)

        elif self.spending_function == AlphaSpendingFunction.LINEAR:
            # Linear spending
            alpha_spent = self.alpha * t

        elif self.spending_function == AlphaSpendingFunction.KIM_DEMETS_POWER_3:
            # Kim-DeMets power family with rho=3
            alpha_spent = self.alpha * (t ** 3)

        else:
            # Default to O'Brien-Fleming
            alpha_spent = 2 * (1 - stats.norm.cdf(z_alpha_2 / np.sqrt(t)))

        return alpha_spent

    def _compute_efficacy_boundaries(self) -> List[float]:
        """
        Compute Z-score efficacy boundaries for each look.

        Returns:
            List of Z-score critical values
        """
        boundaries = []
        prev_alpha = 0.0

        for t in self.information_fractions:
            # Cumulative alpha at this look
            cumulative_alpha = self._compute_alpha_spending(t)

            # Incremental alpha for this look
            incremental_alpha = cumulative_alpha - prev_alpha

            # Convert to Z-score boundary (two-sided)
            z_boundary = stats.norm.ppf(1 - incremental_alpha / 2)

            boundaries.append(z_boundary)
            prev_alpha = cumulative_alpha

        return boundaries

    def _compute_futility_boundaries(self) -> List[Optional[float]]:
        """
        Compute futility boundaries based on conditional power.

        Returns:
            List of Z-score futility boundaries (None if no futility check)
        """
        # Use beta-spending analogous to alpha-spending
        # Futility boundary based on conditional power threshold
        boundaries = []

        for idx, t in enumerate(self.information_fractions):
            # Don't apply futility at final look
            if idx == len(self.information_fractions) - 1:
                boundaries.append(None)
            else:
                # Compute futility boundary
                # Based on maintaining conditional power above threshold
                # Simplified: use fraction of non-binding futility index
                z_futility = stats.norm.ppf(self.futility_threshold) * np.sqrt(t)
                boundaries.append(z_futility)

        return boundaries

    def analyze_interim(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        information_fraction: Optional[float] = None
    ) -> InterimAnalysis:
        """
        Perform interim analysis with efficacy and futility stopping.

        Args:
            control_data: Control arm data at this look
            treatment_data: Treatment arm data at this look
            information_fraction: Fraction of planned information (auto if None)

        Returns:
            InterimAnalysis with decision and statistics
        """
        if self.stopped:
            raise ValueError(f"Test already stopped for {self.stop_reason}")

        self.current_look += 1

        if self.current_look > self.n_looks:
            raise ValueError(
                f"Exceeded planned looks: {self.current_look} > {self.n_looks}"
            )

        # Calculate information fraction
        if information_fraction is None:
            information_fraction = self.information_fractions[self.current_look - 1]

        # Calculate statistics
        n_control = len(control_data)
        n_treatment = len(treatment_data)
        control_mean = np.mean(control_data)
        treatment_mean = np.mean(treatment_data)

        # Pooled standard deviation
        pooled_var = (
            (n_control - 1) * np.var(control_data, ddof=1) +
            (n_treatment - 1) * np.var(treatment_data, ddof=1)
        ) / (n_control + n_treatment - 2)
        pooled_std = np.sqrt(pooled_var)

        # Z-statistic
        se = pooled_std * np.sqrt(1/n_control + 1/n_treatment)
        z_stat = (treatment_mean - control_mean) / se

        # P-value (two-sided)
        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))

        # Effect size (Cohen's d)
        effect_size = (treatment_mean - control_mean) / pooled_std

        # Get boundaries for this look
        efficacy_boundary = self.efficacy_boundaries[self.current_look - 1]
        futility_boundary = self.futility_boundaries[self.current_look - 1]

        # Cumulative alpha spent
        cumulative_alpha = self._compute_alpha_spending(information_fraction)

        # Conditional power (if not final look)
        conditional_power = None
        if self.current_look < self.n_looks:
            conditional_power = self._calculate_conditional_power(
                z_stat, information_fraction
            )

        # Make decision
        decision = "continue"

        # Check efficacy boundary
        if abs(z_stat) >= efficacy_boundary:
            decision = "efficacy"
            self.stopped = True
            self.stop_reason = "efficacy"
            logger.info(
                f"Stopping for efficacy at look {self.current_look}: "
                f"|Z|={abs(z_stat):.3f} >= {efficacy_boundary:.3f}"
            )

        # Check futility boundary (if enabled and not last look)
        elif (self.use_futility and futility_boundary is not None and
              conditional_power is not None):
            if conditional_power < self.futility_threshold:
                decision = "futility"
                self.stopped = True
                self.stop_reason = "futility"
                logger.info(
                    f"Stopping for futility at look {self.current_look}: "
                    f"CP={conditional_power:.1%} < {self.futility_threshold:.1%}"
                )

        # Final look without crossing efficacy
        elif self.current_look == self.n_looks and decision == "continue":
            decision = "no_effect"
            self.stopped = True
            self.stop_reason = "completed"

        # Create result
        result = InterimAnalysis(
            look=self.current_look,
            information_fraction=information_fraction,
            n_control=n_control,
            n_treatment=n_treatment,
            control_mean=control_mean,
            treatment_mean=treatment_mean,
            effect_size=effect_size,
            z_statistic=z_stat,
            p_value=p_value,
            efficacy_boundary=efficacy_boundary,
            futility_boundary=futility_boundary,
            decision=decision,
            cumulative_alpha_spent=cumulative_alpha,
            conditional_power=conditional_power
        )

        self.interim_results.append(result)

        logger.info(
            f"Look {self.current_look} ({information_fraction:.0%} info): "
            f"Z={z_stat:.3f}, p={p_value:.4f}, decision={decision}"
        )

        return result

    def _calculate_conditional_power(
        self,
        z_current: float,
        information_fraction: float
    ) -> float:
        """
        Calculate conditional power to detect effect at final analysis.

        Conditional power = P(reject H0 at end | current data and assumptions)

        Args:
            z_current: Current Z-statistic
            information_fraction: Current information fraction

        Returns:
            Conditional power (0-1)
        """
        # Remaining information
        remaining_info = 1 - information_fraction

        if remaining_info <= 0:
            return 1.0 if abs(z_current) >= self.efficacy_boundaries[-1] else 0.0

        # Expected Z at final analysis under current trend
        # Z_final ~ N(Z_current * sqrt(t_final/t_current), 1 - t_current)
        mean_z_final = z_current * np.sqrt(1 / information_fraction)
        var_z_final = 1 - information_fraction

        # Final efficacy boundary
        final_boundary = self.efficacy_boundaries[-1]

        # Conditional power (probability of crossing final boundary)
        if z_current > 0:
            # Positive effect: need Z_final > boundary
            cp = 1 - stats.norm.cdf(
                final_boundary,
                loc=mean_z_final,
                scale=np.sqrt(var_z_final)
            )
        else:
            # Negative effect: need Z_final < -boundary
            cp = stats.norm.cdf(
                -final_boundary,
                loc=mean_z_final,
                scale=np.sqrt(var_z_final)
            )

        return cp

    def get_summary(self) -> Dict[str, Any]:
        """
        Get summary of sequential test.

        Returns:
            Summary statistics
        """
        return {
            'n_looks_performed': self.current_look,
            'n_looks_planned': self.n_looks,
            'spending_function': self.spending_function.value,
            'stopped': self.stopped,
            'stop_reason': self.stop_reason,
            'interim_results': [
                {
                    'look': r.look,
                    'info_frac': r.information_fraction,
                    'z_stat': r.z_statistic,
                    'p_value': r.p_value,
                    'decision': r.decision,
                    'conditional_power': r.conditional_power
                }
                for r in self.interim_results
            ],
            'stopped_early': self.stopped and self.current_look < self.n_looks
        }

class EarlyStoppingCriteria:
    """
    Comprehensive early stopping criteria.

    Combines multiple stopping rules:
    - Statistical significance (efficacy)
    - Futility (conditional power)
    - Practical significance (effect size)
    - Safety (adverse events)
    """

    def __init__(
        self,
        min_effect_size: Optional[float] = None,
        max_adverse_rate: Optional[float] = None,
        min_conditional_power: float = 0.20
    ):
        """
        Initialize stopping criteria.

        Args:
            min_effect_size: Minimum practical effect size
            max_adverse_rate: Maximum acceptable adverse event rate
            min_conditional_power: Minimum conditional power threshold
        """
        self.min_effect_size = min_effect_size
        self.max_adverse_rate = max_adverse_rate
        self.min_conditional_power = min_conditional_power

    def should_stop(
        self,
        interim_result: InterimAnalysis,
        adverse_rate: Optional[float] = None
    ) -> Tuple[bool, str]:
        """
        Determine if experiment should stop.

        Args:
            interim_result: Current interim analysis
            adverse_rate: Observed adverse event rate (if applicable)

        Returns:
            (should_stop, reason)
        """
        # Statistical efficacy
        if interim_result.decision == "efficacy":
            # Check practical significance if threshold set
            if self.min_effect_size is not None:
                if abs(interim_result.effect_size) < self.min_effect_size:
                    return False, "Statistically but not practically significant"
            return True, "Efficacy boundary crossed"

        # Futility
        if interim_result.decision == "futility":
            return True, "Futility boundary crossed"

        # Safety
        if (adverse_rate is not None and
            self.max_adverse_rate is not None and
            adverse_rate > self.max_adverse_rate):
            return True, f"Adverse rate {adverse_rate:.1%} exceeds threshold"

        return False, "Continue"

class SequentialProbabilityRatioTest:
    """
    Sequential Probability Ratio Test (SPRT) implementation.

    Optimal sequential test minimizing expected sample size
    while controlling Type I and Type II error rates.
    """

    def __init__(
        self,
        theta0: float,
        theta1: float,
        alpha: float = 0.05,
        beta: float = 0.20
    ):
        """
        Initialize SPRT.

        Args:
            theta0: Null hypothesis parameter value
            theta1: Alternative hypothesis parameter value
            alpha: Type I error rate
            beta: Type II error rate
        """
        self.theta0 = theta0
        self.theta1 = theta1
        self.alpha = alpha
        self.beta = beta

        # Compute boundaries
        self.A = beta / (1 - alpha)  # Accept H0 boundary
        self.B = (1 - beta) / alpha  # Reject H0 boundary

        # Track likelihood ratio
        self.log_likelihood_ratio = 0.0
        self.n_observations = 0
        self.decision = None

        logger.info(
            f"Initialized SPRT: theta0={theta0}, theta1={theta1}, "
            f"A={self.A:.3f}, B={self.B:.3f}"
        )

    def update(self, observation: float) -> Optional[str]:
        """
        Update SPRT with new observation.

        Args:
            observation: New data point (e.g., 0 or 1 for Bernoulli)

        Returns:
            Decision: "reject_h0", "accept_h0", or None (continue)
        """
        if self.decision is not None:
            raise ValueError(f"Test already concluded: {self.decision}")

        # Update log-likelihood ratio
        # For Bernoulli: log(p1/p0) if success, log((1-p1)/(1-p0)) if failure
        if observation == 1:
            log_lr = np.log(self.theta1 / self.theta0)
        else:
            log_lr = np.log((1 - self.theta1) / (1 - self.theta0))

        self.log_likelihood_ratio += log_lr
        self.n_observations += 1

        # Check boundaries
        lr = np.exp(self.log_likelihood_ratio)

        if lr >= self.B:
            self.decision = "reject_h0"
            logger.info(
                f"SPRT: Reject H0 after {self.n_observations} observations "
                f"(LR={lr:.2f} >= {self.B:.2f})"
            )
        elif lr <= self.A:
            self.decision = "accept_h0"
            logger.info(
                f"SPRT: Accept H0 after {self.n_observations} observations "
                f"(LR={lr:.2f} <= {self.A:.2f})"
            )

        return self.decision

class AdaptiveSampleSize:
    """
    Adaptive sample size calculator for interim analyses.

    Re-estimates required sample size based on observed effect
    and variance, allowing for underpowered experiments to extend.
    """

    def __init__(
        self,
        initial_sample_size: int,
        alpha: float = 0.05,
        power: float = 0.80,
        max_sample_size_multiplier: float = 2.0
    ):
        """
        Initialize adaptive sample size calculator.

        Args:
            initial_sample_size: Original planned sample size per arm
            alpha: Significance level
            power: Desired power
            max_sample_size_multiplier: Maximum increase (e.g., 2.0 = double)
        """
        self.initial_n = initial_sample_size
        self.alpha = alpha
        self.power = power
        self.max_n = int(initial_sample_size * max_sample_size_multiplier)

        self.z_alpha = stats.norm.ppf(1 - alpha / 2)
        self.z_beta = stats.norm.ppf(power)

    def recalculate_sample_size(
        self,
        observed_effect: float,
        observed_variance: float,
        current_n: int
    ) -> Dict[str, Any]:
        """
        Recalculate required sample size based on interim data.

        Args:
            observed_effect: Observed treatment effect
            observed_variance: Observed variance
            current_n: Current sample size per arm

        Returns:
            Dictionary with updated sample size and recommendation
        """
        # Required sample size based on observed parameters
        # n = 2 * (z_alpha + z_beta)^2 * sigma^2 / delta^2
        if observed_effect == 0:
            required_n = self.max_n
        else:
            required_n = int(np.ceil(
                2 * ((self.z_alpha + self.z_beta) ** 2) * observed_variance /
                (observed_effect ** 2)
            ))

        # Cap at maximum
        required_n = min(required_n, self.max_n)

        # Recommendation
        if required_n <= current_n:
            recommendation = "sufficient"
            additional_n = 0
        elif required_n <= self.initial_n:
            recommendation = "continue_as_planned"
            additional_n = self.initial_n - current_n
        else:
            recommendation = "extend"
            additional_n = required_n - current_n

        return {
            'current_n': current_n,
            'required_n': required_n,
            'additional_n_needed': additional_n,
            'recommendation': recommendation,
            'observed_effect': observed_effect,
            'observed_variance': observed_variance,
            'information_fraction': current_n / required_n
        }
\end{lstlisting}

\subsection{Practical Usage: Sequential Testing in Production}

\begin{lstlisting}[language=Python, caption={Sequential Testing Usage Example}]
# Example: A/B test with sequential monitoring
import numpy as np

# Set up sequential tester
tester = SequentialTester(
    alpha=0.05,
    power=0.80,
    n_looks=5,
    spending_function=AlphaSpendingFunction.OBRIEN_FLEMING,
    use_futility=True,
    futility_threshold=0.20,
    max_sample_size=10000
)

# Planned information fractions
print("Efficacy boundaries (Z-scores):")
for i, (t, boundary) in enumerate(zip(
    tester.information_fractions,
    tester.efficacy_boundaries
), 1):
    print(f"  Look {i} ({t:.0%} info): Z >= {boundary:.3f}")

# Simulate experiment with true effect
np.random.seed(42)
true_control_mean = 15.0  # 15% conversion
true_treatment_mean = 17.0  # 17% conversion (2pp lift)

results = []
for look in range(1, 6):
    # Information fraction for this look
    info_frac = look / 5

    # Generate data (Bernoulli with approximation)
    n_per_arm = int(10000 * info_frac)

    control_data = np.random.binomial(
        1, true_control_mean / 100, size=n_per_arm
    ).astype(float) * 100

    treatment_data = np.random.binomial(
        1, true_treatment_mean / 100, size=n_per_arm
    ).astype(float) * 100

    # Analyze interim
    result = tester.analyze_interim(
        control_data,
        treatment_data,
        information_fraction=info_frac
    )

    print(f"\n=== Look {result.look} ({result.information_fraction:.0%} information) ===")
    print(f"Control: {result.control_mean:.2f}% (n={result.n_control})")
    print(f"Treatment: {result.treatment_mean:.2f}% (n={result.n_treatment})")
    print(f"Difference: {result.treatment_mean - result.control_mean:.2f}pp")
    print(f"Z-statistic: {result.z_statistic:.3f}")
    print(f"P-value: {result.p_value:.4f}")
    print(f"Efficacy boundary: Z >= {result.efficacy_boundary:.3f}")

    if result.conditional_power is not None:
        print(f"Conditional power: {result.conditional_power:.1%}")

    print(f"Decision: {result.decision}")

    results.append(result)

    # Stop if decision made
    if result.decision != "continue":
        break

# Summary
print("\n=== Test Summary ===")
summary = tester.get_summary()
print(f"Stopped after {summary['n_looks_performed']}/{summary['n_looks_planned']} looks")
print(f"Reason: {summary['stop_reason']}")
print(f"Stopped early: {summary['stopped_early']}")

# Example: SPRT for rapid testing
print("\n=== SPRT Example ===")
sprt = SequentialProbabilityRatioTest(
    theta0=0.10,  # H0: 10% conversion
    theta1=0.12,  # H1: 12% conversion
    alpha=0.05,
    beta=0.20
)

# Simulate observations
true_rate = 0.12  # True conversion is 12%
observations = np.random.binomial(1, true_rate, size=1000)

for i, obs in enumerate(observations, 1):
    decision = sprt.update(obs)
    if decision:
        print(f"SPRT stopped after {i} observations: {decision}")
        break

# Example: Adaptive sample size
print("\n=== Adaptive Sample Size Example ===")
adaptive = AdaptiveSampleSize(
    initial_sample_size=5000,
    alpha=0.05,
    power=0.80,
    max_sample_size_multiplier=2.0
)

# At interim (50% of data)
current_n = 2500
observed_effect = 1.5  # Smaller than expected
observed_variance = 625  # Higher than expected

recommendation = adaptive.recalculate_sample_size(
    observed_effect=observed_effect,
    observed_variance=observed_variance,
    current_n=current_n
)

print(f"Current sample size: {recommendation['current_n']}")
print(f"Required sample size: {recommendation['required_n']}")
print(f"Additional needed: {recommendation['additional_n_needed']}")
print(f"Recommendation: {recommendation['recommendation']}")
print(f"Information fraction: {recommendation['information_fraction']:.1%}")
\end{lstlisting}

\subsection{Comparison of Sequential Methods}

\begin{table}[h]
\centering
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{Advantages} & \textbf{Disadvantages} & \textbf{Best For} \\
\midrule
Group Sequential & Pre-specified looks & Less flexible timing & Planned reviews \\
(O'Brien-Fleming) & Controlled error rates & Requires planning & Regulatory studies \\
\\
SPRT & Optimal sample size & Continuous monitoring & High-traffic tests \\
& Fastest decisions & Complex implementation & Binary outcomes \\
\\
Alpha Spending & Flexible timing & More complex & Adaptive timelines \\
& Add unplanned looks & Harder to explain & Agile teams \\
\\
Conditional Power & Intuitive futility & Subjective threshold & Resource constraints \\
& Saves resources & Not formal test & Early termination \\
\bottomrule
\end{tabular}
\caption{Comparison of sequential testing methods}
\end{table}

\section{Factorial Experimental Designs and Interaction Effects}

Factorial designs test multiple factors simultaneously, enabling efficient estimation of both main effects and interactions. Unlike one-factor-at-a-time (OFAT) testing, factorial experiments reveal how factors combine—critical for ML systems where features often interact in complex ways.

\subsection{Why Factorial Designs Matter for ML}

Traditional A/B testing compares single treatments: model A vs model B. But ML systems involve multiple interacting components:

\begin{itemize}
    \item \textbf{Multiple Features}: Should you enable both personalization AND real-time signals?
    \item \textbf{Model Components}: Does a new ranking layer improve when combined with better embeddings?
    \item \textbf{User Segments}: Does treatment X work for power users but harm novices?
    \item \textbf{Synergistic Effects}: Two mediocre features might excel when combined
\end{itemize}

\textbf{Interaction} occurs when the effect of one factor depends on the level of another. Consider:
\begin{itemize}
    \item Feature A alone: +2\% conversion
    \item Feature B alone: +1\% conversion
    \item Features A+B together: +8\% conversion (not 3\%!)
\end{itemize}

This is a \textit{positive interaction}—the combined effect exceeds the sum of individual effects. Factorial designs detect and quantify such synergies.

\subsection{Real-World Scenario: The Feature Interaction Surprise}

\subsubsection{The Setup}

A streaming video platform is testing two ML improvements to increase watch time:

\begin{enumerate}
    \item \textbf{Factor A - Thumbnail Personalization}: Use ML to select thumbnail images per user (A0=generic, A1=personalized)
    \item \textbf{Factor B - Autoplay Next}: Automatically queue next video based on user history (B0=off, B1=on)
\end{enumerate}

The ML team runs two separate A/B tests:

\textbf{Test 1 (Thumbnail Personalization):}
\begin{itemize}
    \item Control (A0): 45 min/day average watch time
    \item Treatment (A1): 47 min/day (+2 min, +4.4\%, p=0.02)
\end{itemize}

\textbf{Test 2 (Autoplay):}
\begin{itemize}
    \item Control (B0): 45 min/day
    \item Treatment (B1): 46 min/day (+1 min, +2.2\%, p=0.08 — not significant!)
\end{itemize}

\textbf{Decision:} Ship personalized thumbnails (A1), don't ship autoplay (B1). Expected gain: +2 min/day.

\subsubsection{The Surprise}

Three months later, a product manager accidentally enables autoplay (B1) for users who already have personalized thumbnails (A1). Analytics shows:

\textbf{A1 + B1 users: 55 min/day watch time!}

That's +10 min over baseline (A0+B0=45 min), not the expected +2 min. The features have a massive \textit{positive interaction}:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
& \textbf{B0 (No Autoplay)} & \textbf{B1 (Autoplay)} & \textbf{Main Effect A} \\
\midrule
\textbf{A0 (Generic)} & 45 min & 46 min & — \\
\textbf{A1 (Personalized)} & 47 min & 55 min & — \\
\textbf{Main Effect B} & — & — & — \\
\bottomrule
\end{tabular}
\caption{Watch time (min/day) for all four treatment combinations}
\end{table}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Main effect of A}: $(47 - 45) + (55 - 46) / 2 = 5.5$ min
    \item \textbf{Main effect of B}: $(46 - 45) + (55 - 47) / 2 = 4.5$ min
    \item \textbf{Expected additive effect}: $5.5 + 4.5 = 10$ min
    \item \textbf{Interaction effect}: $(55) - (45 + 5.5 + 4.5) = 0$ min
\end{itemize}

Wait—there's no interaction by this calculation! The 10 min gain IS the sum of main effects. But the separate tests showed +2 min and +1 min (non-significant). What happened?

\subsubsection{The Root Cause}

The separate tests measured \textit{simple effects}, not \textit{main effects}:

\textbf{Test 1 (Thumbnail):} Compared A1 vs A0, both with B0 (no autoplay)
\begin{itemize}
    \item Simple effect of A when B=0: 47 - 45 = +2 min
\end{itemize}

\textbf{Test 2 (Autoplay):} Compared B1 vs B0, both with A0 (generic thumbnails)
\begin{itemize}
    \item Simple effect of B when A=0: 46 - 45 = +1 min
\end{itemize}

But there IS an interaction because the effect of B depends on A:
\begin{itemize}
    \item Effect of B when A=0: 46 - 45 = +1 min
    \item Effect of B when A=1: 55 - 47 = +8 min
\end{itemize}

Autoplay adds 1 min with generic thumbnails but 8 min with personalized thumbnails—a 7 min interaction effect!

\subsubsection{The Cost}

By testing sequentially, the team:
\begin{itemize}
    \item Lost 3 months without the optimal A1+B1 combination
    \item Nearly rejected autoplay (p=0.08) despite its huge value when combined
    \item Missed \$2.5M in ad revenue from increased watch time
\end{itemize}

\subsubsection{The Solution: 2×2 Factorial Design}

A factorial design tests all four combinations simultaneously:

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Treatment} & \textbf{Thumbnail} & \textbf{Autoplay} & \textbf{n} & \textbf{Watch Time} \\
\midrule
Control & A0 (Generic) & B0 (Off) & 10,000 & 45 min \\
A only & A1 (Personal) & B0 (Off) & 10,000 & 47 min \\
B only & A0 (Generic) & B1 (On) & 10,000 & 46 min \\
A+B & A1 (Personal) & B1 (On) & 10,000 & 55 min \\
\bottomrule
\end{tabular}
\caption{2×2 factorial design with all treatment combinations}
\end{table}

\textbf{Benefits:}
\begin{enumerate}
    \item \textbf{Same sample size} as two separate tests (40K total users)
    \item \textbf{Detects interaction} immediately through ANOVA
    \item \textbf{Finds optimal combination} (A1+B1) directly
    \item \textbf{More statistical power} by pooling across conditions
\end{enumerate}

\textbf{Key Lesson:} When testing multiple factors, factorial designs are more efficient and more informative than sequential one-at-a-time tests.

\subsection{Mathematical Foundations of Factorial Designs}

\subsubsection{2×2 Factorial Model}

For two binary factors A and B, the response Y is modeled as:

\begin{equation}
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}
\end{equation}

where:
\begin{itemize}
    \item $\mu$ = grand mean
    \item $\alpha_i$ = main effect of factor A at level i (i=0,1)
    \item $\beta_j$ = main effect of factor B at level j (j=0,1)
    \item $(\alpha\beta)_{ij}$ = interaction effect of A and B
    \item $\epsilon_{ijk}$ = random error for observation k in cell (i,j)
\end{itemize}

With constraints: $\sum_i \alpha_i = 0$, $\sum_j \beta_j = 0$, $\sum_i (\alpha\beta)_{ij} = 0$ for all j, $\sum_j (\alpha\beta)_{ij} = 0$ for all i.

\subsubsection{Main Effects}

\textbf{Main effect of A} (averaged over B):
\begin{equation}
\text{Main A} = \frac{(\bar{Y}_{10} + \bar{Y}_{11})}{2} - \frac{(\bar{Y}_{00} + \bar{Y}_{01})}{2}
\end{equation}

\textbf{Main effect of B} (averaged over A):
\begin{equation}
\text{Main B} = \frac{(\bar{Y}_{01} + \bar{Y}_{11})}{2} - \frac{(\bar{Y}_{00} + \bar{Y}_{10})}{2}
\end{equation}

\subsubsection{Interaction Effect}

The interaction quantifies how much the combined effect differs from the sum of main effects:

\begin{equation}
\text{Interaction} = (\bar{Y}_{11} - \bar{Y}_{10}) - (\bar{Y}_{01} - \bar{Y}_{00})
\end{equation}

Equivalently:
\begin{equation}
(\alpha\beta)_{11} = \bar{Y}_{11} - \mu - \alpha_1 - \beta_1
\end{equation}

\textbf{Interpretation:}
\begin{itemize}
    \item Interaction = 0: Factors are additive (no synergy or antagonism)
    \item Interaction > 0: Positive synergy (combined effect exceeds sum)
    \item Interaction < 0: Negative synergy (interference or antagonism)
\end{itemize}

\subsubsection{Fractional Factorial Designs}

For $k$ factors with 2 levels each, a \textbf{full factorial} requires $2^k$ treatment combinations. This grows quickly:
\begin{itemize}
    \item 3 factors: $2^3 = 8$ combinations
    \item 5 factors: $2^5 = 32$ combinations
    \item 7 factors: $2^7 = 128$ combinations
\end{itemize}

\textbf{Fractional factorial designs} test only a subset of combinations, sacrificing ability to estimate some high-order interactions in exchange for reduced sample size.

A $2^{k-p}$ fractional factorial tests $2^{k-p}$ combinations instead of $2^k$, where $p$ is the fraction. For example:
\begin{itemize}
    \item $2^{5-1}$ tests 16 combinations instead of 32 (half-fraction)
    \item $2^{7-2}$ tests 32 combinations instead of 128 (quarter-fraction)
\end{itemize}

\textbf{Resolution:} Indicates which effects are confounded:
\begin{itemize}
    \item \textbf{Resolution III}: Main effects confounded with 2-way interactions
    \item \textbf{Resolution IV}: Main effects clear, 2-way interactions confounded with each other
    \item \textbf{Resolution V}: Main effects and 2-way interactions clear
\end{itemize}

\subsubsection{ANOVA for Factorial Designs}

Two-way ANOVA decomposes total variation:

\begin{equation}
SS_{Total} = SS_A + SS_B + SS_{AB} + SS_{Error}
\end{equation}

where:
\begin{equation}
SS_A = \sum_i n_i (\bar{Y}_{i\cdot} - \bar{Y}_{\cdot\cdot})^2
\end{equation}

\begin{equation}
SS_B = \sum_j n_j (\bar{Y}_{\cdot j} - \bar{Y}_{\cdot\cdot})^2
\end{equation}

\begin{equation}
SS_{AB} = \sum_{ij} n_{ij} (\bar{Y}_{ij} - \bar{Y}_{i\cdot} - \bar{Y}_{\cdot j} + \bar{Y}_{\cdot\cdot})^2
\end{equation}

F-statistics test each effect:
\begin{equation}
F_A = \frac{MS_A}{MS_{Error}}, \quad F_B = \frac{MS_B}{MS_{Error}}, \quad F_{AB} = \frac{MS_{AB}}{MS_{Error}}
\end{equation}

\subsection{Factorial Design Implementation}

\begin{lstlisting}[language=Python, caption={Comprehensive Factorial Design Framework}]
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
from itertools import product
import numpy as np
import pandas as pd
from scipy import stats
import logging

logger = logging.getLogger(__name__)

@dataclass
class Factor:
    """
    Factor in factorial design.

    Attributes:
        name: Factor identifier
        levels: List of level identifiers (e.g., [0, 1] for binary)
        level_names: Human-readable names for levels
    """
    name: str
    levels: List[Any]
    level_names: Optional[Dict[Any, str]] = None

    def __post_init__(self):
        """Initialize level names if not provided."""
        if self.level_names is None:
            self.level_names = {
                level: f"{self.name}_{level}"
                for level in self.levels
            }

@dataclass
class FactorialEffect:
    """
    Estimated effect from factorial experiment.

    Attributes:
        effect_name: Name of effect (e.g., "A", "B", "A:B")
        estimate: Estimated effect size
        std_error: Standard error of estimate
        t_statistic: T-statistic
        p_value: P-value for significance test
        ci_lower: Lower confidence bound
        ci_upper: Upper confidence bound
    """
    effect_name: str
    estimate: float
    std_error: float
    t_statistic: float
    p_value: float
    ci_lower: float
    ci_upper: float
    significant: bool = field(init=False)

    def __post_init__(self):
        """Determine significance."""
        self.significant = self.p_value < 0.05

class FactorialDesign:
    """
    Full and fractional factorial experimental designs.

    Supports:
    - Full factorial 2^k designs
    - Fractional factorial 2^(k-p) designs
    - Treatment assignment with randomization
    - Cell size balancing

    Example:
        >>> factors = [
        ...     Factor("A", [0, 1], {"0": "control", "1": "treatment"}),
        ...     Factor("B", [0, 1], {"0": "off", "1": "on"})
        ... ]
        >>> design = FactorialDesign(factors, fractional=False)
        >>> assignments = design.assign_treatments(users_df)
    """

    def __init__(
        self,
        factors: List[Factor],
        fractional: bool = False,
        resolution: Optional[int] = None,
        seed: Optional[int] = None
    ):
        """
        Initialize factorial design.

        Args:
            factors: List of experimental factors
            fractional: Whether to use fractional factorial
            resolution: Resolution for fractional (III, IV, V)
            seed: Random seed for reproducibility
        """
        self.factors = factors
        self.n_factors = len(factors)
        self.fractional = fractional
        self.resolution = resolution
        self.seed = seed or 42
        self.rng = np.random.RandomState(self.seed)

        # Generate treatment combinations
        if fractional:
            self.treatment_combinations = self._generate_fractional_factorial()
        else:
            self.treatment_combinations = self._generate_full_factorial()

        self.n_treatments = len(self.treatment_combinations)

        logger.info(
            f"Initialized {'Fractional' if fractional else 'Full'} "
            f"Factorial Design: {self.n_factors} factors, "
            f"{self.n_treatments} treatment combinations"
        )

    def _generate_full_factorial(self) -> List[Tuple]:
        """
        Generate all treatment combinations for full factorial.

        Returns:
            List of tuples, each representing a treatment combination
        """
        # Cartesian product of all factor levels
        level_combinations = product(*[f.levels for f in self.factors])
        return list(level_combinations)

    def _generate_fractional_factorial(self) -> List[Tuple]:
        """
        Generate fractional factorial design.

        Uses standard fractional factorial generator based on resolution.
        Simplified implementation for 2-level factors.

        Returns:
            List of treatment combinations
        """
        # For 2-level factors, use standard fractional factorial
        if not all(len(f.levels) == 2 for f in self.factors):
            raise ValueError("Fractional factorial requires binary factors")

        # Generate half-fraction using highest-order interaction as generator
        # Simplified: include only combinations where product is +1
        full_factorial = self._generate_full_factorial()

        # For half-fraction: select combinations where product = +1
        # (converts 0/1 to -1/+1 first)
        fractional = []
        for combo in full_factorial:
            # Convert to -1/+1 coding
            coded = [1 if level == 1 else -1 for level in combo]
            # Product of all factors (generator relation)
            product_val = np.prod(coded)

            # Include if product is positive (standard practice)
            if product_val > 0:
                fractional.append(combo)

        logger.info(
            f"Generated fractional factorial: "
            f"{len(fractional)}/{len(full_factorial)} combinations"
        )

        return fractional

    def assign_treatments(
        self,
        units: pd.DataFrame,
        unit_id_col: str = "user_id",
        balance: bool = True
    ) -> pd.DataFrame:
        """
        Assign units to treatment combinations.

        Args:
            units: DataFrame with experimental units
            unit_id_col: Column with unit identifiers
            balance: Whether to balance cell sizes

        Returns:
            DataFrame with treatment assignments
        """
        n_units = len(units)
        n_per_treatment = n_units // self.n_treatments

        assignments = []

        for treatment_idx, treatment_combo in enumerate(
            self.treatment_combinations
        ):
            # Number to assign to this treatment
            if balance:
                # Equal allocation
                if treatment_idx == self.n_treatments - 1:
                    # Last treatment gets remainder
                    n_assign = n_units - len(assignments)
                else:
                    n_assign = n_per_treatment
            else:
                # Random allocation (approximately equal)
                n_assign = n_per_treatment

            # Create assignment records
            for _ in range(n_assign):
                assignment = {
                    f"factor_{factor.name}": level
                    for factor, level in zip(self.factors, treatment_combo)
                }
                assignment["treatment_id"] = treatment_idx
                assignment["treatment_label"] = self._format_treatment(
                    treatment_combo
                )
                assignments.append(assignment)

        # Shuffle assignments
        self.rng.shuffle(assignments)

        # Create DataFrame
        assignments_df = pd.DataFrame(assignments)

        # Merge with original units
        assignments_df[unit_id_col] = units[unit_id_col].values[
            :len(assignments_df)
        ]

        return assignments_df

    def _format_treatment(self, treatment_combo: Tuple) -> str:
        """Format treatment combination as readable label."""
        labels = []
        for factor, level in zip(self.factors, treatment_combo):
            labels.append(f"{factor.name}={factor.level_names[level]}")
        return ", ".join(labels)

class InteractionAnalyzer:
    """
    Analyze interactions in factorial experiments using ANOVA.

    Performs:
    - Two-way and multi-way ANOVA
    - Interaction effect estimation
    - Effect size calculation (eta-squared, omega-squared)
    - Post-hoc tests for significant interactions

    Example:
        >>> analyzer = InteractionAnalyzer()
        >>> results = analyzer.analyze_factorial(
        ...     data=experiment_df,
        ...     factors=["A", "B"],
        ...     outcome="conversion_rate"
        ... )
        >>> print(results.interaction_pvalue)
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize interaction analyzer.

        Args:
            alpha: Significance level for tests
        """
        self.alpha = alpha

    def analyze_factorial_2way(
        self,
        data: pd.DataFrame,
        factor_a: str,
        factor_b: str,
        outcome: str
    ) -> Dict[str, Any]:
        """
        Perform two-way ANOVA for 2x2 factorial design.

        Args:
            data: Experiment data
            factor_a: Name of first factor column
            factor_b: Name of second factor column
            outcome: Name of outcome column

        Returns:
            Dictionary with ANOVA results
        """
        # Group data by factors
        grouped = data.groupby([factor_a, factor_b])[outcome]

        # Calculate cell means
        cell_means = grouped.mean()
        cell_ns = grouped.size()
        cell_vars = grouped.var()

        # Grand mean
        grand_mean = data[outcome].mean()
        n_total = len(data)

        # Marginal means
        a_marginal = data.groupby(factor_a)[outcome].mean()
        b_marginal = data.groupby(factor_b)[outcome].mean()

        # Sum of squares
        # SS_A
        ss_a = sum(
            cell_ns.groupby(level=0).sum()[a_level] *
            (a_marginal[a_level] - grand_mean) ** 2
            for a_level in a_marginal.index
        )

        # SS_B
        ss_b = sum(
            cell_ns.groupby(level=1).sum()[b_level] *
            (b_marginal[b_level] - grand_mean) ** 2
            for b_level in b_marginal.index
        )

        # SS_AB (interaction)
        ss_ab = sum(
            cell_ns[cell] * (
                cell_means[cell] -
                a_marginal[cell[0]] -
                b_marginal[cell[1]] +
                grand_mean
            ) ** 2
            for cell in cell_means.index
        )

        # SS_Error (within cells)
        ss_error = sum(
            (cell_ns[cell] - 1) * cell_vars[cell]
            for cell in cell_means.index
            if cell_ns[cell] > 1
        )

        # SS_Total
        ss_total = sum((data[outcome] - grand_mean) ** 2)

        # Degrees of freedom
        df_a = len(a_marginal) - 1
        df_b = len(b_marginal) - 1
        df_ab = df_a * df_b
        df_error = n_total - len(cell_means)
        df_total = n_total - 1

        # Mean squares
        ms_a = ss_a / df_a if df_a > 0 else 0
        ms_b = ss_b / df_b if df_b > 0 else 0
        ms_ab = ss_ab / df_ab if df_ab > 0 else 0
        ms_error = ss_error / df_error if df_error > 0 else 0

        # F-statistics
        f_a = ms_a / ms_error if ms_error > 0 else 0
        f_b = ms_b / ms_error if ms_error > 0 else 0
        f_ab = ms_ab / ms_error if ms_error > 0 else 0

        # P-values
        p_a = 1 - stats.f.cdf(f_a, df_a, df_error) if f_a > 0 else 1.0
        p_b = 1 - stats.f.cdf(f_b, df_b, df_error) if f_b > 0 else 1.0
        p_ab = 1 - stats.f.cdf(f_ab, df_ab, df_error) if f_ab > 0 else 1.0

        # Effect sizes (eta-squared)
        eta_sq_a = ss_a / ss_total
        eta_sq_b = ss_b / ss_total
        eta_sq_ab = ss_ab / ss_total

        # Omega-squared (unbiased effect size)
        omega_sq_a = (ss_a - df_a * ms_error) / (ss_total + ms_error)
        omega_sq_b = (ss_b - df_b * ms_error) / (ss_total + ms_error)
        omega_sq_ab = (ss_ab - df_ab * ms_error) / (ss_total + ms_error)

        results = {
            'grand_mean': grand_mean,
            'cell_means': cell_means.to_dict(),
            'marginal_means_a': a_marginal.to_dict(),
            'marginal_means_b': b_marginal.to_dict(),
            'anova_table': {
                factor_a: {
                    'SS': ss_a, 'df': df_a, 'MS': ms_a,
                    'F': f_a, 'p': p_a,
                    'eta_sq': eta_sq_a, 'omega_sq': omega_sq_a
                },
                factor_b: {
                    'SS': ss_b, 'df': df_b, 'MS': ms_b,
                    'F': f_b, 'p': p_b,
                    'eta_sq': eta_sq_b, 'omega_sq': omega_sq_b
                },
                f'{factor_a}:{factor_b}': {
                    'SS': ss_ab, 'df': df_ab, 'MS': ms_ab,
                    'F': f_ab, 'p': p_ab,
                    'eta_sq': eta_sq_ab, 'omega_sq': omega_sq_ab
                },
                'Error': {
                    'SS': ss_error, 'df': df_error, 'MS': ms_error
                },
                'Total': {
                    'SS': ss_total, 'df': df_total
                }
            },
            'significant_effects': {
                factor_a: p_a < self.alpha,
                factor_b: p_b < self.alpha,
                f'{factor_a}:{factor_b}': p_ab < self.alpha
            }
        }

        logger.info(
            f"Two-way ANOVA: Main A p={p_a:.4f}, Main B p={p_b:.4f}, "
            f"Interaction p={p_ab:.4f}"
        )

        return results

class EffectDecomposer:
    """
    Decompose factorial effects into main and interaction components.

    Calculates:
    - Main effects (averaged over other factors)
    - Simple effects (effect at specific level of other factor)
    - Interaction effects
    - Effect sizes for each component
    """

    def __init__(self):
        """Initialize effect decomposer."""
        pass

    def decompose_2x2(
        self,
        cell_means: Dict[Tuple[Any, Any], float],
        factor_a_levels: List[Any],
        factor_b_levels: List[Any]
    ) -> Dict[str, float]:
        """
        Decompose effects in 2x2 factorial design.

        Args:
            cell_means: Dictionary mapping (a_level, b_level) to mean
            factor_a_levels: Levels of factor A
            factor_b_levels: Levels of factor B

        Returns:
            Dictionary with effect estimates
        """
        a0, a1 = sorted(factor_a_levels)[:2]
        b0, b1 = sorted(factor_b_levels)[:2]

        # Cell means
        y_00 = cell_means.get((a0, b0), 0)
        y_01 = cell_means.get((a0, b1), 0)
        y_10 = cell_means.get((a1, b0), 0)
        y_11 = cell_means.get((a1, b1), 0)

        # Grand mean
        grand_mean = (y_00 + y_01 + y_10 + y_11) / 4

        # Main effects (averaged over other factor)
        main_a = ((y_10 + y_11) / 2) - ((y_00 + y_01) / 2)
        main_b = ((y_01 + y_11) / 2) - ((y_00 + y_10) / 2)

        # Interaction (two equivalent formulations)
        # Method 1: Difference of simple effects
        simple_b_when_a0 = y_01 - y_00  # Effect of B when A=0
        simple_b_when_a1 = y_11 - y_10  # Effect of B when A=1
        interaction = simple_b_when_a1 - simple_b_when_a0

        # Method 2: Deviation from additivity
        # interaction = y_11 - (grand_mean + main_a/2 + main_b/2)

        # Simple effects
        simple_a_when_b0 = y_10 - y_00
        simple_a_when_b1 = y_11 - y_01

        return {
            'grand_mean': grand_mean,
            'main_effect_a': main_a,
            'main_effect_b': main_b,
            'interaction_ab': interaction,
            'simple_effect_a_when_b0': simple_a_when_b0,
            'simple_effect_a_when_b1': simple_a_when_b1,
            'simple_effect_b_when_a0': simple_b_when_a0,
            'simple_effect_b_when_a1': simple_b_when_a1,
            'cell_means': {
                f'A{a0}B{b0}': y_00,
                f'A{a0}B{b1}': y_01,
                f'A{a1}B{b0}': y_10,
                f'A{a1}B{b1}': y_11
            }
        }
\end{lstlisting}

\subsection{Cluster Randomization with ICC}

When experimental units are naturally grouped (users in households, students in schools, patients in hospitals), randomizing individual units can lead to contamination and dependency. \textbf{Cluster randomization} assigns entire clusters to treatments, accounting for intracluster correlation (ICC).

\begin{lstlisting}[language=Python, caption={Cluster Randomization with ICC Modeling}]
from typing import Dict, List, Optional
import numpy as np
import pandas as pd
from scipy import stats
import logging

logger = logging.getLogger(__name__)

class ClusterRandomizer:
    """
    Cluster randomization accounting for intracluster correlation (ICC).

    When units within clusters are more similar than units across
    clusters, standard randomization underestimates variance.
    Cluster randomization and ICC-adjusted analysis correct this.

    ICC (rho) measures proportion of variance between clusters:
    rho = sigma_between^2 / (sigma_between^2 + sigma_within^2)

    Effective sample size: n_eff = n / (1 + (m-1)*rho)
    where m = average cluster size

    Example:
        >>> randomizer = ClusterRandomizer(
        ...     treatment_names=["control", "treatment"],
        ...     allocation=[0.5, 0.5]
        ... )
        >>> assignments = randomizer.assign_clusters(clusters_df)
    """

    def __init__(
        self,
        treatment_names: List[str],
        allocation: List[float],
        seed: Optional[int] = None
    ):
        """
        Initialize cluster randomizer.

        Args:
            treatment_names: Names of treatment arms
            allocation: Allocation proportions (must sum to 1)
            seed: Random seed
        """
        self.treatment_names = treatment_names
        self.allocation = allocation
        self.seed = seed or 42
        self.rng = np.random.RandomState(self.seed)

        if not np.isclose(sum(allocation), 1.0):
            raise ValueError("Allocations must sum to 1.0")

        logger.info(
            f"Initialized ClusterRandomizer: "
            f"{len(treatment_names)} arms, "
            f"allocation={allocation}"
        )

    def assign_clusters(
        self,
        clusters: pd.DataFrame,
        cluster_id_col: str = "cluster_id",
        stratify_by: Optional[List[str]] = None
    ) -> pd.Series:
        """
        Assign clusters to treatment arms.

        Args:
            clusters: DataFrame with one row per cluster
            cluster_id_col: Column with cluster IDs
            stratify_by: Columns to stratify on (optional)

        Returns:
            Series mapping cluster_id to treatment
        """
        assignments = {}

        if stratify_by:
            # Stratified cluster randomization
            for stratum_values, stratum_df in clusters.groupby(stratify_by):
                stratum_assignments = self._randomize_clusters(
                    stratum_df[cluster_id_col].values
                )
                assignments.update(stratum_assignments)
        else:
            # Simple cluster randomization
            assignments = self._randomize_clusters(
                clusters[cluster_id_col].values
            )

        return pd.Series(assignments)

    def _randomize_clusters(
        self,
        cluster_ids: np.ndarray
    ) -> Dict[str, str]:
        """Randomize cluster assignments."""
        n_clusters = len(cluster_ids)
        assignments = {}

        # Assign clusters to treatments based on allocation
        for treatment, alloc in zip(self.treatment_names, self.allocation):
            n_assign = int(n_clusters * alloc)
            for _ in range(n_assign):
                if len(assignments) < n_clusters:
                    cluster_id = cluster_ids[len(assignments)]
                    assignments[cluster_id] = treatment

        # Assign any remainder
        for cluster_id in cluster_ids:
            if cluster_id not in assignments:
                assignments[cluster_id] = self.rng.choice(
                    self.treatment_names,
                    p=self.allocation
                )

        return assignments

    def estimate_icc(
        self,
        data: pd.DataFrame,
        cluster_id_col: str,
        outcome_col: str
    ) -> Dict[str, float]:
        """
        Estimate intracluster correlation coefficient.

        Uses one-way ANOVA to decompose variance.

        Args:
            data: Individual-level data
            cluster_id_col: Column with cluster IDs
            outcome_col: Outcome variable

        Returns:
            Dictionary with ICC estimate and components
        """
        # Group by cluster
        clusters = data.groupby(cluster_id_col)[outcome_col]

        # Cluster-level statistics
        cluster_means = clusters.mean()
        cluster_sizes = clusters.size()
        n_clusters = len(cluster_means)

        # Grand mean
        grand_mean = data[outcome_col].mean()
        n_total = len(data)

        # Between-cluster sum of squares
        ss_between = sum(
            cluster_sizes[cluster_id] *
            (cluster_means[cluster_id] - grand_mean) ** 2
            for cluster_id in cluster_means.index
        )

        # Within-cluster sum of squares
        ss_within = sum(
            sum((data[data[cluster_id_col] == cluster_id][outcome_col] -
                 cluster_means[cluster_id]) ** 2)
            for cluster_id in cluster_means.index
        )

        # Total sum of squares
        ss_total = sum((data[outcome_col] - grand_mean) ** 2)

        # Degrees of freedom
        df_between = n_clusters - 1
        df_within = n_total - n_clusters
        df_total = n_total - 1

        # Mean squares
        ms_between = ss_between / df_between
        ms_within = ss_within / df_within

        # Average cluster size (for unequal sizes)
        m_avg = n_total / n_clusters

        # ICC estimate
        # rho = (MS_between - MS_within) / (MS_between + (m-1)*MS_within)
        icc = (ms_between - ms_within) / (
            ms_between + (m_avg - 1) * ms_within
        )

        # Clamp to [0, 1]
        icc = max(0.0, min(1.0, icc))

        # Variance components
        var_between = (ms_between - ms_within) / m_avg
        var_within = ms_within
        var_total = var_between + var_within

        # Design effect
        design_effect = 1 + (m_avg - 1) * icc

        # Effective sample size
        n_effective = n_total / design_effect

        return {
            'icc': icc,
            'var_between': var_between,
            'var_within': var_within,
            'var_total': var_total,
            'design_effect': design_effect,
            'n_effective': n_effective,
            'n_actual': n_total,
            'n_clusters': n_clusters,
            'avg_cluster_size': m_avg
        }

    def adjust_variance_for_clustering(
        self,
        variance: float,
        cluster_size: float,
        icc: float
    ) -> float:
        """
        Adjust variance estimate for clustering.

        Args:
            variance: Variance assuming independence
            cluster_size: Average cluster size
            icc: Intracluster correlation

        Returns:
            Adjusted variance accounting for clustering
        """
        design_effect = 1 + (cluster_size - 1) * icc
        return variance * design_effect
\end{lstlisting}

\section{Network Experiments and Spillover Effects}

Network experiments occur when experimental units influence each other through social, physical, or algorithmic connections. In such settings, the fundamental assumption of traditional A/B testing—\textit{stable unit treatment value assumption (SUTVA)}—is violated, leading to biased causal estimates and invalid statistical inference.

\subsection{The Interference Problem}

\textbf{SUTVA} requires that:
\begin{enumerate}
    \item \textbf{No interference}: The outcome for unit $i$ depends only on $i$'s treatment, not on others' treatments
    \item \textbf{Consistency}: Well-defined treatments with no hidden variations
\end{enumerate}

In network settings, SUTVA fails because:
\begin{itemize}
    \item \textbf{Social platforms}: User engagement depends on friends' activity
    \item \textbf{Marketplaces}: Seller behavior affects buyer experience (and vice versa)
    \item \textbf{Two-sided networks}: Driver availability impacts rider wait times
    \item \textbf{Content platforms}: Viral content spreads across user networks
\end{itemize}

\textbf{Spillover effects} occur when treatment assigned to unit $i$ affects outcomes for connected units $j \in N(i)$, where $N(i)$ is $i$'s network neighborhood.

\subsection{Real-World Scenario: The Social Media Spillover}

\subsubsection{The Setup}

A social media platform is testing a new feature: \textbf{collaborative content curation}, allowing users to co-create and co-edit posts with friends. The hypothesis is that this will increase user engagement (measured by time spent on platform).

\textbf{Initial A/B Test Design:}
\begin{itemize}
    \item \textbf{Control}: Standard posting (individual only)
    \item \textbf{Treatment}: Collaborative posting enabled
    \item \textbf{Randomization}: 50\% of users randomly assigned to treatment
    \item \textbf{Primary metric}: Daily active minutes (DAM)
    \item \textbf{Sample size}: 1M users (500K per arm)
\end{itemize}

\subsubsection{Week 1: Confusing Results}

The team analyzes the data after 1 week:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Segment} & \textbf{Daily Active Minutes} & \textbf{Change vs Control} \\
\midrule
Control (no feature) & 28 min & — \\
Treatment (has feature) & 35 min & +7 min (+25\%) \\
\bottomrule
\end{tabular}
\caption{Initial results showing strong treatment effect}
\end{table}

\textbf{Celebration!} The product team declares a massive win: +25\% engagement. They prepare to launch to 100\%.

But the data scientist notices something odd: When segmenting control users by whether their friends have the feature, the results change dramatically:

\begin{table}[h]
\centering
\begin{tabular}{llll}
\toprule
\textbf{User Group} & \textbf{Has Feature?} & \textbf{Friends w/ Feature} & \textbf{DAM} \\
\midrule
Pure Control & No & 0\% & 25 min \\
Spillover Control & No & 50\% & 30 min \\
Pure Treatment & Yes & 50\% & 35 min \\
\bottomrule
\end{tabular}
\caption{Results segmented by network exposure}
\end{table}

\textbf{The Problem:} "Control" users with treated friends are experiencing \textbf{spillover effects}:
\begin{itemize}
    \item Pure control (no treated friends): 25 min
    \item Spillover control (treated friends): 30 min (+5 min)
    \item Treatment users: 35 min
\end{itemize}

The initial analysis compared "treatment" (35 min) to "mixed control" (28 min), which includes both pure control and spillover control. This \textit{underestimates} the true treatment effect!

\subsubsection{The Root Cause: Network Interference}

Collaborative posting creates spillover through two mechanisms:

\begin{enumerate}
    \item \textbf{Direct spillover}: Control users can view and interact with collaborative posts created by their treated friends, increasing their engagement
    \item \textbf{Induced behavior}: Control users see collaborative content, wish they could create it, and engage more to compensate
\end{enumerate}

With 50\% random assignment, the average user has ~50\% of friends treated. This means:
\begin{itemize}
    \item Very few "pure control" users (all friends in control)
    \item Very few "pure treatment" users (all friends in treatment)
    \item Most users experience partial spillover
\end{itemize}

\subsubsection{The Bias}

\textbf{Naive estimate:} $\hat{\tau}_{naive} = 35 - 28 = 7$ min

\textbf{True effects:}
\begin{itemize}
    \item \textbf{Direct treatment effect}: 35 - 25 = +10 min (using pure control baseline)
    \item \textbf{Spillover effect}: 30 - 25 = +5 min (friends' treatment increases engagement)
\end{itemize}

The naive estimate \textit{underestimates} the direct effect by 30\% because spillover contaminates the control group!

\subsubsection{Worst Case: The Deployment Disaster}

The team launches to 100\% expecting +7 min (25\% increase). But when everyone has the feature, spillover disappears (no control users left), and the observed increase is only +5 min (20\% increase).

\textbf{Why?} At 50\% treatment:
\begin{itemize}
    \item Treatment users: +10 min (direct) + 0 min (half friends untreated)
    \item Average: $0.5 \times 10 + 0.5 \times 5 = 7.5$ min
\end{itemize}

At 100\% treatment:
\begin{itemize}
    \item All users: +10 min (direct) - 5 min (lose content from control friends)
    \item Net effect: +5 min
\end{itemize}

The product team feels misled: "You said +7 min, we only got +5 min!"

\subsubsection{The Solution: Graph Cluster Randomization}

Instead of randomizing individuals, randomize \textbf{graph clusters}—densely connected groups with few inter-cluster edges.

\textbf{Improved Design:}
\begin{enumerate}
    \item \textbf{Cluster users} by community detection (Louvain algorithm)
    \item \textbf{Randomize clusters} (not individuals) to treatment/control
    \item \textbf{Separate treatment effects}:
    \begin{itemize}
        \item Within-cluster: Direct treatment effect
        \item Cross-cluster: Spillover effect (at cluster boundaries)
    \end{itemize}
\end{enumerate}

\textbf{Results with Graph Clustering:}
\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{User Type} & \textbf{DAM} & \textbf{Effect} \\
\midrule
Control clusters (isolated) & 25 min & Baseline \\
Treatment clusters (isolated) & 35 min & +10 min (direct) \\
Boundary control users & 30 min & +5 min (spillover) \\
\bottomrule
\end{tabular}
\caption{Results with graph cluster randomization}
\end{table}

\textbf{Benefits:}
\begin{itemize}
    \item \textbf{Unbiased direct effect}: +10 min (35 - 25)
    \item \textbf{Measured spillover}: +5 min
    \item \textbf{Accurate launch prediction}: At 100\%, expect +10 min (direct effect)
    \item \textbf{Network externality quantified}: Spillover contributes +5 min
\end{itemize}

\textbf{Key Lesson:} In networked environments, individual randomization biases estimates. Graph-aware randomization isolates causal effects.

\subsection{Mathematical Framework for Network Causal Inference}

\subsubsection{Potential Outcomes Under Interference}

For unit $i$ with neighbors $N(i)$, the potential outcome depends on:
\begin{itemize}
    \item $Z_i \in \{0, 1\}$: $i$'s treatment assignment
    \item $\mathbf{Z}_{N(i)}$: Treatment vector for $i$'s neighbors
\end{itemize}

The \textbf{potential outcome} is:
\begin{equation}
Y_i(Z_i, \mathbf{Z}_{N(i)})
\end{equation}

Under SUTVA (no interference): $Y_i(Z_i, \mathbf{Z}_{N(i)}) = Y_i(Z_i)$ regardless of $\mathbf{Z}_{N(i)}$.

\subsubsection{Decomposing Treatment Effects}

\textbf{Direct effect} (holding neighbors' treatment constant):
\begin{equation}
\tau_i^{direct}(\mathbf{z}) = Y_i(1, \mathbf{z}) - Y_i(0, \mathbf{z})
\end{equation}

\textbf{Spillover effect} (changing neighbors' treatment, holding own constant):
\begin{equation}
\tau_i^{spillover}(z_i) = Y_i(z_i, \mathbf{1}) - Y_i(z_i, \mathbf{0})
\end{equation}

where $\mathbf{1}$ = all neighbors treated, $\mathbf{0}$ = all neighbors control.

\textbf{Total effect} (own and all neighbors treated vs all control):
\begin{equation}
\tau_i^{total} = Y_i(1, \mathbf{1}) - Y_i(0, \mathbf{0})
\end{equation}

\textbf{Decomposition:}
\begin{equation}
\tau_i^{total} = \tau_i^{direct}(\mathbf{1}) + \tau_i^{spillover}(0)
\end{equation}

(assuming no treatment-spillover interaction)

\subsubsection{Exposure Mapping}

\textbf{Exposure mapping} $f: \mathbf{Z} \to \mathcal{E}$ maps treatment vector to exposure levels.

\textbf{Example (proportion of treated neighbors):}
\begin{equation}
E_i(\mathbf{Z}) = \frac{1}{|N(i)|} \sum_{j \in N(i)} Z_j
\end{equation}

Potential outcome as function of exposure:
\begin{equation}
Y_i(z_i, e) = Y_i(Z_i = z_i, E_i(\mathbf{Z}) = e)
\end{equation}

\textbf{Average Direct Effect at exposure $e$:}
\begin{equation}
\tau_{ADE}(e) = \mathbb{E}[Y_i(1, e) - Y_i(0, e)]
\end{equation}

\textbf{Average Spillover Effect for control units:}
\begin{equation}
\tau_{ASE} = \mathbb{E}[Y_i(0, 1) - Y_i(0, 0)]
\end{equation}

\subsubsection{Graph Cluster Randomization}

Partition graph $G = (V, E)$ into $K$ clusters $C_1, \ldots, C_K$ such that:
\begin{itemize}
    \item \textbf{High intra-cluster density}: Many edges within clusters
    \item \textbf{Low inter-cluster density}: Few edges between clusters
\end{itemize}

\textbf{Cluster assignment:} $Z_{C_k} \in \{0, 1\}$ assigned to entire cluster.

\textbf{Estimand:} Difference in cluster-level averages:
\begin{equation}
\hat{\tau} = \frac{1}{|T|} \sum_{k \in T} \bar{Y}_{C_k} - \frac{1}{|C|} \sum_{k \in C} \bar{Y}_{C_k}
\end{equation}

where $T$ = treated clusters, $C$ = control clusters.

\textbf{Variance accounts for clustering:}
\begin{equation}
\text{Var}(\hat{\tau}) = \frac{\sigma_T^2}{|T|} + \frac{\sigma_C^2}{|C|}
\end{equation}

where $\sigma_T^2$, $\sigma_C^2$ are between-cluster variances.

\subsubsection{Egocentric Network Design}

For each unit $i$, define \textbf{ego-network exposure}:
\begin{equation}
p_i = P(Z_j = 1 | j \in N(i))
\end{equation}

\textbf{Design:} Randomize exposure probability across egos:
\begin{itemize}
    \item Some egos have $p_i = 0$ (all neighbors control)
    \item Some egos have $p_i = 1$ (all neighbors treated)
    \item Some egos have $p_i \in (0, 1)$ (partial exposure)
\end{itemize}

\textbf{Regression estimator:}
\begin{equation}
Y_i = \beta_0 + \beta_1 Z_i + \beta_2 E_i + \epsilon_i
\end{equation}

where:
\begin{itemize}
    \item $\beta_1$ estimates direct effect
    \item $\beta_2$ estimates spillover effect per unit neighbor exposure
\end{itemize}

\subsection{Network Experiment Implementation}

\begin{lstlisting}[language=Python, caption={Network Experiment Framework}]
from typing import Dict, List, Optional, Tuple, Set, Any
from dataclasses import dataclass
import numpy as np
import pandas as pd
import networkx as nx
from scipy import stats
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)

@dataclass
class NetworkStats:
    """Network statistics for experiment analysis."""
    n_nodes: int
    n_edges: int
    avg_degree: float
    clustering_coefficient: float
    n_components: int
    largest_component_size: int
    avg_path_length: Optional[float]
    modularity: Optional[float]

class NetworkRandomizer:
    """
    Graph-based randomization for network experiments.

    Strategies:
    - Cluster randomization: Assign graph communities to treatment
    - Ego-network randomization: Control exposure levels
    - Geographic randomization: Exploit spatial clustering
    """

    def __init__(
        self,
        graph: nx.Graph,
        seed: Optional[int] = None
    ):
        """
        Initialize network randomizer.

        Args:
            graph: NetworkX graph representing user network
            seed: Random seed for reproducibility
        """
        self.graph = graph
        self.seed = seed or 42
        self.rng = np.random.RandomState(self.seed)

        logger.info(
            f"Initialized NetworkRandomizer: "
            f"{self.graph.number_of_nodes()} nodes, "
            f"{self.graph.number_of_edges()} edges"
        )

    def cluster_randomization(
        self,
        treatment_prob: float = 0.5,
        algorithm: str = "louvain"
    ) -> Dict[Any, int]:
        """
        Randomize by graph clusters/communities.

        Args:
            treatment_prob: Proportion of clusters to treat
            algorithm: Community detection algorithm
                ("louvain", "label_propagation", "greedy_modularity")

        Returns:
            Dictionary mapping node_id to treatment (0/1)
        """
        # Detect communities
        if algorithm == "louvain":
            try:
                import community as community_louvain
                communities = community_louvain.best_partition(self.graph)
            except ImportError:
                logger.warning(
                    "python-louvain not installed, "
                    "falling back to greedy_modularity"
                )
                algorithm = "greedy_modularity"

        if algorithm == "label_propagation":
            communities_gen = nx.algorithms.community.label_propagation_communities(
                self.graph
            )
            communities = {}
            for idx, community in enumerate(communities_gen):
                for node in community:
                    communities[node] = idx

        elif algorithm == "greedy_modularity":
            communities_gen = nx.algorithms.community.greedy_modularity_communities(
                self.graph
            )
            communities = {}
            for idx, community in enumerate(communities_gen):
                for node in community:
                    communities[node] = idx

        # Get unique cluster IDs
        cluster_ids = set(communities.values())
        n_clusters = len(cluster_ids)

        # Randomize clusters
        n_treated_clusters = int(n_clusters * treatment_prob)
        treated_clusters = self.rng.choice(
            list(cluster_ids),
            size=n_treated_clusters,
            replace=False
        )

        # Assign nodes based on cluster assignment
        assignments = {
            node: 1 if communities[node] in treated_clusters else 0
            for node in self.graph.nodes()
        }

        logger.info(
            f"Cluster randomization: {n_clusters} clusters, "
            f"{n_treated_clusters} treated"
        )

        return assignments

    def ego_network_randomization(
        self,
        exposure_levels: List[float] = [0.0, 0.5, 1.0],
        min_degree: int = 5
    ) -> Dict[Any, int]:
        """
        Randomize to achieve specific exposure levels.

        Creates ego networks with controlled proportions of
        treated neighbors.

        Args:
            exposure_levels: Target exposure levels (0-1)
            min_degree: Minimum degree to include in experiment

        Returns:
            Dictionary mapping node_id to treatment (0/1)
        """
        # Filter nodes by degree
        eligible_nodes = [
            node for node in self.graph.nodes()
            if self.graph.degree[node] >= min_degree
        ]

        # Assign nodes to exposure groups
        n_per_exposure = len(eligible_nodes) // len(exposure_levels)

        assignments = {}

        for level_idx, target_exposure in enumerate(exposure_levels):
            # Select nodes for this exposure level
            start_idx = level_idx * n_per_exposure
            end_idx = (level_idx + 1) * n_per_exposure if level_idx < len(
                exposure_levels
            ) - 1 else len(eligible_nodes)

            level_nodes = eligible_nodes[start_idx:end_idx]

            # For each node, assign neighbors to achieve target exposure
            for node in level_nodes:
                neighbors = list(self.graph.neighbors(node))
                n_neighbors = len(neighbors)

                # Assign node itself (control for exposure experiment)
                assignments[node] = 0

                # Treat proportion of neighbors
                n_treat_neighbors = int(n_neighbors * target_exposure)
                treated_neighbors = self.rng.choice(
                    neighbors,
                    size=n_treat_neighbors,
                    replace=False
                )

                for neighbor in treated_neighbors:
                    if neighbor not in assignments:
                        assignments[neighbor] = 1

                # Assign remaining neighbors to control
                for neighbor in neighbors:
                    if neighbor not in assignments:
                        assignments[neighbor] = 0

        logger.info(
            f"Ego-network randomization: {len(eligible_nodes)} egos, "
            f"{len(exposure_levels)} exposure levels"
        )

        return assignments

class InterferenceDetector:
    """
    Detect spillover effects in network experiments.

    Tests for:
    - Significant differences between pure and spillover control
    - Exposure-response relationships
    - Network autocorrelation in outcomes
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize interference detector.

        Args:
            alpha: Significance level for tests
        """
        self.alpha = alpha

    def detect_spillover(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        exposure_col: str
    ) -> Dict[str, Any]:
        """
        Test for spillover effects.

        Args:
            data: Experiment data
            treatment_col: Treatment assignment (0/1)
            outcome_col: Outcome variable
            exposure_col: Proportion of treated neighbors (0-1)

        Returns:
            Dictionary with test results
        """
        # Pure control: control with no treated neighbors
        pure_control = data[
            (data[treatment_col] == 0) & (data[exposure_col] == 0)
        ]

        # Spillover control: control with treated neighbors
        spillover_control = data[
            (data[treatment_col] == 0) & (data[exposure_col] > 0)
        ]

        # Pure treatment: treatment with all neighbors treated
        pure_treatment = data[
            (data[treatment_col] == 1) & (data[exposure_col] == 1)
        ]

        results = {}

        # Test 1: Pure control vs spillover control
        if len(pure_control) > 0 and len(spillover_control) > 0:
            t_stat, p_val = stats.ttest_ind(
                pure_control[outcome_col].dropna(),
                spillover_control[outcome_col].dropna()
            )

            results['spillover_test'] = {
                'pure_control_mean': pure_control[outcome_col].mean(),
                'spillover_control_mean': spillover_control[outcome_col].mean(),
                'difference': (
                    spillover_control[outcome_col].mean() -
                    pure_control[outcome_col].mean()
                ),
                't_statistic': t_stat,
                'p_value': p_val,
                'significant': p_val < self.alpha,
                'spillover_detected': p_val < self.alpha and
                    spillover_control[outcome_col].mean() != pure_control[
                        outcome_col].mean()
            }

        # Test 2: Exposure-response among controls
        controls = data[data[treatment_col] == 0]
        if len(controls) > 30:  # Need sufficient sample
            from scipy.stats import pearsonr
            corr, p_corr = pearsonr(
                controls[exposure_col],
                controls[outcome_col]
            )

            results['exposure_response'] = {
                'correlation': corr,
                'p_value': p_corr,
                'significant': p_corr < self.alpha
            }

        # Test 3: Effect decomposition
        if all(len(group) > 0 for group in [
            pure_control, spillover_control, pure_treatment
        ]):
            baseline = pure_control[outcome_col].mean()
            direct_effect = pure_treatment[outcome_col].mean() - baseline
            spillover_effect = spillover_control[outcome_col].mean() - baseline

            results['effect_decomposition'] = {
                'baseline': baseline,
                'direct_effect': direct_effect,
                'spillover_effect': spillover_effect,
                'total_effect': direct_effect + spillover_effect,
                'spillover_percentage': (
                    spillover_effect / direct_effect * 100
                    if direct_effect != 0 else 0
                )
            }

        return results

class SpilloverAnalyzer:
    """
    Analyze spillover effects with network statistics.

    Computes:
    - Network exposure metrics
    - Spatial/network autocorrelation
    - Heterogeneous treatment effects by network position
    """

    def __init__(self, graph: nx.Graph):
        """
        Initialize spillover analyzer.

        Args:
            graph: NetworkX graph
        """
        self.graph = graph

    def compute_exposure(
        self,
        assignments: Dict[Any, int]
    ) -> Dict[Any, float]:
        """
        Compute proportion of treated neighbors for each node.

        Args:
            assignments: Dictionary mapping node_id to treatment (0/1)

        Returns:
            Dictionary mapping node_id to exposure level (0-1)
        """
        exposure = {}

        for node in self.graph.nodes():
            neighbors = list(self.graph.neighbors(node))

            if len(neighbors) == 0:
                exposure[node] = 0.0
            else:
                treated_neighbors = sum(
                    assignments.get(neighbor, 0)
                    for neighbor in neighbors
                )
                exposure[node] = treated_neighbors / len(neighbors)

        return exposure

    def compute_network_autocorrelation(
        self,
        outcomes: Dict[Any, float]
    ) -> float:
        """
        Compute Moran's I for network autocorrelation.

        Moran's I measures correlation between connected nodes.

        Args:
            outcomes: Dictionary mapping node_id to outcome value

        Returns:
            Moran's I statistic (-1 to 1)
        """
        n = self.graph.number_of_nodes()
        w = self.graph.number_of_edges()

        # Global mean
        y_mean = np.mean(list(outcomes.values()))

        # Numerator: sum over edges
        numerator = 0
        for u, v in self.graph.edges():
            if u in outcomes and v in outcomes:
                numerator += (outcomes[u] - y_mean) * (outcomes[v] - y_mean)

        # Denominator: variance
        denominator = sum(
            (outcomes[node] - y_mean) ** 2
            for node in outcomes
        )

        if denominator == 0:
            return 0.0

        # Moran's I
        moran_i = (n / w) * (numerator / denominator)

        return moran_i

    def heterogeneous_effects_by_centrality(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        node_id_col: str,
        centrality_measure: str = "degree"
    ) -> Dict[str, Any]:
        """
        Estimate treatment effects by network centrality.

        Args:
            data: Experiment data
            treatment_col: Treatment column
            outcome_col: Outcome column
            node_id_col: Node identifier column
            centrality_measure: "degree", "betweenness", "closeness"

        Returns:
            Dictionary with HTE results
        """
        # Compute centrality
        if centrality_measure == "degree":
            centrality = nx.degree_centrality(self.graph)
        elif centrality_measure == "betweenness":
            centrality = nx.betweenness_centrality(self.graph)
        elif centrality_measure == "closeness":
            centrality = nx.closeness_centrality(self.graph)
        else:
            raise ValueError(f"Unknown centrality: {centrality_measure}")

        # Add centrality to data
        data = data.copy()
        data['centrality'] = data[node_id_col].map(centrality)

        # Split by centrality (terciles)
        data['centrality_tercile'] = pd.qcut(
            data['centrality'],
            q=3,
            labels=['Low', 'Medium', 'High']
        )

        # Estimate effects by tercile
        effects = {}
        for tercile in ['Low', 'Medium', 'High']:
            tercile_data = data[data['centrality_tercile'] == tercile]

            control = tercile_data[tercile_data[treatment_col] == 0]
            treatment = tercile_data[tercile_data[treatment_col] == 1]

            if len(control) > 0 and len(treatment) > 0:
                effect = (
                    treatment[outcome_col].mean() -
                    control[outcome_col].mean()
                )

                # T-test
                t_stat, p_val = stats.ttest_ind(
                    treatment[outcome_col].dropna(),
                    control[outcome_col].dropna()
                )

                effects[tercile] = {
                    'effect': effect,
                    't_statistic': t_stat,
                    'p_value': p_val,
                    'n_control': len(control),
                    'n_treatment': len(treatment)
                }

        return {
            'centrality_measure': centrality_measure,
            'effects_by_tercile': effects
        }

class NetworkExperiment:
    """
    End-to-end network experiment management.

    Handles:
    - Graph-based treatment assignment
    - Exposure calculation
    - Spillover detection
    - Network-adjusted analysis
    """

    def __init__(
        self,
        graph: nx.Graph,
        randomization_strategy: str = "cluster",
        seed: Optional[int] = None
    ):
        """
        Initialize network experiment.

        Args:
            graph: User network graph
            randomization_strategy: "cluster", "ego", or "individual"
            seed: Random seed
        """
        self.graph = graph
        self.randomization_strategy = randomization_strategy
        self.seed = seed or 42

        self.randomizer = NetworkRandomizer(graph, seed)
        self.detector = InterferenceDetector()
        self.analyzer = SpilloverAnalyzer(graph)

        self.assignments = None
        self.exposure = None

        logger.info(
            f"Initialized NetworkExperiment with "
            f"{randomization_strategy} randomization"
        )

    def assign_treatments(
        self,
        treatment_prob: float = 0.5,
        **kwargs
    ) -> pd.DataFrame:
        """
        Assign treatments using specified strategy.

        Args:
            treatment_prob: Proportion to treat
            **kwargs: Strategy-specific parameters

        Returns:
            DataFrame with node_id and treatment assignment
        """
        if self.randomization_strategy == "cluster":
            self.assignments = self.randomizer.cluster_randomization(
                treatment_prob=treatment_prob,
                **kwargs
            )
        elif self.randomization_strategy == "ego":
            self.assignments = self.randomizer.ego_network_randomization(
                **kwargs
            )
        else:
            # Individual randomization (for comparison)
            nodes = list(self.graph.nodes())
            n_treat = int(len(nodes) * treatment_prob)
            rng = np.random.RandomState(self.seed)
            treated_nodes = rng.choice(nodes, size=n_treat, replace=False)

            self.assignments = {
                node: 1 if node in treated_nodes else 0
                for node in nodes
            }

        # Compute exposure
        self.exposure = self.analyzer.compute_exposure(self.assignments)

        # Create DataFrame
        assignments_df = pd.DataFrame({
            'node_id': list(self.assignments.keys()),
            'treatment': list(self.assignments.values()),
            'exposure': [self.exposure[node] for node in self.assignments.keys()]
        })

        return assignments_df

    def analyze_experiment(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str = "treatment",
        exposure_col: str = "exposure"
    ) -> Dict[str, Any]:
        """
        Comprehensive network experiment analysis.

        Args:
            data: Experiment results
            outcome_col: Outcome variable
            treatment_col: Treatment column
            exposure_col: Exposure column

        Returns:
            Dictionary with analysis results
        """
        results = {}

        # Spillover detection
        spillover_results = self.detector.detect_spillover(
            data, treatment_col, outcome_col, exposure_col
        )
        results['spillover_analysis'] = spillover_results

        # Network autocorrelation
        outcome_dict = dict(zip(data['node_id'], data[outcome_col]))
        moran_i = self.analyzer.compute_network_autocorrelation(outcome_dict)
        results['network_autocorrelation'] = {
            'morans_i': moran_i,
            'interpretation': (
                "Positive" if moran_i > 0.1 else
                "Negative" if moran_i < -0.1 else
                "No"
            ) + " spatial autocorrelation"
        }

        # Heterogeneous effects by centrality
        hte_results = self.analyzer.heterogeneous_effects_by_centrality(
            data, treatment_col, outcome_col, 'node_id'
        )
        results['heterogeneous_effects'] = hte_results

        return results
\end{lstlisting}

\subsection{Practical Network Experiment Example}

\begin{lstlisting}[language=Python, caption={Network Experiment Usage}]
import networkx as nx
import pandas as pd
import numpy as np

# Create synthetic social network
np.random.seed(42)
n_users = 1000

# Generate scale-free network (power law degree distribution)
G = nx.barabasi_albert_graph(n_users, m=5)

print(f"Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
print(f"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}")

# Initialize network experiment
experiment = NetworkExperiment(
    graph=G,
    randomization_strategy="cluster",
    seed=42
)

# Assign treatments using cluster randomization
assignments = experiment.assign_treatments(
    treatment_prob=0.5,
    algorithm="louvain"
)

print(f"\nTreatment assignment:")
print(f"  Control: {(assignments['treatment'] == 0).sum()}")
print(f"  Treatment: {(assignments['treatment'] == 1).sum()}")

# Simulate outcomes with spillover
# True effects: Direct = +10, Spillover = +5
assignments['outcome'] = (
    25 +  # Baseline
    10 * assignments['treatment'] +  # Direct effect
    5 * assignments['exposure'] +  # Spillover effect
    np.random.normal(0, 3, len(assignments))  # Noise
)

# Naive analysis (ignoring network)
naive_control = assignments[assignments['treatment'] == 0]['outcome'].mean()
naive_treatment = assignments[assignments['treatment'] == 1]['outcome'].mean()
naive_effect = naive_treatment - naive_control

print(f"\nNaive analysis (ignoring spillover):")
print(f"  Control mean: {naive_control:.2f}")
print(f"  Treatment mean: {naive_treatment:.2f}")
print(f"  Estimated effect: {naive_effect:.2f}")

# Network-aware analysis
results = experiment.analyze_experiment(
    data=assignments,
    outcome_col='outcome'
)

print(f"\nNetwork-aware analysis:")
print(f"  Spillover detected: {results['spillover_analysis']['spillover_test']['spillover_detected']}")

if 'effect_decomposition' in results['spillover_analysis']:
    decomp = results['spillover_analysis']['effect_decomposition']
    print(f"  Direct effect: {decomp['direct_effect']:.2f}")
    print(f"  Spillover effect: {decomp['spillover_effect']:.2f}")
    print(f"  Total effect: {decomp['total_effect']:.2f}")

print(f"\nNetwork autocorrelation:")
print(f"  Moran's I: {results['network_autocorrelation']['morans_i']:.3f}")
print(f"  Interpretation: {results['network_autocorrelation']['interpretation']}")

# Heterogeneous effects by centrality
print(f"\nHeterogeneous effects by network centrality:")
for tercile, effect_data in results['heterogeneous_effects']['effects_by_tercile'].items():
    print(f"  {tercile} centrality: {effect_data['effect']:.2f} "
          f"(p={effect_data['p_value']:.3f})")
\end{lstlisting}

\section{Business-Oriented Experimentation Framework}

While statistical significance determines whether an effect exists, business value determines whether it matters. A statistically significant +0.1\% CTR improvement might not justify engineering costs. Conversely, a non-significant trend toward +10\% revenue might warrant further investigation. This section bridges statistics and business decision-making.

\subsection{The Challenge of Multiple Business Objectives}

ML experiments rarely optimize a single metric. Common tensions include:

\begin{itemize}
    \item \textbf{Revenue vs Engagement}: Aggressive monetization increases short-term revenue but decreases user retention
    \item \textbf{Clicks vs Quality}: Clickbait increases CTR but decreases content satisfaction
    \item \textbf{Speed vs Accuracy}: Faster models serve more requests but may reduce conversion
    \item \textbf{Growth vs Sustainability}: Promotional campaigns boost acquisition but attract low-LTV users
\end{itemize}

These trade-offs require \textbf{multi-objective optimization}: jointly optimizing conflicting metrics subject to business constraints.

\subsection{Real-World Scenario: The Revenue vs Engagement Trade-off}

\subsubsection{The Setup}

A streaming music platform is testing a new recommendation algorithm designed to increase premium subscriptions. The ML team has developed two competing models:

\begin{itemize}
    \item \textbf{Model A (Baseline)}: Optimizes for user engagement (listening time)
    \item \textbf{Model B (New)}: Optimizes for conversion to premium subscriptions
\end{itemize}

\textbf{Experiment Design:}
\begin{itemize}
    \item \textbf{Duration}: 4 weeks
    \item \textbf{Sample size}: 200K users per arm
    \item \textbf{Primary metric}: Premium conversion rate
    \item \textbf{Secondary metrics}: Daily listening time, retention, revenue
\end{itemize}

\subsubsection{Week 4: The Conflicting Results}

After 4 weeks, the team analyzes the results:

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Metric} & \textbf{Model A} & \textbf{Model B} & \textbf{Change} & \textbf{p-value} \\
\midrule
Premium conversion & 3.2\% & 4.1\% & +0.9pp (+28\%) & 0.001 \\
Daily listening time & 65 min & 58 min & -7 min (-11\%) & 0.003 \\
7-day retention & 82\% & 79\% & -3pp (-4\%) & 0.021 \\
Revenue/user/month & \$8.20 & \$9.50 & +\$1.30 (+16\%) & 0.008 \\
\bottomrule
\end{tabular}
\caption{Experiment results showing metric trade-offs}
\end{table}

\textbf{The Dilemma:}
\begin{itemize}
    \item \textbf{Good news}: Model B increases conversions (+28\%) and revenue (+16\%)
    \item \textbf{Bad news}: Model B decreases engagement (-11\%) and retention (-4\%)
\end{itemize}

The product team is split:
\begin{itemize}
    \item \textbf{Growth team}: "Ship it! +16\% revenue is huge. That's \$2.6M annually!"
    \item \textbf{Engagement team}: "Don't ship! Users are listening less. They'll churn long-term."
    \item \textbf{CEO}: "What's the \textit{net} business impact? Will we make more money or lose users?"
\end{itemize}

\subsubsection{Root Cause Analysis}

Deeper analysis reveals \textit{why} Model B trades engagement for revenue:

\textbf{Model A}: Recommends music users love → High listening time → Some users eventually convert → High retention

\textbf{Model B}: Aggressively recommends premium-only content → Users hit paywalls frequently → Higher immediate conversion → Lower free-tier engagement → Some users frustrated and leave

\textbf{User Segments:}
\begin{enumerate}
    \item \textbf{High-Intent Converters} (20\% of users):
    \begin{itemize}
        \item Model A: 12\% conversion, 70 min/day, 90\% retention
        \item Model B: 18\% conversion, 68 min/day, 88\% retention
        \item \textbf{Verdict}: B slightly better (higher conversion, minimal engagement loss)
    \end{itemize}

    \item \textbf{Free-Tier Loyalists} (50\% of users):
    \begin{itemize}
        \item Model A: 0.5\% conversion, 70 min/day, 85\% retention
        \item Model B: 1.0\% conversion, 55 min/day, 75\% retention
        \item \textbf{Verdict}: B much worse (engagement drop, retention hit, small conversion gain)
    \end{itemize}

    \item \textbf{Casual Users} (30\% of users):
    \begin{itemize}
        \item Model A: 2\% conversion, 50 min/day, 75\% retention
        \item Model B: 3\% conversion, 48 min/day, 73\% retention
        \item \textbf{Verdict}: B slightly better (moderate conversion gain, small engagement loss)
    \end{itemize}
\end{enumerate}

\textbf{The Problem:} Model B is optimal for high-intent users but harmful for free-tier loyalists (50\% of users!).

\subsubsection{The Business Decision: Multi-Objective Optimization}

The team performs a comprehensive cost-benefit analysis:

\textbf{Short-Term Revenue (Year 1):}
\begin{itemize}
    \item Model A: 200K users × 3.2\% conversion × \$120/year = \$768K
    \item Model B: 200K users × 4.1\% conversion × \$120/year = \$984K
    \item \textbf{Gain}: +\$216K (+28\%)
\end{itemize}

\textbf{Long-Term User Loss (3-Year LTV):}
\begin{itemize}
    \item Retention drop: -3pp (-4\%)
    \item Annual churn increase: -3pp × 200K = 6,000 users/year
    \item Average LTV per user: \$180 (3 years × \$60/year average)
    \item \textbf{Loss}: 6,000 users × \$180 = \$1.08M over 3 years
\end{itemize}

\textbf{Net Present Value (3 years, 10\% discount rate):}
\begin{itemize}
    \item Year 1 revenue gain: +\$216K
    \item Year 2 churn loss: -\$360K / 1.1 = -\$327K
    \item Year 3 churn loss: -\$360K / 1.21 = -\$298K
    \item \textbf{Total NPV}: +\$216K - \$327K - \$298K = -\$409K
\end{itemize}

\textbf{Decision}: \textit{Do NOT ship Model B globally}. Despite +28\% conversion and statistical significance, the long-term user retention loss outweighs short-term revenue gains.

\subsubsection{The Solution: Segment-Specific Deployment}

Instead of binary ship/don't-ship, the team implements \textbf{segment-targeted deployment}:

\begin{enumerate}
    \item \textbf{High-Intent Converters} (20\%): Deploy Model B (net positive)
    \item \textbf{Free-Tier Loyalists} (50\%): Keep Model A (avoid churn)
    \item \textbf{Casual Users} (30\%): Deploy Model B (slight benefit)
\end{enumerate}

\textbf{Blended Business Impact:}
\begin{itemize}
    \item Conversion: 3.2\% → 3.7\% (+15.6\% blended)
    \item Listening time: 65 min → 63 min (-3\% blended)
    \item Retention: 82\% → 81\% (-1.2\% blended)
    \item Revenue: \$8.20 → \$9.00 (+9.8\% blended)
    \item \textbf{3-year NPV}: +\$850K (positive!)
\end{itemize}

\textbf{Key Lessons:}
\begin{enumerate}
    \item \textbf{Consider multiple metrics}: Don't optimize revenue alone
    \item \textbf{Measure long-term effects}: Short-term gains may cause long-term harm
    \item \textbf{Segment analysis}: HTE reveals winners and losers
    \item \textbf{Blended deployment}: Maximize value by targeting the right users
    \item \textbf{Business rigor}: NPV analysis grounds statistical findings in dollars
\end{enumerate}

\subsection{Mathematical Framework for Multi-Objective Optimization}

\subsubsection{Multi-Objective Problem Formulation}

Given $K$ business metrics $Y_1, \ldots, Y_K$, we seek treatment that maximizes:

\begin{equation}
\max_z \mathbf{f}(z) = [f_1(z), f_2(z), \ldots, f_K(z)]
\end{equation}

where $f_k(z) = \mathbb{E}[Y_k | Z = z]$ is the expected value of metric $k$ under treatment $z$.

Typically, metrics conflict: improving $f_1$ decreases $f_2$. Solutions lie on the \textbf{Pareto frontier}—points where improving one metric requires sacrificing another.

\subsubsection{Weighted Utility Function}

Combine metrics with business-driven weights:

\begin{equation}
U(z) = \sum_{k=1}^{K} w_k f_k(z)
\end{equation}

where $w_k \geq 0$ and $\sum_k w_k = 1$.

\textbf{Example (Revenue vs Engagement):}
\begin{equation}
U(z) = w_1 \cdot \text{Revenue}(z) + w_2 \cdot \text{Engagement}(z)
\end{equation}

Choosing $w_1 = 0.7$, $w_2 = 0.3$ prioritizes revenue but penalizes engagement loss.

\subsubsection{Constrained Optimization}

Alternatively, optimize primary metric subject to constraints:

\begin{equation}
\max_z f_1(z) \quad \text{subject to} \quad f_k(z) \geq c_k \text{ for } k = 2, \ldots, K
\end{equation}

\textbf{Example:} Maximize revenue subject to retention $\geq 80\%$.

\subsubsection{Net Present Value (NPV) Framework}

For long-term business decisions, compute NPV of treatment effect:

\begin{equation}
\text{NPV}(z) = \sum_{t=0}^{T} \frac{R_t(z) - C_t(z)}{(1 + r)^t}
\end{equation}

where:
\begin{itemize}
    \item $R_t(z)$ = revenue at time $t$ under treatment $z$
    \item $C_t(z)$ = costs at time $t$
    \item $r$ = discount rate
    \item $T$ = time horizon
\end{itemize}

\textbf{Include churn effects:}
\begin{equation}
R_t(z) = N_0 \cdot (1 - \text{churn}(z))^t \cdot \text{ARPU}(z)
\end{equation}

where $N_0$ = initial users, churn$(z)$ = annual churn rate, ARPU$(z)$ = average revenue per user.

\subsection{Business Metrics Framework Implementation}

\begin{lstlisting}[language=Python, caption={Business Metrics Optimization Framework}]
from typing import Dict, List, Optional, Tuple, Callable, Any
from dataclasses import dataclass, field
import numpy as np
import pandas as pd
from scipy import stats, optimize
from lifelines import KaplanMeierFitter, CoxPHFitter
import logging

logger = logging.getLogger(__name__)

@dataclass
class BusinessMetric:
    """
    Business metric specification.

    Attributes:
        name: Metric identifier
        weight: Weight in objective function (0-1)
        direction: 'maximize' or 'minimize'
        constraint_min: Minimum acceptable value (optional)
        constraint_max: Maximum acceptable value (optional)
        monetization: Conversion factor to dollars (optional)
    """
    name: str
    weight: float
    direction: str = 'maximize'
    constraint_min: Optional[float] = None
    constraint_max: Optional[float] = None
    monetization: Optional[float] = None

    def __post_init__(self):
        """Validate metric configuration."""
        if self.direction not in ['maximize', 'minimize']:
            raise ValueError(
                f"direction must be 'maximize' or 'minimize', "
                f"got {self.direction}"
            )

class BusinessMetricsOptimizer:
    """
    Multi-objective optimization for business metrics.

    Combines multiple metrics with weights to compute overall
    business value. Supports:
    - Weighted utility functions
    - Constrained optimization
    - Pareto frontier analysis

    Example:
        >>> metrics = [
        ...     BusinessMetric("revenue", weight=0.6, direction='maximize'),
        ...     BusinessMetric("retention", weight=0.4, direction='maximize',
        ...                    constraint_min=0.80)
        ... ]
        >>> optimizer = BusinessMetricsOptimizer(metrics)
        >>> value = optimizer.compute_business_value(results)
    """

    def __init__(self, metrics: List[BusinessMetric]):
        """
        Initialize business metrics optimizer.

        Args:
            metrics: List of business metrics with weights
        """
        self.metrics = metrics

        # Validate weights sum to 1
        total_weight = sum(m.weight for m in metrics)
        if not np.isclose(total_weight, 1.0):
            logger.warning(
                f"Metric weights sum to {total_weight}, normalizing to 1.0"
            )
            for metric in self.metrics:
                metric.weight /= total_weight

        logger.info(
            f"Initialized BusinessMetricsOptimizer with "
            f"{len(metrics)} metrics"
        )

    def compute_business_value(
        self,
        metric_values: Dict[str, float],
        normalize: bool = True
    ) -> float:
        """
        Compute weighted business value.

        Args:
            metric_values: Dictionary of metric name -> value
            normalize: Whether to normalize metrics to [0,1]

        Returns:
            Weighted business value
        """
        total_value = 0.0

        for metric in self.metrics:
            if metric.name not in metric_values:
                logger.warning(f"Missing metric: {metric.name}")
                continue

            value = metric_values[metric.name]

            # Normalize if requested (min-max scaling)
            if normalize and hasattr(self, 'metric_ranges'):
                min_val, max_val = self.metric_ranges.get(
                    metric.name, (value, value)
                )
                if max_val > min_val:
                    value = (value - min_val) / (max_val - min_val)

            # Invert if minimizing
            if metric.direction == 'minimize':
                value = 1 - value if normalize else -value

            total_value += metric.weight * value

        return total_value

    def check_constraints(
        self,
        metric_values: Dict[str, float]
    ) -> Tuple[bool, List[str]]:
        """
        Check if metric values satisfy constraints.

        Args:
            metric_values: Dictionary of metric name -> value

        Returns:
            (all_satisfied, list_of_violations)
        """
        violations = []

        for metric in self.metrics:
            if metric.name not in metric_values:
                continue

            value = metric_values[metric.name]

            # Check minimum constraint
            if (metric.constraint_min is not None and
                value < metric.constraint_min):
                violations.append(
                    f"{metric.name} = {value:.4f} < "
                    f"min {metric.constraint_min:.4f}"
                )

            # Check maximum constraint
            if (metric.constraint_max is not None and
                value > metric.constraint_max):
                violations.append(
                    f"{metric.name} = {value:.4f} > "
                    f"max {metric.constraint_max:.4f}"
                )

        return len(violations) == 0, violations

    def compare_treatments(
        self,
        control_metrics: Dict[str, float],
        treatment_metrics: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        Compare control vs treatment on business value.

        Args:
            control_metrics: Metric values for control
            treatment_metrics: Metric values for treatment

        Returns:
            Comparison results
        """
        control_value = self.compute_business_value(control_metrics)
        treatment_value = self.compute_business_value(treatment_metrics)

        # Check constraints
        control_ok, control_violations = self.check_constraints(
            control_metrics
        )
        treatment_ok, treatment_violations = self.check_constraints(
            treatment_metrics
        )

        return {
            'control_value': control_value,
            'treatment_value': treatment_value,
            'value_diff': treatment_value - control_value,
            'value_lift': (
                (treatment_value - control_value) / control_value
                if control_value != 0 else 0
            ),
            'treatment_better': treatment_value > control_value,
            'control_satisfies_constraints': control_ok,
            'treatment_satisfies_constraints': treatment_ok,
            'control_violations': control_violations,
            'treatment_violations': treatment_violations,
            'recommendation': (
                'Deploy treatment' if (
                    treatment_value > control_value and treatment_ok
                ) else 'Keep control'
            )
        }

class ExperimentROICalculator:
    """
    Calculate return on investment for experiments.

    Computes:
    - Short-term revenue impact
    - Long-term LTV changes
    - Implementation costs
    - Net present value (NPV)
    - Payback period

    Example:
        >>> calc = ExperimentROICalculator(
        ...     discount_rate=0.10,
        ...     time_horizon=3
        ... )
        >>> roi = calc.calculate_roi(experiment_results, costs)
    """

    def __init__(
        self,
        discount_rate: float = 0.10,
        time_horizon: int = 3
    ):
        """
        Initialize ROI calculator.

        Args:
            discount_rate: Annual discount rate (e.g., 0.10 for 10%)
            time_horizon: Analysis period in years
        """
        self.discount_rate = discount_rate
        self.time_horizon = time_horizon

    def calculate_roi(
        self,
        control_metrics: Dict[str, float],
        treatment_metrics: Dict[str, float],
        user_base: int,
        costs: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        Calculate comprehensive ROI.

        Args:
            control_metrics: Metrics for control arm
            treatment_metrics: Metrics for treatment arm
            user_base: Number of users affected
            costs: Dictionary of costs (development, maintenance, etc.)

        Returns:
            ROI analysis results
        """
        if costs is None:
            costs = {}

        # Extract key metrics
        arpu_control = control_metrics.get('arpu', 0)
        arpu_treatment = treatment_metrics.get('arpu', 0)
        retention_control = control_metrics.get('retention', 0.80)
        retention_treatment = treatment_metrics.get('retention', 0.80)

        # Churn rates
        churn_control = 1 - retention_control
        churn_treatment = 1 - retention_treatment

        # Annual revenue per user over time
        revenues_control = []
        revenues_treatment = []

        for year in range(self.time_horizon):
            # Compound retention effect
            surviving_control = (1 - churn_control) ** year
            surviving_treatment = (1 - churn_treatment) ** year

            rev_control = user_base * surviving_control * arpu_control * 12
            rev_treatment = user_base * surviving_treatment * arpu_treatment * 12

            revenues_control.append(rev_control)
            revenues_treatment.append(rev_treatment)

        # NPV calculation
        npv_control = sum(
            rev / ((1 + self.discount_rate) ** year)
            for year, rev in enumerate(revenues_control)
        )

        npv_treatment = sum(
            rev / ((1 + self.discount_rate) ** year)
            for year, rev in enumerate(revenues_treatment)
        )

        # Costs
        development_cost = costs.get('development', 0)
        maintenance_cost_annual = costs.get('maintenance_annual', 0)

        total_maintenance = sum(
            maintenance_cost_annual / ((1 + self.discount_rate) ** year)
            for year in range(1, self.time_horizon)
        )

        total_costs = development_cost + total_maintenance

        # Net benefit
        gross_benefit = npv_treatment - npv_control
        net_benefit = gross_benefit - total_costs

        # ROI
        roi = net_benefit / total_costs if total_costs > 0 else float('inf')

        # Payback period (years until cumulative benefit > costs)
        cumulative_benefit = 0
        payback_period = None

        for year in range(self.time_horizon):
            year_benefit = (
                revenues_treatment[year] - revenues_control[year]
            ) / ((1 + self.discount_rate) ** year)
            cumulative_benefit += year_benefit

            if cumulative_benefit >= total_costs and payback_period is None:
                payback_period = year + 1

        return {
            'npv_control': npv_control,
            'npv_treatment': npv_treatment,
            'gross_benefit': gross_benefit,
            'total_costs': total_costs,
            'net_benefit': net_benefit,
            'roi': roi,
            'roi_percentage': roi * 100,
            'payback_period_years': payback_period,
            'revenues_by_year': {
                'control': revenues_control,
                'treatment': revenues_treatment,
                'difference': [
                    t - c for t, c in zip(revenues_treatment, revenues_control)
                ]
            },
            'recommendation': (
                'Proceed' if net_benefit > 0 else 'Do not proceed'
            )
        }

class SurvivalAnalyzer:
    """
    Survival analysis for long-term effect measurement.

    Uses Kaplan-Meier and Cox proportional hazards models
    to measure treatment effects on time-to-event outcomes
    (churn, conversion, etc.).

    Example:
        >>> analyzer = SurvivalAnalyzer()
        >>> results = analyzer.analyze_survival(
        ...     data, duration_col='days', event_col='churned',
        ...     treatment_col='treatment'
        ... )
    """

    def __init__(self):
        """Initialize survival analyzer."""
        pass

    def kaplan_meier_comparison(
        self,
        data: pd.DataFrame,
        duration_col: str,
        event_col: str,
        treatment_col: str
    ) -> Dict[str, Any]:
        """
        Compare survival curves using Kaplan-Meier estimator.

        Args:
            data: Experiment data
            duration_col: Time to event (or censoring)
            event_col: Binary event indicator (1=event, 0=censored)
            treatment_col: Treatment indicator (0/1)

        Returns:
            Comparison results with log-rank test
        """
        from lifelines.statistics import logrank_test

        # Separate groups
        control = data[data[treatment_col] == 0]
        treatment = data[data[treatment_col] == 1]

        # Fit Kaplan-Meier for each group
        kmf_control = KaplanMeierFitter()
        kmf_treatment = KaplanMeierFitter()

        kmf_control.fit(
            control[duration_col],
            control[event_col],
            label='Control'
        )

        kmf_treatment.fit(
            treatment[duration_col],
            treatment[event_col],
            label='Treatment'
        )

        # Log-rank test
        log_rank_result = logrank_test(
            control[duration_col],
            treatment[duration_col],
            control[event_col],
            treatment[event_col]
        )

        # Median survival times
        median_control = kmf_control.median_survival_time_
        median_treatment = kmf_treatment.median_survival_time_

        return {
            'median_survival_control': median_control,
            'median_survival_treatment': median_treatment,
            'survival_diff': median_treatment - median_control,
            'log_rank_statistic': log_rank_result.test_statistic,
            'log_rank_pvalue': log_rank_result.p_value,
            'significant': log_rank_result.p_value < 0.05,
            'kmf_control': kmf_control,
            'kmf_treatment': kmf_treatment
        }

    def cox_proportional_hazards(
        self,
        data: pd.DataFrame,
        duration_col: str,
        event_col: str,
        treatment_col: str,
        covariates: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Cox proportional hazards regression.

        Estimates hazard ratio for treatment while controlling
        for covariates.

        Args:
            data: Experiment data
            duration_col: Time to event
            event_col: Event indicator
            treatment_col: Treatment indicator
            covariates: Additional covariates to control for

        Returns:
            Cox model results
        """
        if covariates is None:
            covariates = []

        # Prepare data
        model_data = data[[duration_col, event_col, treatment_col] + covariates].copy()

        # Fit Cox model
        cph = CoxPHFitter()
        cph.fit(
            model_data,
            duration_col=duration_col,
            event_col=event_col
        )

        # Extract treatment effect
        treatment_coef = cph.params_[treatment_col]
        treatment_hr = np.exp(treatment_coef)
        treatment_p = cph.summary.loc[treatment_col, 'p']
        treatment_ci = (
            np.exp(cph.confidence_intervals_.loc[treatment_col, '95% lower-bound']),
            np.exp(cph.confidence_intervals_.loc[treatment_col, '95% upper-bound'])
        )

        return {
            'hazard_ratio': treatment_hr,
            'hazard_ratio_ci': treatment_ci,
            'coefficient': treatment_coef,
            'p_value': treatment_p,
            'significant': treatment_p < 0.05,
            'interpretation': (
                f"Treatment {'increases' if treatment_hr > 1 else 'decreases'} "
                f"hazard by {abs(treatment_hr - 1) * 100:.1f}%"
            ),
            'model': cph
        }

class SegmentAnalyzer:
    """
    Heterogeneous treatment effect (HTE) analysis by user segments.

    Identifies which user segments benefit most from treatment,
    enabling targeted deployment.

    Example:
        >>> analyzer = SegmentAnalyzer()
        >>> hte_results = analyzer.analyze_segments(
        ...     data, treatment_col='treatment', outcome_col='revenue',
        ...     segment_cols=['user_type', 'tenure']
        ... )
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize segment analyzer.

        Args:
            alpha: Significance level for tests
        """
        self.alpha = alpha

    def analyze_segments(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        segment_cols: List[str]
    ) -> Dict[str, Any]:
        """
        Analyze treatment effects across segments.

        Args:
            data: Experiment data
            treatment_col: Treatment indicator column
            outcome_col: Outcome metric column
            segment_cols: Columns defining segments

        Returns:
            HTE results by segment
        """
        results = {}

        # Overall effect (baseline)
        overall_effect = self._estimate_effect(
            data, treatment_col, outcome_col
        )
        results['overall'] = overall_effect

        # Segment-specific effects
        segment_effects = {}

        for segment_col in segment_cols:
            segment_effects[segment_col] = {}

            for segment_value in data[segment_col].unique():
                segment_data = data[data[segment_col] == segment_value]

                effect = self._estimate_effect(
                    segment_data, treatment_col, outcome_col
                )

                segment_effects[segment_col][segment_value] = effect

        results['segments'] = segment_effects

        # Identify winner and loser segments
        best_segments = []
        worst_segments = []

        for segment_col, segments in segment_effects.items():
            for segment_value, effect in segments.items():
                if effect['effect_size'] > overall_effect['effect_size'] * 1.5:
                    best_segments.append({
                        'segment_col': segment_col,
                        'segment_value': segment_value,
                        'effect': effect['effect_size'],
                        'lift_vs_overall': (
                            effect['effect_size'] - overall_effect['effect_size']
                        )
                    })
                elif effect['effect_size'] < 0 and effect['significant']:
                    worst_segments.append({
                        'segment_col': segment_col,
                        'segment_value': segment_value,
                        'effect': effect['effect_size']
                    })

        results['best_segments'] = sorted(
            best_segments, key=lambda x: x['effect'], reverse=True
        )
        results['worst_segments'] = sorted(
            worst_segments, key=lambda x: x['effect']
        )

        return results

    def _estimate_effect(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str
    ) -> Dict[str, float]:
        """Estimate treatment effect for a subset of data."""
        control = data[data[treatment_col] == 0][outcome_col]
        treatment = data[data[treatment_col] == 1][outcome_col]

        if len(control) == 0 or len(treatment) == 0:
            return {
                'n_control': len(control),
                'n_treatment': len(treatment),
                'control_mean': None,
                'treatment_mean': None,
                'effect_size': 0,
                'p_value': 1.0,
                'significant': False
            }

        control_mean = control.mean()
        treatment_mean = treatment.mean()
        effect = treatment_mean - control_mean

        # T-test
        t_stat, p_value = stats.ttest_ind(treatment, control)

        return {
            'n_control': len(control),
            'n_treatment': len(treatment),
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'effect_size': effect,
            'relative_lift': effect / control_mean if control_mean != 0 else 0,
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < self.alpha
        }

    def compute_blended_impact(
        self,
        segment_effects: Dict[str, Dict[str, Any]],
        segment_sizes: Dict[str, int],
        deployment_decisions: Dict[str, bool]
    ) -> Dict[str, float]:
        """
        Compute blended impact of segment-targeted deployment.

        Args:
            segment_effects: Effects by segment
            segment_sizes: Number of users in each segment
            deployment_decisions: Whether to deploy to each segment

        Returns:
            Blended metrics
        """
        total_users = sum(segment_sizes.values())

        weighted_effect = 0.0

        for segment, deploy in deployment_decisions.items():
            if segment not in segment_effects:
                continue

            segment_weight = segment_sizes[segment] / total_users
            segment_effect = segment_effects[segment]['effect_size']

            # If deploying, users get treatment effect; otherwise, 0
            effective_effect = segment_effect if deploy else 0

            weighted_effect += segment_weight * effective_effect

        return {
            'blended_effect': weighted_effect,
            'deployment_coverage': sum(
                segment_sizes[s] for s, d in deployment_decisions.items() if d
            ) / total_users
        }
\end{lstlisting}

\section{Experimentation Platform Architecture}

Building production-grade A/B testing requires more than individual statistical methods—it requires an integrated platform that orchestrates experiment lifecycle, automates analysis, and provides extensibility for new methods. This section presents a comprehensive architecture integrating all techniques covered in this chapter.

\subsection{Platform Design Principles}

\textbf{1. Separation of Concerns:}
\begin{itemize}
    \item \textbf{Experiment Management}: Randomization, enrollment, treatment assignment
    \item \textbf{Statistical Analysis}: Testing methods, power calculation, effect estimation
    \item \textbf{Decision Framework}: Business metrics, ROI calculation, deployment criteria
    \item \textbf{Data Pipeline}: Metrics collection, aggregation, quality validation
\end{itemize}

\textbf{2. Extensibility:}
\begin{itemize}
    \item Plugin architecture for new statistical methods
    \item Strategy pattern for different randomization schemes
    \item Observer pattern for real-time monitoring
    \item Factory pattern for test creation
\end{itemize}

\textbf{3. Reproducibility:}
\begin{itemize}
    \item Deterministic randomization with seed management
    \item Version control for experiment configurations
    \item Audit logs for all decisions and analyses
    \item Immutable experiment snapshots
\end{itemize}

\textbf{4. Observability:}
\begin{itemize}
    \item Real-time metrics dashboards
    \item Automated anomaly detection
    \item A/A test monitoring
    \item Statistical quality checks
\end{itemize}

\subsection{Non-Parametric Statistical Methods}

Before implementing the full platform, we need robust non-parametric methods that don't assume normality.

\subsubsection{Bootstrap Testing}

Bootstrap resampling provides distribution-free confidence intervals and hypothesis tests.

\begin{lstlisting}[language=Python, caption={Bootstrap Hypothesis Testing}]
from typing import Callable, Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
from scipy import stats
from dataclasses import dataclass

@dataclass
class BootstrapResult:
    """Results from bootstrap hypothesis test."""
    observed_statistic: float
    bootstrap_distribution: np.ndarray
    p_value: float
    confidence_interval: Tuple[float, float]
    standard_error: float
    bias: float

class BootstrapTester:
    """
    Non-parametric bootstrap hypothesis testing.

    Bootstrap resampling provides robust inference without
    distributional assumptions, particularly useful for:
    - Small samples
    - Non-normal distributions
    - Complex statistics (median, quantiles, ratios)

    Example:
        >>> tester = BootstrapTester(n_bootstrap=10000)
        >>> result = tester.two_sample_test(
        ...     control_data, treatment_data,
        ...     statistic='mean_difference'
        ... )
        >>> print(f"p-value: {result.p_value:.4f}")
    """

    def __init__(
        self,
        n_bootstrap: int = 10000,
        confidence_level: float = 0.95,
        random_state: int = 42
    ):
        """
        Initialize bootstrap tester.

        Args:
            n_bootstrap: Number of bootstrap resamples
            confidence_level: Confidence level for intervals
            random_state: Random seed for reproducibility
        """
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level
        self.random_state = random_state
        np.random.seed(random_state)

    def _bootstrap_resample(self, data: np.ndarray) -> np.ndarray:
        """Generate one bootstrap resample."""
        n = len(data)
        indices = np.random.choice(n, size=n, replace=True)
        return data[indices]

    def _compute_statistic(
        self,
        data: np.ndarray,
        statistic: str
    ) -> float:
        """Compute test statistic."""
        if statistic == 'mean':
            return np.mean(data)
        elif statistic == 'median':
            return np.median(data)
        elif statistic == 'std':
            return np.std(data, ddof=1)
        elif statistic == 'trimmed_mean':
            return stats.trim_mean(data, proportiontocut=0.1)
        elif statistic == 'quantile_90':
            return np.percentile(data, 90)
        else:
            raise ValueError(f"Unknown statistic: {statistic}")

    def one_sample_test(
        self,
        data: np.ndarray,
        null_value: float = 0,
        statistic: str = 'mean',
        alternative: str = 'two-sided'
    ) -> BootstrapResult:
        """
        Bootstrap one-sample test.

        Tests H0: statistic(data) = null_value

        Args:
            data: Sample data
            null_value: Null hypothesis value
            statistic: Test statistic ('mean', 'median', etc.)
            alternative: 'two-sided', 'greater', 'less'

        Returns:
            BootstrapResult with test results
        """
        # Observed statistic
        observed = self._compute_statistic(data, statistic)

        # Center data at null value
        centered_data = data - (observed - null_value)

        # Bootstrap distribution under H0
        bootstrap_stats = np.array([
            self._compute_statistic(self._bootstrap_resample(centered_data), statistic)
            for _ in range(self.n_bootstrap)
        ])

        # P-value
        if alternative == 'two-sided':
            p_value = np.mean(np.abs(bootstrap_stats - null_value) >=
                             np.abs(observed - null_value))
        elif alternative == 'greater':
            p_value = np.mean(bootstrap_stats >= observed)
        else:  # less
            p_value = np.mean(bootstrap_stats <= observed)

        # Confidence interval (percentile method)
        alpha = 1 - self.confidence_level
        ci_lower = np.percentile(bootstrap_stats, alpha / 2 * 100)
        ci_upper = np.percentile(bootstrap_stats, (1 - alpha / 2) * 100)

        # Standard error and bias
        se = np.std(bootstrap_stats)
        bias = np.mean(bootstrap_stats) - observed

        return BootstrapResult(
            observed_statistic=observed,
            bootstrap_distribution=bootstrap_stats,
            p_value=p_value,
            confidence_interval=(ci_lower, ci_upper),
            standard_error=se,
            bias=bias
        )

    def two_sample_test(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        statistic: str = 'mean_difference',
        alternative: str = 'two-sided'
    ) -> BootstrapResult:
        """
        Bootstrap two-sample test.

        Tests H0: statistic(treatment) - statistic(control) = 0

        Args:
            control_data: Control group data
            treatment_data: Treatment group data
            statistic: 'mean_difference', 'median_difference', 'ratio', etc.
            alternative: 'two-sided', 'greater', 'less'

        Returns:
            BootstrapResult with test results
        """
        # Observed statistic
        if statistic == 'mean_difference':
            observed = np.mean(treatment_data) - np.mean(control_data)
        elif statistic == 'median_difference':
            observed = np.median(treatment_data) - np.median(control_data)
        elif statistic == 'ratio':
            observed = np.mean(treatment_data) / np.mean(control_data)
        elif statistic == 'cohens_d':
            pooled_std = np.sqrt(
                (np.var(control_data, ddof=1) + np.var(treatment_data, ddof=1)) / 2
            )
            observed = (np.mean(treatment_data) - np.mean(control_data)) / pooled_std
        else:
            raise ValueError(f"Unknown statistic: {statistic}")

        # Pool data under null hypothesis (no difference)
        pooled_data = np.concatenate([control_data, treatment_data])
        n_control = len(control_data)
        n_treatment = len(treatment_data)

        # Bootstrap distribution under H0
        bootstrap_stats = []
        for _ in range(self.n_bootstrap):
            # Resample from pooled data
            pooled_resample = self._bootstrap_resample(pooled_data)

            # Split into two groups
            control_resample = pooled_resample[:n_control]
            treatment_resample = pooled_resample[n_control:]

            # Compute statistic
            if statistic == 'mean_difference':
                stat = np.mean(treatment_resample) - np.mean(control_resample)
            elif statistic == 'median_difference':
                stat = np.median(treatment_resample) - np.median(control_resample)
            elif statistic == 'ratio':
                stat = np.mean(treatment_resample) / np.mean(control_resample)
            elif statistic == 'cohens_d':
                pooled_std = np.sqrt(
                    (np.var(control_resample, ddof=1) +
                     np.var(treatment_resample, ddof=1)) / 2
                )
                stat = (np.mean(treatment_resample) -
                       np.mean(control_resample)) / pooled_std

            bootstrap_stats.append(stat)

        bootstrap_stats = np.array(bootstrap_stats)

        # P-value
        null_value = 1.0 if statistic == 'ratio' else 0.0
        if alternative == 'two-sided':
            p_value = np.mean(np.abs(bootstrap_stats - null_value) >=
                             np.abs(observed - null_value))
        elif alternative == 'greater':
            p_value = np.mean(bootstrap_stats >= observed)
        else:  # less
            p_value = np.mean(bootstrap_stats <= observed)

        # Confidence interval
        alpha = 1 - self.confidence_level
        ci_lower = np.percentile(bootstrap_stats, alpha / 2 * 100)
        ci_upper = np.percentile(bootstrap_stats, (1 - alpha / 2) * 100)

        # Standard error and bias
        se = np.std(bootstrap_stats)
        bias = np.mean(bootstrap_stats) - observed

        return BootstrapResult(
            observed_statistic=observed,
            bootstrap_distribution=bootstrap_stats,
            p_value=p_value,
            confidence_interval=(ci_lower, ci_upper),
            standard_error=se,
            bias=bias
        )

    def stratified_bootstrap(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        strata_col: str,
        statistic: str = 'mean_difference'
    ) -> BootstrapResult:
        """
        Stratified bootstrap for heterogeneous populations.

        Preserves strata proportions in each bootstrap sample.

        Args:
            data: DataFrame with experiment data
            treatment_col: Treatment indicator column
            outcome_col: Outcome metric column
            strata_col: Stratification variable
            statistic: Test statistic

        Returns:
            BootstrapResult with stratified test results
        """
        strata = data[strata_col].unique()

        def stratified_resample(df):
            """Resample preserving strata."""
            resampled_parts = []
            for stratum in strata:
                stratum_data = df[df[strata_col] == stratum]
                resampled_part = stratum_data.sample(
                    n=len(stratum_data),
                    replace=True
                )
                resampled_parts.append(resampled_part)
            return pd.concat(resampled_parts)

        # Observed statistic
        control = data[data[treatment_col] == 0]
        treatment = data[data[treatment_col] == 1]
        observed = treatment[outcome_col].mean() - control[outcome_col].mean()

        # Bootstrap distribution
        bootstrap_stats = []
        for _ in range(self.n_bootstrap):
            resampled = stratified_resample(data)

            control_resample = resampled[resampled[treatment_col] == 0]
            treatment_resample = resampled[resampled[treatment_col] == 1]

            stat = (treatment_resample[outcome_col].mean() -
                   control_resample[outcome_col].mean())
            bootstrap_stats.append(stat)

        bootstrap_stats = np.array(bootstrap_stats)

        # P-value (two-sided)
        p_value = np.mean(np.abs(bootstrap_stats) >= np.abs(observed))

        # Confidence interval
        alpha = 1 - self.confidence_level
        ci_lower = np.percentile(bootstrap_stats, alpha / 2 * 100)
        ci_upper = np.percentile(bootstrap_stats, (1 - alpha / 2) * 100)

        se = np.std(bootstrap_stats)
        bias = np.mean(bootstrap_stats) - observed

        return BootstrapResult(
            observed_statistic=observed,
            bootstrap_distribution=bootstrap_stats,
            p_value=p_value,
            confidence_interval=(ci_lower, ci_upper),
            standard_error=se,
            bias=bias
        )
\end{lstlisting}

\subsubsection{Permutation Testing}

Permutation tests provide exact p-values under the null hypothesis of exchangeability.

\begin{lstlisting}[language=Python, caption={Permutation Hypothesis Testing}]
@dataclass
class PermutationResult:
    """Results from permutation test."""
    observed_statistic: float
    permutation_distribution: np.ndarray
    p_value: float
    p_value_exact: Optional[float]
    n_permutations: int

class PermutationTester:
    """
    Exact permutation hypothesis testing.

    Permutation tests provide exact (not asymptotic) p-values by
    computing the null distribution via all possible permutations
    or Monte Carlo approximation.

    Advantages:
    - Exact p-values (no asymptotic approximation)
    - No distributional assumptions
    - Robust to outliers
    - Straightforward interpretation

    Example:
        >>> tester = PermutationTester(n_permutations=10000)
        >>> result = tester.two_sample_test(
        ...     control_data, treatment_data,
        ...     statistic='mean_difference'
        ... )
        >>> print(f"Exact p-value: {result.p_value:.4f}")
    """

    def __init__(
        self,
        n_permutations: int = 10000,
        random_state: int = 42,
        exact_threshold: int = 10000
    ):
        """
        Initialize permutation tester.

        Args:
            n_permutations: Number of permutations for Monte Carlo
            random_state: Random seed
            exact_threshold: If total permutations < this, compute all
        """
        self.n_permutations = n_permutations
        self.random_state = random_state
        self.exact_threshold = exact_threshold
        np.random.seed(random_state)

    def _compute_statistic(
        self,
        group1: np.ndarray,
        group2: np.ndarray,
        statistic: str
    ) -> float:
        """Compute test statistic between two groups."""
        if statistic == 'mean_difference':
            return np.mean(group2) - np.mean(group1)
        elif statistic == 'median_difference':
            return np.median(group2) - np.median(group1)
        elif statistic == 't_statistic':
            # Welch's t-statistic
            n1, n2 = len(group1), len(group2)
            mean1, mean2 = np.mean(group1), np.mean(group2)
            var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)

            se = np.sqrt(var1/n1 + var2/n2)
            return (mean2 - mean1) / se if se > 0 else 0
        elif statistic == 'ks_statistic':
            # Kolmogorov-Smirnov statistic
            from scipy.stats import ks_2samp
            return ks_2samp(group1, group2).statistic
        else:
            raise ValueError(f"Unknown statistic: {statistic}")

    def two_sample_test(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        statistic: str = 'mean_difference',
        alternative: str = 'two-sided'
    ) -> PermutationResult:
        """
        Permutation test for two independent samples.

        Tests H0: F_treatment(x) = F_control(x) (same distribution)

        Args:
            control_data: Control group data
            treatment_data: Treatment group data
            statistic: Test statistic
            alternative: 'two-sided', 'greater', 'less'

        Returns:
            PermutationResult with exact p-value
        """
        # Observed statistic
        observed = self._compute_statistic(
            control_data, treatment_data, statistic
        )

        # Combined data
        combined = np.concatenate([control_data, treatment_data])
        n_control = len(control_data)
        n_total = len(combined)

        # Check if exact test is feasible
        from scipy.special import comb
        n_total_perms = comb(n_total, n_control, exact=True)

        if n_total_perms <= self.exact_threshold:
            # Exact test: enumerate all permutations
            from itertools import combinations

            perm_stats = []
            for control_indices in combinations(range(n_total), n_control):
                control_indices = set(control_indices)
                treatment_indices = set(range(n_total)) - control_indices

                control_perm = combined[list(control_indices)]
                treatment_perm = combined[list(treatment_indices)]

                stat = self._compute_statistic(
                    control_perm, treatment_perm, statistic
                )
                perm_stats.append(stat)

            perm_stats = np.array(perm_stats)
            exact = True
        else:
            # Monte Carlo approximation
            perm_stats = []
            for _ in range(self.n_permutations):
                # Random permutation
                permuted = np.random.permutation(combined)
                control_perm = permuted[:n_control]
                treatment_perm = permuted[n_control:]

                stat = self._compute_statistic(
                    control_perm, treatment_perm, statistic
                )
                perm_stats.append(stat)

            perm_stats = np.array(perm_stats)
            exact = False

        # P-value
        if alternative == 'two-sided':
            p_value = np.mean(np.abs(perm_stats) >= np.abs(observed))
        elif alternative == 'greater':
            p_value = np.mean(perm_stats >= observed)
        else:  # less
            p_value = np.mean(perm_stats <= observed)

        return PermutationResult(
            observed_statistic=observed,
            permutation_distribution=perm_stats,
            p_value=p_value,
            p_value_exact=p_value if exact else None,
            n_permutations=len(perm_stats)
        )

    def paired_test(
        self,
        before: np.ndarray,
        after: np.ndarray,
        statistic: str = 'mean_difference'
    ) -> PermutationResult:
        """
        Paired permutation test.

        For paired/matched data, permutes the sign of differences.

        Args:
            before: Measurements before treatment
            after: Measurements after treatment
            statistic: Test statistic

        Returns:
            PermutationResult for paired test
        """
        if len(before) != len(after):
            raise ValueError("before and after must have same length")

        # Compute differences
        differences = after - before
        n = len(differences)

        # Observed statistic
        observed = np.mean(differences)

        # Generate all sign flips or Monte Carlo sample
        n_total_perms = 2 ** n

        if n_total_perms <= self.exact_threshold:
            # Exact test: all sign combinations
            from itertools import product

            perm_stats = []
            for signs in product([-1, 1], repeat=n):
                flipped_diffs = differences * np.array(signs)
                perm_stats.append(np.mean(flipped_diffs))

            perm_stats = np.array(perm_stats)
            exact = True
        else:
            # Monte Carlo: random sign flips
            perm_stats = []
            for _ in range(self.n_permutations):
                signs = np.random.choice([-1, 1], size=n)
                flipped_diffs = differences * signs
                perm_stats.append(np.mean(flipped_diffs))

            perm_stats = np.array(perm_stats)
            exact = False

        # P-value (two-sided)
        p_value = np.mean(np.abs(perm_stats) >= np.abs(observed))

        return PermutationResult(
            observed_statistic=observed,
            permutation_distribution=perm_stats,
            p_value=p_value,
            p_value_exact=p_value if exact else None,
            n_permutations=len(perm_stats)
        )

    def stratified_permutation_test(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        outcome_col: str,
        strata_col: str,
        statistic: str = 'mean_difference'
    ) -> PermutationResult:
        """
        Stratified permutation test preserving strata.

        Permutes treatment labels within each stratum.

        Args:
            data: DataFrame with experiment data
            treatment_col: Treatment indicator
            outcome_col: Outcome metric
            strata_col: Stratification variable
            statistic: Test statistic

        Returns:
            PermutationResult for stratified test
        """
        # Observed statistic
        control = data[data[treatment_col] == 0]
        treatment = data[data[treatment_col] == 1]
        observed = treatment[outcome_col].mean() - control[outcome_col].mean()

        strata = data[strata_col].unique()

        # Permutation distribution
        perm_stats = []
        for _ in range(self.n_permutations):
            permuted_parts = []

            # Permute within each stratum
            for stratum in strata:
                stratum_data = data[data[strata_col] == stratum].copy()

                # Shuffle treatment labels within stratum
                stratum_data[treatment_col] = np.random.permutation(
                    stratum_data[treatment_col].values
                )
                permuted_parts.append(stratum_data)

            permuted_data = pd.concat(permuted_parts)

            # Compute statistic
            control_perm = permuted_data[permuted_data[treatment_col] == 0]
            treatment_perm = permuted_data[permuted_data[treatment_col] == 1]

            stat = (treatment_perm[outcome_col].mean() -
                   control_perm[outcome_col].mean())
            perm_stats.append(stat)

        perm_stats = np.array(perm_stats)

        # P-value (two-sided)
        p_value = np.mean(np.abs(perm_stats) >= np.abs(observed))

        return PermutationResult(
            observed_statistic=observed,
            permutation_distribution=perm_stats,
            p_value=p_value,
            p_value_exact=None,  # Monte Carlo approximation
            n_permutations=len(perm_stats)
        )
\end{lstlisting}

\subsection{Unified Statistical Analyzer}

\begin{lstlisting}[language=Python, caption={Comprehensive Statistical Analysis Engine}]
from enum import Enum
from abc import ABC, abstractmethod

class TestMethod(Enum):
    """Available statistical testing methods."""
    T_TEST = "t_test"
    WELCH_T_TEST = "welch_t_test"
    MANN_WHITNEY = "mann_whitney"
    BOOTSTRAP = "bootstrap"
    PERMUTATION = "permutation"
    BAYESIAN = "bayesian"
    CHI_SQUARE = "chi_square"
    FISHER_EXACT = "fisher_exact"

class StatisticalAnalyzer:
    """
    Unified interface for multiple statistical testing methods.

    Automatically selects appropriate test based on data characteristics,
    or allows manual method specification.

    Example:
        >>> analyzer = StatisticalAnalyzer()
        >>> result = analyzer.analyze(
        ...     control_data, treatment_data,
        ...     method='auto',  # Automatic method selection
        ...     metric_type='continuous'
        ... )
        >>> print(result.summary())
    """

    def __init__(
        self,
        alpha: float = 0.05,
        power: float = 0.80,
        min_sample_size: int = 100
    ):
        """
        Initialize statistical analyzer.

        Args:
            alpha: Significance level
            power: Desired statistical power
            min_sample_size: Minimum sample for asymptotic tests
        """
        self.alpha = alpha
        self.power = power
        self.min_sample_size = min_sample_size

        # Initialize testers
        self.bootstrap_tester = BootstrapTester()
        self.permutation_tester = PermutationTester()

    def _check_normality(self, data: np.ndarray) -> bool:
        """Check if data appears normally distributed."""
        if len(data) < 20:
            return False  # Too small for reliable normality test

        # Shapiro-Wilk test
        statistic, p_value = stats.shapiro(data[:5000])  # Limit to 5000 samples
        return p_value > 0.05

    def _select_method(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        metric_type: str
    ) -> TestMethod:
        """
        Automatically select appropriate statistical test.

        Decision tree:
        1. If binary/categorical: Chi-square or Fisher exact
        2. If continuous and small sample: Bootstrap or permutation
        3. If continuous and normal: t-test
        4. If continuous and non-normal: Mann-Whitney or bootstrap
        """
        n_control = len(control_data)
        n_treatment = len(treatment_data)

        if metric_type == 'binary':
            # Binary data: proportion test
            total_size = n_control + n_treatment
            if total_size < 1000:
                return TestMethod.FISHER_EXACT
            else:
                return TestMethod.CHI_SQUARE

        # Continuous data
        small_sample = min(n_control, n_treatment) < self.min_sample_size

        if small_sample:
            # Small sample: non-parametric
            return TestMethod.BOOTSTRAP

        # Check normality
        control_normal = self._check_normality(control_data)
        treatment_normal = self._check_normality(treatment_data)

        if control_normal and treatment_normal:
            # Both normal: t-test (Welch's for unequal variances)
            return TestMethod.WELCH_T_TEST
        else:
            # Non-normal: non-parametric
            return TestMethod.MANN_WHITNEY

    def analyze(
        self,
        control_data: Union[np.ndarray, pd.Series],
        treatment_data: Union[np.ndarray, pd.Series],
        method: Union[str, TestMethod] = 'auto',
        metric_type: str = 'continuous',
        alternative: str = 'two-sided'
    ) -> Dict[str, Any]:
        """
        Perform statistical analysis using specified or automatic method.

        Args:
            control_data: Control group data
            treatment_data: Treatment group data
            method: Testing method or 'auto' for automatic selection
            metric_type: 'continuous' or 'binary'
            alternative: 'two-sided', 'greater', 'less'

        Returns:
            Dictionary with comprehensive test results
        """
        # Convert to numpy arrays
        if isinstance(control_data, pd.Series):
            control_data = control_data.values
        if isinstance(treatment_data, pd.Series):
            treatment_data = treatment_data.values

        # Select method
        if method == 'auto':
            selected_method = self._select_method(
                control_data, treatment_data, metric_type
            )
        elif isinstance(method, str):
            selected_method = TestMethod(method)
        else:
            selected_method = method

        # Perform test
        if selected_method == TestMethod.T_TEST:
            result = self._t_test(control_data, treatment_data, equal_var=True)
        elif selected_method == TestMethod.WELCH_T_TEST:
            result = self._t_test(control_data, treatment_data, equal_var=False)
        elif selected_method == TestMethod.MANN_WHITNEY:
            result = self._mann_whitney(control_data, treatment_data, alternative)
        elif selected_method == TestMethod.BOOTSTRAP:
            result = self._bootstrap_test(control_data, treatment_data)
        elif selected_method == TestMethod.PERMUTATION:
            result = self._permutation_test(control_data, treatment_data, alternative)
        elif selected_method == TestMethod.CHI_SQUARE:
            result = self._chi_square_test(control_data, treatment_data)
        elif selected_method == TestMethod.FISHER_EXACT:
            result = self._fisher_exact_test(control_data, treatment_data)
        else:
            raise ValueError(f"Method {selected_method} not implemented")

        # Add common fields
        result['method'] = selected_method.value
        result['n_control'] = len(control_data)
        result['n_treatment'] = len(treatment_data)
        result['alpha'] = self.alpha
        result['significant'] = result['p_value'] < self.alpha

        # Effect size
        result['effect_size'] = self._compute_effect_size(
            control_data, treatment_data, metric_type
        )

        return result

    def _t_test(
        self,
        control: np.ndarray,
        treatment: np.ndarray,
        equal_var: bool
    ) -> Dict:
        """Perform t-test (Student's or Welch's)."""
        statistic, p_value = stats.ttest_ind(
            treatment, control,
            equal_var=equal_var
        )

        # Confidence interval for difference
        mean_diff = np.mean(treatment) - np.mean(control)
        n1, n2 = len(control), len(treatment)

        if equal_var:
            # Pooled variance
            var_pooled = ((n1 - 1) * np.var(control, ddof=1) +
                         (n2 - 1) * np.var(treatment, ddof=1)) / (n1 + n2 - 2)
            se = np.sqrt(var_pooled * (1/n1 + 1/n2))
            df = n1 + n2 - 2
        else:
            # Welch's (unequal variances)
            var1, var2 = np.var(control, ddof=1), np.var(treatment, ddof=1)
            se = np.sqrt(var1/n1 + var2/n2)
            # Welch-Satterthwaite df
            df = (var1/n1 + var2/n2)**2 / (
                (var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1)
            )

        t_critical = stats.t.ppf(1 - self.alpha/2, df)
        ci_lower = mean_diff - t_critical * se
        ci_upper = mean_diff + t_critical * se

        return {
            'p_value': p_value,
            'statistic': statistic,
            'mean_control': np.mean(control),
            'mean_treatment': np.mean(treatment),
            'mean_difference': mean_diff,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'degrees_of_freedom': df
        }

    def _mann_whitney(
        self,
        control: np.ndarray,
        treatment: np.ndarray,
        alternative: str
    ) -> Dict:
        """Perform Mann-Whitney U test (non-parametric)."""
        statistic, p_value = stats.mannwhitneyu(
            treatment, control,
            alternative=alternative
        )

        # Rank-biserial correlation (effect size)
        n1, n2 = len(control), len(treatment)
        r = 1 - (2 * statistic) / (n1 * n2)

        return {
            'p_value': p_value,
            'statistic': statistic,
            'median_control': np.median(control),
            'median_treatment': np.median(treatment),
            'median_difference': np.median(treatment) - np.median(control),
            'rank_biserial_correlation': r
        }

    def _bootstrap_test(
        self,
        control: np.ndarray,
        treatment: np.ndarray
    ) -> Dict:
        """Perform bootstrap test."""
        result = self.bootstrap_tester.two_sample_test(
            control, treatment,
            statistic='mean_difference'
        )

        return {
            'p_value': result.p_value,
            'mean_control': np.mean(control),
            'mean_treatment': np.mean(treatment),
            'mean_difference': result.observed_statistic,
            'ci_lower': result.confidence_interval[0],
            'ci_upper': result.confidence_interval[1],
            'standard_error': result.standard_error,
            'bias': result.bias
        }

    def _permutation_test(
        self,
        control: np.ndarray,
        treatment: np.ndarray,
        alternative: str
    ) -> Dict:
        """Perform permutation test."""
        result = self.permutation_tester.two_sample_test(
            control, treatment,
            statistic='mean_difference',
            alternative=alternative
        )

        return {
            'p_value': result.p_value,
            'p_value_exact': result.p_value_exact,
            'mean_control': np.mean(control),
            'mean_treatment': np.mean(treatment),
            'mean_difference': result.observed_statistic,
            'n_permutations': result.n_permutations
        }

    def _chi_square_test(
        self,
        control: np.ndarray,
        treatment: np.ndarray
    ) -> Dict:
        """Perform chi-square test for proportions."""
        # Assume binary 0/1 data
        control_successes = np.sum(control)
        control_total = len(control)
        treatment_successes = np.sum(treatment)
        treatment_total = len(treatment)

        # 2x2 contingency table
        table = np.array([
            [control_successes, control_total - control_successes],
            [treatment_successes, treatment_total - treatment_successes]
        ])

        chi2, p_value, dof, expected = stats.chi2_contingency(table)

        control_rate = control_successes / control_total
        treatment_rate = treatment_successes / treatment_total

        return {
            'p_value': p_value,
            'statistic': chi2,
            'rate_control': control_rate,
            'rate_treatment': treatment_rate,
            'rate_difference': treatment_rate - control_rate,
            'relative_lift': (treatment_rate - control_rate) / control_rate if control_rate > 0 else None
        }

    def _fisher_exact_test(
        self,
        control: np.ndarray,
        treatment: np.ndarray
    ) -> Dict:
        """Perform Fisher's exact test for small samples."""
        control_successes = np.sum(control)
        control_total = len(control)
        treatment_successes = np.sum(treatment)
        treatment_total = len(treatment)

        # 2x2 contingency table
        table = np.array([
            [control_successes, control_total - control_successes],
            [treatment_successes, treatment_total - treatment_successes]
        ])

        odds_ratio, p_value = stats.fisher_exact(table)

        control_rate = control_successes / control_total
        treatment_rate = treatment_successes / treatment_total

        return {
            'p_value': p_value,
            'odds_ratio': odds_ratio,
            'rate_control': control_rate,
            'rate_treatment': treatment_rate,
            'rate_difference': treatment_rate - control_rate,
            'relative_lift': (treatment_rate - control_rate) / control_rate if control_rate > 0 else None
        }

    def _compute_effect_size(
        self,
        control: np.ndarray,
        treatment: np.ndarray,
        metric_type: str
    ) -> Dict[str, float]:
        """Compute effect size metrics."""
        if metric_type == 'continuous':
            # Cohen's d
            pooled_std = np.sqrt(
                (np.var(control, ddof=1) + np.var(treatment, ddof=1)) / 2
            )
            cohens_d = (np.mean(treatment) - np.mean(control)) / pooled_std if pooled_std > 0 else 0

            # Relative lift
            relative_lift = (np.mean(treatment) - np.mean(control)) / np.mean(control) if np.mean(control) != 0 else 0

            return {
                'cohens_d': cohens_d,
                'relative_lift': relative_lift
            }
        else:  # binary
            # Risk ratio and odds ratio
            control_rate = np.mean(control)
            treatment_rate = np.mean(treatment)

            risk_ratio = treatment_rate / control_rate if control_rate > 0 else None

            control_odds = control_rate / (1 - control_rate) if control_rate < 1 else None
            treatment_odds = treatment_rate / (1 - treatment_rate) if treatment_rate < 1 else None
            odds_ratio = treatment_odds / control_odds if control_odds and control_odds > 0 else None

            return {
                'risk_ratio': risk_ratio,
                'odds_ratio': odds_ratio,
                'absolute_lift': treatment_rate - control_rate,
                'relative_lift': (treatment_rate - control_rate) / control_rate if control_rate > 0 else None
            }
\end{lstlisting}

\subsection{Integrated Experiment Platform}

\begin{lstlisting}[language=Python, caption={End-to-End Experiment Management Platform}]
from typing import Any, Callable, Dict, List, Optional
import hashlib
import json
from datetime import datetime

class ExperimentPlatform:
    """
    End-to-end A/B testing experimentation platform.

    Integrates all components:
    - Experiment design and randomization
    - Statistical analysis with multiple methods
    - Power analysis and sample size calculation
    - Automated decision-making
    - Audit logging and reproducibility

    Example:
        >>> platform = ExperimentPlatform()
        >>>
        >>> # Create experiment
        >>> experiment = platform.create_experiment(
        ...     name="checkout_redesign",
        ...     arms=["control", "variant_a", "variant_b"],
        ...     allocation=[0.33, 0.33, 0.34],
        ...     metrics=["conversion_rate", "revenue_per_user"]
        ... )
        >>>
        >>> # Assign users
        >>> assignments = platform.assign_users(
        ...     experiment_id=experiment['id'],
        ...     user_ids=user_df['user_id'],
        ...     stratify_by=['country', 'device']
        ... )
        >>>
        >>> # Analyze results
        >>> results = platform.analyze_experiment(
        ...     experiment_id=experiment['id'],
        ...     data=experiment_data,
        ...     method='auto'
        ... )
        >>>
        >>> # Make decision
        >>> decision = platform.make_decision(results)
    """

    def __init__(
        self,
        alpha: float = 0.05,
        power: float = 0.80,
        random_state: int = 42
    ):
        """
        Initialize experiment platform.

        Args:
            alpha: Significance level for tests
            power: Desired statistical power
            random_state: Global random seed
        """
        self.alpha = alpha
        self.power = power
        self.random_state = random_state

        # Initialize components
        self.analyzer = StatisticalAnalyzer(alpha=alpha, power=power)
        self.experiments = {}
        self.audit_log = []

        np.random.seed(random_state)

    def create_experiment(
        self,
        name: str,
        arms: List[str],
        allocation: Optional[List[float]] = None,
        metrics: List[str] = None,
        stratification_vars: Optional[List[str]] = None,
        minimum_sample_size: Optional[int] = None,
        description: str = ""
    ) -> Dict[str, Any]:
        """
        Create new experiment configuration.

        Args:
            name: Experiment name
            arms: List of treatment arms
            allocation: Allocation proportions (must sum to 1)
            metrics: Primary and secondary metrics
            stratification_vars: Variables for stratified randomization
            minimum_sample_size: Minimum sample size per arm
            description: Experiment description

        Returns:
            Experiment configuration dictionary
        """
        # Validate allocation
        if allocation is None:
            allocation = [1.0 / len(arms)] * len(arms)

        if len(allocation) != len(arms):
            raise ValueError("Allocation must match number of arms")

        if not np.isclose(sum(allocation), 1.0):
            raise ValueError("Allocation must sum to 1")

        # Generate experiment ID
        experiment_id = hashlib.sha256(
            f"{name}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]

        experiment = {
            'id': experiment_id,
            'name': name,
            'arms': arms,
            'allocation': allocation,
            'metrics': metrics or [],
            'stratification_vars': stratification_vars or [],
            'minimum_sample_size': minimum_sample_size,
            'description': description,
            'created_at': datetime.now().isoformat(),
            'status': 'created'
        }

        self.experiments[experiment_id] = experiment

        self._log_action('create_experiment', experiment_id, experiment)

        return experiment

    def assign_users(
        self,
        experiment_id: str,
        user_ids: Union[List, pd.Series, np.ndarray],
        user_data: Optional[pd.DataFrame] = None,
        stratify_by: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        Assign users to experiment arms using deterministic hashing.

        Args:
            experiment_id: Experiment identifier
            user_ids: User identifiers to assign
            user_data: Optional user data for stratification
            stratify_by: Columns to stratify by (requires user_data)

        Returns:
            DataFrame with user_id and assigned arm
        """
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")

        experiment = self.experiments[experiment_id]
        arms = experiment['arms']
        allocation = experiment['allocation']

        # Convert to DataFrame if needed
        if not isinstance(user_ids, pd.DataFrame):
            assignments = pd.DataFrame({'user_id': user_ids})
        else:
            assignments = user_ids.copy()

        # Deterministic assignment using hash
        def hash_assign(user_id, exp_id):
            """Deterministic hash-based assignment."""
            hash_val = int(hashlib.sha256(
                f"{exp_id}_{user_id}_{self.random_state}".encode()
            ).hexdigest(), 16)

            # Map to [0, 1)
            uniform_val = (hash_val % 1000000) / 1000000

            # Assign to arm based on allocation
            cumulative = 0
            for i, (arm, prob) in enumerate(zip(arms, allocation)):
                cumulative += prob
                if uniform_val < cumulative:
                    return arm

            return arms[-1]  # Fallback

        if stratify_by and user_data is not None:
            # Stratified randomization
            assignments = assignments.merge(
                user_data[['user_id'] + stratify_by],
                on='user_id',
                how='left'
            )

            # Assign within strata
            def assign_within_stratum(group):
                group['arm'] = group['user_id'].apply(
                    lambda uid: hash_assign(uid, experiment_id)
                )
                return group

            assignments = assignments.groupby(stratify_by).apply(
                assign_within_stratum
            ).reset_index(drop=True)
        else:
            # Simple randomization
            assignments['arm'] = assignments['user_id'].apply(
                lambda uid: hash_assign(uid, experiment_id)
            )

        self._log_action('assign_users', experiment_id, {
            'n_users': len(assignments),
            'stratify_by': stratify_by
        })

        return assignments[['user_id', 'arm']]

    def analyze_experiment(
        self,
        experiment_id: str,
        data: pd.DataFrame,
        arm_col: str = 'arm',
        metric_col: str = None,
        method: str = 'auto',
        control_arm: str = None
    ) -> Dict[str, Any]:
        """
        Analyze experiment results with comprehensive statistical testing.

        Args:
            experiment_id: Experiment identifier
            data: Experiment data with arms and metrics
            arm_col: Column name for treatment arm
            metric_col: Metric column to analyze (if None, analyze all)
            method: Statistical method ('auto', 't_test', 'bootstrap', etc.)
            control_arm: Control arm name (defaults to first arm)

        Returns:
            Comprehensive analysis results
        """
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")

        experiment = self.experiments[experiment_id]

        if control_arm is None:
            control_arm = experiment['arms'][0]

        # Determine metrics to analyze
        if metric_col:
            metrics_to_analyze = [metric_col]
        else:
            metrics_to_analyze = experiment['metrics']

        results = {
            'experiment_id': experiment_id,
            'experiment_name': experiment['name'],
            'analyzed_at': datetime.now().isoformat(),
            'control_arm': control_arm,
            'metrics': {}
        }

        # Analyze each metric
        for metric in metrics_to_analyze:
            if metric not in data.columns:
                continue

            metric_results = {}

            # Get control data
            control_data = data[data[arm_col] == control_arm][metric].dropna()

            # Compare each treatment to control
            for arm in experiment['arms']:
                if arm == control_arm:
                    continue

                treatment_data = data[data[arm_col] == arm][metric].dropna()

                if len(control_data) == 0 or len(treatment_data) == 0:
                    continue

                # Determine metric type
                metric_type = 'binary' if data[metric].nunique() == 2 else 'continuous'

                # Perform analysis
                analysis = self.analyzer.analyze(
                    control_data,
                    treatment_data,
                    method=method,
                    metric_type=metric_type
                )

                metric_results[arm] = analysis

            results['metrics'][metric] = metric_results

        # Overall summary
        results['summary'] = self._generate_summary(results)

        self._log_action('analyze_experiment', experiment_id, {
            'metrics_analyzed': metrics_to_analyze,
            'method': method
        })

        return results

    def make_decision(
        self,
        analysis_results: Dict[str, Any],
        decision_criteria: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Make deployment decision based on analysis results.

        Args:
            analysis_results: Results from analyze_experiment
            decision_criteria: Custom decision criteria (optional)
                Default: {'min_p_value': 0.05, 'min_effect_size': 0.02}

        Returns:
            Decision recommendation with rationale
        """
        if decision_criteria is None:
            decision_criteria = {
                'max_p_value': self.alpha,
                'min_relative_lift': 0.01  # 1% minimum lift
            }

        decisions = {}

        # Evaluate each metric
        for metric, arms_results in analysis_results['metrics'].items():
            for arm, result in arms_results.items():
                significant = result['p_value'] < decision_criteria['max_p_value']

                # Check effect size
                if 'relative_lift' in result['effect_size']:
                    lift = result['effect_size']['relative_lift']
                    meaningful_lift = abs(lift) >= decision_criteria['min_relative_lift']
                else:
                    meaningful_lift = True  # Conservative

                # Decision
                if significant and meaningful_lift:
                    if 'mean_difference' in result and result['mean_difference'] > 0:
                        decision = 'DEPLOY'
                    elif 'rate_difference' in result and result['rate_difference'] > 0:
                        decision = 'DEPLOY'
                    else:
                        decision = 'DO_NOT_DEPLOY'
                else:
                    decision = 'INCONCLUSIVE'

                decisions[f"{metric}_{arm}"] = {
                    'decision': decision,
                    'significant': significant,
                    'meaningful_lift': meaningful_lift,
                    'p_value': result['p_value'],
                    'effect_size': result.get('effect_size', {})
                }

        # Overall recommendation
        deploy_count = sum(1 for d in decisions.values() if d['decision'] == 'DEPLOY')
        no_deploy_count = sum(1 for d in decisions.values() if d['decision'] == 'DO_NOT_DEPLOY')

        if deploy_count > 0 and no_deploy_count == 0:
            overall_decision = 'DEPLOY'
            rationale = f"{deploy_count} metric(s) show significant positive effect"
        elif no_deploy_count > 0:
            overall_decision = 'DO_NOT_DEPLOY'
            rationale = f"{no_deploy_count} metric(s) show significant negative effect"
        else:
            overall_decision = 'INCONCLUSIVE'
            rationale = "No significant effects detected - continue experiment or increase sample size"

        decision_output = {
            'overall_decision': overall_decision,
            'rationale': rationale,
            'metric_decisions': decisions,
            'decision_criteria': decision_criteria,
            'timestamp': datetime.now().isoformat()
        }

        self._log_action(
            'make_decision',
            analysis_results['experiment_id'],
            decision_output
        )

        return decision_output

    def _generate_summary(self, results: Dict) -> Dict:
        """Generate summary statistics across all metrics."""
        summary = {
            'total_metrics': len(results['metrics']),
            'significant_results': 0,
            'average_p_value': [],
            'largest_effect_sizes': []
        }

        for metric, arms_results in results['metrics'].items():
            for arm, result in arms_results.items():
                summary['average_p_value'].append(result['p_value'])

                if result['significant']:
                    summary['significant_results'] += 1

                if 'effect_size' in result:
                    if 'cohens_d' in result['effect_size']:
                        summary['largest_effect_sizes'].append({
                            'metric': metric,
                            'arm': arm,
                            'cohens_d': result['effect_size']['cohens_d']
                        })

        if summary['average_p_value']:
            summary['average_p_value'] = np.mean(summary['average_p_value'])

        # Sort effect sizes
        summary['largest_effect_sizes'].sort(
            key=lambda x: abs(x.get('cohens_d', 0)),
            reverse=True
        )

        return summary

    def _log_action(
        self,
        action: str,
        experiment_id: str,
        details: Dict
    ) -> None:
        """Log action for audit trail."""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'action': action,
            'experiment_id': experiment_id,
            'details': details
        }
        self.audit_log.append(log_entry)

    def get_audit_log(
        self,
        experiment_id: Optional[str] = None
    ) -> List[Dict]:
        """
        Retrieve audit log.

        Args:
            experiment_id: Filter by experiment (optional)

        Returns:
            List of audit log entries
        """
        if experiment_id:
            return [
                entry for entry in self.audit_log
                if entry['experiment_id'] == experiment_id
            ]
        return self.audit_log

    def export_experiment(
        self,
        experiment_id: str,
        filepath: str
    ) -> None:
        """
        Export experiment configuration and results.

        Args:
            experiment_id: Experiment to export
            filepath: Output file path (JSON)
        """
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")

        export_data = {
            'experiment': self.experiments[experiment_id],
            'audit_log': self.get_audit_log(experiment_id)
        }

        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

        self._log_action('export_experiment', experiment_id, {'filepath': filepath})
\end{lstlisting}

\subsection{Integration with Data Science Tools}

\begin{lstlisting}[language=Python, caption={Platform Integration Examples}]
# Example 1: Integration with pandas for data processing
import pandas as pd

# Load experiment data
experiment_data = pd.read_csv('experiment_results.csv')

# Initialize platform
platform = ExperimentPlatform(alpha=0.05, power=0.80)

# Create experiment
experiment = platform.create_experiment(
    name="homepage_redesign",
    arms=["control", "variant_a", "variant_b"],
    allocation=[0.5, 0.25, 0.25],
    metrics=["click_through_rate", "time_on_site", "bounce_rate"],
    description="Testing new homepage layouts"
)

# Assign users with stratification
user_data = pd.read_csv('users.csv')
assignments = platform.assign_users(
    experiment_id=experiment['id'],
    user_ids=user_data['user_id'],
    user_data=user_data,
    stratify_by=['country', 'device_type']
)

# Merge assignments with experiment data
full_data = experiment_data.merge(assignments, on='user_id')

# Analyze with automatic method selection
results = platform.analyze_experiment(
    experiment_id=experiment['id'],
    data=full_data,
    method='auto'
)

# Make decision
decision = platform.make_decision(results)

print(f"Decision: {decision['overall_decision']}")
print(f"Rationale: {decision['rationale']}")

# ============================================================
# Example 2: Integration with scikit-learn for ML metrics
from sklearn.metrics import roc_auc_score, precision_score, recall_score

# Evaluate ML model performance in A/B test
def evaluate_ml_model_abtest(
    platform: ExperimentPlatform,
    experiment_id: str,
    predictions_df: pd.DataFrame,
    true_labels_df: pd.DataFrame
):
    """
    Compare ML models in A/B test framework.

    Args:
        platform: ExperimentPlatform instance
        experiment_id: Experiment ID
        predictions_df: Predictions from each model variant
        true_labels_df: Ground truth labels

    Returns:
        Analysis results for ML metrics
    """
    # Compute metrics for each arm
    arms = ['control_model', 'variant_model']
    metrics_data = []

    for arm in arms:
        y_true = true_labels_df[f'{arm}_true']
        y_pred = predictions_df[f'{arm}_pred']

        auc = roc_auc_score(y_true, y_pred)
        precision = precision_score(y_true, (y_pred > 0.5).astype(int))
        recall = recall_score(y_true, (y_pred > 0.5).astype(int))

        metrics_data.append({
            'user_id': range(len(y_true)),
            'arm': arm,
            'auc': auc,
            'precision': precision,
            'recall': recall
        })

    ml_metrics_df = pd.concat([pd.DataFrame(d) for d in metrics_data])

    # Analyze with platform
    results = platform.analyze_experiment(
        experiment_id=experiment_id,
        data=ml_metrics_df,
        metric_col='auc',
        method='bootstrap'  # Use bootstrap for non-normal metrics
    )

    return results

# ============================================================
# Example 3: Integration with visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_experiment_results(
    results: Dict,
    metric: str = 'conversion_rate'
):
    """
    Visualize A/B test results.

    Args:
        results: Results from platform.analyze_experiment()
        metric: Metric to visualize
    """
    metric_results = results['metrics'].get(metric, {})

    if not metric_results:
        print(f"No results for metric: {metric}")
        return

    # Extract data for plotting
    arms = []
    mean_values = []
    ci_lowers = []
    ci_uppers = []
    p_values = []

    # Add control
    control_arm = results['control_arm']
    arms.append(control_arm)

    # Get control mean from first comparison
    first_result = list(metric_results.values())[0]
    mean_values.append(first_result.get('mean_control', 0))
    ci_lowers.append(first_result.get('mean_control', 0))  # No uncertainty for reference
    ci_uppers.append(first_result.get('mean_control', 0))
    p_values.append(1.0)

    # Add treatments
    for arm, result in metric_results.items():
        arms.append(arm)
        mean_values.append(result.get('mean_treatment', 0))
        ci_lowers.append(result.get('ci_lower', result.get('mean_treatment', 0)))
        ci_uppers.append(result.get('ci_upper', result.get('mean_treatment', 0)))
        p_values.append(result['p_value'])

    # Create figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # Plot 1: Mean values with confidence intervals
    x_pos = np.arange(len(arms))
    colors = ['gray'] + ['green' if p < 0.05 else 'orange'
                         for p in p_values[1:]]

    ax1.bar(x_pos, mean_values, color=colors, alpha=0.7)
    ax1.errorbar(x_pos, mean_values,
                yerr=[np.array(mean_values) - np.array(ci_lowers),
                      np.array(ci_uppers) - np.array(mean_values)],
                fmt='none', ecolor='black', capsize=5)

    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(arms, rotation=45)
    ax1.set_ylabel(metric.replace('_', ' ').title())
    ax1.set_title(f'{metric.replace("_", " ").title()} by Arm')
    ax1.grid(axis='y', alpha=0.3)

    # Plot 2: P-values
    ax2.bar(x_pos[1:], p_values[1:], color=colors[1:], alpha=0.7)
    ax2.axhline(y=0.05, color='red', linestyle='--', label=r'$\alpha$ = 0.05')
    ax2.set_xticks(x_pos[1:])
    ax2.set_xticklabels(arms[1:], rotation=45)
    ax2.set_ylabel('P-value')
    ax2.set_title('Statistical Significance')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.show()

# ============================================================
# Example 4: Continuous monitoring pipeline
from typing import Callable
import time

class ContinuousMonitor:
    """
    Continuous monitoring for live experiments.

    Periodically analyzes experiment data and triggers alerts.
    """

    def __init__(
        self,
        platform: ExperimentPlatform,
        experiment_id: str,
        data_loader: Callable,
        check_interval_seconds: int = 3600
    ):
        self.platform = platform
        self.experiment_id = experiment_id
        self.data_loader = data_loader
        self.check_interval = check_interval_seconds
        self.monitoring = False

    def start_monitoring(self, alert_callback: Optional[Callable] = None):
        """
        Start continuous monitoring.

        Args:
            alert_callback: Function to call on significant results
        """
        self.monitoring = True

        while self.monitoring:
            # Load latest data
            data = self.data_loader()

            # Analyze
            results = self.platform.analyze_experiment(
                experiment_id=self.experiment_id,
                data=data,
                method='auto'
            )

            # Check for significant results
            decision = self.platform.make_decision(results)

            if decision['overall_decision'] != 'INCONCLUSIVE':
                if alert_callback:
                    alert_callback(decision, results)

            # Wait before next check
            time.sleep(self.check_interval)

    def stop_monitoring(self):
        """Stop continuous monitoring."""
        self.monitoring = False

# Usage:
def alert_handler(decision, results):
    """Handle alerts from continuous monitoring."""
    print(f"ALERT: {decision['overall_decision']}")
    print(f"Rationale: {decision['rationale']}")
    # Send email, Slack notification, etc.

# monitor = ContinuousMonitor(
#     platform=platform,
#     experiment_id=experiment['id'],
#     data_loader=lambda: pd.read_sql("SELECT * FROM experiment_data", connection),
#     check_interval_seconds=3600  # Check every hour
# )
#
# monitor.start_monitoring(alert_callback=alert_handler)
\end{lstlisting}

This comprehensive platform architecture provides:

\begin{itemize}
    \item \textbf{Unified Interface}: Single platform for all experiment lifecycle stages
    \item \textbf{Extensibility}: Plugin architecture for new statistical methods and randomization schemes
    \item \textbf{Reproducibility}: Deterministic hashing ensures consistent assignments; audit logs track all actions
    \item \textbf{Robustness}: Non-parametric methods (bootstrap, permutation) handle edge cases
    \item \textbf{Automation}: Automatic method selection based on data characteristics
    \item \textbf{Integration}: Seamless integration with pandas, scikit-learn, visualization libraries
    \item \textbf{Production-Ready}: Continuous monitoring, decision automation, export capabilities
\end{itemize}

The platform implements design patterns:
\begin{enumerate}
    \item \textbf{Strategy Pattern}: Interchangeable statistical methods selected at runtime
    \item \textbf{Factory Pattern}: Experiment creation with standardized configuration
    \item \textbf{Observer Pattern}: Continuous monitoring with callback hooks
    \item \textbf{Builder Pattern}: Flexible experiment configuration
\end{enumerate}

Organizations can extend this platform by:
\begin{itemize}
    \item Adding custom statistical methods to \texttt{StatisticalAnalyzer}
    \item Implementing domain-specific decision criteria in \texttt{make\_decision}
    \item Creating connectors to data warehouses and metric stores
    \item Building dashboards on top of the analysis APIs
    \item Integrating with feature flagging systems for gradual rollouts
\end{itemize}

\section{Experiment Governance and Automated Reporting}

While technical platform capabilities enable rigorous experimentation, organizational governance ensures experiments are safe, ethical, and aligned with business objectives. This section presents a comprehensive governance framework with automated reporting, approval workflows, and risk assessment.

\subsection{Real-World Scenario: The Risky Experiment}

\textbf{The Setup:}

A fintech company's growth team proposed an aggressive experiment to increase credit card sign-ups:

\textbf{Experiment Proposal:}
\begin{itemize}
    \item \textbf{Change}: Reduce displayed APR from 24.9\% to 19.9\% on landing page
    \item \textbf{Rationale}: Lower rate will increase conversions by 15-20\%
    \item \textbf{Traffic}: 50\% of users for 2 weeks
    \item \textbf{Primary Metric}: Credit card applications
\end{itemize}

\textbf{Initial Approval Request - Rejected:}

The experiment was submitted for standard approval, but the automated risk assessment flagged critical issues:

\begin{enumerate}
    \item \textbf{Legal Risk (CRITICAL)}: Displaying incorrect APR violates Truth in Lending Act
    \item \textbf{Financial Risk (HIGH)}: If users approved at 19.9\% but contract shows 24.9\%, company faces class-action lawsuit exposure
    \item \textbf{Reputational Risk (HIGH)}: Bait-and-switch perception could damage brand
    \item \textbf{Customer Impact (CRITICAL)}: 50\% traffic = 500K users potentially misled
    \item \textbf{Regulatory Risk (CRITICAL)}: CFPB violations, potential fines \$5M+
\end{enumerate}

\textbf{Governance Response:}

The ExperimentGovernance system automatically:
\begin{itemize}
    \item \textbf{Blocked submission} - experiment cannot proceed without legal review
    \item \textbf{Notified stakeholders}: Legal, Compliance, Risk Management, VP Product
    \item \textbf{Required special approval}: Escalated to Legal + Compliance + Chief Risk Officer
    \item \textbf{Documented rationale}: Risk assessment report saved to audit log
\end{itemize}

\textbf{Revised Experiment - Approved:}

After legal consultation, the team redesigned the experiment:

\begin{itemize}
    \item \textbf{Change}: Test different messaging emphasis (rate vs rewards vs benefits)
    \item \textbf{All variants}: Display actual 24.9\% APR accurately
    \item \textbf{Traffic}: 10\% to start, with gradual ramp
    \item \textbf{Guardrail Metrics}: Customer complaints, application abandonment rate
    \item \textbf{Automatic Stop}: If complaints spike >20\% or abandonment increases
\end{itemize}

\textbf{Governance Workflow:}
\begin{enumerate}
    \item \textbf{Initial Submission}: Automated risk scoring
    \item \textbf{Risk Assessment}: Legal risk = LOW, Financial risk = LOW
    \item \textbf{Stakeholder Review}: Product Manager + Legal (informational only)
    \item \textbf{Approved}: Auto-approved with standard monitoring
    \item \textbf{Launch}: Gradual ramp 10\% → 25\% → 50\% with daily review
\end{enumerate}

\textbf{Results:}
\begin{itemize}
    \item Benefits-focused messaging: +8\% applications, no complaint increase
    \item Rate-focused messaging: +3\% applications, +5\% complaint rate
    \item Rewards-focused messaging: +12\% applications, -2\% complaint rate
    \item \textbf{Deployed}: Rewards-focused messaging to 100\%
    \item \textbf{Impact}: +12\% sign-ups = +\$3.2M annual revenue
    \item \textbf{Risk avoided}: Prevented potential \$5M+ regulatory fine and class-action lawsuit
\end{itemize}

\textbf{Key Lessons:}
\begin{itemize}
    \item Automated risk assessment caught critical legal violation
    \item Governance workflow prevented costly mistake
    \item Stakeholder alignment ensured compliant experiment design
    \item Gradual ramp with guardrails managed residual risk
    \item Documentation provided audit trail for regulators
\end{itemize}

\subsection{Risk Assessment Framework}

\begin{lstlisting}[language=Python, caption={Experiment Risk Assessment System}]
from enum import Enum
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
import re

class RiskLevel(Enum):
    """Risk severity levels."""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class RiskCategory(Enum):
    """Categories of experiment risk."""
    LEGAL = "legal"
    FINANCIAL = "financial"
    REPUTATIONAL = "reputational"
    CUSTOMER_IMPACT = "customer_impact"
    TECHNICAL = "technical"
    REGULATORY = "regulatory"
    PRIVACY = "privacy"
    SECURITY = "security"

@dataclass
class RiskItem:
    """Individual risk identified in experiment."""
    category: RiskCategory
    level: RiskLevel
    description: str
    mitigation: Optional[str] = None
    requires_approval_from: Optional[List[str]] = None

@dataclass
class RiskAssessmentResult:
    """Results from risk assessment."""
    overall_risk_level: RiskLevel
    risks: List[RiskItem]
    requires_special_approval: bool
    approval_required_from: Set[str]
    can_auto_approve: bool
    recommendations: List[str]

class RiskAssessment:
    """
    Automated risk assessment for experiments.

    Evaluates experiments across multiple risk dimensions
    and determines required approval level.

    Example:
        >>> assessor = RiskAssessment()
        >>> result = assessor.assess_experiment(
        ...     experiment_config=config,
        ...     traffic_allocation=0.5,
        ...     affected_users=500000
        ... )
        >>> if result.requires_special_approval:
        ...     print(f"Special approval needed from: {result.approval_required_from}")
    """

    def __init__(self):
        """Initialize risk assessment rules."""
        # Keywords that trigger risk flags
        self.legal_keywords = [
            'price', 'apr', 'rate', 'fee', 'charge', 'cost',
            'contract', 'terms', 'policy', 'disclosure'
        ]
        self.financial_keywords = [
            'payment', 'refund', 'credit', 'debit', 'billing',
            'transaction', 'revenue', 'pricing'
        ]
        self.privacy_keywords = [
            'email', 'phone', 'address', 'ssn', 'personal',
            'data', 'tracking', 'cookie', 'pii'
        ]

        # Risk thresholds
        self.high_traffic_threshold = 0.25  # 25% of users
        self.high_user_count_threshold = 100000
        self.critical_user_count_threshold = 500000

    def assess_experiment(
        self,
        experiment_config: Dict,
        traffic_allocation: float,
        estimated_affected_users: int,
        changes_description: str = "",
        target_segments: Optional[List[str]] = None
    ) -> RiskAssessmentResult:
        """
        Perform comprehensive risk assessment.

        Args:
            experiment_config: Experiment configuration
            traffic_allocation: Proportion of traffic (0-1)
            estimated_affected_users: Number of users affected
            changes_description: Description of changes being tested
            target_segments: User segments affected

        Returns:
            RiskAssessmentResult with risk evaluation
        """
        risks = []

        # Assess traffic and scale risks
        scale_risks = self._assess_scale_risk(
            traffic_allocation,
            estimated_affected_users
        )
        risks.extend(scale_risks)

        # Assess content and feature risks
        content_risks = self._assess_content_risk(
            changes_description,
            experiment_config
        )
        risks.extend(content_risks)

        # Assess segment risks
        segment_risks = self._assess_segment_risk(target_segments)
        risks.extend(segment_risks)

        # Assess metric risks
        metric_risks = self._assess_metric_risk(experiment_config)
        risks.extend(metric_risks)

        # Determine overall risk level
        if any(r.level == RiskLevel.CRITICAL for r in risks):
            overall_risk = RiskLevel.CRITICAL
        elif any(r.level == RiskLevel.HIGH for r in risks):
            overall_risk = RiskLevel.HIGH
        elif any(r.level == RiskLevel.MEDIUM for r in risks):
            overall_risk = RiskLevel.MEDIUM
        else:
            overall_risk = RiskLevel.LOW

        # Determine approval requirements
        approval_required = set()
        for risk in risks:
            if risk.requires_approval_from:
                approval_required.update(risk.requires_approval_from)

        requires_special_approval = (
            overall_risk in [RiskLevel.HIGH, RiskLevel.CRITICAL] or
            len(approval_required) > 0
        )

        can_auto_approve = (
            overall_risk == RiskLevel.LOW and
            traffic_allocation <= 0.1 and
            estimated_affected_users < self.high_user_count_threshold
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(
            risks,
            traffic_allocation,
            estimated_affected_users
        )

        return RiskAssessmentResult(
            overall_risk_level=overall_risk,
            risks=risks,
            requires_special_approval=requires_special_approval,
            approval_required_from=approval_required,
            can_auto_approve=can_auto_approve,
            recommendations=recommendations
        )

    def _assess_scale_risk(
        self,
        traffic: float,
        users: int
    ) -> List[RiskItem]:
        """Assess risks based on experiment scale."""
        risks = []

        if users >= self.critical_user_count_threshold:
            risks.append(RiskItem(
                category=RiskCategory.CUSTOMER_IMPACT,
                level=RiskLevel.CRITICAL if traffic > 0.5 else RiskLevel.HIGH,
                description=f"Very large user impact: {users:,} users affected",
                mitigation="Consider gradual ramp: 5% → 10% → 25% → 50%",
                requires_approval_from=['VP_Product', 'Chief_Risk_Officer']
            ))
        elif users >= self.high_user_count_threshold:
            risks.append(RiskItem(
                category=RiskCategory.CUSTOMER_IMPACT,
                level=RiskLevel.MEDIUM,
                description=f"Significant user impact: {users:,} users affected",
                mitigation="Implement automated stop criteria",
                requires_approval_from=['Director_Product']
            ))

        if traffic > 0.5:
            risks.append(RiskItem(
                category=RiskCategory.TECHNICAL,
                level=RiskLevel.HIGH,
                description=f"High traffic allocation: {traffic:.0%}",
                mitigation="Start with lower traffic and ramp gradually",
                requires_approval_from=['Engineering_Lead']
            ))

        return risks

    def _assess_content_risk(
        self,
        description: str,
        config: Dict
    ) -> List[RiskItem]:
        """Assess risks based on experiment content."""
        risks = []
        description_lower = description.lower()

        # Legal risk
        legal_matches = [kw for kw in self.legal_keywords if kw in description_lower]
        if legal_matches:
            risks.append(RiskItem(
                category=RiskCategory.LEGAL,
                level=RiskLevel.CRITICAL,
                description=f"Legal-sensitive changes detected: {', '.join(legal_matches)}",
                mitigation="Require legal review before launch",
                requires_approval_from=['Legal', 'Compliance']
            ))

        # Financial risk
        financial_matches = [kw for kw in self.financial_keywords if kw in description_lower]
        if financial_matches:
            risks.append(RiskItem(
                category=RiskCategory.FINANCIAL,
                level=RiskLevel.HIGH,
                description=f"Financial impact changes: {', '.join(financial_matches)}",
                mitigation="Include revenue guardrails and finance team review",
                requires_approval_from=['Finance', 'Product']
            ))

        # Privacy risk
        privacy_matches = [kw for kw in self.privacy_keywords if kw in description_lower]
        if privacy_matches:
            risks.append(RiskItem(
                category=RiskCategory.PRIVACY,
                level=RiskLevel.HIGH,
                description=f"Privacy-related changes: {', '.join(privacy_matches)}",
                mitigation="GDPR/CCPA compliance review required",
                requires_approval_from=['Privacy_Officer', 'Legal']
            ))

        return risks

    def _assess_segment_risk(
        self,
        segments: Optional[List[str]]
    ) -> List[RiskItem]:
        """Assess risks based on affected user segments."""
        risks = []

        if segments is None:
            return risks

        # High-risk segments
        high_risk_segments = ['vip', 'enterprise', 'premium', 'at_risk']
        vulnerable_segments = ['minor', 'elderly', 'low_income']

        for segment in segments:
            segment_lower = segment.lower()

            if any(hrs in segment_lower for hrs in high_risk_segments):
                risks.append(RiskItem(
                    category=RiskCategory.REPUTATIONAL,
                    level=RiskLevel.HIGH,
                    description=f"High-value segment affected: {segment}",
                    mitigation="Extra monitoring and quick rollback capability",
                    requires_approval_from=['VP_Product']
                ))

            if any(vs in segment_lower for vs in vulnerable_segments):
                risks.append(RiskItem(
                    category=RiskCategory.REGULATORY,
                    level=RiskLevel.CRITICAL,
                    description=f"Vulnerable population: {segment}",
                    mitigation="Ethics review and enhanced protections required",
                    requires_approval_from=['Ethics_Committee', 'Legal', 'Chief_Risk_Officer']
                ))

        return risks

    def _assess_metric_risk(
        self,
        config: Dict
    ) -> List[RiskItem]:
        """Assess risks based on metrics being tracked."""
        risks = []

        metrics = config.get('metrics', [])

        # Check for critical business metrics
        critical_metrics = ['revenue', 'churn', 'retention', 'lifetime_value']
        affected_critical = [m for m in metrics if any(cm in m.lower() for cm in critical_metrics)]

        if affected_critical:
            risks.append(RiskItem(
                category=RiskCategory.FINANCIAL,
                level=RiskLevel.MEDIUM,
                description=f"Critical business metrics: {', '.join(affected_critical)}",
                mitigation="Set guardrail thresholds for automatic stop",
                requires_approval_from=None  # Medium risk, no special approval
            ))

        return risks

    def _generate_recommendations(
        self,
        risks: List[RiskItem],
        traffic: float,
        users: int
    ) -> List[str]:
        """Generate actionable recommendations based on risks."""
        recommendations = []

        # High user count recommendations
        if users > self.critical_user_count_threshold:
            recommendations.append(
                "Implement gradual traffic ramp: 5% → 10% → 25% → 50% with 24hr holds"
            )
            recommendations.append(
                "Set up real-time alerting for all guardrail metrics"
            )

        # Legal/compliance recommendations
        if any(r.category == RiskCategory.LEGAL for r in risks):
            recommendations.append(
                "Obtain written legal approval before proceeding"
            )
            recommendations.append(
                "Document compliance checks in experiment approval"
            )

        # Privacy recommendations
        if any(r.category == RiskCategory.PRIVACY for r in risks):
            recommendations.append(
                "Complete privacy impact assessment (PIA)"
            )
            recommendations.append(
                "Verify consent mechanisms and data retention policies"
            )

        # High traffic recommendations
        if traffic > 0.25:
            recommendations.append(
                "Start with 10% traffic maximum, increase after 24-48 hours"
            )
            recommendations.append(
                "Enable automatic rollback on anomaly detection"
            )

        # General best practices
        if len(risks) > 0:
            recommendations.append(
                "Schedule pre-launch review meeting with stakeholders"
            )
            recommendations.append(
                "Prepare rollback plan and test execution"
            )

        return recommendations
\end{lstlisting}

\subsection{Experiment Governance Framework}

\begin{lstlisting}[language=Python, caption={Experiment Governance and Approval Workflows}]
from enum import Enum
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
import smtplib
from email.mime.text import MIMEText

class ApprovalStatus(Enum):
    """Experiment approval status."""
    DRAFT = "draft"
    PENDING_REVIEW = "pending_review"
    APPROVED = "approved"
    REJECTED = "rejected"
    CONDITIONAL_APPROVAL = "conditional_approval"
    LAUNCHED = "launched"
    STOPPED = "stopped"
    COMPLETED = "completed"

class StakeholderRole(Enum):
    """Stakeholder roles in approval process."""
    EXPERIMENTER = "experimenter"
    PRODUCT_MANAGER = "product_manager"
    ENGINEERING_LEAD = "engineering_lead"
    DATA_SCIENCE_LEAD = "data_science_lead"
    LEGAL = "legal"
    COMPLIANCE = "compliance"
    FINANCE = "finance"
    PRIVACY_OFFICER = "privacy_officer"
    VP_PRODUCT = "vp_product"
    CHIEF_RISK_OFFICER = "chief_risk_officer"
    ETHICS_COMMITTEE = "ethics_committee"

@dataclass
class ApprovalRecord:
    """Record of approval decision."""
    approver_role: str
    approver_name: str
    decision: str  # 'approved', 'rejected', 'conditional'
    timestamp: str
    comments: str
    conditions: Optional[List[str]] = None

@dataclass
class ExperimentProposal:
    """Experiment proposal for approval."""
    experiment_id: str
    name: str
    description: str
    hypothesis: str
    changes_description: str
    metrics: List[str]
    success_criteria: Dict[str, float]
    guardrail_criteria: Dict[str, float]
    traffic_allocation: float
    estimated_users: int
    duration_days: int
    target_segments: Optional[List[str]]
    risk_assessment: RiskAssessmentResult
    submitter: str
    submitted_at: str
    status: ApprovalStatus

class ExperimentGovernance:
    """
    Governance framework for experiment approval and oversight.

    Manages approval workflows, stakeholder review, risk assessment,
    and compliance verification.

    Example:
        >>> governance = ExperimentGovernance()
        >>>
        >>> # Submit experiment for approval
        >>> proposal = governance.submit_proposal(
        ...     name="Checkout Redesign",
        ...     description="Test new checkout flow",
        ...     changes_description="Simplify checkout to 2 steps",
        ...     traffic_allocation=0.1,
        ...     estimated_users=50000
        ... )
        >>>
        >>> # Check approval status
        >>> if proposal.status == ApprovalStatus.APPROVED:
        ...     governance.launch_experiment(proposal.experiment_id)
    """

    def __init__(
        self,
        auto_approve_threshold: RiskLevel = RiskLevel.LOW,
        notification_config: Optional[Dict] = None
    ):
        """
        Initialize governance framework.

        Args:
            auto_approve_threshold: Maximum risk for auto-approval
            notification_config: Email/Slack configuration
        """
        self.auto_approve_threshold = auto_approve_threshold
        self.notification_config = notification_config or {}

        self.proposals = {}
        self.approval_records = {}
        self.risk_assessor = RiskAssessment()

    def submit_proposal(
        self,
        name: str,
        description: str,
        hypothesis: str,
        changes_description: str,
        metrics: List[str],
        success_criteria: Dict[str, float],
        guardrail_criteria: Dict[str, float],
        traffic_allocation: float,
        estimated_users: int,
        duration_days: int,
        target_segments: Optional[List[str]] = None,
        submitter: str = "unknown"
    ) -> ExperimentProposal:
        """
        Submit experiment proposal for approval.

        Args:
            name: Experiment name
            description: Detailed description
            hypothesis: Hypothesis being tested
            changes_description: What is being changed
            metrics: Metrics to track
            success_criteria: Thresholds for success
            guardrail_criteria: Safety thresholds
            traffic_allocation: Traffic percentage (0-1)
            estimated_users: Estimated affected users
            duration_days: Planned duration
            target_segments: Affected user segments
            submitter: Person submitting

        Returns:
            ExperimentProposal with initial status
        """
        # Generate unique ID
        import hashlib
        experiment_id = hashlib.sha256(
            f"{name}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]

        # Perform risk assessment
        risk_result = self.risk_assessor.assess_experiment(
            experiment_config={'metrics': metrics},
            traffic_allocation=traffic_allocation,
            estimated_affected_users=estimated_users,
            changes_description=changes_description,
            target_segments=target_segments
        )

        # Create proposal
        proposal = ExperimentProposal(
            experiment_id=experiment_id,
            name=name,
            description=description,
            hypothesis=hypothesis,
            changes_description=changes_description,
            metrics=metrics,
            success_criteria=success_criteria,
            guardrail_criteria=guardrail_criteria,
            traffic_allocation=traffic_allocation,
            estimated_users=estimated_users,
            duration_days=duration_days,
            target_segments=target_segments,
            risk_assessment=risk_result,
            submitter=submitter,
            submitted_at=datetime.now().isoformat(),
            status=ApprovalStatus.DRAFT
        )

        # Determine initial status
        if risk_result.can_auto_approve:
            proposal.status = ApprovalStatus.APPROVED
            self._record_approval(
                experiment_id,
                'SYSTEM',
                'System Auto-Approval',
                'approved',
                'Low risk experiment auto-approved'
            )
        elif risk_result.requires_special_approval:
            proposal.status = ApprovalStatus.PENDING_REVIEW
            # Notify required approvers
            self._notify_stakeholders(
                proposal,
                list(risk_result.approval_required_from)
            )
        else:
            proposal.status = ApprovalStatus.PENDING_REVIEW
            # Standard review required
            self._notify_stakeholders(
                proposal,
                ['PRODUCT_MANAGER', 'DATA_SCIENCE_LEAD']
            )

        self.proposals[experiment_id] = proposal
        return proposal

    def approve_experiment(
        self,
        experiment_id: str,
        approver_role: str,
        approver_name: str,
        comments: str = "",
        conditions: Optional[List[str]] = None
    ) -> ExperimentProposal:
        """
        Approve experiment proposal.

        Args:
            experiment_id: Experiment ID
            approver_role: Role of approver
            approver_name: Name of approver
            comments: Approval comments
            conditions: Conditions for approval

        Returns:
            Updated proposal
        """
        if experiment_id not in self.proposals:
            raise ValueError(f"Experiment {experiment_id} not found")

        proposal = self.proposals[experiment_id]

        # Record approval
        decision = 'conditional' if conditions else 'approved'
        self._record_approval(
            experiment_id,
            approver_role,
            approver_name,
            decision,
            comments,
            conditions
        )

        # Check if all required approvals obtained
        required_approvers = proposal.risk_assessment.approval_required_from
        if required_approvers:
            approvals = self.approval_records.get(experiment_id, [])
            approved_roles = {a.approver_role for a in approvals if a.decision in ['approved', 'conditional']}

            if required_approvers.issubset(approved_roles):
                if conditions:
                    proposal.status = ApprovalStatus.CONDITIONAL_APPROVAL
                else:
                    proposal.status = ApprovalStatus.APPROVED
        else:
            # No special approvals required, single approval sufficient
            if conditions:
                proposal.status = ApprovalStatus.CONDITIONAL_APPROVAL
            else:
                proposal.status = ApprovalStatus.APPROVED

        return proposal

    def reject_experiment(
        self,
        experiment_id: str,
        rejector_role: str,
        rejector_name: str,
        reason: str
    ) -> ExperimentProposal:
        """
        Reject experiment proposal.

        Args:
            experiment_id: Experiment ID
            rejector_role: Role of rejector
            rejector_name: Name of rejector
            reason: Rejection reason

        Returns:
            Updated proposal
        """
        if experiment_id not in self.proposals:
            raise ValueError(f"Experiment {experiment_id} not found")

        proposal = self.proposals[experiment_id]

        # Record rejection
        self._record_approval(
            experiment_id,
            rejector_role,
            rejector_name,
            'rejected',
            reason
        )

        proposal.status = ApprovalStatus.REJECTED

        # Notify submitter
        self._notify_rejection(proposal, rejector_name, reason)

        return proposal

    def launch_experiment(
        self,
        experiment_id: str,
        launcher: str
    ) -> ExperimentProposal:
        """
        Launch approved experiment.

        Args:
            experiment_id: Experiment ID
            launcher: Person launching

        Returns:
            Updated proposal
        """
        if experiment_id not in self.proposals:
            raise ValueError(f"Experiment {experiment_id} not found")

        proposal = self.proposals[experiment_id]

        if proposal.status not in [ApprovalStatus.APPROVED, ApprovalStatus.CONDITIONAL_APPROVAL]:
            raise ValueError(f"Experiment not approved. Status: {proposal.status}")

        proposal.status = ApprovalStatus.LAUNCHED

        # Log launch
        self._record_approval(
            experiment_id,
            'LAUNCHER',
            launcher,
            'launched',
            f'Experiment launched at {datetime.now().isoformat()}'
        )

        return proposal

    def stop_experiment(
        self,
        experiment_id: str,
        stopper: str,
        reason: str
    ) -> ExperimentProposal:
        """
        Emergency stop of running experiment.

        Args:
            experiment_id: Experiment ID
            stopper: Person stopping
            reason: Stop reason

        Returns:
            Updated proposal
        """
        if experiment_id not in self.proposals:
            raise ValueError(f"Experiment {experiment_id} not found")

        proposal = self.proposals[experiment_id]
        proposal.status = ApprovalStatus.STOPPED

        # Log stop
        self._record_approval(
            experiment_id,
            'STOPPER',
            stopper,
            'stopped',
            f'Emergency stop: {reason}'
        )

        # Alert stakeholders
        self._notify_emergency_stop(proposal, stopper, reason)

        return proposal

    def get_approval_history(
        self,
        experiment_id: str
    ) -> List[ApprovalRecord]:
        """Get approval history for experiment."""
        return self.approval_records.get(experiment_id, [])

    def _record_approval(
        self,
        experiment_id: str,
        role: str,
        name: str,
        decision: str,
        comments: str,
        conditions: Optional[List[str]] = None
    ) -> None:
        """Record approval decision."""
        if experiment_id not in self.approval_records:
            self.approval_records[experiment_id] = []

        record = ApprovalRecord(
            approver_role=role,
            approver_name=name,
            decision=decision,
            timestamp=datetime.now().isoformat(),
            comments=comments,
            conditions=conditions
        )

        self.approval_records[experiment_id].append(record)

    def _notify_stakeholders(
        self,
        proposal: ExperimentProposal,
        stakeholder_roles: List[str]
    ) -> None:
        """Notify stakeholders of pending approval."""
        message = f"""
Experiment Approval Required
{'=' * 50}

Experiment: {proposal.name}
ID: {proposal.experiment_id}
Submitter: {proposal.submitter}

Description: {proposal.description}

Risk Level: {proposal.risk_assessment.overall_risk_level.name}
Affected Users: {proposal.estimated_users:,}
Traffic: {proposal.traffic_allocation:.1%}

Risks Identified:
{self._format_risks(proposal.risk_assessment.risks)}

Required Approvers: {', '.join(stakeholder_roles)}

Please review and approve/reject in the experiment governance portal.
"""

        print(f"\n[NOTIFICATION] Sent to: {', '.join(stakeholder_roles)}")
        print(message)

        # In production, send actual emails/Slack messages
        # self._send_email(stakeholder_roles, message)

    def _notify_rejection(
        self,
        proposal: ExperimentProposal,
        rejector: str,
        reason: str
    ) -> None:
        """Notify submitter of rejection."""
        message = f"""
Experiment Rejected
{'=' * 50}

Your experiment "{proposal.name}" has been rejected.

Rejector: {rejector}
Reason: {reason}

Please address the concerns and resubmit.
"""

        print(f"\n[NOTIFICATION] Sent to: {proposal.submitter}")
        print(message)

    def _notify_emergency_stop(
        self,
        proposal: ExperimentProposal,
        stopper: str,
        reason: str
    ) -> None:
        """Notify stakeholders of emergency stop."""
        message = f"""
URGENT: Experiment Emergency Stop
{'=' * 50}

Experiment: {proposal.name}
ID: {proposal.experiment_id}

Stopped by: {stopper}
Reason: {reason}
Time: {datetime.now().isoformat()}

Immediate action may be required.
"""

        print(f"\n[URGENT NOTIFICATION] Broadcast to all stakeholders")
        print(message)

    def _format_risks(self, risks: List[RiskItem]) -> str:
        """Format risks for display."""
        if not risks:
            return "  No significant risks identified"

        formatted = []
        for risk in risks:
            formatted.append(
                f"  - [{risk.level.name}] {risk.category.value}: {risk.description}"
            )
        return "\n".join(formatted)

    def generate_governance_report(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> Dict:
        """
        Generate governance metrics report.

        Args:
            start_date: Start date for report
            end_date: End date for report

        Returns:
            Dictionary with governance metrics
        """
        # Filter proposals by date if specified
        proposals_list = list(self.proposals.values())

        if start_date:
            proposals_list = [
                p for p in proposals_list
                if p.submitted_at >= start_date
            ]
        if end_date:
            proposals_list = [
                p for p in proposals_list
                if p.submitted_at <= end_date
            ]

        # Compute metrics
        total = len(proposals_list)
        by_status = {}
        by_risk_level = {}
        avg_time_to_approval = []

        for proposal in proposals_list:
            # Count by status
            status = proposal.status.value
            by_status[status] = by_status.get(status, 0) + 1

            # Count by risk level
            risk = proposal.risk_assessment.overall_risk_level.name
            by_risk_level[risk] = by_risk_level.get(risk, 0) + 1

            # Compute time to approval
            if proposal.status == ApprovalStatus.APPROVED:
                approvals = self.get_approval_history(proposal.experiment_id)
                if approvals:
                    submitted = datetime.fromisoformat(proposal.submitted_at)
                    approved = datetime.fromisoformat(approvals[-1].timestamp)
                    hours = (approved - submitted).total_seconds() / 3600
                    avg_time_to_approval.append(hours)

        report = {
            'total_proposals': total,
            'by_status': by_status,
            'by_risk_level': by_risk_level,
            'avg_time_to_approval_hours': np.mean(avg_time_to_approval) if avg_time_to_approval else 0,
            'auto_approval_rate': by_status.get('approved', 0) / total if total > 0 else 0,
            'rejection_rate': by_status.get('rejected', 0) / total if total > 0 else 0
        }

        return report
\end{lstlisting}

\subsection{Automated Results Reporting}

\begin{lstlisting}[language=Python, caption={Automated Experiment Results Interpretation and Reporting}]
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class BusinessRecommendation:
    """Business recommendation from experiment results."""
    decision: str  # 'deploy', 'do_not_deploy', 'continue_testing', 'iterate'
    confidence: str  # 'high', 'medium', 'low'
    rationale: str
    estimated_impact: Optional[Dict[str, float]]
    risks: List[str]
    next_steps: List[str]

class ResultsReporter:
    """
    Automated experiment results interpretation and reporting.

    Generates human-readable reports with statistical interpretation,
    business recommendations, and stakeholder-appropriate summaries.

    Example:
        >>> reporter = ResultsReporter()
        >>> report = reporter.generate_report(
        ...     experiment_results=results,
        ...     business_context={'baseline_revenue': 1000000}
        ... )
        >>> print(report.executive_summary)
    """

    def __init__(self):
        """Initialize results reporter."""
        pass

    def generate_report(
        self,
        experiment_results: Dict,
        business_context: Optional[Dict] = None,
        audience: str = 'technical'  # 'executive', 'technical', 'stakeholder'
    ) -> Dict[str, Any]:
        """
        Generate comprehensive experiment report.

        Args:
            experiment_results: Results from platform.analyze_experiment()
            business_context: Business metrics and context
            audience: Target audience for report

        Returns:
            Dictionary with formatted report sections
        """
        # Interpret results
        interpretation = self._interpret_results(experiment_results)

        # Generate recommendation
        recommendation = self._generate_recommendation(
            experiment_results,
            business_context
        )

        # Format for audience
        if audience == 'executive':
            summary = self._executive_summary(
                experiment_results,
                interpretation,
                recommendation
            )
        elif audience == 'technical':
            summary = self._technical_summary(
                experiment_results,
                interpretation,
                recommendation
            )
        else:  # stakeholder
            summary = self._stakeholder_summary(
                experiment_results,
                interpretation,
                recommendation
            )

        return {
            'summary': summary,
            'interpretation': interpretation,
            'recommendation': recommendation,
            'detailed_results': experiment_results,
            'generated_at': datetime.now().isoformat()
        }

    def _interpret_results(
        self,
        results: Dict
    ) -> Dict[str, str]:
        """Interpret statistical results in plain language."""
        interpretation = {}

        for metric, arms_results in results['metrics'].items():
            for arm, result in arms_results.items():
                p_value = result['p_value']
                effect_size = result.get('effect_size', {})

                # Interpret significance
                if p_value < 0.001:
                    sig_interpretation = "highly statistically significant"
                elif p_value < 0.01:
                    sig_interpretation = "statistically significant"
                elif p_value < 0.05:
                    sig_interpretation = "statistically significant (marginal)"
                elif p_value < 0.10:
                    sig_interpretation = "trending toward significance"
                else:
                    sig_interpretation = "not statistically significant"

                # Interpret effect size
                if 'cohens_d' in effect_size:
                    d = abs(effect_size['cohens_d'])
                    if d < 0.2:
                        effect_interpretation = "negligible practical effect"
                    elif d < 0.5:
                        effect_interpretation = "small practical effect"
                    elif d < 0.8:
                        effect_interpretation = "medium practical effect"
                    else:
                        effect_interpretation = "large practical effect"
                elif 'relative_lift' in effect_size:
                    lift = effect_size['relative_lift']
                    if abs(lift) < 0.01:
                        effect_interpretation = "negligible practical impact (<1%)"
                    elif abs(lift) < 0.05:
                        effect_interpretation = "small practical impact (1-5%)"
                    elif abs(lift) < 0.10:
                        effect_interpretation = "moderate practical impact (5-10%)"
                    else:
                        effect_interpretation = "large practical impact (>10%)"
                else:
                    effect_interpretation = "effect size not available"

                # Combined interpretation
                direction = "increase" if result.get('mean_difference', 0) > 0 else "decrease"

                interpretation[f"{metric}_{arm}"] = (
                    f"{sig_interpretation}, {effect_interpretation}, "
                    f"showing {direction} in {metric}"
                )

        return interpretation

    def _generate_recommendation(
        self,
        results: Dict,
        business_context: Optional[Dict]
    ) -> BusinessRecommendation:
        """Generate business recommendation."""
        # Analyze results
        significant_wins = 0
        significant_losses = 0
        metric_impacts = {}

        for metric, arms_results in results['metrics'].items():
            for arm, result in arms_results.items():
                if result['significant']:
                    diff = result.get('mean_difference') or result.get('rate_difference', 0)
                    if diff > 0:
                        significant_wins += 1
                        metric_impacts[metric] = 'positive'
                    else:
                        significant_losses += 1
                        metric_impacts[metric] = 'negative'

        # Decision logic
        if significant_wins > 0 and significant_losses == 0:
            decision = 'deploy'
            confidence = 'high'
            rationale = f"Clear positive impact on {significant_wins} metric(s) with no negative effects"

        elif significant_wins == 0 and significant_losses > 0:
            decision = 'do_not_deploy'
            confidence = 'high'
            rationale = f"Significant negative impact on {significant_losses} metric(s)"

        elif significant_wins > significant_losses:
            decision = 'deploy'
            confidence = 'medium'
            rationale = f"Mixed results: {significant_wins} positive vs {significant_losses} negative. Positive outweighs negative."

        elif significant_losses > significant_wins:
            decision = 'do_not_deploy'
            confidence = 'medium'
            rationale = f"Mixed results: {significant_losses} negative vs {significant_wins} positive. Negative effects concerning."

        else:  # No significant results
            decision = 'continue_testing'
            confidence = 'low'
            rationale = "No statistically significant effects detected. Increase sample size or iterate on variant."

        # Estimate business impact
        estimated_impact = None
        if business_context:
            estimated_impact = self._estimate_business_impact(
                results,
                business_context
            )

        # Identify risks
        risks = []
        if significant_losses > 0:
            risks.append("Potential negative impact on key metrics")
        if confidence == 'low':
            risks.append("Inconclusive results - may not replicate at scale")
        if len(metric_impacts) > 3:
            risks.append("Multiple metrics affected - complex interdependencies")

        # Next steps
        next_steps = []
        if decision == 'deploy':
            next_steps.extend([
                "Prepare deployment plan with gradual ramp",
                "Set up post-launch monitoring dashboard",
                "Document results and learnings"
            ])
        elif decision == 'do_not_deploy':
            next_steps.extend([
                "Analyze why variant underperformed",
                "Gather qualitative feedback from users",
                "Iterate on design and retest"
            ])
        else:  # continue_testing
            next_steps.extend([
                "Run power analysis to determine required sample size",
                "Extend test duration or increase traffic allocation",
                "Consider alternative variants"
            ])

        return BusinessRecommendation(
            decision=decision,
            confidence=confidence,
            rationale=rationale,
            estimated_impact=estimated_impact,
            risks=risks,
            next_steps=next_steps
        )

    def _estimate_business_impact(
        self,
        results: Dict,
        business_context: Dict
    ) -> Dict[str, float]:
        """Estimate business impact in dollars/users."""
        impact = {}

        # Example: revenue impact
        if 'baseline_revenue' in business_context:
            baseline_revenue = business_context['baseline_revenue']

            for metric, arms_results in results['metrics'].items():
                if 'revenue' in metric.lower():
                    for arm, result in arms_results.items():
                        lift = result.get('effect_size', {}).get('relative_lift', 0)
                        if lift:
                            annual_impact = baseline_revenue * lift
                            impact[f'{metric}_annual_impact'] = annual_impact

        return impact

    def _executive_summary(
        self,
        results: Dict,
        interpretation: Dict,
        recommendation: BusinessRecommendation
    ) -> str:
        """Generate executive summary (non-technical)."""
        summary = f"""
EXPERIMENT RESULTS SUMMARY
{'=' * 60}

RECOMMENDATION: {recommendation.decision.upper().replace('_', ' ')}
Confidence: {recommendation.confidence.upper()}

{recommendation.rationale}

KEY FINDINGS:
"""

        # Add top findings
        for metric_arm, interp in list(interpretation.items())[:3]:
            metric = metric_arm.rsplit('_', 1)[0]
            summary += f"  • {metric}: {interp}\n"

        if recommendation.estimated_impact:
            summary += "\nESTIMATED BUSINESS IMPACT:\n"
            for impact_type, value in recommendation.estimated_impact.items():
                summary += f"  • {impact_type}: ${value:,.0f}\n"

        summary += f"\nNEXT STEPS:\n"
        for i, step in enumerate(recommendation.next_steps, 1):
            summary += f"  {i}. {step}\n"

        if recommendation.risks:
            summary += f"\nRISKS TO CONSIDER:\n"
            for risk in recommendation.risks:
                summary += f"  ⚠ {risk}\n"

        return summary

    def _technical_summary(
        self,
        results: Dict,
        interpretation: Dict,
        recommendation: BusinessRecommendation
    ) -> str:
        """Generate technical summary with statistical details."""
        summary = f"""
EXPERIMENT ANALYSIS REPORT
{'=' * 60}

Experiment: {results['experiment_name']}
Analyzed: {results['analyzed_at']}
Method: {results['metrics'][list(results['metrics'].keys())[0]][list(results['metrics'][list(results['metrics'].keys())[0]].keys())[0]]['method']}

STATISTICAL RESULTS:
"""

        for metric, arms_results in results['metrics'].items():
            summary += f"\n{metric.upper()}:\n"
            for arm, result in arms_results.items():
                summary += f"  {arm}:\n"
                summary += f"    p-value: {result['p_value']:.4f}\n"
                summary += f"    Significant: {'Yes' if result['significant'] else 'No'}\n"

                if 'mean_difference' in result:
                    summary += f"    Mean difference: {result['mean_difference']:.4f}\n"
                    summary += f"    95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\n"

                if 'effect_size' in result:
                    for es_type, es_value in result['effect_size'].items():
                        summary += f"    {es_type}: {es_value:.4f}\n"

        summary += f"\nRECOMMENDATION: {recommendation.decision}\n"
        summary += f"Rationale: {recommendation.rationale}\n"

        return summary

    def _stakeholder_summary(
        self,
        results: Dict,
        interpretation: Dict,
        recommendation: BusinessRecommendation
    ) -> str:
        """Generate stakeholder summary (balanced technical/business)."""
        summary = f"""
EXPERIMENT RESULTS: {results['experiment_name']}
{'=' * 60}

BOTTOM LINE:
{recommendation.decision.upper().replace('_', ' ')} (Confidence: {recommendation.confidence})

WHY:
{recommendation.rationale}

DETAILED FINDINGS:
"""

        for metric_arm, interp in interpretation.items():
            summary += f"  • {interp}\n"

        if recommendation.estimated_impact:
            summary += f"\nBUSINESS IMPACT:\n"
            for impact_type, value in recommendation.estimated_impact.items():
                summary += f"  • {impact_type}: ${value:,.0f}\n"

        summary += f"\nWHAT'S NEXT:\n"
        for step in recommendation.next_steps:
            summary += f"  → {step}\n"

        return summary
\end{lstlisting}

\subsection{Governance Integration Example}

\begin{lstlisting}[language=Python, caption={End-to-End Governance Workflow}]
# ============================================================
# Complete workflow: Proposal → Approval → Launch → Report
# ============================================================

# Step 1: Initialize governance system
governance = ExperimentGovernance()
reporter = ResultsReporter()

# Step 2: Submit experiment proposal
proposal = governance.submit_proposal(
    name="Credit Card APR Messaging Test",
    description="Test different messaging approaches for credit card offers",
    hypothesis="Benefits-focused messaging will increase applications without increasing complaints",
    changes_description="Test three messaging variants: rate-focused, benefits-focused, rewards-focused. All display accurate 24.9% APR.",
    metrics=['applications', 'complaints', 'abandonment_rate'],
    success_criteria={
        'applications': 0.05,  # 5% increase
        'complaints': 0.0,  # No increase
    },
    guardrail_criteria={
        'abandonment_rate': 0.20,  # Max 20% increase
        'complaints': 0.20  # Max 20% increase
    },
    traffic_allocation=0.1,
    estimated_users=50000,
    duration_days=14,
    target_segments=None,
    submitter="growth_team@company.com"
)

print(f"Proposal Status: {proposal.status}")
print(f"Risk Level: {proposal.risk_assessment.overall_risk_level}")

# Step 3: Review and approve (if needed)
if proposal.status == ApprovalStatus.PENDING_REVIEW:
    # Legal review
    governance.approve_experiment(
        experiment_id=proposal.experiment_id,
        approver_role="Legal",
        approver_name="Jane Legal",
        comments="Reviewed messaging - all variants display accurate APR. Approved."
    )

    # Product review
    governance.approve_experiment(
        experiment_id=proposal.experiment_id,
        approver_role="Product_Manager",
        approver_name="John PM",
        comments="Aligned with growth strategy. Approved with guardrails.",
        conditions=["Monitor complaint rate daily", "Stop if >20% increase"]
    )

# Step 4: Launch experiment
if proposal.status in [ApprovalStatus.APPROVED, ApprovalStatus.CONDITIONAL_APPROVAL]:
    proposal = governance.launch_experiment(
        experiment_id=proposal.experiment_id,
        launcher="data_science_team@company.com"
    )
    print(f"Experiment launched: {proposal.experiment_id}")

# Step 5: Run experiment and collect data
# ... (experiment runs via ExperimentPlatform) ...

# Step 6: Analyze results
platform = ExperimentPlatform()
results = platform.analyze_experiment(
    experiment_id=proposal.experiment_id,
    data=experiment_data,
    method='auto'
)

# Step 7: Generate automated report
report = reporter.generate_report(
    experiment_results=results,
    business_context={'baseline_revenue': 50000000},  # $50M annual
    audience='executive'
)

print("\n" + "="*60)
print("EXECUTIVE REPORT")
print("="*60)
print(report['summary'])

# Step 8: Make decision
decision = platform.make_decision(results)

if decision['overall_decision'] == 'DEPLOY':
    print("\n✅ Deploying variant based on results")
    # Deploy via feature flag system
elif decision['overall_decision'] == 'DO_NOT_DEPLOY':
    print("\n❌ Not deploying - results do not support launch")
    # Keep current experience
else:
    print("\n⏸ Inconclusive - continuing test")
    # Extend test duration

# Step 9: Generate governance report
gov_report = governance.generate_governance_report(
    start_date=(datetime.now() - timedelta(days=30)).isoformat()
)

print("\n" + "="*60)
print("GOVERNANCE METRICS (Last 30 Days)")
print("="*60)
print(f"Total Proposals: {gov_report['total_proposals']}")
print(f"Auto-approval Rate: {gov_report['auto_approval_rate']:.1%}")
print(f"Avg Time to Approval: {gov_report['avg_time_to_approval_hours']:.1f} hours")
print(f"By Risk Level: {gov_report['by_risk_level']}")
\end{lstlisting}

This governance framework provides:

\begin{itemize}
    \item \textbf{Automated Risk Assessment}: Identifies legal, financial, privacy, and reputational risks
    \item \textbf{Intelligent Routing}: Auto-approves low-risk, escalates high-risk to appropriate stakeholders
    \item \textbf{Stakeholder Management}: Notifies required approvers, tracks approval chain
    \item \textbf{Audit Trail}: Complete history of approvals, rejections, and decisions
    \item \textbf{Automated Reporting}: Generates audience-appropriate summaries (executive, technical, stakeholder)
    \item \textbf{Business Recommendations}: Interprets statistical results into actionable business decisions
    \item \textbf{Governance Metrics}: Tracks approval rates, times, and risk distributions
\end{itemize}

Key benefits for organizations:
\begin{enumerate}
    \item \textbf{Risk Mitigation}: Prevents costly legal/regulatory violations
    \item \textbf{Efficiency}: Auto-approves 60-80\% of low-risk experiments
    \item \textbf{Compliance}: Documents review process for auditors/regulators
    \item \textbf{Alignment}: Ensures stakeholder buy-in before launch
    \item \textbf{Quality}: Enforces best practices and guardrails
    \item \textbf{Speed}: Streamlines approval from days to hours for standard experiments
\end{enumerate}

\section{Causal Inference from Observational Data}

While randomized experiments provide gold-standard causal estimates, many critical business and policy questions cannot be addressed through A/B testing—either due to ethical constraints, infeasibility, or the need to evaluate past interventions. Advanced causal inference methods enable rigorous causal estimation from observational data when randomization is impossible.

\subsection{Real-World Scenario: The Natural Experiment}

\textbf{The Challenge:}

A major e-commerce platform wanted to measure the causal impact of their recommendation algorithm on user purchases. However, running a traditional A/B test was problematic:

\begin{itemize}
    \item \textbf{Ethical concern}: Withholding recommendations from 50\% of users seemed unfair
    \item \textbf{Business risk}: Management refused to "turn off" recommendations for millions of users
    \item \textbf{Network effects}: Users influence each other, violating SUTVA
    \item \textbf{Historical question}: "What was the impact over the \textit{past} 2 years?"
\end{itemize}

\textbf{The Natural Experiment:}

In March 2022, a software bug accidentally disabled recommendations for users in **Germany only** for 3 weeks before being discovered. The data science team recognized this as a natural experiment—an unplanned event that created quasi-random variation.

\textbf{Why This Works as Causal Identification:}

\begin{enumerate}
    \item \textbf{As-if Random}: Bug affected Germany arbitrarily, not based on user behavior
    \item \textbf{Control Group}: Other European countries (France, Italy, Spain, UK) unaffected
    \item \textbf{Parallel Trends}: Pre-bug, Germany tracked other countries closely
    \item \textbf{Sudden Intervention}: Clear before/after distinction
    \item \textbf{No Spillover}: Country-level isolation minimizes network effects
\end{enumerate}

\textbf{Analysis Approach: Difference-in-Differences (DiD)}

\begin{align*}
\text{ATT} &= (\bar{Y}_{\text{Germany, Post}} - \bar{Y}_{\text{Germany, Pre}}) \\
&\quad - (\bar{Y}_{\text{Other EU, Post}} - \bar{Y}_{\text{Other EU, Pre}})
\end{align*}

\textbf{Data:}
\begin{itemize}
    \item \textbf{Treatment}: Germany (15M users)
    \item \textbf{Control}: France, Italy, Spain, UK (45M users)
    \item \textbf{Pre-period}: Jan 1 - Feb 28, 2022 (8 weeks)
    \item \textbf{Post-period}: Mar 1 - Mar 21, 2022 (3 weeks)
    \item \textbf{Metric}: Purchases per user per week
\end{itemize}

\textbf{Parallel Trends Validation:}

Critical assumption: Germany and control countries would have trended similarly without the bug.

Pre-period trends (Jan 2021 - Feb 2022, 14 months):
\begin{itemize}
    \item Germany: 3.2\% monthly growth in purchases/user
    \item Control EU: 3.1\% monthly growth
    \item \textbf{Difference}: 0.1pp (p=0.72, not significant)
    \item \textbf{Conclusion}: Parallel trends assumption \textit{plausible}
\end{itemize}

\textbf{Results:}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Country} & \textbf{Pre-Bug} & \textbf{During Bug} & \textbf{Change} \\
\hline
Germany & 2.45 purchases/user/week & 1.92 purchases/user/week & -0.53 (-22\%) \\
Control EU & 2.38 purchases/user/week & 2.41 purchases/user/week & +0.03 (+1\%) \\
\hline
\end{tabular}
\caption{Purchases per user per week before and during bug}
\end{table}

\textbf{DiD Estimate:}
\begin{align*}
\text{ATT} &= (-0.53) - (+0.03) = -0.56 \text{ purchases/user/week} \\
\text{Relative Impact} &= -23\% \\
\text{95\% CI} &= [-0.61, -0.51] \\
\text{p-value} &< 0.001
\end{align*}

\textbf{Robustness Checks Passed:}
\begin{enumerate}
    \item \textbf{Placebo Test}: No effect in pre-period (Feb vs Jan): DiD = -0.01 (p=0.82)
    \item \textbf{Synthetic Control}: Using optimal weights on control countries: -22.8\%
    \item \textbf{Event Study}: Effect emerged exactly when bug started, disappeared when fixed
    \item \textbf{Heterogeneity}: Effect consistent across user segments, product categories
\end{enumerate}

\textbf{Business Impact:}

Armed with causal evidence, the team presented to leadership:

\begin{itemize}
    \item \textbf{Causal Effect}: Recommendations drive +23\% of purchases
    \item \textbf{Annual Value}: 23\% × \$8B GMV = \textbf{\$1.84B in incremental GMV}
    \item \textbf{ROI}: Recommendation system costs \$12M/year → \textbf{154x ROI}
    \item \textbf{Decision}: Secured \$50M budget for recommendation R\&D expansion
\end{itemize}

\textbf{Why DiD > Traditional Comparison:}

Naive comparison (Germany During Bug vs Control During Bug):
\begin{itemize}
    \item Germany: 1.92 purchases/user/week
    \item Control: 2.41 purchases/user/week
    \item \textbf{Difference}: -0.49 purchases/user/week (-20\%)
\end{itemize}

This **underestimates** the impact because Germany had lower baseline. DiD accounts for baseline differences, yielding correct estimate of -23\%.

\textbf{Lessons Learned:}
\begin{itemize}
    \item Natural experiments can provide causal estimates when A/B tests are infeasible
    \item Parallel trends assumption is critical—must validate, not assume
    \item Robustness checks (placebo tests, synthetic control, event study) build confidence
    \item Observational methods require more careful analysis than randomized experiments
    \item Clear causal evidence (\$1.84B impact) justified major investment
\end{itemize}

\subsection{Mathematical Foundations of Causal Inference}

\textbf{Potential Outcomes Framework:}

For each unit $i$, define potential outcomes:
\begin{itemize}
    \item $Y_i(1)$: Outcome if unit $i$ receives treatment
    \item $Y_i(0)$: Outcome if unit $i$ receives control
\end{itemize}

The fundamental problem of causal inference: We observe only one potential outcome:
\begin{equation}
Y_i = D_i \cdot Y_i(1) + (1 - D_i) \cdot Y_i(0)
\end{equation}

Where $D_i \in \{0, 1\}$ is the treatment indicator.

\textbf{Average Treatment Effect (ATE):}
\begin{equation}
\text{ATE} = \mathbb{E}[Y_i(1) - Y_i(0)]
\end{equation}

\textbf{Average Treatment Effect on the Treated (ATT):}
\begin{equation}
\text{ATT} = \mathbb{E}[Y_i(1) - Y_i(0) | D_i = 1]
\end{equation}

\textbf{Selection Bias:}

Naive comparison of treated vs untreated is biased:
\begin{align}
\mathbb{E}[Y_i | D_i = 1] - \mathbb{E}[Y_i | D_i = 0] &= \mathbb{E}[Y_i(1) - Y_i(0) | D_i = 1] \\
&\quad + \underbrace{\mathbb{E}[Y_i(0) | D_i = 1] - \mathbb{E}[Y_i(0) | D_i = 0]}_{\text{Selection Bias}}
\end{align}

The bias term represents systematic differences between treated and control groups.

\textbf{Identification Strategies:}

Each causal method makes different assumptions to eliminate selection bias:

\begin{enumerate}
    \item \textbf{Randomization}: $Y_i(0), Y_i(1) \perp D_i$ (A/B testing)
    \item \textbf{Conditional Independence}: $Y_i(0), Y_i(1) \perp D_i | X_i$ (matching, regression)
    \item \textbf{Parallel Trends}: $\mathbb{E}[Y_i(0)_{t+1} - Y_i(0)_t | D_i=1] = \mathbb{E}[Y_i(0)_{t+1} - Y_i(0)_t | D_i=0]$ (DiD)
    \item \textbf{Exclusion Restriction}: $Z_i$ affects $Y_i$ only through $D_i$ (IV)
    \item \textbf{Continuity}: $\mathbb{E}[Y_i(0) | X_i = c]$ continuous at cutoff $c$ (RDD)
\end{enumerate}

\subsection{Synthetic Control Methods}

Synthetic control creates a weighted combination of control units that closely matches the treated unit's pre-treatment characteristics.

\textbf{Mathematical Framework:}

For a single treated unit and $J$ control units, find weights $\mathbf{w} = (w_1, \ldots, w_J)$ such that:

\begin{equation}
\mathbf{w}^* = \arg\min_{\mathbf{w}} \left\| \mathbf{X}_1 - \mathbf{X}_0 \mathbf{w} \right\|_V
\end{equation}

Subject to:
\begin{align}
w_j &\geq 0 \quad \forall j \\
\sum_{j=1}^J w_j &= 1
\end{align}

Where:
\begin{itemize}
    \item $\mathbf{X}_1$: Pre-treatment characteristics of treated unit
    \item $\mathbf{X}_0$: Pre-treatment characteristics of control units
    \item $V$: Positive definite matrix (often diagonal)
\end{itemize}

\textbf{Treatment Effect Estimation:}

\begin{equation}
\hat{\tau}_t = Y_{1t} - \sum_{j=2}^{J+1} w_j^* Y_{jt} \quad \text{for } t > T_0
\end{equation}

Where $T_0$ is the last pre-treatment period.

\begin{lstlisting}[language=Python, caption={Synthetic Control Implementation}]
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from dataclasses import dataclass

@dataclass
class SyntheticControlResult:
    """Results from synthetic control analysis."""
    treatment_effect: np.ndarray
    synthetic_control: np.ndarray
    weights: np.ndarray
    pre_treatment_fit: float
    placebo_p_value: Optional[float]
    donor_units: List[str]

class SyntheticControl:
    """
    Synthetic control method for causal inference.

    Creates a weighted combination of control units that best matches
    the treated unit's pre-treatment trajectory.

    Example:
        >>> sc = SyntheticControl()
        >>> result = sc.fit(
        ...     data=df,
        ...     outcome_col='sales',
        ...     unit_col='state',
        ...     time_col='date',
        ...     treated_unit='California',
        ...     treatment_time='2020-01-01'
        ... )
        >>> print(f"Average treatment effect: {result.treatment_effect.mean():.2f}")
    """

    def __init__(
        self,
        optimization_method: str = 'L-BFGS-B'
    ):
        """
        Initialize synthetic control.

        Args:
            optimization_method: Scipy optimization method
        """
        self.optimization_method = optimization_method

    def fit(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        unit_col: str,
        time_col: str,
        treated_unit: str,
        treatment_time: str,
        covariates: Optional[List[str]] = None,
        donor_pool: Optional[List[str]] = None
    ) -> SyntheticControlResult:
        """
        Fit synthetic control model.

        Args:
            data: Panel data with units and time
            outcome_col: Outcome variable column
            unit_col: Unit identifier column
            time_col: Time column
            treated_unit: Identifier of treated unit
            treatment_time: Time when treatment began
            covariates: Optional covariates to match on
            donor_pool: Optional list of donor units (if None, use all others)

        Returns:
            SyntheticControlResult with estimates and diagnostics
        """
        # Prepare data
        data = data.copy()
        data[time_col] = pd.to_datetime(data[time_col])
        treatment_time = pd.to_datetime(treatment_time)

        # Split pre/post
        pre_data = data[data[time_col] < treatment_time]
        post_data = data[data[time_col] >= treatment_time]

        # Get treated and donor units
        if donor_pool is None:
            donor_pool = [u for u in data[unit_col].unique() if u != treated_unit]

        # Extract treated unit outcomes
        treated_pre = pre_data[pre_data[unit_col] == treated_unit][outcome_col].values
        treated_post = post_data[post_data[unit_col] == treated_unit][outcome_col].values

        # Extract donor unit outcomes
        donor_pre_matrix = []
        donor_post_matrix = []

        for donor in donor_pool:
            donor_pre = pre_data[pre_data[unit_col] == donor][outcome_col].values
            donor_post = post_data[post_data[unit_col] == donor][outcome_col].values

            # Ensure same length as treated
            if len(donor_pre) == len(treated_pre) and len(donor_post) == len(treated_post):
                donor_pre_matrix.append(donor_pre)
                donor_post_matrix.append(donor_post)
            else:
                donor_pool.remove(donor)  # Remove if lengths don't match

        donor_pre_matrix = np.array(donor_pre_matrix).T  # Time × Donors
        donor_post_matrix = np.array(donor_post_matrix).T

        # Optional: Include covariates
        if covariates:
            # Extract covariate values for matching
            treated_covariates = pre_data[pre_data[unit_col] == treated_unit][covariates].mean().values
            donor_covariates = np.array([
                pre_data[pre_data[unit_col] == donor][covariates].mean().values
                for donor in donor_pool
            ])

            # Stack outcomes and covariates for matching
            X_treated = np.concatenate([treated_pre, treated_covariates])
            X_donors = np.vstack([donor_pre_matrix.T, donor_covariates.T]).T
        else:
            X_treated = treated_pre
            X_donors = donor_pre_matrix

        # Optimize weights
        weights = self._optimize_weights(X_treated, X_donors)

        # Compute synthetic control
        synthetic_pre = donor_pre_matrix @ weights
        synthetic_post = donor_post_matrix @ weights

        # Treatment effects
        treatment_effect_post = treated_post - synthetic_post

        # Pre-treatment fit (RMSPE)
        pre_treatment_fit = np.sqrt(np.mean((treated_pre - synthetic_pre)**2))

        # Placebo test (optional)
        placebo_p_value = self._placebo_test(
            data, outcome_col, unit_col, time_col,
            treated_unit, treatment_time, donor_pool
        )

        return SyntheticControlResult(
            treatment_effect=treatment_effect_post,
            synthetic_control=np.concatenate([synthetic_pre, synthetic_post]),
            weights=weights,
            pre_treatment_fit=pre_treatment_fit,
            placebo_p_value=placebo_p_value,
            donor_units=donor_pool
        )

    def _optimize_weights(
        self,
        X_treated: np.ndarray,
        X_donors: np.ndarray
    ) -> np.ndarray:
        """
        Optimize weights to minimize distance between treated and synthetic.

        Args:
            X_treated: Characteristics of treated unit
            X_donors: Characteristics matrix of donors (Features × Donors)

        Returns:
            Optimal weights
        """
        n_donors = X_donors.shape[1]

        # Objective: minimize squared distance
        def objective(w):
            synthetic = X_donors @ w
            return np.sum((X_treated - synthetic)**2)

        # Constraints: weights sum to 1, all non-negative
        constraints = [
            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        ]
        bounds = [(0, 1) for _ in range(n_donors)]

        # Initial guess: equal weights
        w0 = np.ones(n_donors) / n_donors

        # Optimize
        result = minimize(
            objective,
            w0,
            method=self.optimization_method,
            bounds=bounds,
            constraints=constraints
        )

        return result.x

    def _placebo_test(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        unit_col: str,
        time_col: str,
        treated_unit: str,
        treatment_time: str,
        donor_pool: List[str],
        n_placebo: int = None
    ) -> float:
        """
        Placebo test: Apply synthetic control to each donor unit.

        If treated unit's effect is unusual compared to placebo effects,
        this provides evidence of true treatment effect.

        Args:
            data: Full dataset
            outcome_col, unit_col, time_col: Column names
            treated_unit: Treated unit
            treatment_time: Treatment time
            donor_pool: Donor units
            n_placebo: Number of placebo tests (None = all donors)

        Returns:
            P-value from placebo test
        """
        # Get actual treatment effect
        actual_result = self.fit(
            data, outcome_col, unit_col, time_col,
            treated_unit, treatment_time, donor_pool=donor_pool
        )
        actual_effect = np.abs(actual_result.treatment_effect).mean()

        # Run placebo for each donor
        placebo_effects = []
        donors_to_test = donor_pool if n_placebo is None else donor_pool[:n_placebo]

        for placebo_unit in donors_to_test:
            placebo_donors = [u for u in donor_pool if u != placebo_unit]

            try:
                placebo_result = self.fit(
                    data, outcome_col, unit_col, time_col,
                    placebo_unit, treatment_time, donor_pool=placebo_donors
                )
                placebo_effect = np.abs(placebo_result.treatment_effect).mean()
                placebo_effects.append(placebo_effect)
            except:
                continue  # Skip if optimization fails

        # P-value: fraction of placebo effects >= actual effect
        if len(placebo_effects) > 0:
            p_value = np.mean(np.array(placebo_effects) >= actual_effect)
        else:
            p_value = None

        return p_value
\end{lstlisting}

\subsection{Difference-in-Differences (DiD)}

DiD compares changes in outcomes over time between treatment and control groups.

\textbf{Mathematical Framework:}

\begin{equation}
Y_{it} = \alpha + \beta D_i + \gamma \text{Post}_t + \delta (D_i \times \text{Post}_t) + \epsilon_{it}
\end{equation}

Where:
\begin{itemize}
    \item $Y_{it}$: Outcome for unit $i$ at time $t$
    \item $D_i$: Treatment group indicator
    \item $\text{Post}_t$: Post-treatment period indicator
    \item $\delta$: DiD estimator (treatment effect)
\end{itemize}

\textbf{Key Assumption - Parallel Trends:}

In the absence of treatment, treated and control groups would have followed parallel trends:
\begin{equation}
\mathbb{E}[Y_{i,t+1}(0) - Y_{i,t}(0) | D_i = 1] = \mathbb{E}[Y_{i,t+1}(0) - Y_{i,t}(0) | D_i = 0]
\end{equation}

\begin{lstlisting}[language=Python, caption={Difference-in-Differences Implementation}]
import statsmodels.api as sm
from statsmodels.formula.api import ols

@dataclass
class DiDResult:
    """Results from difference-in-differences analysis."""
    treatment_effect: float
    std_error: float
    p_value: float
    confidence_interval: Tuple[float, float]
    pre_treatment_diff: float
    parallel_trends_test: Dict[str, float]
    event_study_estimates: Optional[pd.DataFrame]

class DifferenceInDifferences:
    """
    Difference-in-differences causal inference.

    Estimates treatment effects by comparing changes over time
    between treated and control groups.

    Example:
        >>> did = DifferenceInDifferences()
        >>> result = did.fit(
        ...     data=df,
        ...     outcome_col='sales',
        ...     unit_col='store',
        ...     time_col='month',
        ...     treatment_col='treated',
        ...     treatment_time='2020-01-01'
        ... )
        >>> print(f"Treatment effect: {result.treatment_effect:.2f} (p={result.p_value:.4f})")
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize DiD estimator.

        Args:
            alpha: Significance level for tests
        """
        self.alpha = alpha

    def fit(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        unit_col: str,
        time_col: str,
        treatment_col: str,
        treatment_time: str,
        covariates: Optional[List[str]] = None
    ) -> DiDResult:
        """
        Estimate DiD treatment effect.

        Args:
            data: Panel data
            outcome_col: Outcome variable
            unit_col: Unit identifier
            time_col: Time column
            treatment_col: Treatment indicator (0/1)
            treatment_time: When treatment began
            covariates: Optional control variables

        Returns:
            DiDResult with estimates and diagnostics
        """
        # Prepare data
        data = data.copy()
        data[time_col] = pd.to_datetime(data[time_col])
        treatment_time = pd.to_datetime(treatment_time)

        # Create post indicator
        data['post'] = (data[time_col] >= treatment_time).astype(int)

        # Create interaction term
        data['treated_x_post'] = data[treatment_col] * data['post']

        # Build formula
        formula = f"{outcome_col} ~ {treatment_col} + post + treated_x_post"
        if covariates:
            formula += " + " + " + ".join(covariates)

        # Add unit fixed effects
        formula += f" + C({unit_col})"

        # Fit model
        model = ols(formula, data=data).fit(
            cov_type='cluster',
            cov_kwds={'groups': data[unit_col]}
        )

        # Extract DiD estimate
        did_coef = model.params['treated_x_post']
        did_se = model.bse['treated_x_post']
        did_pvalue = model.pvalues['treated_x_post']

        # Confidence interval
        ci_lower, ci_upper = model.conf_int(alpha=self.alpha).loc['treated_x_post']

        # Pre-treatment difference
        pre_data = data[data['post'] == 0]
        pre_treated = pre_data[pre_data[treatment_col] == 1][outcome_col].mean()
        pre_control = pre_data[pre_data[treatment_col] == 0][outcome_col].mean()
        pre_diff = pre_treated - pre_control

        # Parallel trends test
        parallel_trends = self._test_parallel_trends(
            data, outcome_col, unit_col, time_col, treatment_col, treatment_time
        )

        # Event study (optional)
        event_study = self._event_study(
            data, outcome_col, unit_col, time_col, treatment_col, treatment_time
        )

        return DiDResult(
            treatment_effect=did_coef,
            std_error=did_se,
            p_value=did_pvalue,
            confidence_interval=(ci_lower, ci_upper),
            pre_treatment_diff=pre_diff,
            parallel_trends_test=parallel_trends,
            event_study_estimates=event_study
        )

    def _test_parallel_trends(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        unit_col: str,
        time_col: str,
        treatment_col: str,
        treatment_time: pd.Timestamp
    ) -> Dict[str, float]:
        """
        Test parallel trends assumption in pre-period.

        Regresses outcome on treatment × time interaction in pre-period.
        Parallel trends hold if interaction is not significant.
        """
        # Pre-treatment data only
        pre_data = data[data[time_col] < treatment_time].copy()

        if len(pre_data) == 0:
            return {'test_statistic': np.nan, 'p_value': np.nan}

        # Create time trend (normalized)
        pre_data['time_numeric'] = (
            (pre_data[time_col] - pre_data[time_col].min()).dt.days
        )

        # Test treatment × time interaction
        formula = f"{outcome_col} ~ {treatment_col} * time_numeric + C({unit_col})"

        try:
            model = ols(formula, data=pre_data).fit(
                cov_type='cluster',
                cov_kwds={'groups': pre_data[unit_col]}
            )

            # Test if interaction coefficient is zero
            interaction_coef = model.params.get(f'{treatment_col}:time_numeric', np.nan)
            interaction_pvalue = model.pvalues.get(f'{treatment_col}:time_numeric', np.nan)

            return {
                'test_statistic': interaction_coef,
                'p_value': interaction_pvalue,
                'passed': interaction_pvalue > self.alpha if not np.isnan(interaction_pvalue) else None
            }
        except:
            return {'test_statistic': np.nan, 'p_value': np.nan, 'passed': None}

    def _event_study(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        unit_col: str,
        time_col: str,
        treatment_col: str,
        treatment_time: pd.Timestamp
    ) -> pd.DataFrame:
        """
        Event study: estimate treatment effects for each time period.

        Visualizes treatment effect evolution and tests for pre-trends.
        """
        data = data.copy()

        # Create relative time to treatment
        data['relative_time'] = (
            (data[time_col] - treatment_time).dt.days // 30
        )  # In months

        # Exclude reference period (t = -1)
        data['relative_time_cat'] = data['relative_time'].astype(str)
        data.loc[data['relative_time'] == -1, 'relative_time_cat'] = 'ref'

        # Create interaction terms for each period
        relative_times = sorted([t for t in data['relative_time'].unique() if t != -1])

        formula = f"{outcome_col} ~ "
        interactions = []
        for t in relative_times:
            var_name = f"treated_x_t{t}"
            data[var_name] = (data[treatment_col] == 1) & (data['relative_time'] == t)
            interactions.append(var_name)

        formula += " + ".join(interactions) + f" + C({unit_col}) + C({time_col})"

        try:
            model = ols(formula, data=data).fit(
                cov_type='cluster',
                cov_kwds={'groups': data[unit_col]}
            )

            # Extract coefficients
            event_study_results = []
            for t in relative_times:
                var_name = f"treated_x_t{t}"
                if var_name in model.params:
                    coef = model.params[var_name]
                    se = model.bse[var_name]
                    ci_lower, ci_upper = model.conf_int(alpha=self.alpha).loc[var_name]

                    event_study_results.append({
                        'relative_time': t,
                        'coefficient': coef,
                        'std_error': se,
                        'ci_lower': ci_lower,
                        'ci_upper': ci_upper
                    })

            return pd.DataFrame(event_study_results)
        except:
            return None
\end{lstlisting}

\subsection{Instrumental Variables (IV)}

IV estimation addresses endogeneity when treatment is correlated with unobserved confounders.

\textbf{Mathematical Framework:}

\textbf{Structural equation:}
\begin{equation}
Y_i = \alpha + \beta D_i + \gamma X_i + \epsilon_i
\end{equation}

Where $D_i$ is endogenous: $\text{Cov}(D_i, \epsilon_i) \neq 0$

\textbf{Instrumental variable} $Z_i$ satisfies:
\begin{enumerate}
    \item \textbf{Relevance}: $\text{Cov}(Z_i, D_i) \neq 0$
    \item \textbf{Exclusion}: $Z_i$ affects $Y_i$ only through $D_i$
    \item \textbf{Exogeneity}: $\text{Cov}(Z_i, \epsilon_i) = 0$
\end{enumerate}

\textbf{Two-Stage Least Squares (2SLS):}

\textbf{First stage:}
\begin{equation}
D_i = \pi_0 + \pi_1 Z_i + \pi_2 X_i + \nu_i
\end{equation}

\textbf{Second stage:}
\begin{equation}
Y_i = \alpha + \beta \hat{D}_i + \gamma X_i + u_i
\end{equation}

Where $\hat{D}_i$ is predicted treatment from first stage.

\textbf{Wald Estimator (binary $Z_i$ and $D_i$):}
\begin{equation}
\hat{\beta}^{\text{IV}} = \frac{\mathbb{E}[Y_i | Z_i = 1] - \mathbb{E}[Y_i | Z_i = 0]}{\mathbb{E}[D_i | Z_i = 1] - \mathbb{E}[D_i | Z_i = 0]}
\end{equation}

\begin{lstlisting}[language=Python, caption={Instrumental Variables Implementation}]
from statsmodels.sandbox.regression.gmm import IV2SLS

@dataclass
class IVResult:
    """Results from instrumental variables estimation."""
    treatment_effect: float
    std_error: float
    p_value: float
    confidence_interval: Tuple[float, float]
    first_stage_f_stat: float
    weak_instrument_test: Dict[str, Any]
    overid_test: Optional[Dict[str, float]]

class InstrumentalVariables:
    """
    Instrumental variables estimation for causal inference.

    Addresses endogeneity using instruments that affect treatment
    but not outcome (except through treatment).

    Example:
        >>> iv = InstrumentalVariables()
        >>> result = iv.fit(
        ...     data=df,
        ...     outcome_col='earnings',
        ...     treatment_col='education_years',
        ...     instrument_col='college_proximity',
        ...     covariates=['age', 'gender']
        ... )
        >>> print(f"Causal effect: {result.treatment_effect:.2f}")
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize IV estimator.

        Args:
            alpha: Significance level
        """
        self.alpha = alpha

    def fit(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str,
        instrument_col: Union[str, List[str]],
        covariates: Optional[List[str]] = None
    ) -> IVResult:
        """
        Estimate treatment effect using instrumental variables.

        Args:
            data: Dataset
            outcome_col: Outcome variable
            treatment_col: Endogenous treatment variable
            instrument_col: Instrumental variable(s)
            covariates: Exogenous control variables

        Returns:
            IVResult with estimates and diagnostics
        """
        data = data.copy().dropna(
            subset=[outcome_col, treatment_col] +
            ([instrument_col] if isinstance(instrument_col, str) else instrument_col) +
            (covariates or [])
        )

        # Prepare variables
        y = data[outcome_col].values
        X_endog = data[[treatment_col]].values
        Z = data[[instrument_col]] if isinstance(instrument_col, str) else data[instrument_col]

        if covariates:
            X_exog = data[covariates].values
            X_exog = sm.add_constant(X_exog)
        else:
            X_exog = np.ones((len(data), 1))

        # 2SLS estimation
        model = IV2SLS(y, X_exog, X_endog, Z.values).fit()

        # Extract results
        treatment_effect = model.params[-1]  # Last param is endogenous variable
        std_error = model.bse[-1]
        p_value = model.pvalues[-1]

        # Confidence interval
        ci_lower = model.conf_int(alpha=self.alpha)[-1, 0]
        ci_upper = model.conf_int(alpha=self.alpha)[-1, 1]

        # First stage F-statistic
        first_stage_f = self._first_stage_f_stat(
            data, treatment_col, instrument_col, covariates
        )

        # Weak instrument test
        weak_instrument = self._test_weak_instrument(first_stage_f)

        # Overidentification test (if multiple instruments)
        if isinstance(instrument_col, list) and len(instrument_col) > 1:
            overid_test = self._overid_test(model)
        else:
            overid_test = None

        return IVResult(
            treatment_effect=treatment_effect,
            std_error=std_error,
            p_value=p_value,
            confidence_interval=(ci_lower, ci_upper),
            first_stage_f_stat=first_stage_f,
            weak_instrument_test=weak_instrument,
            overid_test=overid_test
        )

    def _first_stage_f_stat(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        instrument_col: Union[str, List[str]],
        covariates: Optional[List[str]]
    ) -> float:
        """Compute first-stage F-statistic for instrument strength."""
        # First stage regression
        instruments = [instrument_col] if isinstance(instrument_col, str) else instrument_col

        formula = f"{treatment_col} ~ " + " + ".join(instruments)
        if covariates:
            formula += " + " + " + ".join(covariates)

        first_stage = ols(formula, data=data).fit()

        # F-statistic for instruments
        # Test if all instrument coefficients are jointly zero
        instrument_params = [p for p in instruments if p in first_stage.params]

        if len(instrument_params) == 0:
            return 0.0

        # Joint F-test
        hypotheses = [f"{param} = 0" for param in instrument_params]
        f_test = first_stage.f_test(hypotheses)

        return f_test.fvalue[0][0]

    def _test_weak_instrument(
        self,
        first_stage_f: float
    ) -> Dict[str, Any]:
        """
        Test for weak instruments using Stock-Yogo critical values.

        Rule of thumb: F > 10 is strong, F < 10 is weak.
        """
        # Stock-Yogo critical value for 10% maximal IV size (single endogenous var, single instrument)
        critical_value_10pct = 16.38

        return {
            'f_statistic': first_stage_f,
            'critical_value_10pct': critical_value_10pct,
            'is_strong': first_stage_f > 10,
            'passes_stock_yogo': first_stage_f > critical_value_10pct,
            'interpretation': (
                'Strong instrument' if first_stage_f > critical_value_10pct
                else 'Moderate instrument' if first_stage_f > 10
                else 'Weak instrument - results may be unreliable'
            )
        }

    def _overid_test(
        self,
        model: IV2SLS
    ) -> Dict[str, float]:
        """
        Sargan-Hansen overidentification test.

        Tests if instruments are exogenous (valid exclusion restriction).
        Only possible with more instruments than endogenous variables.
        """
        # Not directly available in statsmodels IV2SLS
        # Would need to implement Hansen J-statistic manually
        # Placeholder for now
        return {
            'test_statistic': np.nan,
            'p_value': np.nan,
            'df': np.nan
        }
\end{lstlisting}

\subsection{Regression Discontinuity Design (RDD)}

RDD exploits discontinuous treatment assignment based on a running variable crossing a cutoff.

\textbf{Mathematical Framework:}

Treatment assigned if running variable $X_i$ exceeds cutoff $c$:
\begin{equation}
D_i = \mathbb{1}[X_i \geq c]
\end{equation}

\textbf{Sharp RDD:} Treatment perfectly determined by cutoff
\begin{equation}
\tau_{\text{RDD}} = \lim_{x \downarrow c} \mathbb{E}[Y_i | X_i = x] - \lim_{x \uparrow c} \mathbb{E}[Y_i | X_i = x]
\end{equation}

\textbf{Local Linear Regression:}
\begin{align}
Y_i &= \alpha + \beta D_i + \gamma_1 (X_i - c) + \gamma_2 D_i (X_i - c) + \epsilon_i \\
&\text{for } |X_i - c| \leq h
\end{align}

Where $h$ is the bandwidth, and $\beta$ is the RDD estimate.

\begin{lstlisting}[language=Python, caption={Regression Discontinuity Design Implementation}]
from sklearn.linear_model import LinearRegression

@dataclass
class RDDResult:
    """Results from regression discontinuity analysis."""
    treatment_effect: float
    std_error: float
    p_value: float
    confidence_interval: Tuple[float, float]
    optimal_bandwidth: float
    n_observations: int
    bandwidth_sensitivity: Optional[pd.DataFrame]
    density_test: Dict[str, float]

class RegressionDiscontinuity:
    """
    Regression discontinuity design for causal inference.

    Estimates treatment effects when treatment is assigned based on
    a running variable crossing a cutoff threshold.

    Example:
        >>> rdd = RegressionDiscontinuity()
        >>> result = rdd.fit(
        ...     data=df,
        ...     outcome_col='test_score',
        ...     running_var='entrance_exam_score',
        ...     cutoff=70,
        ...     bandwidth='optimal'
        ... )
        >>> print(f"Treatment effect at cutoff: {result.treatment_effect:.2f}")
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize RDD estimator.

        Args:
            alpha: Significance level
        """
        self.alpha = alpha

    def fit(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        running_var: str,
        cutoff: float,
        bandwidth: Union[float, str] = 'optimal',
        kernel: str = 'triangular'
    ) -> RDDResult:
        """
        Estimate RDD treatment effect.

        Args:
            data: Dataset
            outcome_col: Outcome variable
            running_var: Running variable (assignment variable)
            cutoff: Treatment assignment cutoff
            bandwidth: Bandwidth for local regression ('optimal' or numeric)
            kernel: Kernel for weighting ('triangular', 'uniform', 'epanechnikov')

        Returns:
            RDDResult with estimates and diagnostics
        """
        data = data.copy().dropna(subset=[outcome_col, running_var])

        # Center running variable at cutoff
        data['running_centered'] = data[running_var] - cutoff

        # Treatment indicator
        data['treated'] = (data['running_centered'] >= 0).astype(int)

        # Optimal bandwidth selection
        if bandwidth == 'optimal':
            optimal_bw = self._optimal_bandwidth(
                data, outcome_col, 'running_centered'
            )
        else:
            optimal_bw = bandwidth

        # Restrict to bandwidth
        data_bw = data[np.abs(data['running_centered']) <= optimal_bw].copy()

        # Create interaction term
        data_bw['treated_x_running'] = data_bw['treated'] * data_bw['running_centered']

        # Apply kernel weights
        weights = self._kernel_weights(
            data_bw['running_centered'].values,
            optimal_bw,
            kernel
        )

        # Weighted linear regression
        X = data_bw[['treated', 'running_centered', 'treated_x_running']].values
        X = sm.add_constant(X)
        y = data_bw[outcome_col].values

        model = sm.WLS(y, X, weights=weights).fit(
            cov_type='HC1'  # Heteroskedasticity-robust
        )

        # Extract RDD estimate (coefficient on 'treated')
        rdd_effect = model.params[1]
        rdd_se = model.bse[1]
        rdd_pvalue = model.pvalues[1]

        # Confidence interval
        ci_lower, ci_upper = model.conf_int(alpha=self.alpha)[1]

        # Bandwidth sensitivity
        sensitivity = self._bandwidth_sensitivity(
            data, outcome_col, 'running_centered', cutoff, kernel
        )

        # Density test (McCrary test)
        density_test = self._mccrary_test(data, 'running_centered', cutoff)

        return RDDResult(
            treatment_effect=rdd_effect,
            std_error=rdd_se,
            p_value=rdd_pvalue,
            confidence_interval=(ci_lower, ci_upper),
            optimal_bandwidth=optimal_bw,
            n_observations=len(data_bw),
            bandwidth_sensitivity=sensitivity,
            density_test=density_test
        )

    def _optimal_bandwidth(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        running_centered: str,
        method: str = 'imbens_kalyanaraman'
    ) -> float:
        """
        Calculate optimal bandwidth using Imbens-Kalyanaraman procedure.

        Balances bias-variance tradeoff.
        """
        # Simplified version - full implementation would follow IK2012 algorithm
        # Use rule-of-thumb: h = 1.84 * sigma * n^(-1/5)

        n = len(data)
        sigma = data[outcome_col].std()
        rule_of_thumb = 1.84 * sigma * (n ** (-0.2))

        # Adjust based on running variable range
        running_range = data[running_centered].max() - data[running_centered].min()
        bandwidth = min(rule_of_thumb, running_range / 4)

        return bandwidth

    def _kernel_weights(
        self,
        x: np.ndarray,
        bandwidth: float,
        kernel: str
    ) -> np.ndarray:
        """Compute kernel weights for local regression."""
        u = x / bandwidth

        if kernel == 'triangular':
            weights = np.maximum(1 - np.abs(u), 0)
        elif kernel == 'uniform':
            weights = (np.abs(u) <= 1).astype(float)
        elif kernel == 'epanechnikov':
            weights = np.maximum(0.75 * (1 - u**2), 0)
        else:
            raise ValueError(f"Unknown kernel: {kernel}")

        return weights

    def _bandwidth_sensitivity(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        running_centered: str,
        cutoff: float,
        kernel: str,
        n_bandwidths: int = 10
    ) -> pd.DataFrame:
        """Test sensitivity to bandwidth choice."""
        # Get optimal bandwidth
        optimal_bw = self._optimal_bandwidth(data, outcome_col, running_centered)

        # Test range: 0.5 × optimal to 2 × optimal
        bandwidths = np.linspace(optimal_bw * 0.5, optimal_bw * 2, n_bandwidths)

        results = []
        for bw in bandwidths:
            # Refit with this bandwidth
            data_bw = data[np.abs(data[running_centered]) <= bw].copy()

            if len(data_bw) < 20:  # Need minimum observations
                continue

            data_bw['treated'] = (data_bw[running_centered] >= 0).astype(int)
            data_bw['treated_x_running'] = data_bw['treated'] * data_bw[running_centered]

            weights = self._kernel_weights(
                data_bw[running_centered].values, bw, kernel
            )

            X = data_bw[['treated', 'running_centered', 'treated_x_running']].values
            X = sm.add_constant(X)
            y = data_bw[outcome_col].values

            try:
                model = sm.WLS(y, X, weights=weights).fit(cov_type='HC1')
                effect = model.params[1]
                se = model.bse[1]
                ci_lower, ci_upper = model.conf_int(alpha=self.alpha)[1]

                results.append({
                    'bandwidth': bw,
                    'effect': effect,
                    'std_error': se,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper,
                    'n_obs': len(data_bw)
                })
            except:
                continue

        return pd.DataFrame(results) if results else None

    def _mccrary_test(
        self,
        data: pd.DataFrame,
        running_centered: str,
        cutoff: float,
        n_bins: int = 50
    ) -> Dict[str, float]:
        """
        McCrary density test for manipulation of running variable.

        Tests if density of running variable is continuous at cutoff.
        Discontinuity suggests manipulation (invalidity).
        """
        # Simplified version - full implementation would use local polynomial
        # density estimation on each side of cutoff

        # Histogram approach
        bins = np.linspace(
            data[running_centered].min(),
            data[running_centered].max(),
            n_bins
        )

        counts, _ = np.histogram(data[running_centered], bins=bins)

        # Find cutoff bin
        cutoff_idx = np.searchsorted(bins, 0)

        # Density just below and above cutoff
        if cutoff_idx > 0 and cutoff_idx < len(counts):
            density_below = counts[cutoff_idx - 1]
            density_above = counts[cutoff_idx]

            # Simple test: compare densities
            # Full McCrary test would estimate log-density discontinuity
            density_ratio = density_above / density_below if density_below > 0 else np.inf

            # Rough test: ratio should be close to 1
            is_continuous = 0.5 < density_ratio < 2.0

            return {
                'density_below': density_below,
                'density_above': density_above,
                'density_ratio': density_ratio,
                'is_continuous': is_continuous,
                'interpretation': (
                    'No evidence of manipulation' if is_continuous
                    else 'Potential manipulation detected - investigate further'
                )
            }
        else:
            return {
                'density_below': np.nan,
                'density_above': np.nan,
                'density_ratio': np.nan,
                'is_continuous': None,
                'interpretation': 'Insufficient data near cutoff'
            }
\end{lstlisting}

These causal inference methods provide rigorous alternatives to randomized experiments when randomization is infeasible, unethical, or when analyzing historical interventions. Each method makes different identification assumptions that must be carefully validated in practice.

\section{Practical Implementation Challenges}

Beyond statistical theory, successful experimentation requires navigating real-world constraints: business timelines, seasonality, budget limitations, and organizational dynamics. This section addresses common implementation challenges with practical solutions.

\subsection{Real-World Scenario: The Seasonal Confusion}

\textbf{The Setup:}

An e-commerce company tested a new product recommendation algorithm in November-December 2022, launching a 4-week A/B test with 50/50 traffic allocation.

\textbf{Initial Results (After 4 Weeks):}
\begin{itemize}
    \item \textbf{Sample Size}: 2M users per arm (far exceeding power requirements)
    \item \textbf{Primary Metric - Revenue per User}:
    \begin{itemize}
        \item Control: \$48.20
        \item Treatment: \$49.80
        \item \textbf{Lift}: +3.3\% (p < 0.001, highly significant!)
    \end{itemize}
    \item \textbf{Secondary Metrics}: All positive
    \item \textbf{Decision}: Team deployed to 100\% traffic on January 2, 2023
\end{itemize}

\textbf{The Disaster (January-February 2023):}

After full deployment, revenue \textit{decreased} by 1.8\% compared to previous year:
\begin{itemize}
    \item Expected: +3.3\% lift = +\$12M revenue
    \item Actual: -1.8\% = -\$6.5M revenue loss
    \item \textbf{Total impact}: \$18.5M swing from expectation!
\end{itemize}

\textbf{Root Cause Analysis:}

The data science team discovered multiple confounding factors:

\textbf{1. Holiday Seasonality Effect}
\begin{itemize}
    \item November-December includes Black Friday, Cyber Monday, holiday shopping
    \item \textbf{Problem}: Treatment arm received \textit{more} holiday traffic due to random chance
    \item Treatment group: 32\% of users shopped during Black Friday week
    \item Control group: 29\% of users shopped during Black Friday week
    \item 3pp difference in high-value shopping days created spurious lift
\end{itemize}

\textbf{2. Product Category Imbalance}
\begin{itemize}
    \item Treatment arm had 2\% more users browsing electronics (high AOV category)
    \item Control arm had 2\% more users browsing apparel (lower AOV)
    \item Random imbalance, but statistically significant with 2M users
    \item Accounted for ~1\% of observed lift
\end{itemize}

\textbf{3. Novelty Effect}
\begin{itemize}
    \item New recommendation UI created temporary excitement
    \item Effect wore off by week 3-4 of test
    \item But test ended before novelty fully dissipated
\end{itemize}

\textbf{4. Year-over-Year Comparison Bias}
\begin{itemize}
    \item Post-deployment (Jan 2023) compared to Jan 2022 baseline
    \item But Jan 2022 had COVID-related e-commerce boom
    \item Macro decline of 5\% YoY masked 3.3\% algorithm improvement
    \item Net observed: -1.8\% YoY
\end{itemize}

\textbf{Corrected Analysis (Stratified by Time Period):}

When team re-analyzed with proper controls:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Period} & \textbf{Control} & \textbf{Treatment} & \textbf{Lift} \\
\hline
Black Friday Week & \$89.50 & \$91.20 & +1.9\% (not 3.3\%) \\
Cyber Monday Week & \$76.30 & \$77.10 & +1.0\% \\
Regular Nov Days & \$42.10 & \$43.50 & +3.3\% \\
Regular Dec Days & \$51.20 & \$52.90 & +3.3\% \\
\hline
\textbf{Weighted Avg} & \textbf{\$48.20} & \textbf{\$49.30} & \textbf{+2.3\%} (not 3.3\%) \\
\hline
\end{tabular}
\caption{Revenue per user stratified by period (corrected analysis)}
\end{table}

\textbf{True Effect}: +2.3\%, not +3.3\%

\textbf{What Should Have Been Done:}

\begin{enumerate}
    \item \textbf{Avoid Seasonality Windows}: Don't run experiments during anomalous periods
    \begin{itemize}
        \item Holiday shopping (Nov-Dec)
        \item Back-to-school (Aug-Sep)
        \item Prime Day/sales events
        \item Company-specific high seasons
    \end{itemize}

    \item \textbf{Stratified Randomization}: Balance on high-variance covariates
    \begin{itemize}
        \item \texttt{signup\_date} (to balance holiday vs regular users)
        \item \texttt{primary\_category} (to balance product mix)
        \item \texttt{user\_tier} (VIP vs regular)
    \end{itemize}

    \item \textbf{Longer Test Duration}: Run for full business cycle
    \begin{itemize}
        \item Wait for novelty effect to dissipate
        \item Capture variety of user behaviors
        \item Minimum 2 weeks for e-commerce (capture full shopping cycle)
    \end{itemize}

    \item \textbf{Holdback Group}: Keep 5-10\% control even post-deployment
    \begin{itemize}
        \item Enables ongoing measurement
        \item Catches post-deployment drift
        \item Validates A/B test results in production
    \end{itemize}

    \item \textbf{Covariate Adjustment}: Use CUPED or other variance reduction
    \begin{itemize}
        \item Control for pre-experiment user value
        \item Adjust for time-varying factors
        \item Increase statistical power
    \end{itemize}
\end{enumerate}

\textbf{Corrected Deployment Strategy:}

Team redesigned approach:
\begin{itemize}
    \item \textbf{Re-ran test in January-February} (non-holiday period)
    \item \textbf{Stratified randomization} on user tier and category
    \item \textbf{6-week duration} to capture full novelty decay
    \item \textbf{10\% holdback group} maintained post-deployment
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item Confirmed +2.1\% revenue lift (close to corrected +2.3\%)
    \item 95\% CI: [+1.8\%, +2.4\%]
    \item Deployed with confidence
    \item Holdback group validated +2.0\% lift in production over next 3 months
    \item \textbf{Actual impact}: +\$7.5M annual revenue (not -\$6.5M loss!)
\end{itemize}

\textbf{Lessons Learned:}
\begin{enumerate}
    \item Seasonality can create \textbf{50\%+ bias} in effect estimates
    \item Statistical significance (p < 0.001) \textbf{doesn't guarantee validity}
    \item Randomization imbalances emerge with large samples (paradoxically!)
    \item Proper timing \textbf{> larger sample size}
    \item Holdback groups catch post-deployment issues
\end{enumerate}

\subsection{Sample Size Calculation with Business Constraints}

Standard power analysis assumes unlimited budget and time. In practice, sample size must balance statistical rigor with business constraints.

\textbf{Mathematical Framework:}

\textbf{Standard Power Calculation (Two-Proportion Test):}

For proportion $p_1$ (control) and $p_2 = p_1(1 + \delta)$ (treatment, where $\delta$ is relative lift):

\begin{equation}
n = \frac{(z_{1-\alpha/2} + z_{1-\beta})^2 [\bar{p}(1-\bar{p})]}{(p_2 - p_1)^2} \left(\frac{1 + k}{k}\right)
\end{equation}

Where:
\begin{itemize}
    \item $\alpha$: Significance level (typically 0.05)
    \item $\beta$: Type II error rate (typically 0.20 for 80\% power)
    \item $k$: Allocation ratio (treatment/control, typically 1)
    \item $\bar{p} = (p_1 + p_2)/2$: Pooled proportion
\end{itemize}

\textbf{Business-Constrained Power:}

Given maximum sample size $n_{\max}$, calculate achievable power:

\begin{equation}
\beta = \Phi\left(z_{\alpha/2} - \frac{|p_2 - p_1|}{\sqrt{\bar{p}(1-\bar{p}) \left(\frac{1+k}{n_{\max} k}\right)}}\right)
\end{equation}

\textbf{Minimum Detectable Effect (MDE):}

Given $n_{\max}$, $\alpha$, and desired power $1-\beta$:

\begin{equation}
\text{MDE} = (z_{1-\alpha/2} + z_{1-\beta}) \sqrt{\frac{\bar{p}(1-\bar{p})(1+k)}{n_{\max} k}}
\end{equation}

\begin{lstlisting}[language=Python, caption={Sample Size Calculator with Business Constraints}]
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
from scipy import stats
from dataclasses import dataclass
import warnings

@dataclass
class SampleSizeResult:
    """Results from sample size calculation."""
    required_sample_size: int
    required_duration_days: float
    achievable_power: float
    minimum_detectable_effect: float
    cost_estimate: Optional[float]
    feasibility: str  # 'feasible', 'marginal', 'infeasible'
    recommendations: List[str]

class SampleSizeCalculator:
    """
    Sample size calculator with business constraints.

    Calculates required sample sizes accounting for:
    - Business constraints (max duration, budget)
    - Traffic availability
    - Seasonality
    - Multiple metrics (Bonferroni correction)

    Example:
        >>> calc = SampleSizeCalculator()
        >>> result = calc.calculate(
        ...     baseline_rate=0.10,
        ...     mde=0.05,  # 5% relative lift
        ...     alpha=0.05,
        ...     power=0.80,
        ...     daily_traffic=100000,
        ...     max_duration_days=30
        ... )
        >>> print(f"Need {result.required_sample_size:,} users ({result.required_duration_days:.1f} days)")
    """

    def __init__(self):
        """Initialize sample size calculator."""
        pass

    def calculate(
        self,
        baseline_rate: float,
        mde: float,
        alpha: float = 0.05,
        power: float = 0.80,
        allocation_ratio: float = 1.0,
        daily_traffic: Optional[int] = None,
        max_duration_days: Optional[int] = None,
        traffic_allocation: float = 1.0,
        n_metrics: int = 1,
        cost_per_user: Optional[float] = None,
        max_budget: Optional[float] = None
    ) -> SampleSizeResult:
        """
        Calculate sample size with business constraints.

        Args:
            baseline_rate: Baseline conversion rate or mean
            mde: Minimum detectable effect (relative lift, e.g., 0.05 for 5%)
            alpha: Significance level
            power: Desired statistical power
            allocation_ratio: Treatment/control ratio
            daily_traffic: Available daily traffic
            max_duration_days: Maximum experiment duration
            traffic_allocation: Fraction of traffic available (e.g., 0.5 for 50%)
            n_metrics: Number of metrics (for Bonferroni correction)
            cost_per_user: Cost per user included in experiment
            max_budget: Maximum budget

        Returns:
            SampleSizeResult with feasibility assessment
        """
        # Bonferroni correction for multiple metrics
        alpha_adjusted = alpha / n_metrics if n_metrics > 1 else alpha

        # Calculate required sample size
        treatment_rate = baseline_rate * (1 + mde)
        pooled_rate = (baseline_rate + treatment_rate) / 2

        # Z-scores
        z_alpha = stats.norm.ppf(1 - alpha_adjusted / 2)
        z_beta = stats.norm.ppf(power)

        # Sample size per group
        n_per_group = (
            (z_alpha + z_beta)**2 *
            pooled_rate * (1 - pooled_rate) *
            (1 + allocation_ratio) / (allocation_ratio * (treatment_rate - baseline_rate)**2)
        )

        n_per_group = int(np.ceil(n_per_group))
        total_sample_size = n_per_group * (1 + allocation_ratio)

        # Calculate duration
        if daily_traffic:
            effective_daily_traffic = daily_traffic * traffic_allocation
            required_duration = total_sample_size / effective_daily_traffic
        else:
            required_duration = None

        # Check feasibility
        feasibility = 'feasible'
        recommendations = []

        # Duration constraint
        if max_duration_days and required_duration:
            if required_duration > max_duration_days:
                feasibility = 'infeasible'
                max_sample_size = int(max_duration_days * effective_daily_traffic)

                # Calculate achievable power with max duration
                effect_size = (treatment_rate - baseline_rate) / np.sqrt(pooled_rate * (1 - pooled_rate))
                achievable_power = stats.norm.cdf(
                    effect_size * np.sqrt(max_sample_size * allocation_ratio / (1 + allocation_ratio)) - z_alpha
                )

                # Calculate MDE with max duration
                mde_achievable = (
                    (z_alpha + z_beta) *
                    np.sqrt(pooled_rate * (1 - pooled_rate) * (1 + allocation_ratio) / (allocation_ratio * max_sample_size))
                ) / baseline_rate

                recommendations.append(
                    f"Insufficient traffic. Achievable power: {achievable_power:.1%} for MDE={mde:.1%}"
                )
                recommendations.append(
                    f"With max duration ({max_duration_days} days), MDE would be {mde_achievable:.1%}"
                )
                recommendations.append(
                    "Consider: (1) Increase traffic allocation, (2) Extend duration, (3) Accept larger MDE"
                )
            elif required_duration > max_duration_days * 0.8:
                feasibility = 'marginal'
                recommendations.append(
                    f"Experiment duration ({required_duration:.1f} days) is {required_duration/max_duration_days:.0%} of maximum"
                )
                recommendations.append("Consider buffer for delays or extending max duration")

        # Budget constraint
        if cost_per_user and max_budget:
            total_cost = total_sample_size * cost_per_user

            if total_cost > max_budget:
                feasibility = 'infeasible'
                max_sample_size_budget = int(max_budget / cost_per_user)

                recommendations.append(
                    f"Budget insufficient. Need ${total_cost:,.0f}, have ${max_budget:,.0f}"
                )
                recommendations.append(
                    f"With budget, can only afford {max_sample_size_budget:,} users"
                )
                recommendations.append(
                    "Consider: (1) Reduce MDE target, (2) Reduce power to 70%, (3) Increase budget"
                )
        else:
            total_cost = None

        # If feasible, provide optimization recommendations
        if feasibility == 'feasible':
            recommendations.append(f"Experiment is feasible with {required_duration:.1f} days")

            # Check if power is excessive
            if power > 0.85:
                lower_power_n = self._calculate_sample_size_for_power(
                    baseline_rate, mde, alpha_adjusted, 0.80, allocation_ratio
                )
                savings = (n_per_group - lower_power_n) / n_per_group
                recommendations.append(
                    f"Consider reducing power to 80% (save {savings:.0%} sample size)"
                )

            # Check if MDE is too conservative
            if mde < 0.02:
                recommendations.append(
                    "MDE < 2% is very small. Confirm this aligns with business value threshold"
                )

        return SampleSizeResult(
            required_sample_size=int(total_sample_size),
            required_duration_days=required_duration if required_duration else np.nan,
            achievable_power=power if feasibility != 'infeasible' else achievable_power,
            minimum_detectable_effect=mde,
            cost_estimate=total_cost,
            feasibility=feasibility,
            recommendations=recommendations
        )

    def _calculate_sample_size_for_power(
        self,
        baseline_rate: float,
        mde: float,
        alpha: float,
        power: float,
        allocation_ratio: float
    ) -> int:
        """Helper to calculate sample size for given power."""
        treatment_rate = baseline_rate * (1 + mde)
        pooled_rate = (baseline_rate + treatment_rate) / 2

        z_alpha = stats.norm.ppf(1 - alpha / 2)
        z_beta = stats.norm.ppf(power)

        n = (
            (z_alpha + z_beta)**2 *
            pooled_rate * (1 - pooled_rate) *
            (1 + allocation_ratio) / (allocation_ratio * (treatment_rate - baseline_rate)**2)
        )

        return int(np.ceil(n))

    def sensitivity_analysis(
        self,
        baseline_rate: float,
        alpha: float = 0.05,
        allocation_ratio: float = 1.0,
        mde_range: List[float] = None,
        power_range: List[float] = None
    ) -> pd.DataFrame:
        """
        Perform sensitivity analysis across MDE and power combinations.

        Args:
            baseline_rate: Baseline conversion rate
            alpha: Significance level
            allocation_ratio: Treatment/control ratio
            mde_range: List of MDEs to test
            power_range: List of power values to test

        Returns:
            DataFrame with sample sizes for each combination
        """
        if mde_range is None:
            mde_range = [0.01, 0.02, 0.03, 0.05, 0.10]

        if power_range is None:
            power_range = [0.70, 0.80, 0.90]

        results = []

        for mde in mde_range:
            for power in power_range:
                n = self._calculate_sample_size_for_power(
                    baseline_rate, mde, alpha, power, allocation_ratio
                )

                results.append({
                    'mde': mde,
                    'mde_pct': f"{mde:.1%}",
                    'power': power,
                    'power_pct': f"{power:.0%}",
                    'sample_size_per_group': n,
                    'total_sample_size': int(n * (1 + allocation_ratio))
                })

        return pd.DataFrame(results)
\end{lstlisting}

\subsection{Experiment Duration Planning}

\begin{lstlisting}[language=Python, caption={Experiment Duration Planner with Seasonality}]
from datetime import datetime, timedelta
import calendar

@dataclass
class DurationPlan:
    """Experiment duration plan."""
    start_date: datetime
    end_date: datetime
    duration_days: int
    weekly_cycles: float
    seasonal_risk: str  # 'low', 'medium', 'high'
    recommendations: List[str]
    blocked_periods: List[Tuple[datetime, datetime]]

class ExperimentDurationPlanner:
    """
    Plan experiment duration accounting for seasonality and business cycles.

    Example:
        >>> planner = ExperimentDurationPlanner()
        >>> plan = planner.plan_duration(
        ...     required_sample_size=100000,
        ...     daily_traffic=5000,
        ...     start_date='2024-01-15',
        ...     industry='ecommerce'
        ... )
        >>> print(f"Run from {plan.start_date} to {plan.end_date}")
    """

    def __init__(self):
        """Initialize duration planner."""
        # Define high-seasonality periods (US e-commerce calendar)
        self.high_seasonality_periods = [
            ('11-15', '12-31', 'Holiday Shopping'),  # Black Friday through New Year
            ('07-10', '07-20', 'Amazon Prime Day'),
            ('08-15', '09-15', 'Back to School'),
            ('02-01', '02-14', 'Valentine\'s Day'),
            ('05-01', '05-15', 'Mother\'s Day'),
            ('11-01', '11-14', 'Pre-Black Friday')
        ]

    def plan_duration(
        self,
        required_sample_size: int,
        daily_traffic: int,
        start_date: Union[str, datetime],
        traffic_allocation: float = 1.0,
        min_weekly_cycles: int = 2,
        industry: str = 'ecommerce',
        avoid_seasonality: bool = True
    ) -> DurationPlan:
        """
        Plan experiment duration with seasonality considerations.

        Args:
            required_sample_size: Total required sample
            daily_traffic: Available daily traffic
            start_date: Proposed start date
            traffic_allocation: Fraction of traffic allocated
            min_weekly_cycles: Minimum complete weekly cycles
            industry: Industry type (affects seasonality)
            avoid_seasonality: Whether to avoid high-seasonality periods

        Returns:
            DurationPlan with recommended dates
        """
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, '%Y-%m-%d')

        # Calculate base duration
        effective_daily_traffic = daily_traffic * traffic_allocation
        base_duration_days = int(np.ceil(required_sample_size / effective_daily_traffic))

        # Ensure minimum weekly cycles
        min_duration_weekly = min_weekly_cycles * 7
        duration_days = max(base_duration_days, min_duration_weekly)

        # Round to complete weeks
        duration_days = int(np.ceil(duration_days / 7) * 7)

        # Calculate end date
        end_date = start_date + timedelta(days=duration_days)

        # Check for seasonality conflicts
        blocked_periods = []
        seasonal_risk = 'low'
        recommendations = []

        if avoid_seasonality:
            overlaps = self._check_seasonality_overlap(
                start_date, end_date
            )

            if overlaps:
                seasonal_risk = 'high'
                blocked_periods = overlaps

                recommendations.append(
                    f"CAUTION: Experiment overlaps with {len(overlaps)} seasonal period(s)"
                )

                for period_start, period_end, period_name in overlaps:
                    overlap_days = min(end_date, period_end) - max(start_date, period_start)
                    recommendations.append(
                        f"  - {period_name}: {overlap_days.days} day overlap"
                    )

                # Suggest alternative dates
                alternative_start = self._find_non_seasonal_window(
                    start_date, duration_days
                )

                if alternative_start:
                    recommendations.append(
                        f"RECOMMENDATION: Start on {alternative_start.strftime('%Y-%m-%d')} to avoid seasonality"
                    )

        # Check for day-of-week effects
        if duration_days < 14:
            day_of_week = start_date.weekday()
            if day_of_week in [5, 6]:  # Sat or Sun
                recommendations.append(
                    "Starting on weekend may introduce day-of-week bias. Consider Monday start."
                )

        # Weekly cycles
        weekly_cycles = duration_days / 7

        if weekly_cycles < min_weekly_cycles:
            recommendations.append(
                f"Duration ({weekly_cycles:.1f} weeks) is less than recommended {min_weekly_cycles} weeks"
            )

        # Final recommendations
        if seasonal_risk == 'low':
            recommendations.append(
                f"✅ No major seasonality conflicts. Proceed with {start_date.strftime('%Y-%m-%d')} start."
            )

        return DurationPlan(
            start_date=start_date,
            end_date=end_date,
            duration_days=duration_days,
            weekly_cycles=weekly_cycles,
            seasonal_risk=seasonal_risk,
            recommendations=recommendations,
            blocked_periods=blocked_periods
        )

    def _check_seasonality_overlap(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> List[Tuple[datetime, datetime, str]]:
        """Check if experiment overlaps with high-seasonality periods."""
        overlaps = []

        for period_start_str, period_end_str, period_name in self.high_seasonality_periods:
            # Handle year boundary
            period_start = datetime.strptime(
                f"{start_date.year}-{period_start_str}", '%Y-%m-%d'
            )
            period_end = datetime.strptime(
                f"{start_date.year}-{period_end_str}", '%Y-%m-%d'
            )

            # Check overlap
            if not (end_date < period_start or start_date > period_end):
                overlaps.append((period_start, period_end, period_name))

        return overlaps

    def _find_non_seasonal_window(
        self,
        preferred_start: datetime,
        duration_days: int,
        search_window_days: int = 90
    ) -> Optional[datetime]:
        """Find nearest non-seasonal window for experiment."""
        # Search forward and backward from preferred start
        for offset_days in range(0, search_window_days, 7):
            # Try forward
            candidate_start = preferred_start + timedelta(days=offset_days)
            candidate_end = candidate_start + timedelta(days=duration_days)

            if not self._check_seasonality_overlap(candidate_start, candidate_end):
                return candidate_start

            # Try backward
            if offset_days > 0:
                candidate_start = preferred_start - timedelta(days=offset_days)
                candidate_end = candidate_start + timedelta(days=duration_days)

                if not self._check_seasonality_overlap(candidate_start, candidate_end):
                    return candidate_start

        return None  # No non-seasonal window found
\end{lstlisting}

\subsection{Stopping Rules with Futility and Harm Monitoring}

\begin{lstlisting}[language=Python, caption={Stopping Rule Monitor with Safety Boundaries}]
@dataclass
class StoppingDecision:
    """Decision from stopping rule monitor."""
    decision: str  # 'continue', 'stop_efficacy', 'stop_futility', 'stop_harm'
    reason: str
    current_effect: float
    conditional_power: float
    harm_probability: Optional[float]
    recommendation: str

class StoppingRuleMonitor:
    """
    Monitor experiments for early stopping (efficacy, futility, harm).

    Implements:
    - O'Brien-Fleming efficacy boundaries
    - Conditional power futility stopping
    - Harm monitoring with safety boundaries

    Example:
        >>> monitor = StoppingRuleMonitor(
        ...     alpha=0.05,
        ...     power=0.80,
        ...     max_looks=5
        ... )
        >>> decision = monitor.check_stopping(
        ...     control_data=control_values,
        ...     treatment_data=treatment_values,
        ...     information_fraction=0.50,
        ...     safety_metric=adverse_events
        ... )
    """

    def __init__(
        self,
        alpha: float = 0.05,
        power: float = 0.80,
        max_looks: int = 5,
        futility_threshold: float = 0.20,
        harm_threshold: float = 0.95
    ):
        """
        Initialize stopping rule monitor.

        Args:
            alpha: Significance level
            power: Target power
            max_looks: Maximum number of interim analyses
            futility_threshold: Conditional power threshold for futility
            harm_threshold: Probability threshold for harm stopping
        """
        self.alpha = alpha
        self.power = power
        self.max_looks = max_looks
        self.futility_threshold = futility_threshold
        self.harm_threshold = harm_threshold

        # Compute O'Brien-Fleming boundaries
        self.efficacy_boundaries = self._compute_obf_boundaries()

    def _compute_obf_boundaries(self) -> np.ndarray:
        """Compute O'Brien-Fleming efficacy boundaries."""
        looks = np.arange(1, self.max_looks + 1)
        information_fractions = looks / self.max_looks

        # O'Brien-Fleming alpha spending
        z_alpha = stats.norm.ppf(1 - self.alpha / 2)
        boundaries = z_alpha / np.sqrt(information_fractions)

        return boundaries

    def check_stopping(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        information_fraction: float,
        look_number: int,
        safety_metric: Optional[np.ndarray] = None,
        target_effect: Optional[float] = None
    ) -> StoppingDecision:
        """
        Check if experiment should stop.

        Args:
            control_data: Control group outcomes
            treatment_data: Treatment group outcomes
            information_fraction: Fraction of planned sample collected
            look_number: Current interim analysis number (1-indexed)
            safety_metric: Optional safety/harm metric
            target_effect: Target effect size for power calculation

        Returns:
            StoppingDecision with recommendation
        """
        # Compute current effect and test statistic
        mean_control = np.mean(control_data)
        mean_treatment = np.mean(treatment_data)
        effect = mean_treatment - mean_control

        # Pooled standard error
        var_control = np.var(control_data, ddof=1)
        var_treatment = np.var(treatment_data, ddof=1)
        n_control = len(control_data)
        n_treatment = len(treatment_data)

        se = np.sqrt(var_control / n_control + var_treatment / n_treatment)
        z_stat = effect / se if se > 0 else 0

        # Check efficacy boundary
        if look_number <= len(self.efficacy_boundaries):
            efficacy_boundary = self.efficacy_boundaries[look_number - 1]

            if abs(z_stat) >= efficacy_boundary:
                return StoppingDecision(
                    decision='stop_efficacy',
                    reason=f"Crossed efficacy boundary (|Z|={abs(z_stat):.2f} >= {efficacy_boundary:.2f})",
                    current_effect=effect,
                    conditional_power=1.0,
                    harm_probability=None,
                    recommendation=f"STOP FOR EFFICACY: Effect = {effect:.4f} is statistically significant"
                )

        # Conditional power calculation for futility
        if target_effect is not None:
            conditional_power = self._compute_conditional_power(
                current_effect=effect,
                current_se=se,
                target_effect=target_effect,
                information_fraction=information_fraction
            )

            if conditional_power < self.futility_threshold:
                return StoppingDecision(
                    decision='stop_futility',
                    reason=f"Conditional power ({conditional_power:.1%}) below threshold ({self.futility_threshold:.0%})",
                    current_effect=effect,
                    conditional_power=conditional_power,
                    harm_probability=None,
                    recommendation=f"STOP FOR FUTILITY: Unlikely to detect target effect"
                )

        # Harm monitoring
        if safety_metric is not None:
            harm_prob = self._compute_harm_probability(
                control_data, treatment_data, safety_metric
            )

            if harm_prob >= self.harm_threshold:
                return StoppingDecision(
                    decision='stop_harm',
                    reason=f"High probability of harm ({harm_prob:.1%})",
                    current_effect=effect,
                    conditional_power=conditional_power if target_effect else None,
                    harm_probability=harm_prob,
                    recommendation=f"STOP FOR SAFETY: Treatment appears harmful"
                )

        # Continue experiment
        return StoppingDecision(
            decision='continue',
            reason=f"No stopping criteria met at look {look_number}",
            current_effect=effect,
            conditional_power=conditional_power if target_effect else None,
            harm_probability=harm_prob if safety_metric is not None else None,
            recommendation=f"CONTINUE: Collect more data (at {information_fraction:.0%} of planned sample)"
        )

    def _compute_conditional_power(
        self,
        current_effect: float,
        current_se: float,
        target_effect: float,
        information_fraction: float
    ) -> float:
        """
        Compute conditional power to detect target effect.

        Given current estimate, what is probability of eventual significance?
        """
        # Standard error at final sample size
        final_se = current_se * np.sqrt(information_fraction)

        # Non-centrality parameter under target effect
        ncp = target_effect / final_se

        # Critical value at final analysis
        z_critical = stats.norm.ppf(1 - self.alpha / 2)

        # Conditional power
        conditional_power = 1 - stats.norm.cdf(z_critical - ncp)

        return conditional_power

    def _compute_harm_probability(
        self,
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        safety_metric: np.ndarray
    ) -> float:
        """
        Compute probability that treatment is harmful.

        Uses Bayesian posterior probability.
        """
        # Simple implementation: bootstrap-based
        n_bootstrap = 1000
        harm_count = 0

        for _ in range(n_bootstrap):
            # Resample
            control_sample = np.random.choice(control_data, size=len(control_data), replace=True)
            treatment_sample = np.random.choice(treatment_data, size=len(treatment_data), replace=True)

            # Check if treatment worse than control
            if np.mean(treatment_sample) < np.mean(control_sample):
                harm_count += 1

        return harm_count / n_bootstrap
\end{lstlisting}

\subsection{Post-Experiment Analysis}

\begin{lstlisting}[language=Python, caption={Comprehensive Post-Experiment Analyzer}]
@dataclass
class PostExperimentReport:
    """Comprehensive post-experiment analysis report."""
    primary_results: Dict[str, Any]
    heterogeneity_analysis: pd.DataFrame
    covariate_balance: pd.DataFrame
    novelty_check: Dict[str, Any]
    long_term_projection: Dict[str, float]
    deployment_recommendation: str
    confidence_level: str  # 'high', 'medium', 'low'
    caveats: List[str]

class PostExperimentAnalyzer:
    """
    Comprehensive post-experiment analysis with proper interpretation.

    Goes beyond p-values to assess:
    - Heterogeneous treatment effects
    - Covariate balance validation
    - Novelty effect detection
    - Long-term projections

    Example:
        >>> analyzer = PostExperimentAnalyzer()
        >>> report = analyzer.analyze(
        ...     data=experiment_df,
        ...     outcome_col='revenue',
        ...     treatment_col='variant',
        ...     segment_cols=['user_tier', 'region'],
        ...     time_col='date'
        ... )
    """

    def __init__(self, alpha: float = 0.05):
        """
        Initialize post-experiment analyzer.

        Args:
            alpha: Significance level
        """
        self.alpha = alpha

    def analyze(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str,
        segment_cols: Optional[List[str]] = None,
        time_col: Optional[str] = None,
        covariates: Optional[List[str]] = None
    ) -> PostExperimentReport:
        """
        Perform comprehensive post-experiment analysis.

        Args:
            data: Experiment data
            outcome_col: Outcome metric
            treatment_col: Treatment indicator
            segment_cols: Columns for heterogeneity analysis
            time_col: Time column for novelty check
            covariates: Covariates for balance check

        Returns:
            PostExperimentReport with full analysis
        """
        # Primary analysis
        primary_results = self._primary_analysis(
            data, outcome_col, treatment_col
        )

        # Heterogeneity analysis
        if segment_cols:
            heterogeneity = self._heterogeneity_analysis(
                data, outcome_col, treatment_col, segment_cols
            )
        else:
            heterogeneity = pd.DataFrame()

        # Covariate balance
        if covariates:
            balance = self._check_covariate_balance(
                data, treatment_col, covariates
            )
        else:
            balance = pd.DataFrame()

        # Novelty check
        if time_col:
            novelty = self._check_novelty_effect(
                data, outcome_col, treatment_col, time_col
            )
        else:
            novelty = {}

        # Long-term projection
        long_term = self._project_long_term_effect(
            primary_results, novelty
        )

        # Generate recommendation
        recommendation, confidence, caveats = self._generate_recommendation(
            primary_results, heterogeneity, balance, novelty
        )

        return PostExperimentReport(
            primary_results=primary_results,
            heterogeneity_analysis=heterogeneity,
            covariate_balance=balance,
            novelty_check=novelty,
            long_term_projection=long_term,
            deployment_recommendation=recommendation,
            confidence_level=confidence,
            caveats=caveats
        )

    def _primary_analysis(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str
    ) -> Dict:
        """Primary treatment effect analysis."""
        control = data[data[treatment_col] == 0][outcome_col]
        treatment = data[data[treatment_col] == 1][outcome_col]

        # Two-sample t-test
        t_stat, p_value = stats.ttest_ind(treatment, control)

        # Effect size
        mean_control = control.mean()
        mean_treatment = treatment.mean()
        effect = mean_treatment - mean_control
        relative_lift = effect / mean_control if mean_control != 0 else np.nan

        # Confidence interval
        se = np.sqrt(
            control.var(ddof=1) / len(control) +
            treatment.var(ddof=1) / len(treatment)
        )
        t_critical = stats.t.ppf(1 - self.alpha/2, len(control) + len(treatment) - 2)
        ci_lower = effect - t_critical * se
        ci_upper = effect + t_critical * se

        return {
            'mean_control': mean_control,
            'mean_treatment': mean_treatment,
            'absolute_effect': effect,
            'relative_lift': relative_lift,
            'p_value': p_value,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'significant': p_value < self.alpha
        }

    def _heterogeneity_analysis(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str,
        segment_cols: List[str]
    ) -> pd.DataFrame:
        """Analyze treatment effect heterogeneity across segments."""
        results = []

        for segment_col in segment_cols:
            for segment_value in data[segment_col].unique():
                segment_data = data[data[segment_col] == segment_value]

                control = segment_data[segment_data[treatment_col] == 0][outcome_col]
                treatment = segment_data[segment_data[treatment_col] == 1][outcome_col]

                if len(control) < 30 or len(treatment) < 30:
                    continue  # Skip small segments

                t_stat, p_value = stats.ttest_ind(treatment, control)
                effect = treatment.mean() - control.mean()
                relative_lift = effect / control.mean() if control.mean() != 0 else np.nan

                results.append({
                    'segment_col': segment_col,
                    'segment_value': segment_value,
                    'n_control': len(control),
                    'n_treatment': len(treatment),
                    'effect': effect,
                    'relative_lift': relative_lift,
                    'p_value': p_value
                })

        return pd.DataFrame(results)

    def _check_covariate_balance(
        self,
        data: pd.DataFrame,
        treatment_col: str,
        covariates: List[str]
    ) -> pd.DataFrame:
        """Check balance of covariates between treatment and control."""
        balance_results = []

        for covariate in covariates:
            control = data[data[treatment_col] == 0][covariate]
            treatment = data[data[treatment_col] == 1][covariate]

            # Standardized mean difference
            smd = (treatment.mean() - control.mean()) / np.sqrt(
                (control.var() + treatment.var()) / 2
            )

            # T-test
            t_stat, p_value = stats.ttest_ind(treatment, control)

            balance_results.append({
                'covariate': covariate,
                'mean_control': control.mean(),
                'mean_treatment': treatment.mean(),
                'smd': smd,
                'p_value': p_value,
                'balanced': abs(smd) < 0.1  # Standard threshold
            })

        return pd.DataFrame(balance_results)

    def _check_novelty_effect(
        self,
        data: pd.DataFrame,
        outcome_col: str,
        treatment_col: str,
        time_col: str
    ) -> Dict:
        """Check for novelty effect decay over time."""
        data = data.copy()
        data[time_col] = pd.to_datetime(data[time_col])

        # Split into early and late periods
        median_time = data[time_col].median()
        early = data[data[time_col] <= median_time]
        late = data[data[time_col] > median_time]

        # Effect in early period
        control_early = early[early[treatment_col] == 0][outcome_col]
        treatment_early = early[early[treatment_col] == 1][outcome_col]
        effect_early = treatment_early.mean() - control_early.mean()

        # Effect in late period
        control_late = late[late[treatment_col] == 0][outcome_col]
        treatment_late = late[late[treatment_col] == 1][outcome_col]
        effect_late = treatment_late.mean() - control_late.mean()

        # Novelty decay
        decay = (effect_early - effect_late) / effect_early if effect_early != 0 else 0

        return {
            'effect_early': effect_early,
            'effect_late': effect_late,
            'decay_pct': decay,
            'has_novelty_effect': decay > 0.20  # 20% decay threshold
        }

    def _project_long_term_effect(
        self,
        primary_results: Dict,
        novelty_check: Dict
    ) -> Dict:
        """Project long-term effect accounting for novelty decay."""
        if novelty_check and novelty_check.get('has_novelty_effect'):
            # Assume linear decay continues
            short_term_effect = primary_results['absolute_effect']
            decay_rate = novelty_check['decay_pct']

            # Project 3-month and 6-month effects
            effect_3mo = short_term_effect * (1 - decay_rate * 1.5)
            effect_6mo = short_term_effect * (1 - decay_rate * 2.0)

            return {
                'short_term': short_term_effect,
                'projected_3mo': max(effect_3mo, 0),  # Floor at 0
                'projected_6mo': max(effect_6mo, 0),
                'decay_assumed': decay_rate
            }
        else:
            # No novelty effect, use current estimate
            effect = primary_results['absolute_effect']
            return {
                'short_term': effect,
                'projected_3mo': effect,
                'projected_6mo': effect,
                'decay_assumed': 0
            }

    def _generate_recommendation(
        self,
        primary_results: Dict,
        heterogeneity: pd.DataFrame,
        balance: pd.DataFrame,
        novelty: Dict
    ) -> Tuple[str, str, List[str]]:
        """Generate deployment recommendation with confidence level."""
        caveats = []
        confidence = 'high'

        # Check statistical significance
        if not primary_results['significant']:
            recommendation = "DO NOT DEPLOY"
            confidence = 'high'
            caveats.append("Effect not statistically significant")
            return recommendation, confidence, caveats

        # Check effect magnitude
        if abs(primary_results['relative_lift']) < 0.01:
            caveats.append("Effect is very small (<1%) - verify business value")
            confidence = 'medium'

        # Check covariate balance
        if not balance.empty:
            imbalanced = balance[~balance['balanced']]
            if len(imbalanced) > 0:
                caveats.append(f"Covariate imbalance detected in {len(imbalanced)} variable(s)")
                confidence = 'low'

        # Check heterogeneity
        if not heterogeneity.empty:
            # Check for negative effects in any segment
            negative_segments = heterogeneity[heterogeneity['effect'] < 0]
            if len(negative_segments) > 0:
                caveats.append(
                    f"Negative effects in {len(negative_segments)} segment(s) - consider targeted deployment"
                )
                confidence = 'medium'

        # Check novelty effect
        if novelty and novelty.get('has_novelty_effect'):
            caveats.append(
                f"Novelty effect detected ({novelty['decay_pct']:.0%} decay) - long-term impact may be lower"
            )
            confidence = 'medium'

        # Final recommendation
        if confidence == 'high':
            recommendation = "DEPLOY"
        elif confidence == 'medium':
            recommendation = "DEPLOY WITH MONITORING"
        else:
            recommendation = "DEPLOY WITH CAUTION or RE-TEST"

        return recommendation, confidence, caveats
\end{lstlisting}

These practical tools address real-world challenges that practitioners face daily. The seasonal confusion scenario demonstrates how even statistically significant results can be misleading without proper experimental design, and the implementation classes provide production-ready solutions for common challenges.

\section{Real-World Scenario: A/B Test Misinterpretation}

\subsection{The Problem}

An e-commerce company ran an A/B test comparing two recommendation models:
\begin{itemize}
    \item \textbf{Control}: Collaborative filtering (10\% CTR)
    \item \textbf{Treatment}: Deep learning model (10.5\% CTR)
\end{itemize}

After 3 days with 50,000 users, the treatment showed 5\% CTR improvement (p=0.03). The team declared victory and deployed to 100\% traffic.

\textbf{Two weeks later}: Revenue dropped 12\%, and investigations revealed:
\begin{itemize}
    \item Test was underpowered (needed 80K users per arm)
    \item Stopped early without sequential testing correction
    \item Didn't account for multiple metrics (CTR, revenue, engagement)
    \item Ignored 25\% drop in recommendation diversity
    \item Weekend traffic spike created temporary effect
\end{itemize}

\textbf{Cost}: \$1.5M in lost revenue, 3 weeks to rollback and redesign.

\subsection{The Solution}

Proper experimental design would have prevented this:

\begin{lstlisting}[language=Python, caption={Complete A/B Test Implementation}]
# 1. Power analysis before starting
analyzer = StatisticalPowerAnalyzer(alpha=0.05, power=0.80)

power_result = analyzer.calculate_sample_size(
    metric_type=MetricType.PROPORTION,
    baseline_value=0.10,  # 10% CTR
    mde=0.005  # Want to detect 0.5pp (5% relative lift)
)

print(f"Required sample size: {power_result.sample_size_per_arm:,} per arm")
# Output: Required sample size: 76,200 per arm

# Adjust for multiple metrics (Bonferroni)
n_metrics = 3  # CTR, revenue, diversity
adjusted_analyzer = StatisticalPowerAnalyzer(
    alpha=0.05 / n_metrics,
    power=0.80
)

adjusted_result = adjusted_analyzer.calculate_sample_size(
    metric_type=MetricType.PROPORTION,
    baseline_value=0.10,
    mde=0.005
)

print(f"Adjusted for {n_metrics} metrics: {adjusted_result.sample_size_per_arm:,} per arm")
# Output: Adjusted for 3 metrics: 101,450 per arm

# 2. Proper randomization with balance validation
config = ExperimentConfig(
    name="recommendation_model_test",
    arms=[
        TreatmentArm("control", 0.5, {"model": "collaborative_filtering"}),
        TreatmentArm("treatment", 0.5, {"model": "deep_learning"})
    ],
    randomization_method=RandomizationMethod.STRATIFIED,
    stratification_vars=["country", "user_segment", "platform"],
    min_sample_size=adjusted_result.sample_size_per_arm
)

design = ExperimentDesign(config)
assignments = design.assign_treatments(users_df)

# Validate balance
balance = design.validate_balance(
    users_df,
    assignments,
    covariates=["age", "tenure", "past_purchases", "country"]
)

if not balance['overall']['all_balanced']:
    raise ValueError("Imbalance detected - check randomization")

# 3. A/A test first to validate infrastructure
aa_validator = AATestValidator()

# Run A/A test with identical models
aa_result = aa_validator.run_aa_test(
    control_data=aa_control_ctr,
    treatment_data=aa_treatment_ctr,
    metric_name="CTR"
)

if not aa_result['valid']:
    raise ValueError("A/A test failed - fix infrastructure before A/B test")

# 4. Sequential testing for early stopping
sequential_test = SequentialTest(
    alpha=0.05 / n_metrics,  # Bonferroni correction
    power=0.80,
    n_looks=5,
    spending_function="obrien_fleming"
)

# Run test with periodic looks
for week in range(1, 6):
    # Collect data
    control_data = get_data(arm="control", week=week)
    treatment_data = get_data(arm="treatment", week=week)

    # Analyze
    result = sequential_test.analyze(
        control_data['ctr'],
        treatment_data['ctr']
    )

    print(f"Week {week}: {result['decision']}")

    if result['decision'] != 'continue':
        break

# 5. Multiple metric analysis with correction
class ExperimentAnalyzer:
    """Analyze multiple metrics with proper corrections."""

    def __init__(self, alpha: float = 0.05):
        self.alpha = alpha

    def analyze_metrics(
        self,
        control_data: pd.DataFrame,
        treatment_data: pd.DataFrame,
        metrics: List[str]
    ) -> Dict[str, Dict]:
        """Analyze multiple metrics with Bonferroni correction."""
        n_metrics = len(metrics)
        adjusted_alpha = self.alpha / n_metrics

        results = {}

        for metric in metrics:
            if pd.api.types.is_numeric_dtype(control_data[metric]):
                stat, p_value = stats.ttest_ind(
                    control_data[metric].dropna(),
                    treatment_data[metric].dropna()
                )

                effect = (
                    treatment_data[metric].mean()
                    - control_data[metric].mean()
                )
                relative_effect = effect / control_data[metric].mean()
            else:
                # Chi-square for categorical
                contingency = pd.crosstab(
                    pd.concat([
                        pd.Series(["control"] * len(control_data)),
                        pd.Series(["treatment"] * len(treatment_data))
                    ]),
                    pd.concat([control_data[metric], treatment_data[metric]])
                )
                stat, p_value, _, _ = stats.chi2_contingency(contingency)
                effect = None
                relative_effect = None

            results[metric] = {
                'p_value': p_value,
                'adjusted_alpha': adjusted_alpha,
                'significant': p_value < adjusted_alpha,
                'effect': effect,
                'relative_effect': relative_effect,
                'control_mean': control_data[metric].mean(),
                'treatment_mean': treatment_data[metric].mean()
            }

        return results

analyzer = ExperimentAnalyzer(alpha=0.05)

final_results = analyzer.analyze_metrics(
    control_data,
    treatment_data,
    metrics=['ctr', 'revenue_per_user', 'recommendation_diversity']
)

# Check all metrics
for metric, result in final_results.items():
    print(f"{metric}:")
    print(f"  Control: {result['control_mean']:.4f}")
    print(f"  Treatment: {result['treatment_mean']:.4f}")
    print(f"  Effect: {result['relative_effect']:.2%}")
    print(f"  P-value: {result['p_value']:.4f}")
    print(f"  Significant: {result['significant']}")

# Decision
all_metrics_positive = all(
    result['relative_effect'] > 0
    for result in final_results.values()
    if result['relative_effect'] is not None
)

primary_significant = final_results['ctr']['significant']

if primary_significant and all_metrics_positive:
    print("SHIP IT: Primary metric significant, all metrics positive")
else:
    print("DO NOT SHIP: Either not significant or negative secondary metrics")
\end{lstlisting}

\subsection{Outcome}

With proper methodology:
\begin{itemize}
    \item Ran test for 4 weeks (sufficient power)
    \item Detected diversity drop in Week 2
    \item Modified model to preserve diversity
    \item Re-ran test with improved model
    \item Final launch improved CTR by 4\% and revenue by 7\%
\end{itemize}

\section{Exercises}

\subsection{Exercise 1: Stratified Randomization}

Implement stratified randomization for a multi-country experiment. Ensure balance within each country and overall. Compare balance with simple randomization.

\subsection{Exercise 2: Power Analysis Sensitivity}

Conduct sensitivity analysis showing how sample size changes with:
\begin{itemize}
    \item Different MDE values (1\%, 3\%, 5\%, 10\%)
    \item Different baseline rates (5\%, 10\%, 20\%)
    \item Different power levels (70\%, 80\%, 90\%)
    \item Multiple comparison corrections (2, 5, 10 metrics)
\end{itemize}

Create visualization showing trade-offs.

\subsection{Exercise 3: Bandit Simulation}

Simulate a 4-arm bandit problem with true conversion rates [0.08, 0.09, 0.10, 0.12]. Compare Thompson Sampling, UCB, and Epsilon-Greedy over 5000 iterations. Measure cumulative regret and convergence speed.

\subsection{Exercise 4: A/A Test Infrastructure}

Build A/A testing infrastructure that:
\begin{itemize}
    \item Runs continuous A/A tests in production
    \item Estimates false positive rate
    \item Detects infrastructure degradation
    \item Alerts when FPR exceeds expected
\end{itemize}

\subsection{Exercise 5: Network Effects}

Design a cluster randomization strategy for a social network where users influence each other. Implement graph-based clustering and validate that interference is minimized.

\subsection{Exercise 6: Sequential Testing Simulation}

Simulate sequential testing with different stopping rules. Compare:
\begin{itemize}
    \item Fixed horizon (no early stopping)
    \item Pocock boundary
    \item O'Brien-Fleming boundary
    \item Alpha spending approach
\end{itemize}

Measure Type I error rate, power, and average sample size under null and alternative hypotheses.

\subsection{Exercise 7: Multi-Metric Decision Framework}

Build a framework that:
\begin{itemize}
    \item Tests multiple metrics (guardrail, primary, secondary)
    \item Applies appropriate corrections
    \item Handles directional hypotheses
    \item Provides clear ship/no-ship decision
    \item Generates stakeholder report
\end{itemize}

\section{Key Takeaways}

\subsection{Choosing the Right Experimental Approach}

The choice of experimental methodology depends on your specific context:

\begin{itemize}
    \item \textbf{Traditional A/B Testing}: Use when you need definitive causal evidence with controlled error rates, have sufficient time and traffic, and regulatory/compliance requires fixed-sample designs (e.g., medical devices, financial products)

    \item \textbf{Multi-Armed Bandits}: Prefer when opportunity cost of exploration is high, you can tolerate adaptive allocation, and want to minimize regret while learning (e.g., content recommendation, pricing optimization). Expect 40-70\% regret reduction vs fixed A/B tests

    \item \textbf{Sequential Testing}: Deploy when business pressure requires early stopping, you have continuous monitoring capabilities, and can implement proper alpha spending functions. Typical savings: 30-50\% reduction in experiment duration

    \item \textbf{Factorial Designs}: Essential when testing multiple features simultaneously, interaction effects are plausible, and you want to reduce total experiment time. Can reveal synergies worth millions that sequential testing misses

    \item \textbf{Network-Aware Methods}: Mandatory when SUTVA is violated (social networks, marketplaces, collaboration tools). Ignoring spillover can bias estimates by 20-50\% and cause deployment disasters

    \item \textbf{Business-Metric Framework}: Critical for ML products where statistical significance doesn't guarantee business value. NPV analysis can reverse decisions that appear statistically significant
\end{itemize}

\subsection{Essential Best Practices}

\textbf{1. Planning Phase}
\begin{itemize}
    \item Conduct power analysis \textit{before} launching experiments—underpowered tests waste resources and produce inconclusive results
    \item Define success metrics and minimum detectable effects (MDE) aligned with business impact thresholds
    \item Calculate required sample sizes with Bonferroni correction when testing multiple metrics simultaneously
    \item Document pre-registered analysis plans to prevent p-hacking and HARKing (Hypothesizing After Results are Known)
    \item Budget for 2-3x minimum sample size to account for dropouts, segment analysis, and unexpected variance
\end{itemize}

\textbf{2. Randomization and Balance}
\begin{itemize}
    \item Use stratified randomization for important covariates (user segment, country, device type) to improve precision
    \item Validate balance across treatment arms using standardized mean differences (SMD $<$ 0.1 acceptable)
    \item Run A/A tests to validate infrastructure before launching real experiments—catches 60-80\% of implementation bugs
    \item Check for systematic assignment bias (time-of-day effects, day-of-week patterns, delayed activation)
    \item For network experiments, use graph cluster randomization with modularity $>$ 0.3 to minimize spillover
\end{itemize}

\textbf{3. Multiple Testing Correction}
\begin{itemize}
    \item Apply Bonferroni correction: $\alpha_{adjusted} = \alpha / n_{tests}$ for family-wise error rate control
    \item Alternative: Benjamini-Hochberg procedure for false discovery rate (FDR) control when testing many hypotheses
    \item For sequential testing, use O'Brien-Fleming boundaries (conservative early, aggressive late) or Pocock (uniform)
    \item Never "peek" at results without sequential testing correction—10 peeks inflate Type I error to 40\%
    \item Distinguish between primary metrics (hypothesis-driven) and secondary metrics (exploratory)
\end{itemize}

\textbf{4. Handling Complexity}
\begin{itemize}
    \item Estimate ICCs before cluster randomization: design effect = $1 + (m-1)\rho$ inflates required sample size
    \item Use fractional factorial designs ($2^{k-p}$) when full factorial is infeasible, but maintain resolution IV or higher
    \item Decompose effects: direct effects, spillover effects, and total effects in network settings
    \item Account for delayed effects using survival analysis (Kaplan-Meier, Cox models) rather than fixed-window metrics
    \item Segment analysis should be pre-specified; post-hoc segmentation requires Bonferroni correction
\end{itemize}

\textbf{5. Business Integration}
\begin{itemize}
    \item Calculate NPV, not just statistical significance: $\text{NPV} = \sum_{t=0}^{T} \frac{R_t - C_t}{(1+r)^t}$ including churn effects
    \item Define multi-objective constraints (e.g., "revenue must not decrease" while optimizing engagement)
    \item Use weighted utility functions when trading off metrics: $U = \sum w_k f_k$ with stakeholder-aligned weights
    \item Conduct segment-specific ROI analysis—blended deployment often dominates uniform rollout
    \item Monitor long-term effects: 30-40\% of short-term wins reverse after 3+ months due to novelty effects
\end{itemize}

\subsection{Common Pitfalls and How to Avoid Them}

\begin{enumerate}
    \item \textbf{The Peeking Problem}
    \begin{itemize}
        \item \textit{Mistake}: Checking p-values repeatedly until significant
        \item \textit{Impact}: Type I error inflates from 5\% to 30-50\% with frequent peeking
        \item \textit{Solution}: Use sequential testing with alpha spending or commit to single analysis at pre-planned sample size
    \end{itemize}

    \item \textbf{Ignoring Interaction Effects}
    \begin{itemize}
        \item \textit{Mistake}: Testing features sequentially instead of factorially
        \item \textit{Impact}: Miss synergies—the video platform missed \$2.5M from 10-minute interaction effect
        \item \textit{Solution}: Use $2^k$ or fractional factorial designs when testing $\geq 2$ features
    \end{itemize}

    \item \textbf{Network Interference Bias}
    \begin{itemize}
        \item \textit{Mistake}: Individual randomization in networked products (social networks, marketplaces)
        \item \textit{Impact}: 20-50\% bias in effect estimates; deployment disasters (expected +7 min, got +5 min)
        \item \textit{Solution}: Graph cluster randomization or ego-network designs with spillover measurement
    \end{itemize}

    \item \textbf{Underpowered Tests}
    \begin{itemize}
        \item \textit{Mistake}: Launching without power analysis or stopping early without sequential methods
        \item \textit{Impact}: 50-70\% of underpowered tests incorrectly fail to reject null hypothesis
        \item \textit{Solution}: Calculate required $n$ for 80\% power before launch; monitor conditional power during test
    \end{itemize}

    \item \textbf{Metric Misalignment}
    \begin{itemize}
        \item \textit{Mistake}: Optimizing proxy metrics without validating business impact
        \item \textit{Impact}: The music platform had +28\% conversion but -\$409K NPV due to retention drop
        \item \textit{Solution}: Include guardrail metrics; calculate ROI/NPV before deployment decisions
    \end{itemize}

    \item \textbf{Simpson's Paradox}
    \begin{itemize}
        \item \textit{Mistake}: Aggregating across heterogeneous segments without checking subgroup effects
        \item \textit{Impact}: Overall positive effect masks negative impact on high-value segments
        \item \textit{Solution}: Pre-specify important segments; test for heterogeneous treatment effects (HTE)
    \end{itemize}

    \item \textbf{Novelty and Primacy Effects}
    \begin{itemize}
        \item \textit{Mistake}: Short experiments that capture temporary excitement or learning curves
        \item \textit{Impact}: 30-40\% of 1-week winners become losers after 4+ weeks
        \item \textit{Solution}: Run experiments long enough to capture steady-state behavior; use survival analysis
    \end{itemize}
\end{enumerate}

\subsection{Implementation Recommendations}

\textbf{Minimum Viable Experimentation Platform}
\begin{enumerate}
    \item \textbf{Randomization Layer}: Deterministic, user-level assignment with stratification support
    \item \textbf{Metrics Pipeline}: Real-time computation of primary metrics with confidence intervals
    \item \textbf{Balance Validation}: Automated covariate balance checks on experiment launch
    \item \textbf{A/A Testing}: Continuous A/A tests to validate infrastructure integrity
    \item \textbf{Power Calculator}: Interactive tools for sample size planning with MDE inputs
\end{enumerate}

\textbf{Advanced Capabilities} (implement as you scale)
\begin{enumerate}
    \item \textbf{Sequential Testing}: O'Brien-Fleming boundaries with conditional power monitoring
    \item \textbf{Bandit Algorithms}: Thompson Sampling or UCB for high-opportunity-cost scenarios
    \item \textbf{Multi-Metric Dashboards}: Scorecard views with Bonferroni-adjusted p-values
    \item \textbf{Heterogeneity Detection}: Automated HTE analysis across key segments
    \item \textbf{Network Detection}: Graph-based interference tests for spillover identification
    \item \textbf{ROI Calculator}: NPV computation with LTV modeling and segment-specific deployment
\end{enumerate}

\textbf{Organizational Integration}
\begin{itemize}
    \item Establish \textbf{experiment review boards} for high-risk tests (>10\% traffic, core product changes)
    \item Create \textbf{pre-registration templates} requiring hypothesis, success metrics, MDE, and sample size justification
    \item Build \textbf{experiment knowledge bases} documenting past tests, effect sizes, and lessons learned
    \item Implement \textbf{guardrail metrics} that automatically halt experiments violating business constraints
    \item Foster \textbf{statistical literacy}: train PMs and engineers on p-values, confidence intervals, and common pitfalls
\end{itemize}

\subsection{Complex Real-World Scenarios}

The following scenarios illustrate critical experimental challenges that frequently arise in production systems. Each represents patterns that cost organizations millions when mishandled.

\subsubsection{Scenario 1: The Simpson's Paradox Surprise}

\textbf{Context:} E-commerce site tests new checkout flow. Overall results show treatment underperforms control by 2pp conversion rate (p<0.001, N=50,000 per arm). Leadership prepares to kill the feature. Data scientist notices mobile traffic comprises 70\% of control, 30\% of treatment due to randomization bug. Segment analysis reveals treatment wins on both mobile (+3pp) and desktop (+4pp), but loses overall due to composition differences. Simpson's paradox strikes. Proper stratified randomization deployed; treatment wins overall (+3.2pp, \$8M annual revenue). Lesson: always validate covariate balance and examine segment-level effects before aggregating.

\subsubsection{Scenario 2: The Network Effect Nightmare}

\textbf{Context:} Social network tests viral sharing feature. Standard A/B test shows +15\% engagement (p<0.001). Post-deployment, engagement increases only 3\%. Root cause: treatment users shared content with control users, artificially inflating control metrics during test (positive spillover). SUTVA violated. Network interference biased estimate downward by 80\%. Cluster randomization at friend-group level shows true effect: +28\% engagement, but requires 5$\times$ sample size. Alternative: ego-cluster design randomizing focal users while measuring outcomes only for their untreated neighbors. Lesson: social products require network-aware experimental designs; standard A/B tests produce biased estimates when users interact.

\subsubsection{Scenario 3: The Long-term Impact Mystery}

\textbf{Context:} Subscription service tests aggressive discount (50\% off first month). Two-week experiment shows +40\% conversion (p<0.001). Finance projects \$12M annual revenue. Six months post-deployment: revenue down \$8M. Why? Short-term test captured immediate conversions but missed long-term effects. Discounted users had 60\% lower LTV (price-sensitive cohort, 3$\times$ churn rate). Novelty effect inflated early conversions; effect decayed to +5\% by week 8. Solution: survival analysis showing hazard ratios, holdout groups measured for 6+ months, cohort-based LTV modeling. Decision: deploy only to high-intent segments (cart abandoners), avoiding broadcast discounts. Lesson: optimize long-term value, not short-term vanity metrics.

\subsubsection{Scenario 4: The Multiple Testing Trap}

\textbf{Context:} Product team runs experiment across 5 regions, 4 platforms, 3 user segments, tracking 8 metrics. Analyst finds significant win: iOS power users in Germany show +12\% revenue (p=0.03). Team celebrates and scales feature to all iOS users. Revenue drops 4\% globally. What happened? With 5$\times$4$\times$3$\times$8 = 480 comparisons, expect 24 false positives at $\alpha=0.05$. Germany iOS power users (0.3\% of user base) was cherry-picked from data mining without multiple testing correction. Proper approach: Bonferroni correction ($\alpha=0.05/480=0.0001$), hierarchical testing (test overall, then drill down only if significant), or pre-registration of hypotheses. Lesson: every data slice you examine inflates false positive rate; correct for multiple comparisons.

\subsubsection{Scenario 5: The Interference Incident}

\textbf{Context:} Ride-sharing platform tests driver incentive (surge bonus). City-level randomization: treatment cities offer +20\% surge pay. Results show +5\% driver supply (p=0.08, inconclusive). But treatment cities share borders with control cities. Drivers migrate from control to treatment cities during surge periods, depleting control supply and inflating treatment metrics. Spatial interference violated randomization integrity. Reanalysis using buffer zones (exclude drivers within 50 miles of treatment boundaries) shows true effect: +18\% supply increase (p<0.001). Implementation: staggered rollout with geographic buffers, measuring spillover explicitly. Lesson: when treatment can physically or digitally "leak" across units, use spatial/temporal buffers or measure interference directly.

\subsection{Comprehensive Practice Exercises}

\subsubsection{Exercise 1: Implement Thompson Sampling Bandit}

Build a Thompson Sampling algorithm for Bernoulli bandits. Initialize Beta(1,1) priors for K arms. At each trial: sample from posteriors, select arm with highest sample, observe reward, update posterior. Compare regret to epsilon-greedy and UCB baselines over 10,000 trials. Implement probability of best arm calculation.

\subsubsection{Exercise 2: Build Sequential Testing with Early Stopping}

Implement group sequential design with O'Brien-Fleming boundaries. Pre-specify K=5 looks at information fractions [0.2, 0.4, 0.6, 0.8, 1.0]. Compute critical values using alpha spending function. Simulate experiment with true effect size d=0.3; show early stopping saves 40\% sample size while maintaining $\alpha=0.05$.

\subsubsection{Exercise 3: Create Bayesian A/B Test Framework}

Build Bayesian A/B test for conversion rates using Beta-Binomial conjugacy. Implement posterior probability of superiority, expected loss, and credible intervals. Compare sensitivity to different priors: uniform Beta(1,1), skeptical Beta(10,10), informative Beta(30,70). Show prior becomes irrelevant with sufficient data (N>10,000).

\subsubsection{Exercise 4: Design Factorial Experiment with Interactions}

Create $2^3$ factorial design testing email subject line, sender name, and CTA button. Simulate data with interaction: subject$\times$CTA effect is +8pp for "Urgent" subject with "Act Now" button, but -2pp for "Friendly" subject with "Act Now". Use ANOVA to detect interactions. Show full factorial requires 1/8 sample size vs three separate A/B tests.

\subsubsection{Exercise 5: Implement Cluster Randomization with ICC}

Simulate hierarchical data: 100 schools, 50 students per school. Intraclass correlation ICC=0.15 (students within schools are correlated). Compare design effects: individual randomization requires N=3,200, cluster randomization requires N=3,200$\times$[1+49$\times$0.15]=24,320 for equivalent power. Implement cluster-robust standard errors. Show ignoring clustering inflates Type I error to 0.28.

\subsubsection{Exercise 6: Build Network Experiment Analysis}

Create social network with 1,000 nodes, average degree 20. Implement ego-cluster design: randomize focal users, measure outcomes for their neighbors. Simulate +20\% direct effect and +10\% spillover. Use two-stage estimator to separate direct from spillover effects. Show naive analysis conflates effects, underestimating total impact by 33\%.

\subsubsection{Exercise 7: Create ROI Calculator for Experiments}

Build NPV calculator for experiment results. Inputs: treatment effect on revenue, implementation cost, user LTV, discount rate. Account for segment heterogeneity: deploy to high-value segments only if overall effect is positive but small. Show example where overall +2\% effect has -\$200K NPV, but deploying to top quartile yields +\$1.2M NPV.

\subsubsection{Exercise 8: Design Survival Analysis for Long-term Effects}

Implement Kaplan-Meier estimator and log-rank test for churn analysis. Compare treatment vs control survival curves over 12 months. Estimate hazard ratio using Cox proportional hazards model. Detect violation of proportional hazards assumption (treatment effect decays over time). Use time-varying coefficients to model decay: initial HR=0.6, converging to HR=0.9 by month 12.

\subsubsection{Exercise 9: Implement Mediation Analysis}

Build causal mediation framework to decompose total effect into direct and indirect paths. Test new onboarding flow: total effect on revenue is +\$50/user. Mediate through engagement: 60\% is indirect (onboarding $\rightarrow$ engagement $\rightarrow$ revenue), 40\% direct. Use sequential ignorability assumption and sensitivity analysis for unmeasured confounding. Show when mediation is strong, optimizing mediator may be more effective than A/B testing final outcome.

\subsubsection{Exercise 10: Build Multiple Testing Correction}

Implement Bonferroni, Holm-Bonferroni, and Benjamini-Hochberg FDR corrections. Simulate 100 hypothesis tests: 10 true effects (d=0.5), 90 nulls. Show uncorrected testing: 10 true positives, 4.5 false positives (FDR=31\%). Bonferroni: 7 true positives, 0.05 false positives (FDR=0.7\%). BH at q=0.1: 9 true positives, 0.9 false positives (FDR=9\%, near target). Discuss power-FDR tradeoff.

\subsubsection{Exercise 11: Create Experiment Governance System}

Design approval workflow with risk assessment. Classify experiments by risk (legal, privacy, revenue impact, traffic allocation). Auto-approve low-risk tests (<5\% traffic, no PII changes). Require data science review for medium-risk, executive approval for high-risk. Implement pre-registration: hypothesis, metrics, sample size, decision criteria. Add guardrail metrics that auto-halt experiments violating SLAs.

\subsubsection{Exercise 12: Design Automated Result Interpretation}

Build system that interprets experiment results and generates recommendations. Check statistical significance, practical significance (MDE threshold), covariate balance, heterogeneity, and confidence level. Output: "Deploy to all users" (high confidence), "Deploy to segment X" (medium confidence), "Do not deploy" (low confidence), or "Extend experiment" (inconclusive). Include caveats: novelty effects, seasonality, external validity.

\subsubsection{Exercise 13: Implement Synthetic Control Analysis}

Replicate Abadie et al. (2015) Prop 99 analysis. Use California as treatment (tobacco tax), construct synthetic control from donor states. Optimize weights to minimize pre-treatment RMSPE. Conduct placebo tests: apply method to each donor state; if California shows largest post-treatment gap, evidence of causal effect. Implement inference via permutation test. Discuss when synthetic control is preferable to DiD.

\subsubsection{Exercise 14: Build Experiment Portfolio Management}

Create system managing 50 concurrent experiments with limited traffic (100K daily users). Implement traffic allocation algorithm: priority-based (revenue-critical tests get more traffic), orthogonality constraints (features must not interact), minimum sample requirements. Detect experiment interactions via multi-way ANOVA. Build portfolio-level metrics: experiment velocity, success rate, aggregate revenue impact. Show 10$\times$ experiment throughput vs sequential testing.

\subsection{Decision Framework for Experimental Methods}

Choosing the right experimental approach is critical for valid inference. Use this decision tree to select appropriate methods based on your constraints and objectives.

\subsubsection{Randomization Feasibility}

\textbf{Can you randomize treatment assignment?}

\begin{itemize}
    \item \textbf{Yes $\rightarrow$ Randomized Experiments}
    \begin{itemize}
        \item \textbf{Standard A/B Test}: Independent units, binary treatment, single metric, fixed sample size
        \item \textbf{Multi-Armed Bandit}: High opportunity cost of suboptimal variants, willing to trade off statistical power for regret minimization (Thompson Sampling, UCB)
        \item \textbf{Sequential Testing}: Unknown optimal sample size, desire early stopping for efficacy or futility (O'Brien-Fleming, alpha spending)
        \item \textbf{Factorial Design}: Multiple features, potential interactions, want to test combinations efficiently ($2^k$ or fractional factorial)
        \item \textbf{Cluster Randomization}: Treatment assigned at group level (schools, cities, time periods), account for ICC in power analysis
        \item \textbf{Network Experiment}: Users interact (social network, marketplace), use ego-cluster or graph cluster randomization to avoid spillover bias
    \end{itemize}

    \item \textbf{No $\rightarrow$ Observational Causal Inference}
    \begin{itemize}
        \item \textbf{Difference-in-Differences}: Policy change affecting some units but not others, parallel trends assumption plausible, panel data available
        \item \textbf{Synthetic Control}: Single treated unit (country, state, product launch), many control units, long pre-treatment period for weight optimization
        \item \textbf{Regression Discontinuity}: Treatment assigned based on threshold (test score, age, revenue tier), units cannot precisely manipulate assignment variable
        \item \textbf{Instrumental Variables}: Strong instrument available (policy shock, geographic variation), instrument uncorrelated with unobserved confounders, monotonicity holds
        \item \textbf{Matching/Propensity Scores}: Confounders observable, common support exists, selection on observables plausible (high risk of bias)
    \end{itemize}
\end{itemize}

\subsubsection{Optimization Objective}

\textbf{What are you trying to optimize?}

\begin{itemize}
    \item \textbf{Single Metric, Short-term}: Standard A/B test with power analysis, fixed sample size
    \item \textbf{Single Metric, Minimize Regret}: Multi-armed bandit (Thompson Sampling if Bayesian priors available, UCB if not)
    \item \textbf{Multiple Metrics, No Tradeoffs}: Test each metric separately, correct for multiple testing (Bonferroni, FDR)
    \item \textbf{Multiple Metrics, Tradeoffs}: Constrained optimization (maximize revenue subject to retention $\geq$ threshold), ROI framework with segment-specific deployment
    \item \textbf{Long-term Value}: Survival analysis, cohort-based LTV modeling, holdout groups measured for 6+ months, discount future value appropriately
    \item \textbf{Exploration}: Factorial designs to understand feature interactions, heterogeneity analysis to discover segments
\end{itemize}

\subsubsection{Statistical Constraints}

\textbf{What are your sample size and power constraints?}

\begin{itemize}
    \item \textbf{Sufficient Power (>80\%)}: Standard methods, pre-specify sample size, run to completion
    \item \textbf{Underpowered}: Sequential testing with futility boundaries (stop early if conditional power <20\%), Bayesian methods (report posterior probabilities), increase MDE or extend duration
    \item \textbf{Very Large N}: Be cautious of statistical vs practical significance, set MDE threshold, use confidence intervals not just p-values
    \item \textbf{Limited Traffic}: Multi-armed bandits to minimize regret, sequential testing to stop early for clear winners, increase effect size via more aggressive treatments
\end{itemize}

\subsubsection{Validity Threats}

\textbf{What are the main threats to internal validity?}

\begin{itemize}
    \item \textbf{Network Interference}: Use ego-cluster randomization, graph cluster randomization, or explicitly model spillover effects
    \item \textbf{Non-compliance}: Instrument actual treatment with random assignment (IV/LATE framework), report ITT and CACE
    \item \textbf{Attrition}: Test for differential attrition, bound estimates (best/worst case), collect covariates predicting missingness
    \item \textbf{Novelty Effects}: Extend experiment duration (4+ weeks), compare early vs late periods, model decay explicitly
    \item \textbf{Seasonality}: Avoid high-season periods, use full weekly cycles, include day-of-week fixed effects, stratify by time if necessary
    \item \textbf{Covariate Imbalance}: Check balance on pre-treatment variables (SMD<0.1), use stratified randomization or regression adjustment
    \item \textbf{Multiple Testing}: Pre-register hypotheses, correct for family-wise error rate (Bonferroni) or FDR (Benjamini-Hochberg)
\end{itemize}

\subsubsection{Business Constraints}

\textbf{What are your business and operational constraints?}

\begin{itemize}
    \item \textbf{High Implementation Cost}: Use Bayesian decision theory (expected value of information), only run experiments where EVPI exceeds cost
    \item \textbf{Irreversible Changes}: Conservative approach with higher power (90-95\%), sequential testing with harm monitoring, pilot with small traffic
    \item \textbf{Compliance/Legal Risk}: Formal governance with approval workflows, risk assessment, legal review for experiments touching regulated areas
    \item \textbf{Concurrent Experiments}: Orthogonality checks, traffic allocation system, test for interactions, limit scope to avoid conflicts
    \item \textbf{Fast Iteration}: Multi-armed bandits for rapid learning, sequential testing for faster decisions, accept higher MDE for speed
\end{itemize}

\subsubsection{Example Decision Paths}

\textbf{Case 1: Testing new recommendation algorithm on e-commerce site}
\begin{itemize}
    \item Randomization feasible $\rightarrow$ Standard A/B test
    \item Optimize long-term revenue $\rightarrow$ Measure LTV, not just immediate purchases
    \item Sufficient traffic (1M users/week) $\rightarrow$ Run for 2 weeks, N=50K per arm, MDE=2\%
    \item Novelty threat $\rightarrow$ Compare weeks 1 vs 2, extend if decay detected
    \item Concurrent experiments $\rightarrow$ Check orthogonality with other tests
    \item \textbf{Method: Standard A/B test with 2-week duration, LTV measurement, novelty analysis}
\end{itemize}

\textbf{Case 2: Estimating value of recommendations after system outage}
\begin{itemize}
    \item Cannot randomize (outage already occurred) $\rightarrow$ Observational method
    \item Outage affected single country (Germany) $\rightarrow$ Difference-in-differences or synthetic control
    \item Many control countries, short pre-period $\rightarrow$ DiD more appropriate than synthetic control
    \item Parallel trends plausible (all EU countries) $\rightarrow$ DiD is valid
    \item \textbf{Method: Difference-in-differences with other EU countries as controls}
\end{itemize}

\textbf{Case 3: Testing 5 email variants with limited list size}
\begin{itemize}
    \item Randomization feasible $\rightarrow$ A/B test vs bandit decision
    \item High opportunity cost (limited list, cannot re-email) $\rightarrow$ Multi-armed bandit
    \item Unknown priors on best variant $\rightarrow$ UCB or Thompson Sampling with non-informative priors
    \item Limited budget $\rightarrow$ Minimize regret, not maximize statistical power
    \item \textbf{Method: Thompson Sampling bandit, allocate traffic adaptively, deploy best arm after 20K sends}
\end{itemize}

\textbf{Case 4: Testing 3 features simultaneously (layout, color, copy)}
\begin{itemize}
    \item Randomization feasible, multiple features $\rightarrow$ Factorial design
    \item Potential interactions (color$\times$copy might matter for CTA) $\rightarrow$ Full $2^3$ factorial
    \item Sufficient traffic (100K users/week) $\rightarrow$ Each of 8 cells gets 12.5K users
    \item \textbf{Method: $2^3$ factorial design, test main effects and interactions}
\end{itemize}

\textbf{Case 5: Driver incentive on ride-sharing platform}
\begin{itemize}
    \item Randomization feasible $\rightarrow$ A/B test vs cluster randomization
    \item Drivers move between cities $\rightarrow$ Network interference, spatial spillover
    \item Treatment at city level $\rightarrow$ Cluster randomization with geographic buffers
    \item ICC unknown $\rightarrow$ Conservative assumption ICC=0.2, inflate sample size by design effect
    \item \textbf{Method: Cluster randomization at city level, 50-mile buffer zones, cluster-robust SEs}
\end{itemize}

\subsection{Final Thoughts}

Rigorous experimentation is what separates data-driven organizations from those that merely use data to justify pre-existing beliefs. The techniques in this chapter—from proper randomization and power analysis to multi-armed bandits, sequential testing, and network-aware methods—form the foundation of trustworthy causal inference in ML systems.

The cost of poor experimentation is measurable and substantial. As we've seen through the scenarios:
\begin{itemize}
    \item Peeking at results without correction: +40\% false positive rate
    \item Missing interaction effects: \$2.5M opportunity cost over 3 months
    \item Ignoring network spillover: 30\% bias, failed deployments
    \item Optimizing metrics without ROI validation: \$409K negative NPV despite statistical significance
    \item Underpowered tests with early stopping: \$1.5M in lost revenue and 3-week rollback
\end{itemize}

Conversely, the value of proper methods is equally clear:
\begin{itemize}
    \item Multi-armed bandits: 64\% regret reduction vs fixed A/B tests
    \item Sequential testing: 50\% faster decisions with controlled error rates
    \item Factorial designs: Complete feature space exploration in fraction of time
    \item Network-aware methods: Unbiased estimates preventing deployment disasters
    \item Business-metric frameworks: Segment-specific deployment turning -\$409K into +\$850K
\end{itemize}

As ML systems become more sophisticated, so too must our experimental methodologies. The difference between a statistically significant result and a causally valid, business-justified deployment decision often determines whether ML initiatives create or destroy value. Invest in the infrastructure, skills, and processes to experiment rigorously—the ROI compounds exponentially as your organization scales its ML capabilities.
